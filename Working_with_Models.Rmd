```{r setup, echo = FALSE, warnings = FALSE, eval = TRUE, message=F}
purp.mcmc = F

source("RMDR.R")

CE <- c("Uncanny", "BrowsingAB", "CUE8", "Hugme", "StroopMW", "Chapter_WM")
load_CE_(CE)


```


# Working with models

<!--
Linear models are a language. Different to natural language it is unambiguous in its mathematical confinement. Like human language, thought preceeds any meaningful statements. Research with upfront thought is called *theory-driven* research. 



BrowsinAB is partly theory-driven (age), partly *opportunistic* (A,B).

[...]
Theory-driven research is interested in prediction of behaviour based on assumptions made by reason. Reasons can be: [...]


So, prediction models are usually theory-driven. This is not to be confused by models that aim for *theory testing*. In terms of statistical modelling, theory testing always implies at least two models. Often these two differ in just one parameter, that is present in one model but absent in another.

-->


## Model building


<!--
### Including predictors

+ iterative
+ liberal theory-driven inclusion of predictors
+ model driven removal of predictors (pruning)
+ interaction effects
+ rules for random effects


Arabesque statements can be construed with human language and linear models alike. Formally, nothing keeps a researcher from creating a questionnaire on the influence of color schemes on perceived usability, collect 32.537 responses via crowdsourcing and enter all, let's say 10 demographic variables into a linear model, including all interaction effects `PU ~ p1 * p2 * p3 * p4 * p5`. Obviously, this is not theory-driven, as *only theories have predictive value that also deny*.

What the researcher probably had in mind is to develop a *prediction model* for example, to be able to predict the most beneficial color scheme for young adults in South Korea. That is a completely rightous research question, but it appears rather opportunistic, rather than theory-driven. Make no mistake! Not even in theory are prediction models free of theory. The aim of the research question may be opportunistic, or market-driven, but the research itself is far from being a complete shot into the blue. Expecting cultural  differences in color appreciation is quite an assumption. The only difference to might be that really is that theory-driven parameters have an expected direction of effect.

-->



### Choosing priors




<!-- start with an obvious -->


The first chapter of this book linked Bayesian statistics to rational decision making. You recall there was much a do about incorporating prior knowledge. In Bayes' formula, prior knowledge is a central component and was expressed as a probability. 

So, how are priors specified? In chapter 




<!-- #104 explain Bayes formula once again. -->

If this is so awesome as it sounds, why haven't we used any priors, so far? In fact, both Bayesian regression engines we used, implicitely add default priors to our models. The defaults are too weak to have unbearable effects on the posterior distribution. Still, they help to tame the MCMC algorithm, when complex models encounter sparse data.

 Consider the case BrowsingAB. In [CLM], we estimated the linear slope between age and ToT.

```{r}
attach(BrowsingAB)
```


```{r prior_BAB_age, opts.label = "future"}
fixef(M_2)
```

The default prior specification can be found in the manuals of rstanarm (or brms). Or, you use the following command on the model object `M_2`:

```{r opts.label = "future"}
M_2$prior.info
```

```{r}
detach(BrowsingAB)
```

+ extract default prior from M_2
+ extreme example: guessing a persons IQ, without a measure (100+-15)

People had been using the data to come to conclusions, but they also relied on their own convictions. This sounds subjective and idiosyncratic, even. Is that still research, strictly? In fact, some have labelled Bayesian statistics subjective, and therefore inadmissible, for the concept of priors. What has been overlooked by those is:

1. Frequentist statistics has priors, too. They are just always flat (non-informative).
1. Non-informative priors in Bayesian estimation are routinely used, too.
1. By putting prior knowledge into a formula, it is objective and becomes criticizable. Every reviewer, stakeholder or follow-up researcher can inspect the chosen prior and follow it, or not. When research is published in a reproducable way, people can even reproduce the estimation  using their own priors, when disagreeing. 





### Monitoring model convergence

Throughout this book I have fooled you into thinking that the regression engine is something where you put your data and model in and you get some tidy MCMC chains as output, like putting gras into a coow produces milk. MCMC chains are random walks and they are not at all as cows. MCMC chains are like a bulk of young kitten, everyone on their own route, bumping into things or getting stuck in corners. Stan is very good with kitten and safely guides their path through the unknowns of a multidimensional parameter space. Most of the time.

Pathologies of MCMC chains:

1. too few cycles
2. chains plateau for longer periods
3. chains do not arrive at similar distributions
4. chains wipe over tiny gaps in the distribution



+ MCMC convergence
+ eff sample size
+ Tuning the Stan engine


### Model criticism

+ residual analysis
+ linearity 
+ colinearity
+ posterior predictive checks

Gelman, a, Gelman, a, Meng, X.-L., Meng, X.-L., Stern, H., & Stern, H. (1996). Posterior predictive assessment of model fitness via realized discrepancies. Vol.6, No.4. Statistica Sinica, 6(4), 733â€“807. http://doi.org/10.1.1.142.9951








## Model criticism

Parameter estimation can be understood as a model being *trained by data*. 

It is not too much of a stretch to expect that after estimation, the model is *trained to data*. Recall that every statistical model (presented in this book) assumes that the shape of data composed of

1. the fixed term (likelihood), that describes the associations between measured variables
1. the random term, that collects the noise, i.e. all fluctuations by unmeasured impact factors 

From a good model we expect that its predictions resemble the associations in and shape of the training data. That is the idea of assessing the *model fit*. The first section introduces graphical methods to examine how well the model accomodates features of the data. The second section 




The first section introduces 

That is commonly called *goodness-of-fit*.







### Graphical model criticism

In our childhood, the first objects we learn to recognize are faces and stay fascinated by faces for our whole life; just imagine, how desperate everyone must have at the early performances of The Doors, where lead singer Jim Morrison used to turn the audience his back. Even cars are designed to have a face (or make one, sometimes). The prevalence for seeing faces could lead to the crude theory that th more something looks like a face, the more we like it. Not so fast! (says professor Mori) Objects that  *almost* liken real faces fall into the Uncanny Valley.

We estimated both models, $M_0$ (linear regression) and $M_1$ (third degree polynomial) in chapter [REF]. In the following, we will see three methods for eye-balling how well the model accomodates the data, ordered by sophistication:

1. point predictions
1. predictive density
1. simulation


#### Point predictions

The classic (and easiest) method is to use the point estimates that are produced by the command `predict`. For creating the first set of diagnostic plots, we first join the predictions of both models with the original data.


```{r}
attach(Uncanny)
```

```{r}
UV_1_point <-
  UV_1 %>% 
  #mutate(age = rnorm(200, age, .2)) %>% 
  left_join(predict(M_0), by = "Obs") %>% 
  rename(M_0 = center) %>% 
  left_join(predict(M_1), by = "Obs") %>% 
  rename(M_1 = center) %>% 
  select(Obs, huMech, avg_like, M_0, M_1) %>% 
  #sample_frac(.3) %>% 
  gather(key = Model, value = predicted, starts_with("M_"))

UV_1_point %>% sample_n(8)
```

The following graph shows the observed values against model predictions. With the bare eye, we can discern two  features in the data: a shoulder on the left edge and an upwards trend at the right edge. Ironically, the middle section, where the emotional breakdown actually happens, the data is sparse and rather undecisive. Still, there is a lot of noise  in the data and I would not place a large bet on the polynomial model, at this stage.

```{r}
UV_1_point %>% 
  ggplot(aes(x = huMech, y = avg_like)) +
  facet_grid(Model ~ .) +
  geom_text(aes(label = Obs), size = 3) +
  geom_smooth(aes(y = predicted, color = "predictions"), se = F) +
  geom_segment(aes(xend = huMech, yend = predicted))
```

Especially for noisy data, it helps to add a LOESS line to the plot. Simply spoken, a LOESS smoother is like an unbreakable pipe cleaner, highly flexible, with a certain stiffness that lets it always retain smoothness. i.e. it never breaks. The default stiffness of the LOESS wire in `geom_smooth` is comparable to a 8mm pipe cleaner wire. It would go smoothly through a slightly bended 50mm pipeline, but it takes some force to push it through a siphon. As you can see on the plot below, LOESS and the polynomial model agree accurately. But, the stiffness of the LOESS is arbitrary, so just to be sure, I added more pipe cleaners of various strengths. Here is what we observe: 

+ They all three agree with the model, that there is a valley and also on where it is.
+ Only the stiffest LOESS seems less impressed with the depth of the valley. 
+ It also does not agree, that there is an initial rise in emotional response.
+ The default stiffness of `geom_smooth`, as you almost cannot see, is so close to the polynomial model $M_1$, one could think the developers have used the Uncanny Valley for calibration. 
+ The weak smoother is getting a little excited in the sparsely populated horizontal center area, resulting in some wiggliness. But, who knows what it may be up to?




```{r opts.label = "future"}
UV_1_point %>% 
  ggplot(aes(x = huMech, y = avg_like)) +
  facet_grid(Model ~ .) +
  geom_text(aes(label = Obs), size = 3) +
  geom_smooth(aes(y = predicted, color = "predictions"), se = F) +
  geom_smooth(aes(color = "LOESS stiff"), se = F, span = 1.6) +
  geom_smooth(aes(color = "LOESS default"), se = F) + # span = 0.8
  geom_smooth(aes(color = "LOESS weak"), se = F, span = 0.5) 
```



```{r}
detach(Uncanny)
```



+ predicted
+ predictive distribution
+ observed versus 
+ residuals: qq-plots, histograms


#### Shape of randomness


### Goodness-of-Fit scores {#GOF}



```{r}
attach(BrowsingAB)
```

```{r}
BAB1 %>% 
  mutate(age = rnorm(200, age, .2)) %>% 
  left_join(predict(M_0), by = "Obs") %>% 
  rename(M_0 = center) %>% 
  left_join(predict(M_ia1), by = "Obs") %>% 
  rename(M_ia1 = center) %>% 
  select(Obs, age, Design, ToT, M_0, M_ia1) %>% 
  sample_frac(.3) %>% 
  gather(key = Model, value = fitted, starts_with("M_")) %>% 
  ggplot(aes(x = age, y = ToT, color = Design)) +
  facet_grid(Model ~ .) +
  geom_text(aes(label = Obs), size = 3) +
  geom_smooth(aes(y = fitted), se = F, method = "lm") +
  geom_segment(aes(x = age, xend = age, y = ToT, yend = fitted), curvature = 0.1)
```

Note:

+ how the variable age is "jittered" by `rnorm` to prevent overplotting




```{r}
detach(BrowsingAB)
```

+ pointwise likelihood and deviance
+ Assessing likelihood versus error distribution
+ residuals
+ RMSE
+ deviance
+ total deviance



The problem with RMSE is that it is based on residuals. The problem with residuals is that they weigh negative and positive deviations the same, which only make sense with symmetric distributions, like the Gaussian. For the classes of models introduced in [REF GLM] a more general score for goodness-of-fit is needed. This is the *deviance* score.

In \@ref(likelihood) we have learned how the *log-likelihood function* calculates the joint probability of all observations $y$, given a certain parameter value vector $\theta$:

$$ LL = \log \sum_i p(y_i|\theta)$$

Opposed to residuals, a likelihood function exists for all models in this book and beyond. Therefore, it is an attractive choice for a goodness of fit score. In classic maximum likelihood estimation, the *total deviance* $D$ is routinely being used for that purpose. It is merely double the negative $LL$:

$D = -2 LL$

In classic MLE, the fit of two models can be compared by computing the deviance of both models. Lower deviance indicates a better fit.






## Model Comparison


The tenor of this book on new statistics is quantification and up to this point all interpretation focussed on the model coefficients: under uncertainty, the center estimate serves as a best guess for effect size and by the credibility limits we express how good our best guess is. Researchers trained in classic statistics often completely ignore coefficient tables. Their perspective merely is whether an effect stands out from the noise (i.e., uncertainty) enough to be called statistically significant. This practice has been criticized for decades and good reasons. One fundamental criticism being that the concept of statistical significance artificially divides the world in significant and non-significant results. That stands in stark contrast to the intuitions that reality is continuous and research is incremental.

With the models, this is such a thing on the one hand, the model would fit well with a data set and on the other hand, the rule of parsimony applies. The difficulty is that a model does not knows exactly the border between signal and error. And unfortunately, models often tend to take advantage of the available flexibility and cut off a fair amount of error. As we will see, this means that more complex models may have less prediction accuracy.

And that is the criterion with which models can be compared. However, predicting the accuracy of a model can be very expensive. The gold standard is cross validation and that means you have to collect data again. This can be avoided with the relatively simple methods of leave-one-out Crossvalidation, but the computational effort can be unbearable. Most statistics practitioners will therefore be well aware of the informational criteria that can effectively approximate the relative predictive power of a model.

What many classic researchers seem to be unaware of, in addition, is that the approach of statistical significance, typically employing p-values, is a special case of a much broader class of statistical procedures called model selection. More specifically, an analysis of variance for the purpose of rendering the difference between two groups of observations is a basic comparison of two models: the alternative model is associated with the hypothesis the researcher has in mind and carries a coefficient to represent the effect. The null model assumes that there is no effect and is the alternative model reduced by the respective coefficient.

$$
H_1: y_i = \beta_0 + \beta_1x_{1i}\\
H_0: y_i = \beta_0
$$

Another way to put it is that $H_0$ is a special case of $H_1$, where the slope coefficient is fixed to zero, $\beta_1 = 0$. If one model is a special case of another, these models are called *nested models*.

All classic tests only work with nested models. The famous F-test estimates both models, calculates the residual sum of squares under both models and evaluates whether the more general model sufficiently reduces the residuals, relative to the nested model,  to be called statistically significant. As we will see in the remainder of this chapter, comparing models relative to each other has useful applications beyond classic hypothesis testing, in particular:

1. Models that contain superfluous predictors have lower predictive power compared to a more parsimonious model.
1. When the main purpose is predicting future observations with optimal accuracy, *model pruning* is a procedure for selecting a subset of parameters with the optimal predictive accuracy.
1. In chapter [REF] we used graphical methods to choose an appropriate error distribution for reaction time measures. By comparison models, we can identify the optimal error distribution for the data at hand.

As it will turn out, the predictive accuracy of a model depends on two counteracting aspects: *model fit*, that is how neatly the model aligns with the data and *model parsimony*. Generally spoken, adding a parameter to a model gives you an advantage in fit, but potentially reduces parsimony, resulting in less-than-optimal forecasts. Even classic ANOVA contains both elements: the F-statistic represents model fit by the reduction in residuals and parsimony enters the F-test as degrees of freedom, i.e. the number of free parameters.

This chapter will not re-iterate on classic tests, but will approach model comparison from the perspective of predictive accuracy. One very practical reason for that is, that classic tests *only* apply to nested models, whereas there are many situations, where  comparing non-nested models is very useful.


The remainder of this chapter will first elaborate on model parsimony and demonstrate that parsimony is crucial for predictive accuracy. Then, a generic procedure for measuring predictive accuracy of models will be outlined, the *leave-one-out cross validation*. Then we introduce a very practical class of statistics for model comparison called *information criteria*. As we will see, information criteria apply to a much wider class of models, including non-nested models, MLM and GLM, but also non-linear models or psychometric models.




### The principle of parsimony

Occam's razor sounds bloody business. In words of Bertrand Russell it says:

>if one can explain a phenomenon without assuming this or that hypothetical entity, there is no ground for assuming it.

Why not? And mind how the speaker carefully avoids to condempt unnecessary hypotheses right away. Occam's razor is sometimes taken as what mathematicians call an axiom. Axioms are assumptions that  
don't have another Why. Often, these axioms are intuitive, like: Every natural number has a successor."

Occam's razor, or the  *parsimony principle*, is intuitive, because people reportedly assume things for their own grounds a lot. It seems fully in order to request a counter balance. Or staying with the original metaphor, a blade is needed that prunes  exuberant creativity of the researcher.


<!-- The other good reason for parsimony is that unnecessary hypotheses inflate the posterior distribution. 


If you are an experimental scientist Occam's razor applies to you in the following form: 

> ..., there is no ground for assuming it, even if it is your darling

-->

The Popperian Philosophy of Science elevates the principle of parsimony to a driving force in scientific progress even. This goes about like this: Popper's fundamental criticism was that researchers aimed at proving their theories by gathering a lot of confirmatorty data. Popper argued, that you can never prove a universal scientific law, because that would require you to gather all possible outcomes of that law. Popperians requests that a  theory must be *falsifiable to be called a scientific theory*. As it turns out, a required condition for a theory to be falsifiable, is that it makes *testable predictions*. In turn, a testable prediction is one where you can find *counter examples*. This is best explained by example:

The infamous Stroop effect <!--[REF]--> can be observed when people see color words, but have to respond the 
ink color of the typeface. Then, as has been reported abundantly, people respond with a delay, when the word and the ink color are incongruent, i.e. they do not match. Multiple theories emerged from decades of research on this, not strange, but extremely robust effect. Here are some:

*Incongruent sources of verbal information compete for processing capacity of a certain module in the brain*, therefore: Everyone who has a brain is falling for the Stroop effect. This is roughly what many cognitive theories on the Stroop effect assume in more sophisticated ways. At the same time, the Stroop effect has been shown to happen in hundreds of studies, and I do not know of any counter-examples. Is this theory a certain endeavour? Not so fast! The above theory speaks of everyone, not in every study. That means specifically, that the Stroop effect is apparent for every single one of the partiucipants, not just en grosse. Unfortunately, most studies on the Stroop effect only report the population-level group effects, but not participant-level effects. We can test this theory by taking a closer look at teh Stroop data set [REF]. Are all participant-level coefficients positive in the expected direction? Yes, they are. Although, there exist individuals who come close to be unaffected.

<!-- #96 -->

```{r}
attach(StroopMW)
```

```{r}
Scores <- 
  D_stroopmw %>% 
  distinct(Part) %>% 
  bind_cols(
    ranef(M_1_exg) %>% 
      filter(fixef == "Conditionincongruent") %>% 
      select(ranef = center)
      ) %>% 
  mutate( fixef = fixef(M_1_exg) %>% 
            filter(fixef == "Conditionincongruent") %>% 
            select(fixef = center) %>% 
            pull(1)
          ) %>% 
  mutate(incongruent = (fixef + ranef) * 1000)

Scores %>% 
  ggplot(aes(x = incongruent)) +
  geom_histogram()
```

```{r}
detach(StroopMW)
```


Another observer of the Stroop effect could draw more on the obvious lack of obedience that participants show. They were not asked to read a single word in this experiment, but apparently they do. This lack of obidience towards instructions is what cognitive experimentalists often call bottom-up processing. It is commonly interpreted as involuntary cognition triggered by sensations. In brief this theory goes: *Reading is highly over-learned and whenever literate persons see a written word, they read it.* This theory proposes that we are so attached to words that we cannot read not. Does always it? Here is my one counter-examples. I remember the day, when at my workplace everyone got a major upgrade on their computers. One highly praised feature of the new user interface was a ubiquitous search function, which was placed in the programs menu, just left of the power-down button. The other day, a colleague entered my office and told me that he had just discovered a usability problem with the search function. Following a habit, he had entered a search term and pressed the button right next to it. This button clearly said "Shut down computer". He had not read. This theory is falsified.

Falsifiability is a continuum and depends on universality and precision of a theory. For *universality*, we have just seen an example. The theory of overlearned reading is a rather universal and anecdotal data made it fail.
Popper concluded that a theory is more falsifiable when it is more universal. In order to rescue the theory, we have to make it more specific, such as: *People always read when forced to see a word*. Are they? Take five minutes to stare at the following letters.  Notice the moment when you stop reading it. Notice the many moments, your thoughts go astray, but always return to the letters. Notice the moment when you stop seeing a word, at all.

```{}
                        W O R D 
```

In the previous chapter Falsifiability also depends on *precision* of a theory; here is an example: Many theories on the Stroop task are nominating a certain information processing pathway as a cause and predict *a* delay in the incongruent condition. Any of those theories is consistent with the above results. A funny fact is that most cognition researchers would call the Stroop task a complex task. That somehow implies longer processing times and, perhaps, one should only call it Stroop effect when it sits north of 50ms. With precision comes falsifiability: one participant can make such a  theory fail.

Comparing two theories by their parsimony is not easy, as it requires a good deal of formalization. Statistical models are formal  and there seems to exist a straight-forward definition for *model parsimony: of two models, the one with fewer independent parameters is more parsimonous and the one with more is more opulent.* In single-level linear models, the nominal number of parameters is precisely the number of coefficients in the likelihood plus another parameter for the error distribution, such as $\sigma^2$.

<!-- #97 -->


### Model complexity {#model_complexity}

In single-level linear models all parameters are independent and counting the nominal parameters is a valid estimate for model opulence. In multi-level models there can be cross speak between levels and parameters influence each other. It is no longer adequate to count all parameters for model opulence.

As explained in [REF: Shrinkage], participant-level parameters in multi-level models are not independent. By estimating participant variance alongside, the individual random coefficients are influencing each other. Not totally unlike human cognition bottom-up and top-down influence takes place: the population level learns from the participant level how much clumping there is and participants with extreme outcomes (or little data) are adjusted towards the center of the population-level distribution (shrinkage). Let us see that on a simple example. The following simulation function creates two-level data sets with repeated measures of participant performance. We will see down the line, how module opulence varies with the model, but also with properties of the data.


```{r}
sim_1 <- 
  function(N_Obs = 20, N_Part = 40, beta_0 = 1, beta_0_sd = 1, error = 1, seed = 42){
    set.seed(seed)
    Part <- data_frame(Part = as.factor(1:N_Part),
                       beta_0p = rnorm(N_Part, 0, beta_0_sd))
    expand_grid(Part = Part$Part, Obs = as.factor(1: N_Obs)) %>% 
      left_join(Part, by = "Part") %>% 
      mutate(perf = rnorm(N_Obs * N_Part, beta_0 + beta_0p, error))
  }

```


```{r}
attach(Chapter_WM)
```


```{r}
sim_1() %>% 
  ggplot(aes(x = Part, y = perf)) +
  geom_jitter()
```


Although we can easily spot the participant-level effect, as a baseline for model opulence  we use a grand mean model with two completely independent parameters $\beta_0$ and $\sigma$. Because they are independent, the effective number of parameters is equal to the nominal parameters in the model.


```{r opts.label = "mcmc"}
M_1 <- 
  sim_1() %>% 
  stan_glm(perf ~ 1, data = .)

M_1
```


In multi-level models, parameters are not independent. The *effective number of parameters* is smaller than nominal and *needs to be estimated*. At this point suffice it to say that the *Widely Applicable Information Criterion (WAIC)* introduces such an estimate [REF] of effective parameter (opulence). Calling the function `waic()` on rstanarm models produces, among others, the statistic $p_waic$. Of course, it also works on models with independent parameters, such as as the grand mean model:


```{r}
waic(M_1)$p_waic
```

As expected, the estimated number of parameters is close to the nominal two. Note that $p_\textrm{waic}$ is an estimate on the draws of the MCMC random walk and this is why the result will never be fully exact (but arbitrarily exact by running longer MCMC chains). Next, we introduce a participant-level effect to the model. We will do that as a fixed effect, first, allowing no top-down influence, the nominal number of parameters of this model is 41:

+ intercept
+ 39 differences
+ residual standard deviation

```{r opts.label = "mcmc"}
M_2 <- 
  sim_1() %>% 
  stan_glm(perf ~ 1 + Part,
                  data = .)
```

```{r}
waic(M_2)$p_waic
```


The opulence estimate is only slightly smaller than the nominal number of parameters. The next model introduces a genuine random effect, where participant-level coefficients are no longer unconstrained. The nominal number of parameters is 43:

+ population intercept
+ population standard deviation
+ 40 participant intercepts
+ residual standard deviation

```{r opts.label = "mcmc"}
M_3 <- 
  sim_1() %>% 
  stan_glmer(perf ~ 1 + (1|Part),
                  data = .)

```

```{r}
waic(M_3)$p_waic
```


Replacing the fixed effect by a random effect introduced two more parameters, but model opulence actually goes down a bit. This is a subtle shrinkage effect at work. Next, we simulate an extreme case of an extremely tight population-level variance. Under such circumstances individual differences are negligible and the population mean rules. The estimated model becomes more parsimonious, almost approaching the parsimony of the grand mean model.


```{r opts.label = "mcmc"}

M_4 <- 
  sim_1(beta_0_sd = 0.001) %>% 
  stan_glmer(perf ~ 1 + (1|Part),
                  data = .)
```
```{r}
waic(M_4)$p_waic
```


Finally, the strength of bottom-up influence depends on how certain those participant-level coefficients are . Up to this point we have simulated a solid 20 observations per participant and received determined participant intercepts.  Reducing to five measures per individual results in considerably  lower model opulence.


```{r opts.label = "mcmc"}
M_5 <- 
  sim_1(N_Obs = 5, N_Part = 40) %>% 
  stan_glmer(perf ~ 1 + (1|Part),
                  data = .)
```
```{r}
waic(M_5)$p_waic
```

```{r}
detach(Chapter_WM)
```
```{r opts.label = "mcsync"}
sync_CE(Chapter_WM, M_1, M_2, M_3, M_4, M_5)

```


To sum it up: this section introduced the effective number of parameters as a measure for model opulence (the opposite of model parsimony). We have seen how opulence  depends on the model, but also strongly on the structure of data. This means that a researcher cannot determine upfront what would be a model with reasonable parsimony. But why should we actually care about parsimonious models? The next section we demonstrates what sounds counter-intuitive, at first: a more opulent models usually fits the existing data better, but can be worse at forecasting future observations.



<!--
In the following case, the population-level variance is extreme to the point that it is indistinguishable from a uniform distribution. There is little to learn from the population (top-down) and participant-level coefficients get less constrained. This model has the same nominal number of parameters, as the one before, but is more opulent.

```{r opts.label = "future"}

M_4 <- 
  sim_1(beta_0_sd = 1000) %>% 
  stan_glmer(perf ~ 1 + (1|Part),
                  data = .)

waic(M_4)$p_waic

```
-->


### Predictive accuracy

At several occasions in this book, we used the `predict(model)` function to produce a plot for model criticism, where fitted lines are shown next to the original data. First, a note on terminology is in order: In common language, a prediction is an informed idea about future events, hence, observations we have not yet made. This is strictly not what the `predict` function does, as it produces the best guess for the observations we already have in the data set. This best guess is what the model "thinks" is the true underlying property, minus the error. For that reason, other authors have suggested to call predicted values *retrodictions*.



### Too complex: over-fitting

Model fit and predictive accuracy should never be confused because they have quite the opposite relation with parsimony. Forecasting future observations is a completely different business, as we will now see. In the following, we will use a small simulated data set and impose two models: a very parsimonious model that matches the data generating process and a very opulent model, that carries more parameters than there are observations, even. As it will turn out, the parsomonious model produces much tighter retrodicted values, but performs considerably worse in a forecasting.

The following simulation draws two variables, $x_i$ and $y_i$ in such a way that they are completely unrelated. are totally unrelated to the predictor $x_i$ (with levels 1 to 4):


```{r}
attach(Chapter_WM)
```

```{r}
sim_2 <- function(n) data_frame(x = 1:n, y = rnorm(n))

set.seed(42)
D_4 <- sim_2(4)

D_4 %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line()
```
 
A naive observer (with some fancy theoretical expectations) may come to the idea of a curvi-linear relationship between $x_i$ and $y_i$, similar to the one we have used to capture the Uncanny Valley effect in the emotional judgment of robot faces [REF]. The presence of a parabolic element and a shoulder suggests a polynomial model of third degree. This model has five nominal parameters:

+ four polynomial coefficients
+ residual standard error


```{r opts.label = "mcmc"}
M_6 <- 
  D_4 %>% 
  stan_glm(y ~ poly(x, 3), data = .)
```



```{r}
D_4 %>% 
  mutate(pred_M_6 = predict(M_6)$center) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(aes(y = pred_M_6), se = F)
```

The fit of the model is almost perfect. But, remember that these are retrodictions, not forecasts. A more critical observer would suggest to start with a parsimonious grand mean model, as it happens to be the true data simulation process:

```{r opts.label = "mcmc"}
M_7 <- 
  D_4 %>% 
  stan_glm(y ~ 1, data = .)
```


Comparing the fitted values of both models seemingly is in favor of the opulent polynomial model:

```{r}
D_4 <- 
  D_4 %>% 
  mutate(pred_M_6 = predict(M_6)$center,
         pred_M_7 = predict(M_7)$center)


D_4 %>%  
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(aes(y = pred_M_6, color = "3rd degree polynomial")) +
  geom_smooth(aes(y = pred_M_7, color = "grand mean"))

```

But, how do both models compare in a forecasting exercise? The following generates 100 new observations, using the same generating process. The graph reveals that parsimonious model predicts the new data much better. In particular, the opulent polynomial model performs poorly for levels 1 and 2 of $x_i$.


```{r}
D_5 <- 
  rerun(25, sim_2(4)) %>% 
  map_df(bind_rows) %>% 
  left_join(select(D_4, x, starts_with("pred_")))

D_5 %>%  
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(aes(y = pred_M_6, color = "3rd degree polynomial")) +
  geom_smooth(aes(y = pred_M_7, color = "grand mean"))

```

For real data sets, differences in predictive performance are more subtle and we will need formal methods, which will be developed in the coming sections. Here, we use the root mean square error (RMSE) once again.

```{r}
RMSE <- 
  function(forecast, observed) sqrt(mean((forecast - observed)^2))

D_5 %>% 
  gather(key = "Model", value = "forecast", starts_with("pred_")) %>% 
  group_by(Model) %>% 
  summarize(RMSE(y, forecast))


```

```{r}
detach(Chapter_WM)
```


```{r opts.label = "mcsync"}
sync_CE(Chapter_WM, M_6, M_7)
```


The grand mean model has a much smaller RSME, which means that on average, its forecasts are closer to the new observations. How can it happen, that a simpler model has a much worse fit on face value, but predicts future data better? And the grand mean model is not just simpler than the polynomial, it even is a special case of the latter, which is also called *nested models*. If we force three of the polynomial coefficients $\beta_1, \beta_2$ and $\beta_3$ to zero, what remains is a flat line, $y_i = \beta_0$.

The polynomial model can take any curved path around one local minimum and one local maximum, as well as all lower-degree shapes, such as paraboles, slopes and flat lines.  

In conclusion, when forcasting matters, researchers should aim for parsimonous models. In the following section, we will see how forecasting performance can be assessed without the need to gather additional data, the leave-one-out (LOO) cross-validation method. This method can be seen as the gold standard, but is very computing intensive. The subsequent section will finally introduce information criteria as computationally more efficient method to compare predictive accuracy of models.


### Cross validation

The gold standard for assessing predictive accuracy is to measure how well new data is predicted. In its undisputed pure form, one predicts how a new data set would look like , gathers a new data set and assesses how accurate these predictions were. 





### Leave-one-out cross validation {#forecasting_LOO}

In the previous section we evaluated the predictive accuracy of models by comparing  retrodictions to new data. Waiting for new data before you can do model evaluation sounds awful, but this is what cross validation requires. The good news is that new data does not have to come from a new study. More precisely, cross validation only requires that the forecast data is not part of the sample you trained the model with. Psychometricians, for example, use the split half technique to assess the reliability of a test. The items of the test are split in half, one training set and one forecasting set. If the estimated participant scores correlate strongly, the test is called reliable.

So, model evaluation can be done, by selecting on part of the data to train the model, i.e. estimate the coefficients, and try to forecast the other part of the data. However, data is precious and reserving half of it for forecasting will considerably be at the cost of certainty. Fortunately, nobody actually said it has to be half the data. Another method of splitting has become common, *leave-one-out (LOO) cross validation*. The idea is simple:

1. Remove observation $i$ from the data set.
1. Estimate the model $M_{/i}$.
1. Predict observation $i$ with Model $M_{/i}$.
1. Measure the predictive accuracy for $i$.
1. Repeat steps 1 to 4 until all observations have been left out and forecast once.

The following code implements a generic function to run LOO.

<!-- #47 -->


```{r}


do_loo <- 
  function(data,
           f_fit, 
           f_predict = function(fit, obs) predict(fit, newdata = obs)$center[1]
           ) 
    { 
    model_name <- as.character(substitute(f_fit))
    leave_out <- function(obs) data %>% slice(-obs) # Quosure
    left_out  <- function(obs) data %>% slice(obs)  # Quosure
    out <-   data_frame(Obs     = 1:nrow(data),
                        Model   = model_name,
                        Sample  = Obs %>% map(leave_out), # meta function
                        OOS     = Obs %>% map(left_out), ## out-of-sample obs
                        Fitted  = Sample %>% map(f_fit)) %>% 
      mutate(forecast = unlist(map2(Fitted, OOS, f_predict)))
    return(out)
  }

```



Before we put `do_loo` to use, some notes on the programming seem in order. Despite its brevity, the function is highly generic in that it can compute leave-one-out scores no matter what model you throw at it. This is mainly achieved by using advanced techniques from *functional programming*:

1. The argument `f_fit` takes an arbitrary function to estimate the model. This should work with all standard regression engines.
1. The argument `f_predict` takes a function as argument that produces the predicted values for the left out observations. The default is a function based on `predict` from the bayr package, but this can be accomodated.
1. The two functions that are defined inside `do_loo` are so-called *quosures*. Quosures are functions that bring their own copy of the data. They can be conceived as the functional programming counterpart to  objects: Not the object brings the function, but the function brings its own data. The advantage is mostly computational as it prevents data to be copied every time the function is invoked. 
1. `map` is a meta function from package purrr. It takes a list of objects and applies an arbitrary function, provided as the second argument.
1. `map2` takes two parallel input lists and applies a function. Here the forecast is created by matching observations with the model they had been excluded from.
1. The function output is created as a *tibble*, which is the tidy re-implementation of data frames. Different to original `data.frame` objects, tibbles can also store complex objects. Here, the outcome of LOO stores every single sample and estimated model, neatly aligned with its forecast value.
1. Other than one might expect, the function does not return a single score for predictive accuracy, but a dataframe with inidividual forecasts. This is on purpose as there is more than one possible function to choose from for calculating a single accuracy score.
1. The function also makes use of what is called non-standard evaluation. This is a very advanced programming concept in R. Suffice it to say that `substitute()` captures an expression, here this is the fitting function argument, without executing it, immediatly. Here the provided argument is converted to character and put as an identifier into the dataframe. That makes it very easy to use `do_loo` for multiple models, as we will see next.

In the following illustration, we perform a LOO cross validation on a small data set `D_5`, with two predictors $x_1$ and $x_2$, where $x_1$ has a linear relationship with outcome $y$ and $x_2$ is competely unrelated. In the following, we apply LOO to estimate three models: a grand mean model, a linear regression model on $x_1$ and a double linear regression  with $x_1$ and $x_2$ as predictors. 


```{r}
attach(Chapter_WM)
```


```{r}
sim_3 <- function(n) data_frame(Obs = 1:n, 
                                x_1 = runif(n), 
                                x_2 = runif(n), 
                                y = rnorm(n, x_1))
set.seed(42)
D_6 <- sim_3(20)

D_6 %>% 
  gather("predictor", "value", x_1:x_2) %>% 
  ggplot(aes(x = value, y = y, color = predictor)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)
```

Next, we define the three estimation functions and apply LOO. Given the data generation process (`sim_3`), M_08 underfits the data, M_10 overfits and M_09 is right on spot.

```{r opts.label = "mcmc"}
niter = 400

fit_M_08  <-  function(sample) 
  stan_glm(y ~    1, 
           iter = niter, data = sample)

fit_M_09  <-  function(sample) 
  stan_glm(y ~    x_1, 
           iter = niter, chain = 4, data = sample)

fit_M_10  <-  function(sample) 
  stan_glm(y ~    x_1 + x_2, 
           iter = niter, chain = 4, data = sample)


Loo <- bind_rows(do_loo(D_6, fit_M_08),
                 do_loo(D_6, fit_M_09),
                 do_loo(D_6, fit_M_10))
```

Notice how the output of the three runs of `do_loo` stacked to become one dataframe, from which we can create a combined summary based on the predictive root mean square error (\@ref(GOF)). As expected, model M_09 outperforms M_8 and M_10 in predictive accuracy:

```{r}
Loo %>% 
  left_join(D_6, by = "Obs") %>% 
  group_by(Model) %>% 
  summarize(predRMSE = RMSE(forecast, y))
```


```{r}
detach(Chapter_WM)
```
```{r opts.label = "mcsync"}
sync_CE(D_6, Chapter_WM, Loo)
```




### Information criteria

So far, we have seen that the right level of parsimony is essential for good predictive accuracy. While LOO can be considered gold standard for assessing predictive accuracy, it has a severe downside. Estimating Bayesian models with MCMC is very computing intensive and for some models in this book, doing a single estimating is in the range of dozens of minutes to more than an hour. LOO estimates the model as many times as there are observations, which makes it a very inconvenient choice for more complex  models and larger data sets.

Using highly oversaturated models is catastrophic for prediction. When the saturated model is asked to predict future observations, it will recover the full noise that was in the data, which is $mu_i + \epsilon_i$. Residuals represents the stochastical component, which by its very nature does not reproduce on the next occasion. Any more complex model reduces deviation from the observed data, but has less predictive value. The AIC accounts for the reduced predictive accuracy by penalizing the number of parameters. 

The oldest of all IC is the *Akaike Information Criterion (AIC)*. Compared to its younger siblings, it is less broad, but its formula will be instructive to point out how  goodness-of-fit and complexy are balanced. To represent goodness-of-fit, the AIC employs the deviance, which makes it applicable for all classes of models that are suited for maximum likelihood estimation (\@ref(likelihood)). Model complexity is given by just two times the numner of parameters, as implausibly simple this may sound.

$$
D = -2 \log(p(y|\hat\theta)).\\ 
AIC = D + 2k
$$

By these two simple terms the AIC brings model fit and complexity into balance. Note that lower deviance is better and so is lower complexity. In effect, with information criteria *the lower score wins*. As it grounds on the likelihood, it is routinely been used to compare models estimated by classic maximum likelihood estimation. The AIC formula is ridiculously simple and still has a solid foundation in mathematical information theory. It is easily computed, as model deviance is a byproduct of parameter estimation. And if that was not enough, the *AIC is an approximation of LOO cross-validation*, beating it in computational efficiency. The following runs our three models using the lightning-fast frequentist implementation that comes with R:

```{r}
M_08_frq <- glm(y ~ 1, data = D_6)
M_09_frq <- glm(y ~ 1 + x_1, data = D_6)
M_10_frq <- glm(y ~ 1 + x_1 + x_2, data = D_6)

AIC(M_08_frq, M_09_frq, M_10_frq)
```

We get what we expected: the true model $M_09$ wins with the lowest AIC score. But, it seems one has to take a very close look to see the differences, as they are so small. That, indeed, is one of the most common misconceptions about information criteria. In theory, these information criteria contain a large constant, which is unknown. Also, IC scores grow with sample size. In practice, the only thing one know about this unkown constant is that it is smaller than the smallest score observed among the tested models. Therefore, when comparing models *the lowest IC score is the point of reference*. It is the differences that are important, as small they may be. It is common even, to report the difference of a model $i$ to the known model with the smallest score. $\Delta_i = \textrm{AIC}_i - \textrm{AIC}_\textrm{min}$. That is equivalent to setting the model with the lowest score to zero.

```{r}
AIC(M_08_frq, M_09_frq, M_10_frq) %>% 
  as_tibble(rownames = "Model") %>% 
  mutate(Delta = AIC - min(AIC))
```



Still, the AIC has limitations. While it covers the full Generalized Linear Model, it is not suited for multi-level models with random effects. A random effect, we recall, is a factor, where the individual means $mu_i$ are assumed to come from a population-level distribution $mu_i ~ N(\mu, \sigma)$. Fixed effects, say the two group means in a comparison of two designs, are allowed to vary freely. In random effects parameters are more or less closely tied to each other, which causes shrinkage (\@ref(shrinkage)). The smaller the dispersion, the stronger the pull and the less independent are these parameters. As section \@ref{model_complexity} has demonstrated, with random effects, the term $2k$  over-estimates model complexity *and* depends on the data.

The *Deviance Information Criterion (DIC)* was the first generalization of the AIC that solves the problem of model complexity. The DIC estimates the effective complexity of a model from the MCMC samples. It is considered the first Bayesian information criterion, as it operates on the posterior mean rather than the likelihood. In addition, it takes researcher-defined priors into account. These ideas have more recently been carried on and further refined by the *Widely Applicable Information Criterion (WAIC)*. 

WAIC estimates the *expected* log pointwise predictive density using the log-likelihood, which in the respective literature is referred to as *log pointwise predictive density* as goodness-of-fit term. For model complexity, WAIC brings a refined estimate for effective model complexity:

$$
\textrm{elpd}_\textrm{WAIC} = \textrm{lpd} - p_\textrm{WAIC}
$$

The following simple transformation brings $\textrm{elpd}_\textrm{WAIC}$ to the scale of deviance, as do all information criteria by convention:

$$
\textrm{WAIC} = -{1 \over 2}\textrm{elpd}_\textrm{WAIC}
$$


The standard implementation of WAIC, provided by the package loo, and works with rstanarm and brms models. It returns all three estimates:

```{r}
attach(Chapter_WM)
```

```{r opts.label = "mcmc"}
M_08 <- fit_M_08(D_6)
M_09 <- fit_M_09(D_6)
M_10 <- fit_M_10(D_6)
```

```{r}
loo::waic(M_09)
```


While following the original logic as AIC, the WAIC output reveals another extension of the concept. In maximum likelihood estimation, there is only one (maximum) likelihood and, hence, one deviance. In effect, AIC becomes a point estimate. In Bayesian statistics, the likelihood is computed at every single draw of the MCMC procedure, resulting in a distribution of the likelihood. The WAIC even goes one step further and uses the log likelihood of every single observation in the data at every draw. For model $M_09$, this results in a log likelihood matrix with number of draws times number of observations. We extract the matrix and bring it into a tidy form:

```{r}
LL_09 <- 
  log_lik(M_09) %>% 
  as_tibble() %>% 
  mutate(Draw = row_number()) %>% 
  gather(key = "Obs", value = logLik, -Draw) %>% 
  mutate(Obs = as.integer(Obs)) %>% 
  mascutils::go_arrange(Obs, Draw)

LL_09 %>% sample_n(8)
```

The next step computes the Bayesian deviance per observation. Because we are on a logarithmic scale, division by the number of samples becomes a subtraction term. The result is a vector of deviances. The model complexity term $p_{WAIC}$ is the variance of log-likelihoods, per observation. Finally, deviance and complexity are combined into a single score per observation:


```{r}
n_draws <- max(LL_09$Draw)

LPPD_09 <- 
  LL_09 %>% 
  group_by(Obs) %>% 
  summarize(llpd = log(sum(exp(logLik))) - log(n_draws),
            p_WAIC = var(logLik),
            WAIC = -2 * (llpd - p_WAIC))

LPPD_09 %>% sample_n(8)

LPPD_09 %>% 
  gather(key = "Score", value = "value", llpd:WAIC) %>% 
  group_by(Score) %>% 
  summarize(Estimate = sum(value))
```


```{r}
detach(Chapter_WM)
```


```{r opts.label = "mcsync"}
sync_CE(Chapter_WM, D_6, Loo, M_08, M_09, M_10)
```



```{r}
data(cars)

cars

m <- stan_glm(dist ~ speed, data = cars, iter = 1000)
ll <- log_lik(m)

n_cases <- ncol(ll)
n_draws <- nrow(ll)

lppd <- sapply(1:n_cases, function(i) rethinking::log_sum_exp(ll[,i]) - log(n_draws))
p_WAIC <- sapply(1:n_cases, function(i) var(ll[,i]))

sum(lppd)
sum(p_WAIC)


```




### Model pruning

If one measures two predictors $x_1$, $x_2$ and one outcome variable $y$, formally there exist four potential linear models to choose from:

+ `y ~ 1` (grand mean)
+ `y ~ x_1` (main effect 1)
+ `y ~ x_2` (main effect 2)
+ `y ~ x_1 + x_2` (both main effects)
+ `y ~ x_1 * x_2` (both main effects and interaction)

For a data set with three predictors, the set of possible models is already 18. 



<!--Some models can be discarded by more or less informal reasoning. For example, even though most studies routinely gather the variable gender,  there is often little reason to assume that it makes a difference. -->

For a number of reasons, it can be appealing to have more formalized criteria for which model to prefer (and by how much):

1. wanting to prune the model to ease its computation
1. wanting to prune the model to ease its presentation (conciseness)
1. wanting to obtain the best predictions for future values
1. selecting one of two very similar predictors, e.g. two procedures to score results form the same test
1. wanting to test a theory


<!--Testing hypothesis is the prevalent statistical paradigm in academic research. Typically, a researcher formulates (or re-iterates) on a more or less profound theory on human behaviour and derives predictions that are observable. Two contradicting hypotheses are formulated on these claims. Both hypothesis are formalized as two separate statistical models and these models are ultimately compared to select the better one.

The most basic case is that one hypothesis states a variable to have an influence on the outcome, the alternative hypothesis says, it has not. In order to make a case, we re-iterate on the XY case study. Recall that a classic linear model was used for estimation, and flat priors were applied. In other words: no prior knowledge was applied and the estimation should neatly match any maximum likelihood estimates. In such a simple case, the oldest of all information criteria may be applied, the *Akaike Information Criterion (AIC)*. -->

Workflow:

1. Building the theoretically plausible model $M_\textrm{theo}$.
  1. Add variables that are essential for your research question as *primary predictors*. This may already include interaction effects.
  1. Review possible *secondary predictors*. For every candidate try to conceive a plausible story, why it would have an effect on the response variable. If you succeed, add the predictor.
  1. Review possible interaction effects. For every pair (or triple, quadrupel) of primary and secondary predictors try to conceive a story of how they could produce conditional effects. If you succeed, add the interaction term.

2. Prune the model. In a stepwise procedure, the currently most complex plausible model $M_\textrm{compl}$ competes against a more parsimonious candidate model $M_\textrm{pars}$. Start with $M_\textrm{theo}$ as your first $M_\textrm{compl}$.
  1. Start with interaction effects (that are *not* primary predictors). Build $M_\text{pars}$ by leaving out one interaction term. Use WAIC or LOO to compare against $M_\textrm{compl}$. If $M_\text{pars}$ wins in terms of predictive accuracy, make it the new $M_{compl}$.  Start with the highest order effects, and remove one term at a time.
  1. Follow the same procedure with all secondary predictors.



The uncanny valley observation suggests that participants likeablity judgements follow a third-degree polynomial, as this is the smallest polynomial to allow for the trough and the shoulder left of it. Previously, we have already confirmed that the 3-deg polynomial predicts more accurately than a straight linear regression model. However, it could still be the case that a 2-deg polynomial is sufficient, as it allows for the valley. The cubic term just adds some more flexibility in the curvature. But, is that really required?

```{r}
attach(Uncanny)
```

```{r opts.label = "mcmc"}
M_2 <- 
  UV_1 %>% 
  mutate(huMech_0 = 1,
         huMech_1 = huMech,
         huMech_2 = huMech^2) %>% 
stan_glm(avg_like ~ huMech_1 + huMech_2, data = .) 
```

```{r}
loo::waic(M_1)
loo::waic(M_2)
```

```{r}
detach(Uncanny)
```
```{r opts.label = "mcsync"}
sync_CE(Uncanny, M_2)
```



### Choosing response distributions

In hypothesis tests, two (typically nested) models are compared for the purpose of deciding between theories. Model pruning proceeds from a full model and removes predictors for the sake of better certainty in predictions. Both are special cases of model selection. But, with modern evaluation criteria, there is more we can do to arrive at an optimal model.

The new Bayesian model selection criteria LOO and WAIC surpass the classic ones (F-test, AIC and DIC) in that they also allow to compare models with different response distributions. Recall the rather informal comparison of ToT distributions, Gaussian, gamma and exgaussian from \@ref(exgaussian-regression). By visual exploration of residuals, we concluded that the exgaussian is most suited for ToT and RT outcomes. Can we confirm this by formal model selection criteria?

```{r}
attach(CUE8)
```


```{r}
brms::waic(M_4_gau)
brms::waic(M_4_gam)
brms::waic(M_4_exg)
```

```{r}
detach(CUE8)
```



```{r}
attach(Hugme)
```


```{r}
brms::waic(M_1_gau)
brms::waic(M_1_gam)
brms::waic(M_1_exg)
```

```{r}
detach(Hugme)
```






### Towards a new regime

Let me introduce an analogy: In the past decades it has been recognized that car brands make their profit at costs of the environment. Every sold over-sized, over-powered car that hits the road pollutes the environment. Public policy in most countries has reacted to that in a number of ways, mostly putting extra costs on actual usage of the car by the customer. The policy in Germany went one step closer to the root of the problem and pushed the national car brands to reduce the total energy consumption of their fleet.

When a researcher genuinely aims for predictive accuracy, this basically means this person has a stake, often an economic one. If you put the envisioned advantages of your design to a test, let's say, users can more efficiently rent a car online, by cheating on the test you harm yourself, when  after rolling out the system it turns out, these claims were false. Using information criteria will reduce this personal risk. 

The regime of p-value does actually not bother with individual faith, but with the faith of a discipline as a whole. Namely, the vision associated with the p-value is to keep the proportion of unreplicable results in a discipline under a certain threshold, i.e. 5% in  social sciences. In contrast to predictive accuracy, this regime of the p-value does not bother with the faith of a single researcher, but aims at a scientific discipline as a whole. However, this evidently has not worked out well. In modern Psychology, the real proportion of unreplicable results is situated more in the region of 50%. The vision of a 95% clean science has been missed by roughly a magnitude. The reasons for this phenomenal failure are manifold, but some are at least indirectly tied to the p-value:

1. People tend to care for their own faith more than for the faith of the system. As long as scientific careers require $p \le .05%$ (but not much else), people will continue to hack the system for getting their work published.
1. People tend to ignore rules they do not fully understand. The mathematical foundations of the p-value are arcane. High school level math suffices to calculate the RMSE from LOO and the quantity makes immediate sense. I know very few people, who understand how  the ratio of sum-of-squares follows an F distribution.
1. People tend to misapply rules they do not understand, especially when that is inconvenient. In my experience, only few researchers are fully aware that within the p-value paradigm, you must *never*:
  + stop data collection early, when you see that $p \le .05$ has been reached at that point
  + continue data collection beyond the planned sample size, when $p \le .05$ has not been reached, yet.
  + try more than one statistical test or model

However, as of current, the debate on the replication crisis has not settled and no new regime has been established to keep science clean of bogus. My fear is that it will take a long while to get to a new consensus on statistical rigor, mostly because there are too many gray backs that have build their careers on publishing "significant" papers, and these are afraid that their life work are getting depreciated, or, if statistical education would move away from p-values, that future students will no longer be able to read their precious papers.

The other problem with establishing a new regime is that, different to the tight organizational structure of an automotive enterprise, with straight command lines and a well defined communication channel with national governments, a scientific discipline is more like a horde of monkeys in the jungle, everyone primarily interested in jumping on the tallest tree and babbling the loudest. In terms of Nassim N. Taleb, author of the Black Swan, contemporary academic researchers trade short-term personal benefits (getting a paper published in a prestigious journal) for long-term desastrous risks for the discipline. Why should a government invest tax payers' money into such a messy discipline, whose reputation has been diminished in public newspapers? For a discipline such as Psychology, that would be the end, because it almost solely draws on its reputation, and lacks  more tangible assets, like building and selling cars.

p-values operate on science as a whole and encompass a long term perspective. Scientific progress is usually slow, because it moves by trial and error. The collective endeavour to arrive at good theories is (or should be) dominated by time spent on discarding the bad ones, and they probably are a vast majority. With the paradigm of model choice by predictive accuracy, it could be possible to put individual reseachers skin back into the game. 


1. *There is always a next experiment*. Even an individual line of research usually is a sequence of similar experimental studies. Often these experiments are of a very similar kind. For example, I have conducted a sequence of experiments on the Uncanny Valley phenomenon, that replicate and extend the original [REF M&R], using mostly the same set of stimuli. When I read their paper for the first time, I was impressed by how they were able to trace the full UV landscape and I was even so delighted that they shared the data, publicly. In the process of deciding on whether to jump on their experimental paradigm or not, my first move was to estimate my own polynomial models of various degrees and compare them by an information criterion. Why? Because, if it had turned out that a polynomial model with a degree lower than three has the best predictive accuracy on the M&R data, that would have been my best guess for my own future data. A more simple form, parabolic or linear, would have been detrimental for any further reaching conclusions I desire to draw from the data. By identifying a better fitting model by predictive accuracy, a reseacher reduces the risk of running fruitless future experiment. That is a personal incentive rather than a collective.

1. The present debate encompasses a variety of positions:
  + It's all not so bad (and I'll retire soon).
  + p-values must continue to exist, but the convention be lowered to, say $\alpha \le .0001$
  + p-values are not recommended, but tolerated next to other criteria
  + p-values must strictly be replaced by Bayes Factors
  + model selection should be banned, as there is nothing such as a null effect



<!--A governmental agent who requests a car brand to reduce its CO2 footprint, just has to utter a plausible threat to the boss of the company and can rest assured that this will make it all the way down to engine engineering. A scientific discipline has no one -->



The p-value regime operates on the level of a scientific 

I Model selection with information criteria and hypothesis testing with p-values aim at  


### Testing of hypotheses

For everyone who, for once in their lives, have to publish  in an academic fashion, this book has so far left one question unanswered: How can I use Bayesian regression models to test hypotheses? Information criteria, maybe?

If practice-oriented research is the right hand in applied sciences, the left hand is the body of theories that *explain* phenomena in the human-design encounter. In common practice, hypothesis tests are not quantitative, they are categorial, to be more precise: they decide whether a predictor is statistically significant. Every effect undergoing an hypothesis test can either be accepted or be rejected by a set threshold, say $\alpha < .05$. Then, either the data jumps over it, or it does not.

Formally, all such hypothesis tests are comparisons of two models, one contains the parameter in question (the alternative model), the other one does not (the null model). The test itself estimates both models and compares them. If the null model wins, the hypothesis is rejected, if the alternative model wins, the hypothesis is accepted and indirectly, its source, the theory, gains credibility.

From this perspective, replacing p-values with information criteria seems like an obvious improvement, as it allows the researcher to choose from a much wider range of models, and if that does not suffice, one can always resort to LOO cross validation. So, should you do that and if so, how?

For everyone doing applied or practical reseach the answer should be short: If your research implies or even targets real costs, benefits and risks, what you really do is exploring impact factors,  not testing hypotheses. Use the methods based on estimates, as well as the language presented in PART II  of this book. Also, it will be easy to re-formulate the hypothesis into a quantitative research question. And, if you can, imply the potential costs  of the design, system, organization, training or intervention, you are testing. When you are in an early stage of your study design, it can be liberating to forget about the strictly experimental idea of comparing conditions. Perhaps, you do not even need another design.

<!-- # example. Ipump? -->

```{r}
attach(IPump)

D_agg %>% 
  ggplot(aes(x = Design, y = ToT)) +
  geom_boxplot() +
  ggtitle("H_1: The novel design N is more efficient to use than legacy Design L.")
```



```{r}
D_agg %>% 
  filter(Design == "Novel") %>% 
  ggplot(aes(x = Session, y = deviations)) +
  geom_boxplot() +
  ggtitle("RQ_1: Can everyone safely use the novel design after minimal training?")
  
```


```{r}
detach(IPump)
```


If you are comparing two conditions derived from theory (or if your supervisor or editor "convinces" you to do hypothesis testing), information criteria definitely are an option, if you under4stand, what you are doing.

The p-value, although itself a quantity, reigns a dichotomy. The null model, which is accepted when the results is "not significant" truly assumes that there is absolutely no effect. In reality though, a fly leaves a dent on the anvil; there almost always is an effect, be it a miniscule one. A frequent mistake in reporting statistical analysis with p-values is to conclude that there is no effect, when the p-value is not "significant". The same holds for predictive accuracy scores: If a predictor is removed during model pruning, that does not mean the underlying impact factor has a zero effect. When demonstrating the detrimental effects of over-fitting, we have seen how a model gets very excited about "features" in the noise, especially when data is sparse. This tendency is not exclusive to null effects, but all effects are contaminated, even the strong ones. Therefore, to discard a predictor during model selection strictly means that the gain in model fit is out-weighed by the detrimental effect of over-fitting. We reject the null model not because it is untrue, but because it performs worse at predicting the future.






### Model selection

Recall the uncanny valley phenomenon: over a long range the emotional response to artificial faces gets more positive when faces get more humanlike. But, when the appearance of faces are getting really close to real, it drops -- so the theory.

In chapter \@ref(polynomial_regression) the non-linear relationship between human likeness of robot faces and emotional response was modelled as a 3-deg polynomial. We had taken the uncanny valley at face value, but now it is time to test it. Just by visual inspection of the graph below, it seems equally possible, that the relationship follows a much simpler trajectory, such as a straight regression line ($M_0$). As we know, such a model would have only two parameters, slope and intercept. Formally, that is just a 1-deg polynomial and we can regard it as a special case of the 3-deg polynomial model ($M_1$), where the quadratic and the cubic term are fixed to zero. Fixing a parameter is nothing else but creating a more parsimonous model.

$$
M_1: \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 \\
M_0: \beta_0 + \beta_1 x\\
$$

Should we find, that the more parsimonous model wins the comparison, we had to take a critical stance on Mori's theory of the uncanny valley. We estimate the linear regression model $M_0$ and compare it to the polynomial model:

```{r}
attach(Uncanny)
```



```{r opts.label = "mcmc"}
M_0 <- stan_glm(avg_like ~ huMech, 
               data = UV_1)
```



```{r opts.label = "future"}
T_pred <- 
  bind_rows(post_pred(M_0),
            post_pred(M_1)) %>% 
  group_by(Obs, model) %>% 
  summarize(predicted = median(value)) %>% 
  left_join(Uncanny$UV_1, by = "Obs")


T_pred %>% 
  ggplot(aes(x = huMech, y = avg_like)) +
  geom_point() +
  geom_line(aes(y = predicted, col = model))

```


```{r}
brms::waic(M_0)
brms::waic(M_1)
```


```{r}
detach(Uncanny)
```

```{r opts.label = "mcsync"}
sync_CE(Uncanny,M_0)
```





<!-- At the same time, a 5-deg polynomial would add considerable more flex to the curvature, possibly  aligning much neatlier to the data -->


<!--
Using the [XIC] for model comparison, we make a clean decision in favor of M1, justfying the Yerkes-Dodson law. However, the model is, in fact, under-specified. The 2-deg model would also win, when the extremes of arousal were both beneficial. Even die-hard theorists should consult the coefficient table, next to the model comparison. In our case, $\beta_2$ is strongly positive, just the right direction.
-->

<!--### Model pruning

Wanting too much to be true can lead to theory tautology, which is dangerous (Reportedly, tautological theories can be die hard and survive eons.) For statistical models the effect of including too many variables is not dangerous, actually, but it is working against you. 
A model that includes factors of little significance will luckily not give any biased predictions. 

demonstrate by adding fantasy irrelevant variables to the age-by-AB (humpty, deppert, greenwing, ...). Show in a series of effect plots that 

In industrial design research, being able to predict crucial properties of a design reduces economical risks.

*Theory testing* [...] *a prior*

What the researcher on color schemes had not in mind, hopefully, is to mine for effects with certainty of more than 95%, create a coefficient table with only these effects and then, *a posterior*, fabulate a matching theory. In psychology, and probably most social sciences, this has happened and had let to ... [crisis]. [Garden of forking paths]
Researcher who walk the garden of forking paths are producing nothing but *babble*, the lingual equivalent to random noise. 

So, what is the power of models? Two answers:

1. develop theories
1. test theories
1. predict the future


Irrespective of the research purpose, a basic question on the model at hand is:  does it capture all features that stand out from the data, or are their any quirks that let us doubt the model's fidelity. This process is called *model criticism*, and frequently centers around details of lesser theoretical implication, such as: convergence of MCMC estimation, which distribution a variable takes, or whether the chosen priors are realistic. 

Model quality can be approached in two ways: either one focusses just on the model at hand and evaluates its quality against a set of absolute  criteria, or one evaluates the quality of a *model in comparison* to another model, or even a set of models. -->



<!-- ### Cross validation and information criteria

Models are all about their predictive ability. Predictive ability seems to imply that the data must speak about the future, which sounds like gathering more data. In fact, it suffices to have just *out-of-samnple data*, not true forecasting.

A very common procedure to assess reliability of a scale is splitting the items into two parts and assess how the average person score on the first part predicts the second. These two parts are often called *training sample* and  *validation sample*. The procedure of *leave-one-out cross validation* generalizes this idea in that:

1. The validation sample is just one observation at a time
2. As many models as there are observations to leave out
3. A suitable cost function determines how well each model has predicted the left-out observation

In theory, every practitioner who does research for decision making on a specific case should construct a specific cost function and use it to evaluate models. This is an ideal and almost nobody ever does it, because it at least expensive, if not impossible. That is why most researchers evaluate their model by generic cost functions, also called *score functions*. 

One in
-->






<!--

In fact, model selection can be as easy as computing a single criterion per model and rank models by this criterion. Unfortunately, there appears to be a zoo of model selection criteria, and the question arises which to choose. A closer look reveals, that it actually is a family of criteria. One can best think of it as a dynasty, where there is one omnipotent matriarch. All offsprings admire the matriarch and aim at imitating her, which is sometimes more succesful, sometimes less. However, the matriarch is a diva. Getting her to act in a fruitful way can be a tedious endeavour. While being omnipotent, she is not very willing to answer more than the most simple questions. As an advice seeker with a non-trivial problem, you either go with the potentially imperfect answer of an offspring, or go without any advice at all.

The matriarch/diva's name is *Bayes Factor (BF)*. When comparing two models, BF is the odds of betting on one model or the other. A BF of 4 between two models $M_1$ and $M_2$ means that after seeing the data. It deems four times more likely than $M_2$. As a rational gambler, if an opponent puts 1 EUR on $M_2$, you would go 4 EUR 4 against it. 

BF is defined as:

[tbc]

Computing the BF for common models of moderate complexity is wicked and requires advanced skills. For, complex or exotic models BF is practically impossible.
The divas more accessible offsprings are the so-called *information criteria (IC)*. Basically, information criteria are approximations of BF that are sufficiently accurate under additional assumptions, only. These assumptions typically include certain classes of models, and exclude others. On the upside, information critera are computed in more straight forward manner and routinely reported by MCMC fitting engines. While information criteria are single numbers, they don't have a natural interpretation. Practical use of information criteria is simple, though: when comparing two or more models, every model receives an individual score on the IC and the model with the lowest score wins. 

All model comparison underlies a  primary conflict: cling and parsimony. Cling is a models tendency to neatly align the observed data points. Recall the distinction of observed and predicted values:

$y_i = \hat\mu_i + \hat\epsilon_i$

While you know $y_i$, precisely, the separation of fitted value and residual is result of the fitting and therefore uncertain. The ideal situation from a predictive perspective is that $\epsilon_i = 0$: with no noise in the data, prediction is perfect. Naively, one could now try to find a model such that $\hat\epsilon_i = 0$. 

Good fit between model and data certainly is a desireable property. But, there is a twist: models with many parameters tend to be more flexible. Recall the Uncanny study, where a third-degree polynomial was used to fit the association between human likeness and likeability. The third degree polynomial was chosen because it can bend around two stationary points: the trough and the shoulder. Third degree polynomials *can* do that, but can also take any form of lower degree polynomials. Ultimately, the grand mean model can be written as a third degree polynomials, 

$$y_i(x_i) = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \beta_3x_i^3$$

if we just set \beta_1 = \beta_2 = \beta_3 = 0. As lower degree polynomials are special cases of higher degree polynomials, where some parameters is fixed. Fixing one parameter has as a consequence that the function is less flexible and can describe fewer patterns that occur in observations. A model that is more parsimonous on parameters will always be less clingy. Too much cling is harmfull, though, which is best understood  at an extreme example: 

[Extreme example where parameters match $N_Obs$, it truly is a linear relation with noise]

 


-->









<!-- ### Model selection

The broadest application of model comparison arises, when a researcher just seeks to identify the one model with optimal predictive accuracy. In contrast to hypothesis testing and model pruning, this can take a fully optimistic route, without being hampered by prior expectations. 

A carefully pruned model results in optimal prediction using as many variables as is needed. In some cases, though, the researcher wants to select just one exclusive measure from a set of possibilities. Using the same model fit criteria one again, this results in the comparison of two non-nested models.

This situation is routine in research on measurement methods. A measure is an way to quantify a certain property of interest, be it perceived user experience, the general processing speed of a user or how extrovert a person is. In theoretical design research, measures operationalize the quantitative elements of theories. In applied design research, measures are used to evaluate and compare designs and examine differential design effects. Some measures are trivial to assess, such as age. Others require time anf advanced material, such as validated questionnaires or usability tests. Some require expensive equipment. For example, automotive designers use dedicated interfaces to capture measures of driving performance in high fidelity car simulators.

[Case study]

Measurement methods are ordered by three criteria:

1. unbiasedness: how accurately is the property of interest measured?
1. reliability:  does the method possess a good signal-to-noise ratio, providing sufficiently certain estimates?
1. costs: can the method be used with little money and time? Costs can also involve more subtle limitations or concerns, for example, the anatural situation when placed in an fMRI scanner.
-->

### Exercises:

1. When testing the Unvcanny Valley hypothesis, we fixed two parameters at once, the quadratic and the cubic term. While the quadratic term is essential for creating the valley, the cubic term just adds flexibility to the curvature. Make an attempt of model pruning.

```{r opts.label = "invisible"}


```









## Literature

+ Gelman's paper on information criteria
+ MacElreaths statistical distributions
+ XY chapter on prior specification
+ MacElreaths information criteria
+ W&L on Bayes factor and model selection
+ Stan Manual
