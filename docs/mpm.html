<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Multi-predictor models | New statistics for design researchers</title>
  <meta name="description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works to make this world an easier place." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Multi-predictor models | New statistics for design researchers" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works to make this world an easier place." />
  <meta name="github-repo" content="schmettow/New_Stats" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Multi-predictor models | New statistics for design researchers" />
  
  <meta name="twitter:description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works to make this world an easier place." />
  

<meta name="author" content="Martin Schmettow" />


<meta name="date" content="2021-12-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lm.html"/>
<link rel="next" href="mlm.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="NewStats.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="part"><span><b>I Preparations</b></span></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#whom-for"><i class="fa fa-check"></i><b>1.1</b> Whom this book is for</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#quant-design-research"><i class="fa fa-check"></i><b>1.2</b> Quantitative design research</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#what-new-stats"><i class="fa fa-check"></i><b>1.3</b> What is New Statistics?</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#how-to-use"><i class="fa fa-check"></i><b>1.4</b> How to use this book</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#routes"><i class="fa fa-check"></i><b>1.4.1</b> Routes</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#classroom"><i class="fa fa-check"></i><b>1.4.2</b> In the classroom</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#rosetta"><i class="fa fa-check"></i><b>1.4.3</b> The stone of Rosetta</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#thank-you-and-supplementary-readings"><i class="fa fa-check"></i><b>1.5</b> Thank you and supplementary readings</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="gsr.html"><a href="gsr.html"><i class="fa fa-check"></i><b>2</b> Getting started with R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="gsr.html"><a href="gsr.html#setting-up-r"><i class="fa fa-check"></i><b>2.1</b> Setting up the R environment</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="gsr.html"><a href="gsr.html#install-cran"><i class="fa fa-check"></i><b>2.1.1</b> Installing CRAN packages</a></li>
<li class="chapter" data-level="2.1.2" data-path="gsr.html"><a href="gsr.html#install-github"><i class="fa fa-check"></i><b>2.1.2</b> Installing packages from Github</a></li>
<li class="chapter" data-level="2.1.3" data-path="gsr.html"><a href="gsr.html#first-program"><i class="fa fa-check"></i><b>2.1.3</b> A first statistical program</a></li>
<li class="chapter" data-level="2.1.4" data-path="gsr.html"><a href="gsr.html#bibliographic-notes"><i class="fa fa-check"></i><b>2.1.4</b> Bibliographic notes</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="gsr.html"><a href="gsr.html#r-primer"><i class="fa fa-check"></i><b>2.2</b> Learning R: a primer</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="gsr.html"><a href="gsr.html#objects"><i class="fa fa-check"></i><b>2.2.1</b> Assigning and calling Objects</a></li>
<li class="chapter" data-level="2.2.2" data-path="gsr.html"><a href="gsr.html#vectors"><i class="fa fa-check"></i><b>2.2.2</b> Vectors</a></li>
<li class="chapter" data-level="2.2.3" data-path="gsr.html"><a href="gsr.html#object-types"><i class="fa fa-check"></i><b>2.2.3</b> Basic object types</a></li>
<li class="chapter" data-level="2.2.4" data-path="gsr.html"><a href="gsr.html#operators-functions"><i class="fa fa-check"></i><b>2.2.4</b> Operators and functions</a></li>
<li class="chapter" data-level="2.2.5" data-path="gsr.html"><a href="gsr.html#data-frames"><i class="fa fa-check"></i><b>2.2.5</b> Storing data in data frames</a></li>
<li class="chapter" data-level="2.2.6" data-path="gsr.html"><a href="gsr.html#import-export"><i class="fa fa-check"></i><b>2.2.6</b> Import, export and archiving</a></li>
<li class="chapter" data-level="2.2.7" data-path="gsr.html"><a href="gsr.html#case-env"><i class="fa fa-check"></i><b>2.2.7</b> Case environments</a></li>
<li class="chapter" data-level="2.2.8" data-path="gsr.html"><a href="gsr.html#structuring-data"><i class="fa fa-check"></i><b>2.2.8</b> Structuring data</a></li>
<li class="chapter" data-level="2.2.9" data-path="gsr.html"><a href="gsr.html#data-transformation"><i class="fa fa-check"></i><b>2.2.9</b> Data transformation</a></li>
<li class="chapter" data-level="2.2.10" data-path="gsr.html"><a href="gsr.html#plotting"><i class="fa fa-check"></i><b>2.2.10</b> Plotting data</a></li>
<li class="chapter" data-level="2.2.11" data-path="gsr.html"><a href="gsr.html#fitting"><i class="fa fa-check"></i><b>2.2.11</b> Fitting regression models</a></li>
<li class="chapter" data-level="2.2.12" data-path="gsr.html"><a href="gsr.html#knitting"><i class="fa fa-check"></i><b>2.2.12</b> Knitting statistical reports</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="gsr.html"><a href="gsr.html#lib_gsr"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ebs.html"><a href="ebs.html"><i class="fa fa-check"></i><b>3</b> Elements of Bayesian statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ebs.html"><a href="ebs.html#decision-making"><i class="fa fa-check"></i><b>3.1</b> Rational decision making in design research</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ebs.html"><a href="ebs.html#measuring-uncertainty"><i class="fa fa-check"></i><b>3.1.1</b> Measuring uncertainty</a></li>
<li class="chapter" data-level="3.1.2" data-path="ebs.html"><a href="ebs.html#benchmarking-designs"><i class="fa fa-check"></i><b>3.1.2</b> Benchmarking designs</a></li>
<li class="chapter" data-level="3.1.3" data-path="ebs.html"><a href="ebs.html#comparing-designs"><i class="fa fa-check"></i><b>3.1.3</b> Comparison of designs</a></li>
<li class="chapter" data-level="3.1.4" data-path="ebs.html"><a href="ebs.html#prior-knowledge"><i class="fa fa-check"></i><b>3.1.4</b> Prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ebs.html"><a href="ebs.html#observations-measures"><i class="fa fa-check"></i><b>3.2</b> Observations and measures</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ebs.html"><a href="ebs.html#interaction-seq"><i class="fa fa-check"></i><b>3.2.1</b> Interaction sequences</a></li>
<li class="chapter" data-level="3.2.2" data-path="ebs.html"><a href="ebs.html#perf-measures"><i class="fa fa-check"></i><b>3.2.2</b> Performance measures</a></li>
<li class="chapter" data-level="3.2.3" data-path="ebs.html"><a href="ebs.html#satisfaction-and-other-feelings"><i class="fa fa-check"></i><b>3.2.3</b> Satisfaction and other feelings</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ebs.html"><a href="ebs.html#descriptive-stats"><i class="fa fa-check"></i><b>3.3</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ebs.html"><a href="ebs.html#frequencies"><i class="fa fa-check"></i><b>3.3.1</b> Frequencies</a></li>
<li class="chapter" data-level="3.3.2" data-path="ebs.html"><a href="ebs.html#central-tendency"><i class="fa fa-check"></i><b>3.3.2</b> Central tendency</a></li>
<li class="chapter" data-level="3.3.3" data-path="ebs.html"><a href="ebs.html#dispersion"><i class="fa fa-check"></i><b>3.3.3</b> Dispersion</a></li>
<li class="chapter" data-level="3.3.4" data-path="ebs.html"><a href="ebs.html#associations"><i class="fa fa-check"></i><b>3.3.4</b> Associations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ebs.html"><a href="ebs.html#bayes-prob-theory"><i class="fa fa-check"></i><b>3.4</b> Bayesian probability theory</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ebs.html"><a href="ebs.html#set-theory"><i class="fa fa-check"></i><b>3.4.1</b> Some set theory</a></li>
<li class="chapter" data-level="3.4.2" data-path="ebs.html"><a href="ebs.html#probability"><i class="fa fa-check"></i><b>3.4.2</b> Probability</a></li>
<li class="chapter" data-level="3.4.3" data-path="ebs.html"><a href="ebs.html#likelihood"><i class="fa fa-check"></i><b>3.4.3</b> Likelihood</a></li>
<li class="chapter" data-level="3.4.4" data-path="ebs.html"><a href="ebs.html#bayes-freq-prob"><i class="fa fa-check"></i><b>3.4.4</b> Bayesian and frequentist probability</a></li>
<li class="chapter" data-level="3.4.5" data-path="ebs.html"><a href="ebs.html#bayes-theorem"><i class="fa fa-check"></i><b>3.4.5</b> Bayes theorem</a></li>
<li class="chapter" data-level="3.4.6" data-path="ebs.html"><a href="ebs.html#dynamics-belief"><i class="fa fa-check"></i><b>3.4.6</b> Bayesian dynamics of belief</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ebs.html"><a href="ebs.html#statmod"><i class="fa fa-check"></i><b>3.5</b> Statistical models</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="ebs.html"><a href="ebs.html#structural-part"><i class="fa fa-check"></i><b>3.5.1</b> The structural part</a></li>
<li class="chapter" data-level="3.5.2" data-path="ebs.html"><a href="ebs.html#distributions"><i class="fa fa-check"></i><b>3.5.2</b> Distributions: shapes of randomness</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="ebs.html"><a href="ebs.html#bayes-estimation"><i class="fa fa-check"></i><b>3.6</b> Towards Bayesian estimation</a></li>
<li class="chapter" data-level="3.7" data-path="ebs.html"><a href="ebs.html#priors-defaults"><i class="fa fa-check"></i><b>3.7</b> On priors and defaults</a></li>
<li class="chapter" data-level="3.8" data-path="ebs.html"><a href="ebs.html#further-readings"><i class="fa fa-check"></i><b>3.8</b> Further readings</a></li>
</ul></li>
<li class="part"><span><b>II Models</b></span></li>
<li class="chapter" data-level="4" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>4</b> Basic Linear models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lm.html"><a href="lm.html#gmm"><i class="fa fa-check"></i><b>4.1</b> Quantification at work: grand mean models</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="lm.html"><a href="lm.html#random-walk"><i class="fa fa-check"></i><b>4.1.1</b> Do the random walk: Markov Chain Monte Carlo sampling</a></li>
<li class="chapter" data-level="4.1.2" data-path="lm.html"><a href="lm.html#likelihood-random-term"><i class="fa fa-check"></i><b>4.1.2</b> Likelihood and random term</a></li>
<li class="chapter" data-level="4.1.3" data-path="lm.html"><a href="lm.html#posterior-dist"><i class="fa fa-check"></i><b>4.1.3</b> Working with the posterior distribution</a></li>
<li class="chapter" data-level="4.1.4" data-path="lm.html"><a href="lm.html#clu"><i class="fa fa-check"></i><b>4.1.4</b> Center and interval estimates</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lm.html"><a href="lm.html#lrm"><i class="fa fa-check"></i><b>4.2</b> Walk the line: linear regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="lm.html"><a href="lm.html#transform-measures"><i class="fa fa-check"></i><b>4.2.1</b> Transforming measures</a></li>
<li class="chapter" data-level="4.2.2" data-path="lm.html"><a href="lm.html#correlations"><i class="fa fa-check"></i><b>4.2.2</b> Correlations</a></li>
<li class="chapter" data-level="4.2.3" data-path="lm.html"><a href="lm.html#endless-linear"><i class="fa fa-check"></i><b>4.2.3</b> Endlessly linear</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lm.html"><a href="lm.html#factorial-models"><i class="fa fa-check"></i><b>4.3</b> Factorial Models</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="lm.html"><a href="lm.html#cgm"><i class="fa fa-check"></i><b>4.3.1</b> A versus B: Comparison of groups</a></li>
<li class="chapter" data-level="4.3.2" data-path="lm.html"><a href="lm.html#dummy"><i class="fa fa-check"></i><b>4.3.2</b> Not stupid: dummy variables</a></li>
<li class="chapter" data-level="4.3.3" data-path="lm.html"><a href="lm.html#treatment-contrasts"><i class="fa fa-check"></i><b>4.3.3</b> Treatment contrast coding</a></li>
<li class="chapter" data-level="4.3.4" data-path="lm.html"><a href="lm.html#amm"><i class="fa fa-check"></i><b>4.3.4</b> Absolute Means Model</a></li>
<li class="chapter" data-level="4.3.5" data-path="lm.html"><a href="lm.html#ofm"><i class="fa fa-check"></i><b>4.3.5</b> Ordered Factorial Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mpm.html"><a href="mpm.html"><i class="fa fa-check"></i><b>5</b> Multi-predictor models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mpm.html"><a href="mpm.html#mrm"><i class="fa fa-check"></i><b>5.1</b> On surface: multiple regression models</a></li>
<li class="chapter" data-level="5.2" data-path="mpm.html"><a href="mpm.html#mfm"><i class="fa fa-check"></i><b>5.2</b> Crossover: multifactorial models</a></li>
<li class="chapter" data-level="5.3" data-path="mpm.html"><a href="mpm.html#grm"><i class="fa fa-check"></i><b>5.3</b> Line-by-line: grouped regression models</a></li>
<li class="chapter" data-level="5.4" data-path="mpm.html"><a href="mpm.html#cfxm"><i class="fa fa-check"></i><b>5.4</b> Conditional effects models</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="mpm.html"><a href="mpm.html#cmrm"><i class="fa fa-check"></i><b>5.4.1</b> Conditional multiple regression</a></li>
<li class="chapter" data-level="5.4.2" data-path="mpm.html"><a href="mpm.html#cmfm"><i class="fa fa-check"></i><b>5.4.2</b> Conditional multifactorial models</a></li>
<li class="chapter" data-level="5.4.3" data-path="mpm.html"><a href="mpm.html#saturation"><i class="fa fa-check"></i><b>5.4.3</b> Saturation: hitting the boundaries</a></li>
<li class="chapter" data-level="5.4.4" data-path="mpm.html"><a href="mpm.html#amplification"><i class="fa fa-check"></i><b>5.4.4</b> Amplification: more than the sum</a></li>
<li class="chapter" data-level="5.4.5" data-path="mpm.html"><a href="mpm.html#cfx-theory"><i class="fa fa-check"></i><b>5.4.5</b> Conditional effects and design theory</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="mpm.html"><a href="mpm.html#prm"><i class="fa fa-check"></i><b>5.5</b> Doing the rollercoaster: polynomial regression models</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="mpm.html"><a href="mpm.html#test-stat"><i class="fa fa-check"></i><b>5.5.1</b> Make yourself a test statistic</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="mpm.html"><a href="mpm.html#further-readings-1"><i class="fa fa-check"></i><b>5.6</b> Further readings</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mlm.html"><a href="mlm.html"><i class="fa fa-check"></i><b>6</b> Multilevel models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mlm.html"><a href="mlm.html#intercept-re"><i class="fa fa-check"></i><b>6.1</b> The Human Factor: Intercept random effects</a></li>
<li class="chapter" data-level="6.2" data-path="mlm.html"><a href="mlm.html#slope-re"><i class="fa fa-check"></i><b>6.2</b> Multi-level linear regression: variance in change</a></li>
<li class="chapter" data-level="6.3" data-path="mlm.html"><a href="mlm.html#thinking-multi-level"><i class="fa fa-check"></i><b>6.3</b> Thinking multi-level</a></li>
<li class="chapter" data-level="6.4" data-path="mlm.html"><a href="mlm.html#universality"><i class="fa fa-check"></i><b>6.4</b> Testing universality of theories</a></li>
<li class="chapter" data-level="6.5" data-path="mlm.html"><a href="mlm.html#non-human-populations"><i class="fa fa-check"></i><b>6.5</b> Non-human populations and cross-overs</a></li>
<li class="chapter" data-level="6.6" data-path="mlm.html"><a href="mlm.html#nested-re"><i class="fa fa-check"></i><b>6.6</b> Nested random effects</a></li>
<li class="chapter" data-level="6.7" data-path="mlm.html"><a href="mlm.html#pool-shrink"><i class="fa fa-check"></i><b>6.7</b> What are random effects? On pooling and shrinkage</a></li>
<li class="chapter" data-level="6.8" data-path="mlm.html"><a href="mlm.html#psychometrics"><i class="fa fa-check"></i><b>6.8</b> Psychometrics and design-o-metric models</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="mlm.html"><a href="mlm.html#coverage"><i class="fa fa-check"></i><b>6.8.1</b> Coverage</a></li>
<li class="chapter" data-level="6.8.2" data-path="mlm.html"><a href="mlm.html#reliability"><i class="fa fa-check"></i><b>6.8.2</b> Reliability</a></li>
<li class="chapter" data-level="6.8.3" data-path="mlm.html"><a href="mlm.html#validity"><i class="fa fa-check"></i><b>6.8.3</b> Validity</a></li>
<li class="chapter" data-level="6.8.4" data-path="mlm.html"><a href="mlm.html#designometrix"><i class="fa fa-check"></i><b>6.8.4</b> Towards Design-o-metrix</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="mlm.html"><a href="mlm.html#further-readings-2"><i class="fa fa-check"></i><b>6.9</b> Further readings</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="glm.html"><a href="glm.html#elements-glm"><i class="fa fa-check"></i><b>7.1</b> Elements of Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="glm.html"><a href="glm.html#relinking-linearity"><i class="fa fa-check"></i><b>7.1.1</b> Re-linking linearity</a></li>
<li class="chapter" data-level="7.1.2" data-path="glm.html"><a href="glm.html#choosing-randomness"><i class="fa fa-check"></i><b>7.1.2</b> Choosing patterns of randomness</a></li>
<li class="chapter" data-level="7.1.3" data-path="glm.html"><a href="glm.html#mean-var-rel"><i class="fa fa-check"></i><b>7.1.3</b> Mean-variance relationship</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="glm.html"><a href="glm.html#count-data"><i class="fa fa-check"></i><b>7.2</b> Count data</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="glm.html"><a href="glm.html#pois-reg"><i class="fa fa-check"></i><b>7.2.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="glm.html"><a href="glm.html#logistic-reg"><i class="fa fa-check"></i><b>7.2.2</b> Logistic (aka Binomial) regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="glm.html"><a href="glm.html#overdispersion"><i class="fa fa-check"></i><b>7.2.3</b> Modelling overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="glm.html"><a href="glm.html#duration-measures"><i class="fa fa-check"></i><b>7.3</b> Duration measures</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="glm.html"><a href="glm.html#exp-gam-reg"><i class="fa fa-check"></i><b>7.3.1</b> Exponential and Gamma regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="glm.html"><a href="glm.html#exgauss-reg"><i class="fa fa-check"></i><b>7.3.2</b> ExGaussian regression</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="glm.html"><a href="glm.html#rating-scales"><i class="fa fa-check"></i><b>7.4</b> Rating scales</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="glm.html"><a href="glm.html#ord-logist-reg"><i class="fa fa-check"></i><b>7.4.1</b> Ordered logistic regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="glm.html"><a href="glm.html#beta-reg"><i class="fa fa-check"></i><b>7.4.2</b> Beta regression</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="glm.html"><a href="glm.html#distributional-models"><i class="fa fa-check"></i><b>7.5</b> Beyond mean: distributional models</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="glm.html"><a href="glm.html#item-anchoring"><i class="fa fa-check"></i><b>7.5.1</b> Item-level anchoring in rating scales</a></li>
<li class="chapter" data-level="7.5.2" data-path="glm.html"><a href="glm.html#part-employment"><i class="fa fa-check"></i><b>7.5.2</b> Participant-level employment of scale</a></li>
<li class="chapter" data-level="7.5.3" data-path="glm.html"><a href="glm.html#part-skew"><i class="fa fa-check"></i><b>7.5.3</b> Participant-level skew in reaction times</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="glm.html"><a href="glm.html#further-readings-3"><i class="fa fa-check"></i><b>7.6</b> Further readings</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="wwm.html"><a href="wwm.html"><i class="fa fa-check"></i><b>8</b> Working with models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="wwm.html"><a href="wwm.html#model-criticism"><i class="fa fa-check"></i><b>8.1</b> Model criticism</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="wwm.html"><a href="wwm.html#residual-analysis"><i class="fa fa-check"></i><b>8.1.1</b> Residual analysis</a></li>
<li class="chapter" data-level="8.1.2" data-path="wwm.html"><a href="wwm.html#fitted-responses"><i class="fa fa-check"></i><b>8.1.2</b> Fitted responses analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="wwm.html"><a href="wwm.html#model-comp"><i class="fa fa-check"></i><b>8.2</b> Model comparison</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="wwm.html"><a href="wwm.html#overfitting"><i class="fa fa-check"></i><b>8.2.1</b> The problem of over-fitting</a></li>
<li class="chapter" data-level="8.2.2" data-path="wwm.html"><a href="wwm.html#cross-validation"><i class="fa fa-check"></i><b>8.2.2</b> Cross validation and LOO</a></li>
<li class="chapter" data-level="8.2.3" data-path="wwm.html"><a href="wwm.html#ic"><i class="fa fa-check"></i><b>8.2.3</b> Information Criteria</a></li>
<li class="chapter" data-level="8.2.4" data-path="wwm.html"><a href="wwm.html#model-selection"><i class="fa fa-check"></i><b>8.2.4</b> Model selection</a></li>
<li class="chapter" data-level="8.2.5" data-path="wwm.html"><a href="wwm.html#choose-dist"><i class="fa fa-check"></i><b>8.2.5</b> Comparing response distributions</a></li>
<li class="chapter" data-level="8.2.6" data-path="wwm.html"><a href="wwm.html#testing-theories"><i class="fa fa-check"></i><b>8.2.6</b> Testing hypotheses</a></li>
<li class="chapter" data-level="8.2.7" data-path="wwm.html"><a href="wwm.html#bayes-factor"><i class="fa fa-check"></i><b>8.2.7</b> A note on Bayes Factor</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="wwm.html"><a href="wwm.html#further-readings-4"><i class="fa fa-check"></i><b>8.3</b> Further readings</a></li>
</ul></li>
<li class="part"><span><b>III Preview Chapters</b></span></li>
<li class="chapter" data-level="9" data-path="LCM.html"><a href="LCM.html"><i class="fa fa-check"></i><b>9</b> Learning curve models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="LCM.html"><a href="LCM.html#the-tweak-finder-model-of-building-skills"><i class="fa fa-check"></i><b>9.1</b> The tweak-finder model of building skills</a></li>
<li class="chapter" data-level="9.2" data-path="LCM.html"><a href="LCM.html#the-exponential-law-of-finding-tweaks"><i class="fa fa-check"></i><b>9.2</b> The Exponential law of finding tweaks</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="LCM.html"><a href="LCM.html#linearization-of-parameters"><i class="fa fa-check"></i><b>9.2.1</b> Linearization of parameters</a></li>
<li class="chapter" data-level="9.2.2" data-path="LCM.html"><a href="LCM.html#priors"><i class="fa fa-check"></i><b>9.2.2</b> Priors</a></li>
<li class="chapter" data-level="9.2.3" data-path="LCM.html"><a href="LCM.html#building-the-model"><i class="fa fa-check"></i><b>9.2.3</b> Building the model</a></li>
<li class="chapter" data-level="9.2.4" data-path="LCM.html"><a href="LCM.html#analyzing-the-results"><i class="fa fa-check"></i><b>9.2.4</b> Analyzing the results</a></li>
<li class="chapter" data-level="9.2.5" data-path="LCM.html"><a href="LCM.html#a-bunch-of-girls-and-their-lacies"><i class="fa fa-check"></i><b>9.2.5</b> A bunch of girls and their Lacies</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="LCM.html"><a href="LCM.html#how-to-deal-with-flat-lines"><i class="fa fa-check"></i><b>9.3</b> How to deal with flat lines</a></li>
<li class="chapter" data-level="9.4" data-path="LCM.html"><a href="LCM.html#conditional-learning-curve-models"><i class="fa fa-check"></i><b>9.4</b> Conditional Learning Curve Models</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="LCM.html"><a href="LCM.html#differences-in-experience"><i class="fa fa-check"></i><b>9.4.1</b> Differences in experience</a></li>
<li class="chapter" data-level="9.4.2" data-path="LCM.html"><a href="LCM.html#conditional-rate-and-amplitude"><i class="fa fa-check"></i><b>9.4.2</b> Conditional Rate and Amplitude</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="LCM.html"><a href="LCM.html#compound-learning-curves-experimental"><i class="fa fa-check"></i><b>9.5</b> Compound learning curves (EXPERIMENTAL)</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="LCM.html"><a href="LCM.html#adjusting-for-fatigue-and-finding-a-separation"><i class="fa fa-check"></i><b>9.5.1</b> Adjusting for fatigue and finding a separation</a></li>
<li class="chapter" data-level="9.5.2" data-path="LCM.html"><a href="LCM.html#lace-acy"><i class="fa fa-check"></i><b>9.5.2</b> Lace ACY!</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="cases.html"><a href="cases.html"><i class="fa fa-check"></i><b>A</b> Cases</a>
<ul>
<li class="chapter" data-level="A.1" data-path="cases.html"><a href="cases.html#real-cases"><i class="fa fa-check"></i><b>A.1</b> Real cases</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="cases.html"><a href="cases.html#hugme"><i class="fa fa-check"></i><b>A.1.1</b> Hugme</a></li>
<li class="chapter" data-level="A.1.2" data-path="cases.html"><a href="cases.html#cue8"><i class="fa fa-check"></i><b>A.1.2</b> CUE8</a></li>
<li class="chapter" data-level="A.1.3" data-path="cases.html"><a href="cases.html#uncanny"><i class="fa fa-check"></i><b>A.1.3</b> Uncanny Valley</a></li>
<li class="chapter" data-level="A.1.4" data-path="cases.html"><a href="cases.html#ipump"><i class="fa fa-check"></i><b>A.1.4</b> IPump</a></li>
<li class="chapter" data-level="A.1.5" data-path="cases.html"><a href="cases.html#sleepstudy"><i class="fa fa-check"></i><b>A.1.5</b> Case Sleepstudy</a></li>
<li class="chapter" data-level="A.1.6" data-path="cases.html"><a href="cases.html#egan"><i class="fa fa-check"></i><b>A.1.6</b> Egan</a></li>
<li class="chapter" data-level="A.1.7" data-path="cases.html"><a href="cases.html#mmn"><i class="fa fa-check"></i><b>A.1.7</b> Case: Millers Magic Number</a></li>
<li class="chapter" data-level="A.1.8" data-path="cases.html"><a href="cases.html#aup"><i class="fa fa-check"></i><b>A.1.8</b> AUP</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="cases.html"><a href="cases.html#synthetic-data"><i class="fa fa-check"></i><b>A.2</b> Synthetic data sets</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="cases.html"><a href="cases.html#rainfall"><i class="fa fa-check"></i><b>A.2.1</b> Rainfall</a></li>
<li class="chapter" data-level="A.2.2" data-path="cases.html"><a href="cases.html#sec99"><i class="fa fa-check"></i><b>A.2.2</b> 99 seconds</a></li>
<li class="chapter" data-level="A.2.3" data-path="cases.html"><a href="cases.html#rational"><i class="fa fa-check"></i><b>A.2.3</b> Rational</a></li>
<li class="chapter" data-level="A.2.4" data-path="cases.html"><a href="cases.html#browsingab"><i class="fa fa-check"></i><b>A.2.4</b> BrowsingAB</a></li>
<li class="chapter" data-level="A.2.5" data-path="cases.html"><a href="cases.html#headache"><i class="fa fa-check"></i><b>A.2.5</b> Headache</a></li>
<li class="chapter" data-level="A.2.6" data-path="cases.html"><a href="cases.html#reading"><i class="fa fa-check"></i><b>A.2.6</b> Reading time</a></li>
<li class="chapter" data-level="A.2.7" data-path="cases.html"><a href="cases.html#argame"><i class="fa fa-check"></i><b>A.2.7</b> AR_game</a></li>
<li class="chapter" data-level="A.2.8" data-path="cases.html"><a href="cases.html#sleep"><i class="fa fa-check"></i><b>A.2.8</b> Sleep</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">New statistics for design researchers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mpm" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Multi-predictor models</h1>
<p>Design researchers are often collecting data under rather wild conditions.
Users of municipal websites, consumer products, enterprise information systems and cars can be extremely diverse.
At the same time, Designs vary in many attributes, affecting the user in many different ways.
There are many variables in the game, and even more possible relations.
With <em>multi predictor models</em> we can examine the simultaneous influence of everything we have recorded.</p>
<p>The first part of the chapter deals with predictors that act independent of each other:
Section <a href="mpm.html#mrm">5.1</a> demonstrates how two continuous linear predictors form a surface in a three-dimensional space. Subsequently, we address the case of multi-factorial models (<a href="mpm.html#mfm">5.2</a>), which are very common in experimental research. In section <a href="lm.html#dummy">4.3.2</a> we have seen how linear models unite factorial with continuous predictors. This lays the ground for combining them into grouped regression models <a href="mpm.html#grm">5.3</a>.</p>
<p>That being said, in reality it frequently happens that predictors are not acting independent on each other. Rather, the influence of one predictor changes dependent on the value of another predictor. In <a href="mpm.html#cfxm">5.4</a>, conditional effects models will be introduced. As will turn out, by adding conditional effects a linear model is capable of rendering non-linear associations. The final section introduces polynomial regression as a general way to estimate even wildly non-linear relationship between a continuous predictor and the outcome variable.</p>
<div id="mrm" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> On surface: multiple regression models</h2>
<!-- #62 -->
<p>Productivity software, like word processors, presentation and calculation software or graphics programs have evolved over decades.
For every new release, dozens of developers have worked hard to make the handling more efficient and the user experience more pleasant.
Consider a program for drawing illustrations: basic functionality, such as drawing lines, selecting objects, moving or colourizing them, have practically always been there.
A user wanting to draw six rectangles, painting them red and arranging them in a grid pattern, can readily do that using basic functionality.
At a certain point of system evolution, it may have been recognized that this is what users repeatedly do: creating a grid of alike objects.
With the basic functions this is rather repetitive and a new function was created, called “copy-and-arrange.”
Users may now create a single object, specify rows and columns of the grid and give it a run.</p>
<p>The new function saves time and leads to better results.
Users should be very excited about the new feature, should they not?
Not quite, as <span class="citation">(Carroll and Rosson 1987)</span> made a very troubling observation: adding functionality for the good of efficiency may turn out ineffective in practice, as users have a strong tendency to stick with their old routines, ignoring new functionality right away.
This is called the <em>active user paradox (AUP)</em>.</p>
<p>Do all users behave that way?
Or can we find users of certain traits that are different?
What type of person would be less likely to fall for the AUP?
And how can we measure resistance towards the AUP?
We did a study, where we explored the impact of two user traits <em>need-for-cognition (ncs)</em> and <em>geekism (gex)</em> on AUP resistance.
To measure AUP resistance we observed users while they were doing drawing tasks with a graphics software.
User actions were noted down and classified by a behavioral coding scheme. From the frequencies of exploration and elaboration actions an individual AUP resistance score was derived. So, are users with high need-for-cognition and geekism more resistant to the AUP?</p>
<p>In Figure <a href="mpm.html#fig:mrm-1">5.1</a> we first look at the two predictors, separately:</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="mpm.html#cb375-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(AUP)</span>
<span id="cb375-2"><a href="mpm.html#cb375-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb375-3"><a href="mpm.html#cb375-3" aria-hidden="true" tabindex="-1"></a>AUP_1 <span class="sc">%&gt;%</span></span>
<span id="cb375-4"><a href="mpm.html#cb375-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> ncs, <span class="at">y =</span> resistance)) <span class="sc">+</span></span>
<span id="cb375-5"><a href="mpm.html#cb375-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb375-6"><a href="mpm.html#cb375-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> F)</span>
<span id="cb375-7"><a href="mpm.html#cb375-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb375-8"><a href="mpm.html#cb375-8" aria-hidden="true" tabindex="-1"></a>AUP_1 <span class="sc">%&gt;%</span></span>
<span id="cb375-9"><a href="mpm.html#cb375-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> gex, <span class="at">y =</span> resistance)) <span class="sc">+</span></span>
<span id="cb375-10"><a href="mpm.html#cb375-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb375-11"><a href="mpm.html#cb375-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> F)</span></code></pre></div>
<div class="figure"><span id="fig:mrm-1"></span>
<img src="Classic_linear_models_files/figure-html/mrm-1-1.png" alt="Linear associations of NCS and Gex with Resistance" width="50%" /><img src="Classic_linear_models_files/figure-html/mrm-1-2.png" alt="Linear associations of NCS and Gex with Resistance" width="50%" />
<p class="caption">
Figure 5.1: Linear associations of NCS and Gex with Resistance
</p>
</div>
<p>As we will see, it is preferable to build one model with two simultaneous predictors, and this is what the present section is all about.
Still, we begin with two separate LRMs, one for each predictor, and z-transformed scores, then we create one model with two predictors.</p>
<p><span class="math display">\[
\begin{aligned}
&amp;M1: \mu_i = \beta_0 + \beta_\mathrm{ncs} x_\mathrm{ncs}\\
&amp;M2: \mu_i = \beta_0 + \beta_\mathrm{gex} x_\mathrm{gex}
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="mpm.html#cb376-1" aria-hidden="true" tabindex="-1"></a>M_1 <span class="ot">&lt;-</span></span>
<span id="cb376-2"><a href="mpm.html#cb376-2" aria-hidden="true" tabindex="-1"></a>  AUP_1 <span class="sc">%&gt;%</span></span>
<span id="cb376-3"><a href="mpm.html#cb376-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(zresistance <span class="sc">~</span> zncs, <span class="at">data =</span> .)</span>
<span id="cb376-4"><a href="mpm.html#cb376-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb376-5"><a href="mpm.html#cb376-5" aria-hidden="true" tabindex="-1"></a>M_2 <span class="ot">&lt;-</span></span>
<span id="cb376-6"><a href="mpm.html#cb376-6" aria-hidden="true" tabindex="-1"></a>  AUP_1 <span class="sc">%&gt;%</span></span>
<span id="cb376-7"><a href="mpm.html#cb376-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(zresistance <span class="sc">~</span> zgex, <span class="at">data =</span> .)</span></code></pre></div>
<p>Next, we estimate a model that includes both predictors <em>simultaneously</em>.
The most practical property of the linear term is that we can include multiple predictor terms (and the intercept), just by forming the sum.
In this case, this is a <code>multiple regression model (MRM)</code>:</p>
<p><span class="math display">\[
\mu_i = \beta_0 + \beta_\mathrm{ncs} x_\mathrm{ncs} + \beta_\mathrm{gex} x_\mathrm{gex}
\]</span></p>
<p>In R’s regression formula language, this is similarly straight-forward.
The <code>+</code> operator directly corresponds with the <code>+</code> in the likelihood formula.</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="mpm.html#cb377-1" aria-hidden="true" tabindex="-1"></a>M_3 <span class="ot">&lt;-</span></span>
<span id="cb377-2"><a href="mpm.html#cb377-2" aria-hidden="true" tabindex="-1"></a>  AUP_1 <span class="sc">%&gt;%</span></span>
<span id="cb377-3"><a href="mpm.html#cb377-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(zresistance <span class="sc">~</span> zncs <span class="sc">+</span> zgex, <span class="at">data =</span> .) <span class="co"># &lt;--</span></span></code></pre></div>
<p>For the comparison of the three models we make use of a feature of package Bayr: the posterior distributions of arbitrary models can be combined into one multi-model posterior object, by just stacking them upon each other with <code>bind_rows</code> (Table <a href="mpm.html#tab:AUP-bind">5.1</a>). In efect, the coefficient table shows all models simultaneously (Table <a href="mpm.html#tab:AUP-coef">5.2</a>):</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="mpm.html#cb378-1" aria-hidden="true" tabindex="-1"></a>P_multi <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb378-2"><a href="mpm.html#cb378-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_1),</span>
<span id="cb378-3"><a href="mpm.html#cb378-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_2),</span>
<span id="cb378-4"><a href="mpm.html#cb378-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_3)</span>
<span id="cb378-5"><a href="mpm.html#cb378-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb378-6"><a href="mpm.html#cb378-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb378-7"><a href="mpm.html#cb378-7" aria-hidden="true" tabindex="-1"></a>P_multi</span></code></pre></div>
<table>
<caption><span id="tab:AUP-bind">Table 5.1: </span>MCMC posterior with 4000 samples of 10 parameters in 3 model(s)</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">parameter</th>
<th align="left">type</th>
<th align="left">fixef</th>
<th align="right">count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">M_1</td>
<td align="left">Intercept</td>
<td align="left">fixef</td>
<td align="left">Intercept</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">M_1</td>
<td align="left">zncs</td>
<td align="left">fixef</td>
<td align="left">zncs</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">M_2</td>
<td align="left">Intercept</td>
<td align="left">fixef</td>
<td align="left">Intercept</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">M_2</td>
<td align="left">zgex</td>
<td align="left">fixef</td>
<td align="left">zgex</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">M_3</td>
<td align="left">Intercept</td>
<td align="left">fixef</td>
<td align="left">Intercept</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">M_3</td>
<td align="left">zgex</td>
<td align="left">fixef</td>
<td align="left">zgex</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">M_3</td>
<td align="left">zncs</td>
<td align="left">fixef</td>
<td align="left">zncs</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">M_1</td>
<td align="left">sigma_resid</td>
<td align="left">disp</td>
<td align="left"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">M_2</td>
<td align="left">sigma_resid</td>
<td align="left">disp</td>
<td align="left"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">M_3</td>
<td align="left">sigma_resid</td>
<td align="left">disp</td>
<td align="left"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="mpm.html#cb379-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(P_multi)</span></code></pre></div>
<table>
<caption><span id="tab:AUP-coef">Table 5.2: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">M_1</td>
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">-0.001</td>
<td align="right">-0.325</td>
<td align="right">0.314</td>
</tr>
<tr class="even">
<td align="left">M_1</td>
<td align="left">zncs</td>
<td align="left">zncs</td>
<td align="right">0.372</td>
<td align="right">0.065</td>
<td align="right">0.687</td>
</tr>
<tr class="odd">
<td align="left">M_2</td>
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">0.001</td>
<td align="right">-0.312</td>
<td align="right">0.305</td>
</tr>
<tr class="even">
<td align="left">M_2</td>
<td align="left">zgex</td>
<td align="left">zgex</td>
<td align="right">0.291</td>
<td align="right">-0.034</td>
<td align="right">0.618</td>
</tr>
<tr class="odd">
<td align="left">M_3</td>
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">-0.001</td>
<td align="right">-0.302</td>
<td align="right">0.306</td>
</tr>
<tr class="even">
<td align="left">M_3</td>
<td align="left">zncs</td>
<td align="left">zncs</td>
<td align="right">0.295</td>
<td align="right">-0.079</td>
<td align="right">0.687</td>
</tr>
<tr class="odd">
<td align="left">M_3</td>
<td align="left">zgex</td>
<td align="left">zgex</td>
<td align="right">0.122</td>
<td align="right">-0.259</td>
<td align="right">0.505</td>
</tr>
</tbody>
</table>
<p>Now, we can easily compare the coefficients. The intercepts of all three models are practically zero, which is a consequence of the z-transformation.
Recall, that the intercept in an LRM is the point, where the predictor variable is zero.
In MRM this is just the same: here, the intercept is the predicted AUP resistance score, when NCS and GEX are both zero.</p>
<p>When using the two predictors simultaneously, the overall positive tendency remains.
However, we observe major and minor shifts: in the MRM, the coefficient of the geekism score is reduced to less than half: <span class="math inline">\(0.12 [-0.26, 0.5]_{CI95}\)</span>.
In contrast, the impact of NCS is reduced just by a little, compared to M_1 : <span class="math inline">\(0.29 [-0.08, 0.69]_{CI95}\)</span>.</p>
<p>For any researcher who has carefully conceived a research question this appears to be a disappointing outcome. Indeed, putting multiple predictors into a model sometimes reduces the size of the effect, compared to single-predictor models. More precisely, this happens, when <em>predictors are correlated</em>.
In this study, participants who are high on NCS also tend to have more pronounced geekism (Figure <a href="mpm.html#fig:AUP-corr-predictors">5.2</a>))</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="mpm.html#cb380-1" aria-hidden="true" tabindex="-1"></a>AUP_1 <span class="sc">%&gt;%</span></span>
<span id="cb380-2"><a href="mpm.html#cb380-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> zncs, <span class="at">y =</span> zgex)) <span class="sc">+</span></span>
<span id="cb380-3"><a href="mpm.html#cb380-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb380-4"><a href="mpm.html#cb380-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> F) <span class="sc">+</span></span>
<span id="cb380-5"><a href="mpm.html#cb380-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="fu">str_c</span>(</span>
<span id="cb380-6"><a href="mpm.html#cb380-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Correlation between ncs and gex: &quot;</span>,</span>
<span id="cb380-7"><a href="mpm.html#cb380-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">cor</span>(AUP_1<span class="sc">$</span>zncs, AUP_1<span class="sc">$</span>zgex), <span class="dv">2</span>)</span>
<span id="cb380-8"><a href="mpm.html#cb380-8" aria-hidden="true" tabindex="-1"></a>  ))</span></code></pre></div>
<div class="figure"><span id="fig:AUP-corr-predictors"></span>
<img src="Classic_linear_models_files/figure-html/AUP-corr-predictors-1.png" alt="Correlated predictors Gex and NCS" width="90%" />
<p class="caption">
Figure 5.2: Correlated predictors Gex and NCS
</p>
</div>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>Participants with a higher NCS also tend to score higher on geekism.
Is that surprising?
Actually, it is not.
People high on NCS love to think.
Computers are a good choice for them, because these are complicated devices that make you think.
(Many users may even agree that computers help you think, for example when analyzing your data with R.) In turn, geekism is a positive attitude towards working with computers in sophisticated ways, which means such people are more resistant towards the AUP.</p>
<p><code>[NCS: love to think] --&gt; [GEX: love computers] --&gt; [resist AUP]</code></p>
<p>When such a causal chain can be established without doubt, some researchers speak of a <em>mediating variable</em> GEX.
Although a bit outdated <span class="citation">(Iacobucci, Saldanha, and Deng 2007)</span>, <em>mediator analysis is correct when the causal direction of the three variables is known</em>.
The classic method to deal with mediation is a so-called step-wise regression.
However, structural equation modeling <span class="citation">(Merkle and Rosseel 2018)</span> can be regarded a better option to deal with three or more variables that have complex causal connections (in theory).</p>
<p>We can exclude that the resistance test has influenced the personality scores, because of the order of appearance in the study. Unfortunately, in the situation here, the causal direction remains ambiguous for NCS and Gex. We can make up a story, like above, where NCS precedes GEX, but we can tell another story with a reversed causal connection, for example:
Computers reward you for thinking hard and, hence, you get used to it and make it your lifestyle. If you like thinking hard, then you probably also like the challenge that was given in the experiment.</p>
<p><code>[GEX: love computers] --&gt; [NCS: love to think] --&gt; [resist AUP]</code></p>
<p>In the current case, we can not distinguish between these two competing theories with this data alone.
This is a central problem in empirical research.
An example, routinely re-iterated in social science methods courses is the observation that people who are more intelligent tend to consume more fresh vegetables.
Do carrots make us smart?
Perhaps, but it is equally plausible that eating carrots is what smart people do.
The basic issue is that a particular direction of causality can only be established, when all reverse directions can be excluded by logic.</p>
<!-- Behavioural science researchers know of two ways to do so: -->
<!-- 1.  By the *arrow of time*, it is excluded that a later event caused a preceding one. In the AUP study, we can exclude that experiencing the computer task influences the participants, when   they score themselves on NCS and GEX, because of the temporal order. -->
<!-- 2.  In *strictly controlled experiments*, the researcher controls the cparticipants are assigned to the conditions, randomly. -->
<p>To come back to the AUP study: There is no way to establish a causal order of predictors NCS and Gex.
If nothing is known but covariation, they just enter the model simultaneously, as in model <code>M_3</code>.
This results in a redistribution of the overall covariance and the predictors are <em>mutually controlled</em>.
In <code>M_2</code> the effect of GEX was promising at first, but now seems spurious in the simultaneous model.
Most of the strength was just borrowed from NCS by covariation.
The model suggests that loving-to-think has a considerably stronger association with AUP resistance than loving-computers.</p>
<p>That <em>may</em> suggest, but not prove, that NCS precedes GEX, as in a chain of causal effects, elements that are closer to the final outcome (AUP resistance) tend to exert more salient influence.
But, without further theorizing and experimenting this is weak evidence of causal order.</p>
<p>If I would want to write a paper on geekism, NCS and the AUP, I might be tempted to report the two separate LRMs, that showed at least moderate effects.
The reason why one should not do that is that separate analyses suggest that the predictors are independent.
To illustrate this at an extreme example, think of a study where users were asked to rate their agreement with an interface by the following two questions, before ToT is recorded:</p>
<ol style="list-style-type: decimal">
<li>Is the interface beautiful?</li>
<li>Does the interface have an aesthetic appearance?</li>
</ol>
<p>Initial separate analyses show strong effects for both predictors.
Still, it would not make sense to give the report the title: “Beauty and aesthetics predict usability.”
Beauty and aesthetics are practically synonyms.
For Gex and NCS this may be not so clear, but we cannot exclude the possibility that they are linked to a common factor, perhaps a third trait that makes people more explorative, no matter whether it be thoughts or computers.</p>
<p>So, what to do if two predictors correlate strongly?
First, we always report just a single model.
Per default, this is the model with both predictors simultaneously.
The second possibility is to use a disciplined method of <em>model selection</em> and remove the predictor (or predictors) that does not actually contribute to prediction.
The third possibility is, that the results with both predictors become more interesting when including conditional effects <a href="mpm.html#cfxm">5.4</a>.</p>
</div>
<div id="mfm" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Crossover: multifactorial models</h2>
<p>The very common situation in research is that multiple factors are of interest.
In <a href="lm.html#ofm">4.3.5</a>, we have seen how we can use an OGM to model a short learning sequence.
That was only useing half of the data, because in the IPump study, we compared two designs against each other, and both were tested in three sessions. That makes 2 x 3 conditions.
Here, I introduce a multi-factorial model, that has <em>main effects only</em>.
Such a model actually is of very limited use for the IPump case, where we need <em>conditional effects</em> to get to a valid model (<a href="mpm.html#cfxm">5.4</a>).</p>
<p>We take as an example the BrowsingAB study: the primary research question regarded the design difference, but the careful researcher also recorded gender of participants.
One can always just explore variables that one has.
The following model estimates the gender effect alongside the design effect (Table <a href="mpm.html#tab:mfm-1">5.3</a>)).</p>
<!-- What happened to the likelihood function when we moved from GMM to CGM and LRM? The effect of age was simply added to the intercept. For a model on education level effects, we expanded the dummy variables and then added them all up. Indeed, the linear model is defined as a succession of linear terms  $x_i\beta_i$ and nothing keeps us from adding further predictors to the model. Seeing is believing! The following code estimates a model with design and gender as predictors. -->
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="mpm.html#cb382-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(BrowsingAB)</span></code></pre></div>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="mpm.html#cb383-1" aria-hidden="true" tabindex="-1"></a>M_mfm_1 <span class="ot">&lt;-</span></span>
<span id="cb383-2"><a href="mpm.html#cb383-2" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb383-3"><a href="mpm.html#cb383-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Design <span class="sc">+</span> Gender, <span class="at">data =</span> .)</span></code></pre></div>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="mpm.html#cb384-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_mfm_1)</span></code></pre></div>
<table>
<caption><span id="tab:mfm-1">Table 5.3: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">105.98</td>
<td align="right">93.6</td>
<td align="right">117.94</td>
</tr>
<tr class="even">
<td align="left">DesignB</td>
<td align="left">DesignB</td>
<td align="right">-17.47</td>
<td align="right">-31.2</td>
<td align="right">-2.99</td>
</tr>
<tr class="odd">
<td align="left">GenderM</td>
<td align="left">GenderM</td>
<td align="right">1.35</td>
<td align="right">-12.3</td>
<td align="right">15.00</td>
</tr>
</tbody>
</table>
<p>By adding gender to the model, both effects are estimated simultaneously.
In the following <em>multi-factorial model (MFM)</em> the intercept is a reference group, once again.
Consider that both factors have two levels, forming a <span class="math inline">\(2 x 2\)</span> matrix, like Table <a href="mpm.html#tab:mfm-2">5.4</a>).</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="mpm.html#cb385-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tribble</span>(</span>
<span id="cb385-2"><a href="mpm.html#cb385-2" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>Condition, <span class="sc">~</span>F, <span class="sc">~</span>M,</span>
<span id="cb385-3"><a href="mpm.html#cb385-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;A&quot;</span>, <span class="st">&quot;reference&quot;</span>, <span class="st">&quot;difference&quot;</span>,</span>
<span id="cb385-4"><a href="mpm.html#cb385-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;B&quot;</span>, <span class="st">&quot;difference&quot;</span>, <span class="st">&quot;&quot;</span></span>
<span id="cb385-5"><a href="mpm.html#cb385-5" aria-hidden="true" tabindex="-1"></a>) </span></code></pre></div>
<table>
<caption><span id="tab:mfm-2">Table 5.4: </span>A two-factorial model has one r4eference level and two differences</caption>
<thead>
<tr class="header">
<th align="left">Condition</th>
<th align="left">F</th>
<th align="left">M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="left">reference</td>
<td align="left">difference</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="left">difference</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>The first one, A-F, has been set as reference group.
In Table <a href="mpm.html#tab:mfm-2">5.4</a> the intercept coefficient tells that women in condition A have an average ToT of <span class="math inline">\(105.98 [93.65, 117.94]_{CI95}\)</span> seconds.
The second coefficient says that design B is slightly faster and that there seemingly is no gender effect.</p>
<p>How comes that the model only has three parameters, when there are four groups?
In a CGM, the number of parameters always equals the number of levels, why not here?
We can think of the 2 x 2 conditions as flat four groups, A-F, A-M, B-F and B-M and we would expect four coefficients, say absolute group means.
This model regards the two effects as independent, that means they are not influencing each other: Design is assumed to have the <em>same effect</em> for men and women.</p>
<p>In many multi-factorial situations, one is better advised to use a model with conditional effects.
Broadly, with conditional effect we can assess, how much effects influence each other. We will come back to that, but here I am introducing an extension of the AMM (<a href="lm.html#amm">4.3.4</a>), the <em>multifactorial AMM</em> (MAMM):</p>
<!-- Basically, a model with conditional effects is a re-parametrization of *a multifactorial AMM* (MAMM), with the following dummy coding: -->
<!-- ```{r mfm-3, echo=FALSE} -->
<!-- tribble(~DesignA, ~DesignB, ~`GenderF`, ~GenderM, -->
<!--         1, 0, 1, 0, -->
<!--         1, 0, 0, 1, -->
<!--         0, 1, 1, 0, -->
<!--         0, 1, 0, 1) %>%  -->
<!--   kable(caption = "Dummy coding ") -->
<!-- ``` -->
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="mpm.html#cb386-1" aria-hidden="true" tabindex="-1"></a>M_amfm_1 <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> Design<span class="sc">:</span>Gender, <span class="at">data =</span> BAB1)</span></code></pre></div>
<p>The R formula for an AMM supresses the intercept and uses an interaction term without main effects (as will be explained in <a href="mpm.html#cmfm">5.4.2</a>). Table <a href="mpm.html#tab:mfm-3">5.5</a> carries the four group means and be can further processed as a <em>conditional plot</em>, as in Figure <a href="mpm.html#fig:mfm-4">5.3</a>.</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="mpm.html#cb387-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_amfm_1)</span></code></pre></div>
<table>
<caption><span id="tab:mfm-3">Table 5.5: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">DesignA:GenderF</td>
<td align="left">DesignA:GenderF</td>
<td align="right">104.3</td>
<td align="right">89.5</td>
<td align="right">119</td>
</tr>
<tr class="even">
<td align="left">DesignB:GenderF</td>
<td align="left">DesignB:GenderF</td>
<td align="right">89.9</td>
<td align="right">75.5</td>
<td align="right">104</td>
</tr>
<tr class="odd">
<td align="left">DesignA:GenderM</td>
<td align="left">DesignA:GenderM</td>
<td align="right">108.4</td>
<td align="right">95.1</td>
<td align="right">122</td>
</tr>
<tr class="even">
<td align="left">DesignB:GenderM</td>
<td align="left">DesignB:GenderM</td>
<td align="right">88.8</td>
<td align="right">75.8</td>
<td align="right">102</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="mpm.html#cb388-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_amfm_1) <span class="sc">%&gt;%</span></span>
<span id="cb388-2"><a href="mpm.html#cb388-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">separate</span>(parameter, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">&quot;Design&quot;</span>, <span class="st">&quot;Gender&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb388-3"><a href="mpm.html#cb388-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Design, <span class="at">col =</span> Gender, <span class="at">y =</span> center)) <span class="sc">+</span></span>
<span id="cb388-4"><a href="mpm.html#cb388-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb388-5"><a href="mpm.html#cb388-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> Gender))</span></code></pre></div>
<div class="figure"><span id="fig:mfm-4"></span>
<img src="Classic_linear_models_files/figure-html/mfm-4-1.png" alt="Line graph showing conditional effects in a tow-factorial model" width="90%" />
<p class="caption">
Figure 5.3: Line graph showing conditional effects in a tow-factorial model
</p>
</div>
<p>If the two effects were truly independent, these two lines had to be parallel, because the effect of Gender had to be constant.
What this graph now suggests is that there is an interaction between the two effects.
There is a tiny advantage for female users with design A, whereas men are faster with B with about the same difference.
Because these two effects cancel each other out, the combined effect of Gender in model <code>M_mfm_1</code> was so close to zero.</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<!-- #124 -->
</div>
<div id="grm" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Line-by-line: grouped regression models</h2>
<p>Recall, that dummy variables make factors compatible with linear regression.
We have seen how two metric predictors make a surface and how factors can be visualized by straight lines in a conditional plot.
When a factor is combined with a metric predictor, we get a group of lines, one per factor level.
For example, we can estimate the effects of age and design simultaneously, as shown in Table <a href="mpm.html#tab:grm-1">5.6</a>.</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="mpm.html#cb390-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(BrowsingAB)</span></code></pre></div>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb391-1"><a href="mpm.html#cb391-1" aria-hidden="true" tabindex="-1"></a>M_grm_1 <span class="ot">&lt;-</span></span>
<span id="cb391-2"><a href="mpm.html#cb391-2" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb391-3"><a href="mpm.html#cb391-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Design <span class="sc">+</span> age_shft, <span class="at">data =</span> .)</span></code></pre></div>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="mpm.html#cb392-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_grm_1)</span></code></pre></div>
<table>
<caption><span id="tab:grm-1">Table 5.6: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">82.035</td>
<td align="right">66.435</td>
<td align="right">96.85</td>
</tr>
<tr class="even">
<td align="left">DesignB</td>
<td align="left">DesignB</td>
<td align="right">-17.381</td>
<td align="right">-30.354</td>
<td align="right">-3.67</td>
</tr>
<tr class="odd">
<td align="left">age_shft</td>
<td align="left">age_shft</td>
<td align="right">0.817</td>
<td align="right">0.413</td>
<td align="right">1.22</td>
</tr>
</tbody>
</table>
<p>Once again, we get an intercept first.
Recall, that in LRM the intercept is the the performance of a 20-year old (age was shifted!).
In the CGM the intercept is the mean of the reference group.
When marrying factors with continuous predictors, the <em>intercept is point zero in the reference group</em>.
The predicted average performance of 20-year old with design A is <span class="math inline">\(82.03 [66.44, 96.85]_{CI95}\)</span>.
The age effect has the usual meaning: by year of life, participants get <span class="math inline">\(0.82 [0.41, 1.22]_{CI95}\)</span> seconds slower.
The <em>factorial effect</em> B is a <em>vertical shift of the intercept</em>.
A 20-year old in condition B is <span class="math inline">\(17.38 [30.35, 3.67]_{CI95}\)</span> seconds faster.</p>
<p>It is important to that this is a model of parallel lines, implying that the age effect is the same everywhere.
The following model estimates intercepts and slopes separately for every level, making it an <em>absolute mixed-predictor model (AMPM)</em>.
The following formula produces such a model and results are shown in Table <a href="mpm.html#tab:grm-2">5.7</a>.</p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="mpm.html#cb393-1" aria-hidden="true" tabindex="-1"></a>M_ampm_1 <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> (<span class="dv">0</span> <span class="sc">+</span> Design <span class="sc">+</span> Design<span class="sc">:</span>age_shft),</span>
<span id="cb393-2"><a href="mpm.html#cb393-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> BAB1</span>
<span id="cb393-3"><a href="mpm.html#cb393-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="mpm.html#cb394-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_ampm_1)</span></code></pre></div>
<table>
<caption><span id="tab:grm-2">Table 5.7: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">DesignA</td>
<td align="left">DesignA</td>
<td align="right">99.435</td>
<td align="right">79.884</td>
<td align="right">119.042</td>
</tr>
<tr class="even">
<td align="left">DesignB</td>
<td align="left">DesignB</td>
<td align="right">46.298</td>
<td align="right">26.843</td>
<td align="right">65.444</td>
</tr>
<tr class="odd">
<td align="left">DesignA:age_shft</td>
<td align="left">DesignA:age_shft</td>
<td align="right">0.233</td>
<td align="right">-0.346</td>
<td align="right">0.813</td>
</tr>
<tr class="even">
<td align="left">DesignB:age_shft</td>
<td align="left">DesignB:age_shft</td>
<td align="right">1.420</td>
<td align="right">0.854</td>
<td align="right">1.980</td>
</tr>
</tbody>
</table>
<p>It turns out, the intercepts and slopes are very different for the two designs.
For a 20 year old, design B works much better, but at the same time design B puts a much stronger penalty on every year of age.
With these coefficients we can also produce a conditional plot, with one line per Design condition (Figure <a href="mpm.html#fig:grm-3">5.4</a>).</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="mpm.html#cb395-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_ampm_1) <span class="sc">%&gt;%</span></span>
<span id="cb395-2"><a href="mpm.html#cb395-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(fixef, center) <span class="sc">%&gt;%</span></span>
<span id="cb395-3"><a href="mpm.html#cb395-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb395-4"><a href="mpm.html#cb395-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Design =</span> <span class="fu">str_extract</span>(fixef, <span class="st">&quot;[AB]&quot;</span>),</span>
<span id="cb395-5"><a href="mpm.html#cb395-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">Coef =</span> <span class="fu">if_else</span>(<span class="fu">str_detect</span>(fixef, <span class="st">&quot;age&quot;</span>),</span>
<span id="cb395-6"><a href="mpm.html#cb395-6" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;Slope&quot;</span>,</span>
<span id="cb395-7"><a href="mpm.html#cb395-7" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;Intercept&quot;</span></span>
<span id="cb395-8"><a href="mpm.html#cb395-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb395-9"><a href="mpm.html#cb395-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb395-10"><a href="mpm.html#cb395-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Design, Coef, center) <span class="sc">%&gt;%</span></span>
<span id="cb395-11"><a href="mpm.html#cb395-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spread</span>(<span class="at">key =</span> Coef, <span class="at">value =</span> center) <span class="sc">%&gt;%</span></span>
<span id="cb395-12"><a href="mpm.html#cb395-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb395-13"><a href="mpm.html#cb395-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(</span>
<span id="cb395-14"><a href="mpm.html#cb395-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> Design,</span>
<span id="cb395-15"><a href="mpm.html#cb395-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> Intercept,</span>
<span id="cb395-16"><a href="mpm.html#cb395-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">slope =</span> Slope</span>
<span id="cb395-17"><a href="mpm.html#cb395-17" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb395-18"><a href="mpm.html#cb395-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> BAB1, <span class="fu">aes</span>(</span>
<span id="cb395-19"><a href="mpm.html#cb395-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> age,</span>
<span id="cb395-20"><a href="mpm.html#cb395-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">col =</span> Design,</span>
<span id="cb395-21"><a href="mpm.html#cb395-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> ToT</span>
<span id="cb395-22"><a href="mpm.html#cb395-22" aria-hidden="true" tabindex="-1"></a>  ))</span></code></pre></div>
<div class="figure"><span id="fig:grm-3"></span>
<img src="Classic_linear_models_files/figure-html/grm-3-1.png" alt="Creating a conditional plot from intercept and slope effects" width="90%" />
<p class="caption">
Figure 5.4: Creating a conditional plot from intercept and slope effects
</p>
</div>
<p>Note</p>
<ul>
<li>how the coefficient table is first made flat with <code>spread</code>, where Intercept and Slope become variables.</li>
<li>that the Abline geometry is specialized on plotting linear graphs, but it requires its own aesthetic mapping (the global will not work).</li>
</ul>
<p>So, if we can already fit a model with separate group means (an AMFM) or a bunch of straight lines (AMPM), why do we need a more elaborate account of conditional effects, as in chapter <a href="mpm.html#cmfm">5.4.2</a>?
The answer is that conditional effects often carry important information, but are notoriously difficult to interpret.
As it will turn out, conditional effects sometimes are due to rather trivial effects, such as saturation.
But, like in this case, they can give the final clue.
It is hard to deny that design features can work differently to different people.
The hypothetical situation is BrowsingAB is that design B uses a smaller font-size, which makes it harder to read with elderly users, whereas younger users have a benefit from more compactly written text.</p>
<p>And, sometimes, experimental hypotheses are even formulated as conditional effects, like the following: some control tasks involve long episodes of vigilance, where mind wandering can interrupt attention on the task.
If this is so, we could expect people who meditate to perform better at a long duration task, but showing no difference at short tasks.
In a very simple experiment participants reaction time could be measured in a long and short task condition.</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<!-- ## Empirical versus statistical control {#emp-stat-control} -->
<!-- Fundamental researchers have a knack for the experimental method. -->
<!-- An *experiment*, strictly, is a study where you measure the effects of variables you *manipulate*. -->
<!-- Manipulation is, almost literally, that it is *in your hands*, who receives the treatment. -->
<!-- The fantastic thing about manipulation is that it allows for *causal conclusions*. -->
<!-- A *strictly controlled experiment* is when all influencing variables are either manipulated or kept constant. -->
<!-- That is an ideal and would not even be the case if you test the same person over-and-over again (like researchers in psychophysics often do). -->
<!-- You never jump into the same river twice. -->
<!-- Sometimes an influencing variable lends itself to be kept constant. -->
<!-- For example, in cognitive psychological experiments environment and equipment is usually kept constant. -->
<!-- For applied research, keeping things constant comes at a major disadvantage: it limits the possible conclusions drawn from the study. -->
<!-- Imagine, you tested a smartphone app with participants, all students, comfortably sitting in a quiet environment. -->
<!-- Would you dare to make conclusions on how any users perform in real life situations, say while driving a car? -->
<!-- When keeping conditions constant in applied design research, *generalizability* suffers and the results may not be valid predictions for the real world. -->
<!-- In most applied design studies we need ecological validity and generalizability. -->
<!-- If performance differs under certain conditions, you certainly want to know that. -->
<!-- The solution is to *let conditions vary and record them* as variables, as good as possible. -->
<!-- For example, if you were to compare two voice-controlled intelligent agent apps, you could manipulate the ambient noise level, if you are in the lab. -->
<!-- In practically all applied studies, variables may exist which you cannot manipulate. -->
<!-- Especially, user traits are impossible to manipulate; If someone has an extrovert character or did a lot of gaming in the past, you cannot change that. -->
<!-- Diversity of users is a fact and people come as they are. -->
<!-- Field studies usually aim for high ecological validity. -->
<!-- Participants are supposed to use the system in the situations they encounter. -->
<!-- If a smartphone app is being used sitting, walking, driving or at a secret place, it is crucial to observe all situations. -->
<!-- Consider a car navigation system that is tested in a long, lonely highway situation only. -->
<!-- How much would the results tell your for performance in dense city traffic? -->
<!-- Design researchers frequently need results that are highly representative for various users and situations of use. -->
<!-- Fundamental lab researchers are afraid of individual differences, too. -->
<!-- The reasons are different, though: all non-manipulated influencing factors add noise to the study, which makes it harder to find the effects of interest. -->
<!-- While lab researchers do there best to keep the environment constant, they cannot keep all participant traits constant. -->
<!-- Lab researchers have two solutions to the problem: matching and randomized control. -->
<!-- With *pair matching*, potentially relevant participant traits are recorded upfront; then participants are assigned to conditions such that groups have about the same composition. -->
<!-- For example, one makes sure that the age distribution is about the same and both genders are equally represented. -->
<!-- When all other influencing variables are constant between groups, the lab researcher can be sure that the effect is unambiguously caused by the manipulation. -->
<!-- So they say and routinely record participants age, gender and nationality. -->
<!-- However, there are better alternatives: the best pair match is the person herself. -->
<!-- Experimental studies that expose the same person to several conditions are called *within-subject*. -->
<!-- In the special case that all participants encounter all conditions, the variable is *complete within-subject*. -->
<!-- In the following chapter, we use mixed-effects models \@ref(mlm) to deal with within-subject designs, gracefully. -->
<!-- In design research, pair matching applies for situations where designs are compared. -->
<!-- In the simple situation that a design is evaluated against a set standard (e.g. 111 seconds to rent a car), it is more important to do *population matching*. -->
<!-- The sample of participants is drawn to be *representative for the target population*. -->
<!-- Representativeness comes in two levels: *coverage representation* is reached when all influencing properties have occurred a few times during observation. -->
<!-- So, if your target population contains several subgroups, such as age groups, experience or people with different goals, they should all be covered to some extent. -->
<!-- *Proportional representation* means all user and situational properties are covered *and* they have about the same proportion in the sample as in the population. -->
<!-- You can only match what you can measure and you only measure what you expect. -->
<!-- Human behaviour in everyday life is influenced by many factors in complex ways. -->
<!-- Although a plethora of personality inventories exists, doing them all prior to the real study is impossible. -->
<!-- It would probably not even be effective. -->
<!-- Never have I seen a design research study, where even the most established personality tests explain more than a few percent of variation. -->
<!-- As another example, take the primacy effect: what you experienced first, has the strongest influence. -->
<!-- In real life, impressions are constantly pouring on people and you will never be able to record and match that to a reasonable extent. -->
<!-- When influencing variables cannot be measured for matching or statistical control, the last resort is *randomized control*. -->
<!-- This is a misleading term, insofar as what the researcher actually does is to *let go to chance*. -->
<!-- Indeed, if the process of drawing participants and assigning them to manipulations is completely left to chance, then *in the long-term*, the sample will be proportional representative and all groups will have the same composition of traits. -->
<!-- *randomization* works well with larger samples. -->
<!-- With small samples, it can still easily happen that one ends up with more or less biased samples or heterogeneous groups. -->
<!-- Just by chance, more higher-educated people could have ended up in condition A of BrowsingAB. -->
<!-- Using manipulation, matching or randomization in in-the-wild research may work in some cases. -->
<!-- In other cases, it will be ineffective or impractical. -->
<!-- The ultimate problem is the attempt to keep things constant. -->
<!-- In applied design research the questions rarely come down to a "Is A better than B?". -->
<!-- If there is an age effect, you may certainly want to know it and see how the design effect compares to it. -->
<!-- But, you can only examine what is varied and recorded. -->
<!-- The approach of *statistical control* is to record (instead of manipulate) all variables that may influence the results and add them to the statistical model. -->
<!-- As we have seen in this section now, the linear model puts no limits on the number of predictors. -->
<!-- That allows us to use control variables and evaluate *multiple research questions in a single model*. -->
<!-- In the next section we will take multi-predictor models to a new level. -->
<!-- As we have seen, multiple effects can be conditional upon each other and I gave you a straight-forward way to check this with AMFMs and AMPMs. -->
<!-- What you could not do with these models is interpret how strong the conditional effect is. -->
<!-- In the following section, I will elaborate on what conditional effects can mean and how these can be quantified as differences, using treatment contrasts. -->
</div>
<div id="cfxm" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Conditional effects models</h2>
<p>With the framework of MPM, we can use an arbitrary number of predictors.
These can represent properties on different levels, for example, two design proposals for a website can differ in font size, or participants differ in age.
So, with MPM we gain much greater flexibility in handling data from applied design research, which allows us to examine user-design interactions more closely.</p>
<p>The catch is that if you would ask an arbitrary design researcher:</p>
<blockquote>
<p>Do you think that all users are equal?
Or, could it be that one design is better for some users, but inferior for others?</p>
</blockquote>
<p>you would in most cases get the answer:</p>
<blockquote>
<p>Of course users differ in many ways and it is crucial to know your target group.</p>
</blockquote>
<p>Some will also refer to the concept of usability by the ISO 9241-11, which contains the famous phrase:</p>
<blockquote>
<p>“… for a specified user …”</p>
</blockquote>
<p>The definition explicitly requires you to state for <em>for whom</em> you intended to design.
It thereby implicitly acknowledges that usability of a design could be very different for another user group.
Statements on usability are by the ISO 9241-11 definition <em>conditional</em> on the target user group.</p>
<p>In statistical terms, conditional statements have this form:</p>
<blockquote>
<p>the effect of design <em>depends on</em> the user group.</p>
</blockquote>
<p>In regression models, conditional statements like these are represented by <em>conditional effects</em>.
Interactions between user properties and designs are central in design research, and deserve a neologism: <em>differential design effects models (DDM)</em><!-- #64-->.</p>
<p>However, conditional effects are often needed for a less interesting reason: <em>saturation</em> occurs when physical (or other) boundaries are reached and the steps are getting smaller, for example, the more you train, the less net effect it usually has.
Saturations counter part is <em>amplification</em>, a rare one, which acts like compound glue: it will harden only if the two components are present.</p>
<div id="cmrm" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Conditional multiple regression</h3>
<p>In section <a href="lm.html#lrm">4.2</a> we have seen how the relationship between predictor and outcome variable can be modelled as a linear term.
We analysed the relationship between age and ToT in the (fictional) BrowsingAB case and over both designs combined and observed just a faint decline in performance, which also seemed to take a wavey form.</p>
<p>It is commonly held that older people tend to have lower performance than younger users.
A number of factors are called responsible, such as: slower processing speed, lower working memory capacity, lower motor speed and visual problems.
All these capabilities interact with properties of designs, such as legibility, visual simplicity and how well the interaction design is mapped to a user’s task.
It is not a stretch to assume that designs can differ in how much performance degrades with age.</p>
<p>In turn, a design can also contain compromises that limit the performance of younger users.
For example, the main difference between design A and B in the BrowsingAB example is that A uses larger letters than B.
Would that create the same benefit for everybody?
It is not unlikely, that larger letters really only matter for users that have issues with vision.
Unfortunately, large letters have an adverse side-effect for younger users, as larger font size takes up more space on screen and more scrolling is required.
In Figure <a href="mpm.html#fig:cmrm-1">5.5</a>, we take a first look at the situation.</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="mpm.html#cb397-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(BrowsingAB)</span>
<span id="cb397-2"><a href="mpm.html#cb397-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb397-3"><a href="mpm.html#cb397-3" aria-hidden="true" tabindex="-1"></a>BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb397-4"><a href="mpm.html#cb397-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(</span>
<span id="cb397-5"><a href="mpm.html#cb397-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> age,</span>
<span id="cb397-6"><a href="mpm.html#cb397-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">col =</span> Design,</span>
<span id="cb397-7"><a href="mpm.html#cb397-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> ToT</span>
<span id="cb397-8"><a href="mpm.html#cb397-8" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb397-9"><a href="mpm.html#cb397-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb397-10"><a href="mpm.html#cb397-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">se =</span> F) <span class="sc">+</span></span>
<span id="cb397-11"><a href="mpm.html#cb397-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="fu">aes</span>(<span class="at">col =</span> <span class="st">&quot;combined&quot;</span>), <span class="at">se =</span> F)</span></code></pre></div>
<div class="figure"><span id="fig:cmrm-1"></span>
<img src="Classic_linear_models_files/figure-html/cmrm-1-1.png" alt="The effect of  age, combined and conditional on Design" width="90%" />
<p class="caption">
Figure 5.5: The effect of age, combined and conditional on Design
</p>
</div>
<p>The graph suggests that designs A and B differ in the effect of age.
Design B appears to perform much better with younger users.
At the same time, it seems as if A could be more favorable for users at a high age.
By adding the conditional effect <code>Design:age_shft</code> the following model estimates the linear relationship for the designs separately.
This is essentially the same model as the absolute mixed-predictor model M_ampm_1 <a href="mpm.html#mpm">5</a>, which also had four coefficients, the intercepts and slopes of two straight lines.
We have already seen how the GRM and the AMPM produce different fitted responses.
Predictions are independent of contrast coding, but coefficients are not.
The following conditional model uses treatment contrasts, like the GRM, and we can compare the coefficients side-by-side (Table <a href="mpm.html#tab:cmrm-3">5.8</a>).</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="mpm.html#cb398-1" aria-hidden="true" tabindex="-1"></a>M_cmrm <span class="ot">&lt;-</span></span>
<span id="cb398-2"><a href="mpm.html#cb398-2" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb398-3"><a href="mpm.html#cb398-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> Design <span class="sc">+</span> age_shft <span class="sc">+</span> Design<span class="sc">:</span>age_shft,</span>
<span id="cb398-4"><a href="mpm.html#cb398-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb398-5"><a href="mpm.html#cb398-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="mpm.html#cb399-1" aria-hidden="true" tabindex="-1"></a>P_comb <span class="ot">&lt;-</span></span>
<span id="cb399-2"><a href="mpm.html#cb399-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(</span>
<span id="cb399-3"><a href="mpm.html#cb399-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">posterior</span>(M_grm_1),</span>
<span id="cb399-4"><a href="mpm.html#cb399-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">posterior</span>(M_cmrm)</span>
<span id="cb399-5"><a href="mpm.html#cb399-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb399-6"><a href="mpm.html#cb399-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb399-7"><a href="mpm.html#cb399-7" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(P_comb)</span></code></pre></div>
<table>
<caption><span id="tab:cmrm-3">Table 5.8: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">M_cmrm</td>
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">99.992</td>
<td align="right">80.392</td>
<td align="right">119.858</td>
</tr>
<tr class="even">
<td align="left">M_cmrm</td>
<td align="left">DesignB</td>
<td align="left">DesignB</td>
<td align="right">-53.112</td>
<td align="right">-80.641</td>
<td align="right">-25.913</td>
</tr>
<tr class="odd">
<td align="left">M_cmrm</td>
<td align="left">age_shft</td>
<td align="left">age_shft</td>
<td align="right">0.219</td>
<td align="right">-0.346</td>
<td align="right">0.805</td>
</tr>
<tr class="even">
<td align="left">M_cmrm</td>
<td align="left">DesignB:age_shft</td>
<td align="left">DesignB:age_shft</td>
<td align="right">1.191</td>
<td align="right">0.405</td>
<td align="right">2.007</td>
</tr>
<tr class="odd">
<td align="left">M_grm_1</td>
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">82.035</td>
<td align="right">66.435</td>
<td align="right">96.848</td>
</tr>
<tr class="even">
<td align="left">M_grm_1</td>
<td align="left">DesignB</td>
<td align="left">DesignB</td>
<td align="right">-17.381</td>
<td align="right">-30.354</td>
<td align="right">-3.670</td>
</tr>
<tr class="odd">
<td align="left">M_grm_1</td>
<td align="left">age_shft</td>
<td align="left">age_shft</td>
<td align="right">0.817</td>
<td align="right">0.413</td>
<td align="right">1.222</td>
</tr>
</tbody>
</table>
<p>The conditional model shares the first three coefficients with the unconditional model, but only the first two, Intercept and DesignB have the same meaning.</p>
<p>The intercept is the performance of an average twenty-year-old using design A, but the two models diverge in where to place this and the conditional model is less in favor of design A (<span class="math inline">\(99.99 [80.39, 119.86]_{CI95}\)</span> seconds).
Conversely, the effect of design B at age of 20 improved dramatically: accordingly, a twenty-year-old is <span class="math inline">\(53.11 [80.64, 25.91]_{CI95}\)</span> faster with B.</p>
<p>The third coefficient Age_shift appears in both models, but really means something different.
The GRM assumes that both designs have the same slope of <span class="math inline">\(0.82\)</span> seconds per year.
The conditional model produces one slope per design and here the coefficient refers to design A only, as this is the reference group.
Due to the treatment effects, <code>DesignB:age_shft</code> is the <em>difference in slopes</em>: users loose <span class="math inline">\(0.82\)</span> seconds per year with A, and on top of that <span class="math inline">\(0.22 [-0.35, 0.81]_{CI95}\)</span> with design B.</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="cmfm" class="section level3" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Conditional multifactorial models</h3>
<p>In a conditional multifactorial model (CMFM), the effect of one factor level, depends on depends on the level on another factor.
<!-- When the second factor changes in level, this influences the coefficients. -->
<!-- Because of that a CMFM is more flexible. -->
A full CMFM has as many coefficients as there are multi-level groups and is flexible enough that all group means can be completely independent, just like an AMM does it.
Let us see this on an almost trivial example, first.
In the fictional BrowsingAB case, a variable <code>rating</code> has been gathered.
Let us imagine this be a vague emotional rating in the spirit of user satisfaction.
Some claim that emotional experience is what makes the sexes different, so one could ask whether this makes a difference for the comparison two designs A and B (Figure <a href="mpm.html#fig:cmfm-1">5.6</a>).</p>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="mpm.html#cb401-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(BrowsingAB)</span></code></pre></div>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="mpm.html#cb402-1" aria-hidden="true" tabindex="-1"></a>BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb402-2"><a href="mpm.html#cb402-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> rating, <span class="at">x =</span> Gender, <span class="at">color =</span> Design)) <span class="sc">+</span></span>
<span id="cb402-3"><a href="mpm.html#cb402-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>()</span></code></pre></div>
<div class="figure"><span id="fig:cmfm-1"></span>
<img src="Classic_linear_models_files/figure-html/cmfm-1-1.png" alt="Comparing satisfaction ratings by design and gender" width="90%" />
<p class="caption">
Figure 5.6: Comparing satisfaction ratings by design and gender
</p>
</div>
<p>In a first exploratory plot it looks like the ratings are pretty consistent across gender, but with a sensitive topic like that, we better run a model, or rather two, a plain MFM and a conditional MFM:</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="mpm.html#cb403-1" aria-hidden="true" tabindex="-1"></a>M_mfm_2 <span class="ot">&lt;-</span></span>
<span id="cb403-2"><a href="mpm.html#cb403-2" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb403-3"><a href="mpm.html#cb403-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(rating <span class="sc">~</span> Design <span class="sc">+</span> Gender,</span>
<span id="cb403-4"><a href="mpm.html#cb403-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb403-5"><a href="mpm.html#cb403-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb403-6"><a href="mpm.html#cb403-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb403-7"><a href="mpm.html#cb403-7" aria-hidden="true" tabindex="-1"></a>M_cmfm_1 <span class="ot">&lt;-</span></span>
<span id="cb403-8"><a href="mpm.html#cb403-8" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb403-9"><a href="mpm.html#cb403-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(rating <span class="sc">~</span> Design <span class="sc">+</span> Gender <span class="sc">+</span> Design<span class="sc">:</span>Gender,</span>
<span id="cb403-10"><a href="mpm.html#cb403-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb403-11"><a href="mpm.html#cb403-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb403-12"><a href="mpm.html#cb403-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb403-13"><a href="mpm.html#cb403-13" aria-hidden="true" tabindex="-1"></a><span class="co"># T_resid &lt;- mutate(T_resid, M_ia2 = residuals(M_ia2))</span></span></code></pre></div>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="mpm.html#cb404-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb404-2"><a href="mpm.html#cb404-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_mfm_2),</span>
<span id="cb404-3"><a href="mpm.html#cb404-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_cmfm_1)</span>
<span id="cb404-4"><a href="mpm.html#cb404-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb404-5"><a href="mpm.html#cb404-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>() <span class="sc">%&gt;%</span></span>
<span id="cb404-6"><a href="mpm.html#cb404-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(</span>
<span id="cb404-7"><a href="mpm.html#cb404-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> parameter, <span class="at">col =</span> model,</span>
<span id="cb404-8"><a href="mpm.html#cb404-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">xmin =</span> lower, <span class="at">xmax =</span> upper, <span class="at">x =</span> center</span>
<span id="cb404-9"><a href="mpm.html#cb404-9" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb404-10"><a href="mpm.html#cb404-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_crossbar</span>(<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>) <span class="sc">+</span></span>
<span id="cb404-11"><a href="mpm.html#cb404-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;effect&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:cmfm-2"></span>
<img src="Classic_linear_models_files/figure-html/cmfm-2-1.png" alt="Comparison of non-conditional and conditionbal two-factorial models, center estimates and 95 percent credibility intervals" width="90%" />
<p class="caption">
Figure 5.7: Comparison of non-conditional and conditionbal two-factorial models, center estimates and 95 percent credibility intervals
</p>
</div>
<p>The CLU plots above show both models in comparison.
Both models use treatment effects and put the intercept on female users with design A.
We observe that there is barely a difference in the estimated intercepts.
The coefficient DesignB means something different in the models: in the MFM it represents <em>the</em> difference between Designs.
In the CMFM, it is the difference design B makes <em>for female users</em>.
The same is true for GenderM: In the MFM it is <em>the</em> gender effect, whereas in the CMFM it is the the gender difference <em>for design A</em>.</p>
<p>A note on terminology: many researchers divide the effect of conditional models into main effects (DesignB, GenderM) and interaction effects. That it is <em>incorrect</em>, because DesignB and GenderM are not main, in the sense of global. That is precisely what a conditional model does: it makes two (or more) local effects out of one global effect.</p>
<!-- Onbly plain MFM have main effects, They are main effects in a plain MFM, but once the effects become conditional, there is nothing such as a main effect any more. -->
<!-- At least, this is the case for treatment effect coding and stairways coding (as we will see next). -->
<p>None of the three coefficients of the MFM noticably changed by introducing the conditional effect <code>DesignB:GenderM</code>.
Recall that in the MFM, the group mean of design B among men is calculated by adding the two main effects to the intercept.
This group mean is fixed.
The conditional effect <code>DesignB:GenderM</code> is the difference to the fixed group in the MFM.
It can be imagined as an adjustment parameter, that gives the fourth group its own degree of freedom.
In the current CMFM the conditional coefficient is very close to zero, with a difference of just <span class="math inline">\(-0.1 [-0.65, 0.44]_{CI95}\)</span>.</p>
<p>It seems we are getting into a lot of null results here.
If you have a background in classic statistics, you may get nervous at such a point, because you remember that in case of null results someone said: “one cannot say anything.”
This is true when you are testing null hypotheses and divide the world into the classes significant/non-significant.
But, when you interpret coefficients, you are speaking quantities and zero is a quantity.
What the MFM tells us is that male users really don’t give any higher or lower ratings, <em>in total</em>, although there remains some uncertainty.</p>
<p>Actually, the purpose of estimating a CMFM can just be to show that some effect is unconditional.
As we have seen earlier <a href="mpm.html#mfm">5.2</a>, conditional effects can cancel each other out, when combined into global effects.
Take a look at the following hypothetical results of the study.
Here, male and female users do not agree.
If we would run an MFM in such a situation, we would get very similar coefficients, but would overlook that the relationship between design and rating is just poorly rendered (Figure <a href="mpm.html#fig:cmfm-3">5.8</a>).</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="mpm.html#cb405-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tribble</span>(</span>
<span id="cb405-2"><a href="mpm.html#cb405-2" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>Design, <span class="sc">~</span>Gender, <span class="sc">~</span>mean_rating,</span>
<span id="cb405-3"><a href="mpm.html#cb405-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;A&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="fl">5.6</span>,</span>
<span id="cb405-4"><a href="mpm.html#cb405-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;A&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="fl">5.6</span> <span class="sc">+</span> .<span class="dv">4</span>,</span>
<span id="cb405-5"><a href="mpm.html#cb405-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;A&quot;</span>, <span class="st">&quot;Total&quot;</span>, <span class="fu">mean</span>(<span class="fu">c</span>(<span class="fl">5.6</span>, <span class="fl">6.0</span>)),</span>
<span id="cb405-6"><a href="mpm.html#cb405-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;B&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="fl">5.6</span> <span class="sc">-</span> .<span class="dv">3</span>,</span>
<span id="cb405-7"><a href="mpm.html#cb405-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;B&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="fl">5.6</span> <span class="sc">+</span> .<span class="dv">4</span> <span class="sc">-</span> .<span class="dv">3</span> <span class="sc">-</span> .<span class="dv">6</span>,</span>
<span id="cb405-8"><a href="mpm.html#cb405-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;B&quot;</span>, <span class="st">&quot;Total&quot;</span>, <span class="fu">mean</span>(<span class="fu">c</span>(<span class="fl">5.3</span>, <span class="fl">5.1</span>))</span>
<span id="cb405-9"><a href="mpm.html#cb405-9" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb405-10"><a href="mpm.html#cb405-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Design, <span class="at">col =</span> Gender, <span class="at">y =</span> mean_rating)) <span class="sc">+</span></span>
<span id="cb405-11"><a href="mpm.html#cb405-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb405-12"><a href="mpm.html#cb405-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> Gender))</span></code></pre></div>
<div class="figure"><span id="fig:cmfm-3"></span>
<img src="Classic_linear_models_files/figure-html/cmfm-3-1.png" alt="Total versus conditional effects in a factorial model" width="90%" />
<p class="caption">
Figure 5.8: Total versus conditional effects in a factorial model
</p>
</div>
<p>If something like this happens in a real design study, it may be a good idea to find out, why this difference appears and whether there is a way to make everyone equally happy.
These are questions a model cannot answer.
But a CMFM can show, when effects are conditional and when they are not.
Much of the time, gender effects is what you rather don’t want to have, as it can become a political problem.
If conditional adjustment effects are close to zero, that is proof (under uncertainty) that an effect is unconditional. That actually justifies the use of an MFM with global effects, only.</p>
<p>Let’s see a more complex example of conditional MFMs, where conditional effects are really needed.
In the IPump study, two infusion pump designs were compared in three successive sessions.
In <a href="lm.html#ofm">4.3.5</a> we saw how a factorial model can render a learning curve using stairway dummies.
With two designs, we can estimate separate learning curves and make comparisons.
Let’s take a look at the raw data in Figure <a href="mpm.html#fig:cmfm-4">5.9</a>.</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="mpm.html#cb406-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(IPump)</span></code></pre></div>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="mpm.html#cb407-1" aria-hidden="true" tabindex="-1"></a>D_agg <span class="sc">%&gt;%</span></span>
<span id="cb407-2"><a href="mpm.html#cb407-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Design, Session) <span class="sc">%&gt;%</span></span>
<span id="cb407-3"><a href="mpm.html#cb407-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_ToT =</span> <span class="fu">mean</span>(ToT)) <span class="sc">%&gt;%</span></span>
<span id="cb407-4"><a href="mpm.html#cb407-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Session, <span class="at">y =</span> mean_ToT, <span class="at">color =</span> Design)) <span class="sc">+</span></span>
<span id="cb407-5"><a href="mpm.html#cb407-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb407-6"><a href="mpm.html#cb407-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> Design)) <span class="sc">+</span></span>
<span id="cb407-7"><a href="mpm.html#cb407-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">350</span>)</span></code></pre></div>
<div class="figure"><span id="fig:cmfm-4"></span>
<img src="Classic_linear_models_files/figure-html/cmfm-4-1.png" alt="ToT by design and session" width="90%" />
<p class="caption">
Figure 5.9: ToT by design and session
</p>
</div>
<p>We note that the learning curves do not cross, but are not parallel either, which means the stairway coefficients will be different.
We need a conditional model.</p>
<p>The first choice to make is between treatment dummies and stairway dummies (<a href="lm.html#dummy">4.3.2</a>, <a href="lm.html#ofm">4.3.5</a>) both have their applications.
With treatment effects, we would get an estimate for the total learning between session 1 and 3.
That does not make much sense here, but could be interesting to compare trainings by the total effect of a training sequence.</p>
<p>We’ll keep the stairway effects on the sessions, but have to now make a choice on where to fix the intercept, and that depends on what aspect of learning is more important.
If this were any walk-up-and-use device or a website for making your annual tax report, higher initial performance would indicate that the system is intuitive to use.
Medical infusion pumps are used routinely by trained staff.
Nurses are using infusion pumps every day, which makes long-term performance more important. The final session is the best estimate we have for that.
We create stairway dummies for session and make this conditional on Design. The results are shown in Table <a href="mpm.html#tab:cmfm-5">5.9</a>.</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="mpm.html#cb408-1" aria-hidden="true" tabindex="-1"></a>T_dummy <span class="ot">&lt;-</span></span>
<span id="cb408-2"><a href="mpm.html#cb408-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tribble</span>(</span>
<span id="cb408-3"><a href="mpm.html#cb408-3" aria-hidden="true" tabindex="-1"></a>    <span class="sc">~</span>Session, <span class="sc">~</span>Session_3, <span class="sc">~</span>Step3_2, <span class="sc">~</span>Step2_1,</span>
<span id="cb408-4"><a href="mpm.html#cb408-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;1&quot;</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>,</span>
<span id="cb408-5"><a href="mpm.html#cb408-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;2&quot;</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>,</span>
<span id="cb408-6"><a href="mpm.html#cb408-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;3&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb408-7"><a href="mpm.html#cb408-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb408-8"><a href="mpm.html#cb408-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb408-9"><a href="mpm.html#cb408-9" aria-hidden="true" tabindex="-1"></a>D_agg <span class="ot">&lt;-</span></span>
<span id="cb408-10"><a href="mpm.html#cb408-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(D_agg,</span>
<span id="cb408-11"><a href="mpm.html#cb408-11" aria-hidden="true" tabindex="-1"></a>    T_dummy,</span>
<span id="cb408-12"><a href="mpm.html#cb408-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">by =</span> <span class="st">&quot;Session&quot;</span>,</span>
<span id="cb408-13"><a href="mpm.html#cb408-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">copy =</span> T</span>
<span id="cb408-14"><a href="mpm.html#cb408-14" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb408-15"><a href="mpm.html#cb408-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Obs, Design, Session, Session_3, Step3_2, Step2_1, ToT)</span></code></pre></div>
<!-- #################### HERE ################ -->
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="mpm.html#cb409-1" aria-hidden="true" tabindex="-1"></a>M_cmfm_2 <span class="ot">&lt;-</span></span>
<span id="cb409-2"><a href="mpm.html#cb409-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Design <span class="sc">+</span> Step3_2 <span class="sc">+</span> Step2_1 <span class="sc">+</span></span>
<span id="cb409-3"><a href="mpm.html#cb409-3" aria-hidden="true" tabindex="-1"></a>    Design<span class="sc">:</span>(Step3_2 <span class="sc">+</span> Step2_1), <span class="at">data =</span> D_agg)</span></code></pre></div>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="mpm.html#cb410-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_cmfm_2)</span></code></pre></div>
<table>
<caption><span id="tab:cmfm-5">Table 5.9: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">151.1</td>
<td align="right">119.406</td>
<td align="right">180.4</td>
</tr>
<tr class="even">
<td align="left">DesignNovel</td>
<td align="left">DesignNovel</td>
<td align="right">-63.6</td>
<td align="right">-105.107</td>
<td align="right">-19.4</td>
</tr>
<tr class="odd">
<td align="left">Step3_2</td>
<td align="left">Step3_2</td>
<td align="right">41.8</td>
<td align="right">0.363</td>
<td align="right">84.9</td>
</tr>
<tr class="even">
<td align="left">Step2_1</td>
<td align="left">Step2_1</td>
<td align="right">131.4</td>
<td align="right">87.140</td>
<td align="right">172.4</td>
</tr>
<tr class="odd">
<td align="left">DesignNovel:Step3_2</td>
<td align="left">DesignNovel:Step3_2</td>
<td align="right">-22.4</td>
<td align="right">-84.576</td>
<td align="right">35.1</td>
</tr>
<tr class="even">
<td align="left">DesignNovel:Step2_1</td>
<td align="left">DesignNovel:Step2_1</td>
<td align="right">-48.2</td>
<td align="right">-106.155</td>
<td align="right">12.6</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>Note that …</p>
<ul>
<li>here I demonstrate a different technique to attach dummies to the data. First a coding table <code>T_dummy</code> is created, which is then combined with the data, using a (tidy) <em>join</em> operation.</li>
<li>we have expanded the factor Session into three dummy variables and we have to make every single one conditional. <code>Design:(Step3_2 + Step2_1)</code> is short for <code>Design:Step3_2 + Design:Step2_1</code>. But, you should <em>never</em> use the fully factorial expansion (<code>Factor1 * Factor2</code>), a this would make dummy variables conditional.</li>
</ul>
<p>In conditional learning curve model, the intercept coefficient tells us that the average ToT with the Legacy in the final session is <span class="math inline">\(151.11 [119.41, 180.38]_{CI95}\)</span> seconds.
Using the Novel design the nurses were <span class="math inline">\(63.61 [105.11, 19.37]_{CI95}\)</span> seconds faster and that is our best estimate for the long-term improvement in efficiency.</p>
<p>Again, the learning step coefficients are not “main” effects, but is local to Legacy.
The first step <code>Step2_1</code> is much larger than the second, as is typical for learning curves.
The adjustment coefficients for Novel have the opposite direction, meaning that the learning steps in Novel are smaller.
That is not as bad as it sounds, for two reasons: first, in this study, the final performance counts, not the training progress.
Second, and more generally, we have misused a linear model to smooth a non-linear model.
Learning processes are exponential (<a href="glm.html#learning-curves">7.2.1.2</a>).</p>
<!-- $\beta_0 - \beta_1x_ {1i}$ is a linear term. -->
<!-- But, when we put it into an exponent, like $\exp(\beta_0 - \beta_1 x_ {1i})$ this is the same as the *quotient* $\exp(\beta_0)/ \beta_1 x_ {1i}$. -->
<!-- Linear models and "linear-in-exponent" models differ in one more property: linear models can be negative or positive, depending on which number is larger. -->
<!-- But the linear-in-exponent model will always stay in the positive range. -->
<p>Learning curves are saturation processes, which can look linear when viewed in segments, but unlike linear models, they never cross the lower boundary.
This is simply, because there is a maximum performance limit, which can only be reached asymptotically.
In the following section, I will argue that basically all measures we take have natural boundaries.
Under common circumstances, this can lead to conditional effects which are due to saturation.
<!-- In chapter \@ref(glm), we will pick up again the idea of putting the linear term into the exponent. -->
<!-- This is what some Generalized Linear Models do to avoid crossing natural boundaries of measures. --></p>
<!-- #### ASYNC: Make this IPump, catching up with OFM.  -->
<!--  The effect of middle education has doubled, whereas it remains stable for high education.  -->
<!-- That may appear strange at first, but keep in mind, that by the interaction term, the education main effects are no longer "main": they only refer to group A, now. In the main effects model, the same education effects are assumed for both designs. Here, it is conditional on design, which brings us to the two conditional effects `B:Middle` and `B:High`. These are, once again, differences. The effect of middle education in B is more seconds than in A <!-- #66 ->. There practically is no net effect of middle education in B. In contrast, the high education conditional effect is small, with practically makes `DesignB` a *main effect: it is the same in both groups*. -->
<!-- Count the number of parameters in both models! We have six with interaction and four without. Six is just the number of groups and, indeed, with conditional effects all group means can vary freely. That is best demonstrated by estimating a variant of the model, where six parameters represent six group means, directly. In other words, it is an AGM, without an intercept and without main effects: -->
<!-- ```{r fit-BAB_cmfm_2, opts.label = "deprecated"} -->
<!-- M_ia3 <- -->
<!--   BAB1 %>%  -->
<!--   stan_glm(ToT ~ 0 + Design : Education, -->
<!--                 data = ., iter = 100) -->
<!-- # T_resid <- mutate(T_resid, M_ia2 = residuals(M_ia2)) -->
<!-- ``` -->
<!-- ```{r opts.label = "mcsync"} -->
<!-- sync_CE(BrowsingAB, M_ia3) -->
<!-- ``` -->
<!-- We extract the coefficients and, with a little preparation, we create an *conditional plot*. It shows the same pattern as the exploratory plot, but is fully inferential, with error bars indicate the 95% credibility interval. -->
<!-- ```{r BAB_ia3} -->
<!-- P_ia3 <- posterior(M_ia3) -->
<!-- T_ia3 <- coef(M_ia3) -->
<!-- T_ia3 -->
<!-- ``` -->
<!-- ```{r BAB_ia3, opts.label = "deprecated"} -->
<!-- T_ia3 %>%  -->
<!--   select(fixef, ToT = center, lower, upper) %>%  -->
<!--   separate(fixef, c("Design", "Education"), ":") %>%  -->
<!--   mutate(Design = str_replace(Design, "Design", ""), -->
<!--          Education = str_replace(Education, "Education", ""), -->
<!--          Education = forcats::fct_inorder(Education)) %>%  -->
<!--   ggplot(aes(y = ToT, ymin  = lower, ymax = upper, -->
<!--              x = Education,  -->
<!--              color = Design, group = Design)) + -->
<!--   geom_pointrange(alpha = .5) + -->
<!--   geom_line() -->
<!-- ``` -->
<!-- <!-- #67 -> -->
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="saturation" class="section level3" number="5.4.3">
<h3><span class="header-section-number">5.4.3</span> Saturation: hitting the boundaries</h3>
<p>Recall taht the three main assumptions of linear regression are Normal distributed residuals, variance homogeneity and linearity. The last arises from the basic regression formula:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{1i}
\]</span></p>
<p>The formula basically says, that if we increase <span class="math inline">\(x_1\)</span> (or any other influencing variable) by one unit, <span class="math inline">\(y\)</span> will increase by <span class="math inline">\(\beta_1\)</span>.
It also says that <span class="math inline">\(y\)</span> is composed as a mere sum.
In this section, we will discover that these innocent assumptions often do not hold.</p>
<p>In this and the next section, we will use conditional effects to account for non-linearity.
We can distinguish between <em>saturation effects</em>, which are more common, and <em>amplification effects</em>. Figure <a href="mpm.html#fig:interaction-effects">5.10</a> shows two attempts at visualizing saturation (and amplification).
<!-- #68 --></p>
<div class="figure"><span id="fig:interaction-effects"></span>
<img src="Classic_linear_models_files/figure-html/interaction-effects-1.png" alt="Two forms of conditional effects: amplification and saturation" width="50%" /><img src="Classic_linear_models_files/figure-html/interaction-effects-2.png" alt="Two forms of conditional effects: amplification and saturation" width="50%" />
<p class="caption">
Figure 5.10: Two forms of conditional effects: amplification and saturation
</p>
</div>
<p>A major flaw with the linear model is that it presumes the regression line to rise or fall infinitely.
However, <em>in an endless universe everything has boundaries</em>.
Just think about your performance in reading this text.
Several things could be done to improve reading performance, such as larger font size, simpler sentence structure or translation into your native language.
Still, there is a hard lower limit for time to read, just by the fact, that reading involves saccades (eye movements) and these cannot be accelerated any further.
The time someone needs to read a text is limited by fundamental cognitive processing speed.
We may be able to reduce the inconvenience of deciphering small text, but once an optimum is reached, there is no further improvement.
Such boundaries of performance inevitably lead to non-linear relationships between predictors and outcome.</p>
<p>Modern statistics knows several means to deal with non-linearity, some of them are introduced in <a href="glm.html#glm">7</a>).
Still, most researchers use linear models, and it often can be regarded a reasonable approximation under particular circumstances.
Mostly, this is that measures keep a distance to the hard boundaries, to avoid saturation.</p>
<p>When there is just one treatment repeatedly pushing towards a boundary, we get the diminishing returns effect seen in learning curves <a href="lm.html#ofm">4.3.5</a>.
If two or more variables are pushing simultaneously, saturation can appear, too. Letter size and contrast both influence the readability of a text, but once the letters are huge, contrast no longer matters.</p>
<p>Before we turn to a genuine design research case, let me explain saturation effects by an example that I hope is intuitive.
The hypothetical question is: do two headache pills have twice the effect of one?
Consider a pharmaceutical study on the effectiveness of two pain killer pills A and B. The day after a huge campus party, random strolling students have been asked to participate.
First, they rate their experienced headache on a Likert scale ranging from “fresh like the kiss of morning dew” to “dead possum on the highway.”
Participants are randomly assigned to four groups, each group getting a different combination of pills: no pill, only A, only B, A and B.
After 30 minutes, headache is measured again and the difference between both measures is taken as the outcome measure: headache reduction. Figure</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="mpm.html#cb413-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Headache)</span>
<span id="cb413-2"><a href="mpm.html#cb413-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb413-3"><a href="mpm.html#cb413-3" aria-hidden="true" tabindex="-1"></a>T_means <span class="ot">&lt;-</span></span>
<span id="cb413-4"><a href="mpm.html#cb413-4" aria-hidden="true" tabindex="-1"></a>  Pills <span class="sc">%&gt;%</span></span>
<span id="cb413-5"><a href="mpm.html#cb413-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(PillA, PillB) <span class="sc">%&gt;%</span></span>
<span id="cb413-6"><a href="mpm.html#cb413-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean_reduction =</span> <span class="fu">round</span>(<span class="fu">mean</span>(reduction), <span class="dv">1</span>))</span>
<span id="cb413-7"><a href="mpm.html#cb413-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb413-8"><a href="mpm.html#cb413-8" aria-hidden="true" tabindex="-1"></a>T_means <span class="sc">%&gt;%</span></span>
<span id="cb413-9"><a href="mpm.html#cb413-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> PillA, <span class="at">col =</span> PillB, mean_reduction)) <span class="sc">+</span></span>
<span id="cb413-10"><a href="mpm.html#cb413-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb413-11"><a href="mpm.html#cb413-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> PillB)) <span class="sc">+</span></span>
<span id="cb413-12"><a href="mpm.html#cb413-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fl">2.5</span>)</span></code></pre></div>
<div class="figure"><span id="fig:headache-1"></span>
<img src="Classic_linear_models_files/figure-html/headache-1-1.png" alt="Group means of a two-factorial model" width="90%" />
<p class="caption">
Figure 5.11: Group means of a two-factorial model
</p>
</div>
<p>When neither pill is given a slight spontaneous recovery seems to occur.
Both pills alone are much stronger than the placebo and when giving them both, participants recover the best.
However, looking more closely, the effect of wto pills is just a tad stronger than the effect of A and stays far from being the sum.
One could also say, that the net effect of B is weaker when given together with A.
When the effect of one predictor depends on the level of another, this is just a conditional effect.</p>
<p>So, why is the effect of two pills not the sum of the two one-pill effects?
This question can be answered by contemplating what may happen when not two but five headache pills are given.
If we would assume linear addition of effects, we also have to assume that participants in the group with all pills make the breathtaking experience of <em>negative</em> headache.
So, certainly, the effect cannot be truly linear.
Al headache pills are pushing into the direction of the boundary called absence of headache.</p>
<p>At the example of headache pills, I will now demonstrate that saturation can cause a severe bias when not accounted for by a conditional effect.
We estimate both models: a factorial unconditional MFM and a conditional MFM. Table <a href="mpm.html#tab:headache-2">5.10</a> puts the center estimates of both models side-by-side.</p>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="mpm.html#cb414-1" aria-hidden="true" tabindex="-1"></a>M_mfm <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(reduction <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> PillA <span class="sc">+</span> PillB,</span>
<span id="cb414-2"><a href="mpm.html#cb414-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> Pills</span>
<span id="cb414-3"><a href="mpm.html#cb414-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb414-4"><a href="mpm.html#cb414-4" aria-hidden="true" tabindex="-1"></a>M_cmfm <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(reduction <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> PillA <span class="sc">+</span> PillB <span class="sc">+</span> PillA<span class="sc">:</span>PillB,</span>
<span id="cb414-5"><a href="mpm.html#cb414-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> Pills</span>
<span id="cb414-6"><a href="mpm.html#cb414-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb414-7"><a href="mpm.html#cb414-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb414-8"><a href="mpm.html#cb414-8" aria-hidden="true" tabindex="-1"></a>P_1 <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb414-9"><a href="mpm.html#cb414-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_mfm),</span>
<span id="cb414-10"><a href="mpm.html#cb414-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_cmfm)</span>
<span id="cb414-11"><a href="mpm.html#cb414-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb415-1"><a href="mpm.html#cb415-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(P_1) <span class="sc">%&gt;%</span></span>
<span id="cb415-2"><a href="mpm.html#cb415-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(model, fixef, center) <span class="sc">%&gt;%</span></span>
<span id="cb415-3"><a href="mpm.html#cb415-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spread</span>(<span class="at">key =</span> model, <span class="at">value =</span> center) <span class="sc">%&gt;%</span></span>
<span id="cb415-4"><a href="mpm.html#cb415-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mutate(diff = M_cmfm - M_mfm) </span></span></code></pre></div>
<table>
<caption><span id="tab:headache-2">Table 5.10: </span>Comparing the coefficients of conidtional and unconditional multi-factorial models</caption>
<thead>
<tr class="header">
<th align="left">fixef</th>
<th align="right">M_1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">106</td>
</tr>
</tbody>
</table>
<p>Both intercepts indicate that headache diminishes due to the placebo alone, but <code>M_mfm</code> over-estimates spontaneous recovery.
At the same time, the treatment effects PillA and PillB are under-estimated by <code>M_mfm</code>.
That happens, because the unconditional model averages over two conditions, under which pill A or B are given: with the other pill or without.
As <code>M_cmfm</code> tells, when taken with the another pill, effectiveness is reduced by <span class="math inline">\(-0.4\)</span>.
This example shows that multi-predictor models can severely under-estimate effect strength of individual impact variables, when effects are conditional.</p>
<!-- One can have headache to a certain degree or no headache at all. -->
<!-- If it's gone, any more pills have no additional effects. -->
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>In general, if two predictors work into the same direction (here the positive direction) and the consitional adjustment effect has the opposite direction, this is most likely a <em>saturation effect</em>: the more of similar is given, the closer it gets to the natural boundaries and the less it adds.</p>
<p>Remember that this is really not about side effects in conjunction with other medicines.
Quite the opposite: if two type of pills effectively reduce headache, but in conjunction produce a rash, this would actually be an amplification effect <a href="mpm.html#amplification">5.4.4</a>.
Whereas amplification effects are theoretically interesting, not only for pharmacists,
saturation effects are a boring nuisance stemming from the basic fact that there always are boundaries of performance. Saturation effects only tell us that we have been applying <em>more of the similar</em> and that we are running against a set limit of how much we can improve things.</p>
<p>Maybe, saturation affects are not so boring for design research. A saturation effect indicates that two impact factors work in a similar way. That can be used to trade one impact factor for another. There is more than one way to fight pain. Distraction or, quite the opposite, meditation can also reduce the suffering. Combining meditation practice with medication attacks the problem from different angles and that may add up much better than taking more pills. For people who are allergic to pill A, though, it is good that B is a similar replacement.</p>
<p>If two design impact factors work in similar ways, we may also be able to trade in one for the other. Imagine a study aiming at ergonomics of reading for informational websites.
In a first experiment, the researcher found that 12pt font effectively reduces reading time as compared to 10pt by about 5 seconds. But, is it reasonable to believe that increasing to 18pt would truly reduce reading time to 15 seconds?</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="mpm.html#cb417-1" aria-hidden="true" tabindex="-1"></a>D_reading_time <span class="ot">&lt;-</span></span>
<span id="cb417-2"><a href="mpm.html#cb417-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb417-3"><a href="mpm.html#cb417-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">14</span>, <span class="dv">16</span>, <span class="dv">18</span>),</span>
<span id="cb417-4"><a href="mpm.html#cb417-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">observed_time =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">40</span>, <span class="dv">30</span>, <span class="cn">NA</span>, <span class="cn">NA</span>, <span class="cn">NA</span>),</span>
<span id="cb417-5"><a href="mpm.html#cb417-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">predicted_time =</span> <span class="dv">60</span> <span class="sc">-</span> font_size <span class="sc">/</span> <span class="dv">4</span> <span class="sc">*</span> <span class="dv">10</span></span>
<span id="cb417-6"><a href="mpm.html#cb417-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb417-7"><a href="mpm.html#cb417-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb417-8"><a href="mpm.html#cb417-8" aria-hidden="true" tabindex="-1"></a>D_reading_time</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">font_size</th>
<th align="right">observed_time</th>
<th align="right">predicted_time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4</td>
<td align="right"></td>
<td align="right">50</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">40</td>
<td align="right">35</td>
</tr>
<tr class="odd">
<td align="right">12</td>
<td align="right">30</td>
<td align="right">30</td>
</tr>
<tr class="even">
<td align="right">14</td>
<td align="right"></td>
<td align="right">25</td>
</tr>
<tr class="odd">
<td align="right">16</td>
<td align="right"></td>
<td align="right">20</td>
</tr>
<tr class="even">
<td align="right">18</td>
<td align="right"></td>
<td align="right">15</td>
</tr>
</tbody>
</table>
<p>Probably not. For normally sighted persons, a font size of 12 is easy enough to decipher and another increase will not have the same effect.</p>
<!-- Taking this further, one would even arrive at absurdly short or impossible negative reading times. -->
<!-- At the opposite, a font size of four point may just render unreadable on a computer screen. -->
<!-- Instead of a moderate increase by 10 seconds, participants may have to decipher and guess the individual words, which will take much longer. -->
<p>Researching the effect of font sizes between 8pt and 12pt font size probably keeps the right distance, with approximate linearity within that range.
But what happens if you bring a second manipulation into the game with a functionally similar effect? For example, readability also improves with contrast.</p>
<p>A common conflict of interests is between the aesthetic appearance and the ergonomic properties.
From an ergonomic point of view, one would probably favor a typesetting design with crisp fonts and maximum contrast.
However, if a design researcher would suggest using 12pt black Arial on white background as body font, this is asking for trouble with anyone claiming a sense of beauty.
Someone will insist on a fancy serif font in an understating blueish-grey tone.
For creating a relaxed reading experience, the only option left is to increase the font size.</p>
<p>The general question arises: can one sufficiently compensate lack of contrast by setting the text in the maximum reasonable font size 12pt, as compared to the more typical 10pt?
In the fictional study Reading, a 2x2 experimental study has been simulated: the same page of text is presented in four versions, with either 10pt or 12pt, and grey versus black font colour.</p>
<!--#69-->
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="mpm.html#cb418-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Reading)</span>
<span id="cb418-2"><a href="mpm.html#cb418-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb418-3"><a href="mpm.html#cb418-3" aria-hidden="true" tabindex="-1"></a>D_1</span></code></pre></div>
<table>
<caption><span id="tab:reading-data">Table 5.11: </span>Data set with 6 variables, showing 8 of 40 observations.</caption>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="right">Part</th>
<th align="left">font_size</th>
<th align="left">font_color</th>
<th align="right">mu</th>
<th align="right">ToT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">1</td>
<td align="left">10pt</td>
<td align="left">gray</td>
<td align="right">60</td>
<td align="right">66.9</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">2</td>
<td align="left">12pt</td>
<td align="left">gray</td>
<td align="right">48</td>
<td align="right">45.2</td>
</tr>
<tr class="odd">
<td align="right">12</td>
<td align="right">12</td>
<td align="left">12pt</td>
<td align="left">gray</td>
<td align="right">48</td>
<td align="right">59.4</td>
</tr>
<tr class="even">
<td align="right">23</td>
<td align="right">23</td>
<td align="left">10pt</td>
<td align="left">black</td>
<td align="right">50</td>
<td align="right">49.1</td>
</tr>
<tr class="odd">
<td align="right">24</td>
<td align="right">24</td>
<td align="left">12pt</td>
<td align="left">black</td>
<td align="right">46</td>
<td align="right">52.1</td>
</tr>
<tr class="even">
<td align="right">25</td>
<td align="right">25</td>
<td align="left">10pt</td>
<td align="left">black</td>
<td align="right">50</td>
<td align="right">59.5</td>
</tr>
<tr class="odd">
<td align="right">26</td>
<td align="right">26</td>
<td align="left">12pt</td>
<td align="left">black</td>
<td align="right">46</td>
<td align="right">43.8</td>
</tr>
<tr class="even">
<td align="right">34</td>
<td align="right">34</td>
<td align="left">12pt</td>
<td align="left">black</td>
<td align="right">46</td>
<td align="right">43.0</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="mpm.html#cb419-1" aria-hidden="true" tabindex="-1"></a>D_1 <span class="sc">%&gt;%</span></span>
<span id="cb419-2"><a href="mpm.html#cb419-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(</span>
<span id="cb419-3"><a href="mpm.html#cb419-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">col =</span> font_color,</span>
<span id="cb419-4"><a href="mpm.html#cb419-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> font_size,</span>
<span id="cb419-5"><a href="mpm.html#cb419-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> ToT</span>
<span id="cb419-6"><a href="mpm.html#cb419-6" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb419-7"><a href="mpm.html#cb419-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>()</span></code></pre></div>
<div class="figure"><span id="fig:read-2"></span>
<img src="Classic_linear_models_files/figure-html/read-2-1.png" alt="A boxplot showing groups in a 2x2 experiment." width="90%" />
<p class="caption">
Figure 5.12: A boxplot showing groups in a 2x2 experiment.
</p>
</div>
<p>In Figure <a href="mpm.html#fig:read-2">5.12</a> we see , that both design choices have an impact: black letters, as well as larger letters are faster to read.
But, do they add up?
Or do both factors behave like headache pills, where more is more, but less than the sum.
Clearly, the 12pt-black group could read fastest on average.
Neither with large font, nor with optimal contrast alone has the design reached a boundary, i.e. saturation.
We run two regression models, a plain MFM and a conditional MFM, that adds an interaction term.
We extract the coefficients from both models and view them side-by-side:</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="mpm.html#cb420-1" aria-hidden="true" tabindex="-1"></a>M_mfm <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> font_size <span class="sc">+</span> font_color,</span>
<span id="cb420-2"><a href="mpm.html#cb420-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_1</span>
<span id="cb420-3"><a href="mpm.html#cb420-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb420-4"><a href="mpm.html#cb420-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb420-5"><a href="mpm.html#cb420-5" aria-hidden="true" tabindex="-1"></a>M_cmfm <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> font_size <span class="sc">+</span> font_color <span class="sc">+</span></span>
<span id="cb420-6"><a href="mpm.html#cb420-6" aria-hidden="true" tabindex="-1"></a>  font_size<span class="sc">:</span>font_color,</span>
<span id="cb420-7"><a href="mpm.html#cb420-7" aria-hidden="true" tabindex="-1"></a><span class="at">data =</span> D_1</span>
<span id="cb420-8"><a href="mpm.html#cb420-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="mpm.html#cb421-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb421-2"><a href="mpm.html#cb421-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_mfm),</span>
<span id="cb421-3"><a href="mpm.html#cb421-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_cmfm)</span>
<span id="cb421-4"><a href="mpm.html#cb421-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb421-5"><a href="mpm.html#cb421-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>()</span></code></pre></div>
<table>
<caption><span id="tab:read-3">Table 5.12: </span>Coefficient estimates with 95% credibility limits</caption>
<colgroup>
<col width="8%" />
<col width="34%" />
<col width="34%" />
<col width="8%" />
<col width="8%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">M_cmfm</td>
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">61.33</td>
<td align="right">57.41</td>
<td align="right">65.18</td>
</tr>
<tr class="even">
<td align="left">M_cmfm</td>
<td align="left">font_size12pt</td>
<td align="left">font_size12pt</td>
<td align="right">-12.73</td>
<td align="right">-18.00</td>
<td align="right">-7.10</td>
</tr>
<tr class="odd">
<td align="left">M_cmfm</td>
<td align="left">font_colorblack</td>
<td align="left">font_colorblack</td>
<td align="right">-11.05</td>
<td align="right">-16.50</td>
<td align="right">-5.75</td>
</tr>
<tr class="even">
<td align="left">M_cmfm</td>
<td align="left">font_size12pt:font_colorblack</td>
<td align="left">font_size12pt:font_colorblack</td>
<td align="right">5.55</td>
<td align="right">-2.23</td>
<td align="right">13.38</td>
</tr>
<tr class="odd">
<td align="left">M_mfm</td>
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">59.89</td>
<td align="right">56.52</td>
<td align="right">63.44</td>
</tr>
<tr class="even">
<td align="left">M_mfm</td>
<td align="left">font_size12pt</td>
<td align="left">font_size12pt</td>
<td align="right">-9.90</td>
<td align="right">-14.16</td>
<td align="right">-5.86</td>
</tr>
<tr class="odd">
<td align="left">M_mfm</td>
<td align="left">font_colorblack</td>
<td align="left">font_colorblack</td>
<td align="right">-8.28</td>
<td align="right">-12.38</td>
<td align="right">-4.31</td>
</tr>
</tbody>
</table>
<p>The estimates confirm, that both manipulations have a considerable effect in reducing reading time.
But, as the conditional effect works in the opposite direction, this reeks of saturation: in this hypothetical case, font size act as contrast are more-of-the-similar and the combined effects is less than the sum.
This is just like taking two headache pills.</p>
<!-- And, there is a conditional effect as well, correcting the additive effect of font colour and size. The combined effect of high contrast and large font is therefore -->
<!-- $$ -->
<!-- \mu_{12pt, black} = -->
<!-- `C_read_fixef[1]` + `C_read_fixef[2]` +  -->
<!-- `C_read_fixef[3]` + `C_read_fixef[4]` =  -->
<!-- `sum(C_read_fixef)` -->
<!-- $$ -->
<!-- In this research scenario, that was not strictly the question, as we were primarily interested in comparing the effects of font size and contrast. Also, if we see the credibility interval of the conditional effect it is not highly certain (`frm_coef(coef(Reading$M_2), row = 4)`). Still, including the  interaction term is the right choice for two reasons: first,  -->
<p>If this was real data, we could assign the saturation effect a deeper meaning.
It is not always obvious that two factors work in a similar way.
From a psychological perspective this would indicate that both manipulations work on similar cognitive processes, for example, visual letter recognition.
Knowing more than one way to improve on a certain mode of processing can be very helpful in design, where conflicting demands arise often and in the current case, a reasonable compromise between ergonomics and aesthetics would be to either use large fonts or black letters.
Both have the strongest ergonomic net effect when they come alone.</p>
<!-- From a theoretical perspective, we would thus expect saturation. This is simply the application of prior knowledge and in a perfect world one would  use a prior distribution for the interaction term, creating a more certain estimate. That being said, the data set is synthetic, and the simulation definitely included the conditional effect.  Second, in Table [XY], all other coefficients change considerably with introduction of the conditional effect. Especially the effects of the two manipulations get considerably under-estimated.  -->
<!-- Why are the main effects under-estimated, if the interaction term is not included? The pure main-effects model has three parameters. This allows it to represent the same number of independent group means. Formally, the number of groups in the study is four, however, the three-parameter model assumes that the fourth group (black, 12pt) can sufficiently be specified by the existing three parameters. If saturation occurs, the group of participants in the 12pt group is not homogeneous: in the grey group, they experience a stronger improvement than in the black group. The three parameter model is forced to make the best out of situation and adjusts the net effect of font size to be slightly lower than it actually is. The same occurs for the font colour effect. -->
<p>Conditional effects are notoriously neglected in research and they are often hard to grasp for audience, even when people have a classic statistics education.
Clear communication is often crucial and conditional models are best understood by using conditional plots.
A conditional plot for the 2x2 design contains the four estimated group means.
These can be computed from the linear model coefficients, but often it easier to just estimate an absolute means model alongside (Table <a href="mpm.html#tab:read-4">5.13</a>))</p>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="mpm.html#cb422-1" aria-hidden="true" tabindex="-1"></a>M_amm <span class="ot">&lt;-</span></span>
<span id="cb422-2"><a href="mpm.html#cb422-2" aria-hidden="true" tabindex="-1"></a>  D_1 <span class="sc">%&gt;%</span></span>
<span id="cb422-3"><a href="mpm.html#cb422-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> font_size<span class="sc">:</span>font_color,</span>
<span id="cb422-4"><a href="mpm.html#cb422-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb422-5"><a href="mpm.html#cb422-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb423-1"><a href="mpm.html#cb423-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_amm)</span></code></pre></div>
<table>
<caption><span id="tab:read-4">Table 5.13: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">font_size10pt:font_colorgray</td>
<td align="left">font_size10pt:font_colorgray</td>
<td align="right">61.2</td>
<td align="right">57.5</td>
<td align="right">65.2</td>
</tr>
<tr class="even">
<td align="left">font_size12pt:font_colorgray</td>
<td align="left">font_size12pt:font_colorgray</td>
<td align="right">48.5</td>
<td align="right">44.5</td>
<td align="right">52.1</td>
</tr>
<tr class="odd">
<td align="left">font_size10pt:font_colorblack</td>
<td align="left">font_size10pt:font_colorblack</td>
<td align="right">50.1</td>
<td align="right">46.1</td>
<td align="right">54.2</td>
</tr>
<tr class="even">
<td align="left">font_size12pt:font_colorblack</td>
<td align="left">font_size12pt:font_colorblack</td>
<td align="right">43.0</td>
<td align="right">38.9</td>
<td align="right">46.9</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="mpm.html#cb424-1" aria-hidden="true" tabindex="-1"></a>T_amm <span class="ot">&lt;-</span></span>
<span id="cb424-2"><a href="mpm.html#cb424-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>(M_amm) <span class="sc">%&gt;%</span></span>
<span id="cb424-3"><a href="mpm.html#cb424-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">separate</span>(fixef, <span class="fu">c</span>(<span class="st">&quot;font_size&quot;</span>, <span class="st">&quot;font_color&quot;</span>), <span class="at">sep =</span> <span class="st">&quot;:&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb424-4"><a href="mpm.html#cb424-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb424-5"><a href="mpm.html#cb424-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size =</span> <span class="fu">str_remove</span>(font_size, <span class="st">&quot;font_size&quot;</span>),</span>
<span id="cb424-6"><a href="mpm.html#cb424-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_color =</span> <span class="fu">str_remove</span>(font_color, <span class="st">&quot;font_color&quot;</span>)</span>
<span id="cb424-7"><a href="mpm.html#cb424-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb424-8"><a href="mpm.html#cb424-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb424-9"><a href="mpm.html#cb424-9" aria-hidden="true" tabindex="-1"></a>G_amm <span class="ot">&lt;-</span> T_amm <span class="sc">%&gt;%</span></span>
<span id="cb424-10"><a href="mpm.html#cb424-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(</span>
<span id="cb424-11"><a href="mpm.html#cb424-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> font_color,</span>
<span id="cb424-12"><a href="mpm.html#cb424-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> font_size, <span class="at">shape =</span> font_size,</span>
<span id="cb424-13"><a href="mpm.html#cb424-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> center</span>
<span id="cb424-14"><a href="mpm.html#cb424-14" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb424-15"><a href="mpm.html#cb424-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb424-16"><a href="mpm.html#cb424-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> font_size))</span></code></pre></div>
<p>Note that in a CLU table the column <code>fixef</code> stores two identifiers, the level of font-size and the level of font_color.
For putting them on different GGplot aesthetics we first have to rip them apart using <code>separate</code> before using <code>mutate</code> and <code>str_replace</code> to strip the group labels off the factor names. Since the coefficient table also contains the 95% certainty limits, we can produce a conditional plot with credibility intervals in another layer (<code>geom_errorbar</code>).
These limits belong to the group means, and generally cannot be used to tell about the treatment effects.</p>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="mpm.html#cb425-1" aria-hidden="true" tabindex="-1"></a>G_amm <span class="sc">+</span></span>
<span id="cb425-2"><a href="mpm.html#cb425-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_crossbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper),</span>
<span id="cb425-3"><a href="mpm.html#cb425-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">width =</span> .<span class="dv">1</span></span>
<span id="cb425-4"><a href="mpm.html#cb425-4" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb425-5"><a href="mpm.html#cb425-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;ToT&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:read-5"></span>
<img src="Classic_linear_models_files/figure-html/read-5-1.png" alt="Conditional effects of font size and font color" width="90%" />
<p class="caption">
Figure 5.13: Conditional effects of font size and font color
</p>
</div>
<p>Still, it gives the reader of a report some sense of the overall level of certainty.
Usually, when two 95% CIs do not overlap, that means that the difference is almost certainly not zero. Another useful Ggplot geometry is the violin plot, as these make the overlap between posterior distributions visible and reduce visual clutter caused by vertical error bars.</p>
<p>However, a violin plot requires more that just three CLU estimates.
Recall from <a href="lm.html#random-walk">4.1.1</a> that the posterior object, obtained with <code>posterior</code> stores the full certainty information gained by the MCMC estimation walk.
The CLU estimates we so commonly use, are just condensing this information into three numbers (CLU).
By pulling the estimated posterior distribution into the plot, we can produce a conditional plot that conveys more information and is easier on the eye.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="mpm.html#cb426-1" aria-hidden="true" tabindex="-1"></a>P_amm <span class="ot">&lt;-</span></span>
<span id="cb426-2"><a href="mpm.html#cb426-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_amm) <span class="sc">%&gt;%</span></span>
<span id="cb426-3"><a href="mpm.html#cb426-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(type <span class="sc">==</span> <span class="st">&quot;fixef&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb426-4"><a href="mpm.html#cb426-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(fixef, value) <span class="sc">%&gt;%</span></span>
<span id="cb426-5"><a href="mpm.html#cb426-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">separate</span>(fixef, <span class="fu">c</span>(<span class="st">&quot;font_size&quot;</span>, <span class="st">&quot;font_color&quot;</span>), <span class="at">sep =</span> <span class="st">&quot;:&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb426-6"><a href="mpm.html#cb426-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb426-7"><a href="mpm.html#cb426-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_size =</span> <span class="fu">str_replace</span>(font_size, <span class="st">&quot;font_size&quot;</span>, <span class="st">&quot;&quot;</span>),</span>
<span id="cb426-8"><a href="mpm.html#cb426-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">font_color =</span> <span class="fu">str_replace</span>(font_color, <span class="st">&quot;font_color&quot;</span>, <span class="st">&quot;&quot;</span>)</span>
<span id="cb426-9"><a href="mpm.html#cb426-9" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb426-10"><a href="mpm.html#cb426-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb426-11"><a href="mpm.html#cb426-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb426-12"><a href="mpm.html#cb426-12" aria-hidden="true" tabindex="-1"></a>G_amm <span class="sc">+</span></span>
<span id="cb426-13"><a href="mpm.html#cb426-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_violin</span>(</span>
<span id="cb426-14"><a href="mpm.html#cb426-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> P_amm,</span>
<span id="cb426-15"><a href="mpm.html#cb426-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(</span>
<span id="cb426-16"><a href="mpm.html#cb426-16" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> value,</span>
<span id="cb426-17"><a href="mpm.html#cb426-17" aria-hidden="true" tabindex="-1"></a>      <span class="at">fill =</span> font_size</span>
<span id="cb426-18"><a href="mpm.html#cb426-18" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb426-19"><a href="mpm.html#cb426-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.5</span>,</span>
<span id="cb426-20"><a href="mpm.html#cb426-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">position =</span> <span class="fu">position_identity</span>(),</span>
<span id="cb426-21"><a href="mpm.html#cb426-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">width =</span> .<span class="dv">2</span></span>
<span id="cb426-22"><a href="mpm.html#cb426-22" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb426-23"><a href="mpm.html#cb426-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;ToT&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:read-6"></span>
<img src="Classic_linear_models_files/figure-html/read-6-1.png" alt="Another way to plot conditional effects from an AMM includes posterior distributions" width="90%" />
<p class="caption">
Figure 5.14: Another way to plot conditional effects from an AMM includes posterior distributions
</p>
</div>
<p>Note how we just add one alternative layer to the original line plot object <code>G_amm</code> to get the violin plot. The violin layer here gets its own data set, which is another feature of the GGplot engine.</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>As Figures <a href="mpm.html#fig:read-5">5.13</a> and <a href="mpm.html#fig:read-6">5.14</a> show, ergonomics is maximized by using large fonts and high contrast. Still, there is saturation and therefore it does little harm to go with the gray font, as long as it is 12pt.</p>
<!-- We have seen in the Headache example that conditional effects occur as non-linearity. -->
<!-- The more a participant approaches the natural boundary of zero headache, the less benefit is created by additional effort. -->
<!-- This we call *saturation*. -->
<p>Saturation is likely to occur when multiple factors influence the same cognitive or physical system or functioning.
In quantitative comparative design studies, we gain a more detailed picture on the co-impact of design interventions and can come to more sophisticated decisions.</p>
<p>If we don’t account for saturation by introducing conditional terms, we are prone to underestimate the net effect of any of these measures and may falsely conclude that a certain treatment is rather ineffective.
Consider a large scale study, that assesses the simultaneous impact of many demographic and psychological variables on how willing customers are to take certain energy saving actions in their homes.
It is very likely that impact factors are associated, like higher income and size of houses.
Certain action require little effort (such as switching off lights in unoccupied rooms), whereas others are time-consuming (drying the laundry outside).
At the same time, customers may vary in the overall eagerness (motivation).
For high effort actions the impact of motivation level probably makes more of a difference than when effort is low.
Not including the conditional effect would result in the false conclusion that suggesting high effort actions is rather ineffective.</p>
</div>
<div id="amplification" class="section level3" number="5.4.4">
<h3><span class="header-section-number">5.4.4</span> Amplification: more than the sum</h3>
<p>Saturation effects occur, when multiple impact factors act on the same system and work in the same direction.
When reaching the boundaries, the change per unit diminishes. We can also think of such factors as exchangeable.
<em>Amplification</em> conditional effects are the opposite: Something only really works, if all conditions are fulfilled.
Conceiving good examples for amplification effects is far more challenging as compared to saturation effects.
Probably this is because saturation is a rather trivial phenomenon, whereas amplification involves complex orchestration of cognitive or physiological subprocesses.
Here, a fictional case on technology acceptance will serve to illustrate amplification effects.
Imagine a start-up company that seeks funding for a novel augmented reality game, where groups of gamers compete for territory.
For a fund raiser, they need to know their market potential, i.e. which fraction of the population is potentially interested.
The entrepreneurs have two hypotheses they want to verify:</p>
<ol style="list-style-type: decimal">
<li>Only technophile persons will dare to play the game, because it requires some top-notch equipment.</li>
<li>The game is strongly cooperative and therefore more attractive for people with a strong social motif.</li>
</ol>
<p>Imagine a study, where a larger sample of participants is asked to rate their own technophilia and sociophilia. Subsequently, participants are given a description of the planned game and were asked how much they intended to participate in the game.</p>
<p>While the example primarily serves to introduce amplification effects, it is also an opportunity to get familiar with conditional effects between metric predictors.
Although this is not very different to conditional effects on groups, there are a few peculiarities, one being that we cannot straight-forwardly make an exploratory plot.
For factors, we have used box plots, but these do not apply for metric predictors.
In fact, it is very difficult to come up with a good graphical representation.
One might think of 3D wire-frame plots, but these transfer poorly to the 2D medium of these pages.</p>
<p>Another option is to create a scatter-plot with the predictors on the axes and encode the outcome variable by shades or size of dots <!-- #71 -->.
These options may suffice to see any present main effects, but are too coarse to discover subtle non-linearity.
The closest we can get to a good illustration is to artificially create groups and continue as if we had factor variables. Note, that turning metric predictors into factors is just a hack to create exploratory graphs, it is not recommended practice for linear models.</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="mpm.html#cb428-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(AR_game)</span></code></pre></div>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb429-1"><a href="mpm.html#cb429-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(forcats) <span class="co"># fct_rev</span></span>
<span id="cb429-2"><a href="mpm.html#cb429-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb429-3"><a href="mpm.html#cb429-3" aria-hidden="true" tabindex="-1"></a>D_1 <span class="sc">%&gt;%</span></span>
<span id="cb429-4"><a href="mpm.html#cb429-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb429-5"><a href="mpm.html#cb429-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">Sociophile =</span></span>
<span id="cb429-6"><a href="mpm.html#cb429-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">fct_rev</span>(<span class="fu">if_else</span>(sociophile <span class="sc">&gt;</span> <span class="fu">median</span>(sociophile),</span>
<span id="cb429-7"><a href="mpm.html#cb429-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;high&quot;</span>, <span class="st">&quot;low&quot;</span></span>
<span id="cb429-8"><a href="mpm.html#cb429-8" aria-hidden="true" tabindex="-1"></a>      )),</span>
<span id="cb429-9"><a href="mpm.html#cb429-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">Technophile =</span></span>
<span id="cb429-10"><a href="mpm.html#cb429-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">fct_rev</span>(<span class="fu">if_else</span>(technophile <span class="sc">&gt;</span> <span class="fu">median</span>(technophile),</span>
<span id="cb429-11"><a href="mpm.html#cb429-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;high&quot;</span>, <span class="st">&quot;low&quot;</span></span>
<span id="cb429-12"><a href="mpm.html#cb429-12" aria-hidden="true" tabindex="-1"></a>      ))</span>
<span id="cb429-13"><a href="mpm.html#cb429-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb429-14"><a href="mpm.html#cb429-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> intention, <span class="at">x =</span> Technophile, <span class="at">col =</span> Sociophile)) <span class="sc">+</span></span>
<span id="cb429-15"><a href="mpm.html#cb429-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb429-16"><a href="mpm.html#cb429-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fl">0.5</span>)</span></code></pre></div>
<div class="figure"><span id="fig:ar-1"></span>
<img src="Classic_linear_models_files/figure-html/ar-1-1.png" alt="Visualizing continuous conditional effects as factors" width="90%" />
<p class="caption">
Figure 5.15: Visualizing continuous conditional effects as factors
</p>
</div>
<p>From Figure <a href="mpm.html#fig:ar-1">5.15</a> it seems that both predictors have a positive effect on intention to play.
However, it remains unclear whether there is a conditional effect.
In absence of a better visualization, we have to rely fully on the numerical estimates of a conditional linear regression model (CMRM)</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="mpm.html#cb430-1" aria-hidden="true" tabindex="-1"></a>M_cmrm <span class="ot">&lt;-</span></span>
<span id="cb430-2"><a href="mpm.html#cb430-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(intention <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> sociophile <span class="sc">+</span> technophile <span class="sc">+</span></span>
<span id="cb430-3"><a href="mpm.html#cb430-3" aria-hidden="true" tabindex="-1"></a>    sociophile<span class="sc">:</span>technophile,</span>
<span id="cb430-4"><a href="mpm.html#cb430-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_1</span>
<span id="cb430-5"><a href="mpm.html#cb430-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb431-1"><a href="mpm.html#cb431-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_cmrm)</span></code></pre></div>
<table>
<caption><span id="tab:ar-2">Table 5.14: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">0.273</td>
<td align="right">0.258</td>
<td align="right">0.288</td>
</tr>
<tr class="even">
<td align="left">sociophile</td>
<td align="left">sociophile</td>
<td align="right">0.114</td>
<td align="right">0.076</td>
<td align="right">0.153</td>
</tr>
<tr class="odd">
<td align="left">technophile</td>
<td align="left">technophile</td>
<td align="right">0.183</td>
<td align="right">0.153</td>
<td align="right">0.214</td>
</tr>
<tr class="even">
<td align="left">sociophile:technophile</td>
<td align="left">sociophile:technophile</td>
<td align="right">0.164</td>
<td align="right">0.081</td>
<td align="right">0.244</td>
</tr>
</tbody>
</table>
<p>Table <a href="mpm.html#tab:ar-2">5.14</a> tells the following:</p>
<ul>
<li>Intention to buy is <span class="math inline">\(0.27 [0.26, 0.29]_{CI95}\)</span> for people with minimum (i.e. zero) technophily and sociophily.</li>
<li>One unit change on sociophily, which is the whole range of the measure, adds <span class="math inline">\(0.11 [0.08, 0.15]_{CI95}\)</span> to intention (with sociophily constant zero).</li>
<li>One unit change on technophily adds <span class="math inline">\(0.11 [0.08, 0.15]_{CI95}\)</span> to intention (with sociophily constant zero).</li>
<li>If both predictors change one unit, <span class="math inline">\(0.18 [0.15, 0.21]_{CI95}\)</span> is added <em>on top</em></li>
</ul>
<p>confirms that sociophily and technophily both have a positive effect on intention.
Both effects are clearly in the positive range.
Yet, when both increase, the outcome increases over-linearly.
The sociophile-technophile personality is the primary target group.</p>
<!-- While plotting the relationship between three metric variables is difficult, there are alternative ways to illustrate the effect. -->
<!-- For example, we could ask: how does intention change by .1 unit sociophily for two imaginary participants with extreme positions on technophily (e.g., .2 and .8). -->
<!-- We could calculate these values arithmetically using the model formula and the center estimates. -->
<!-- The better and genuinely Bayesian way of doing it is sampling from the *posterior predictive distribution* (PPD). -->
<!-- This distribution is generated during parameter estimation. -->
<!-- While the posterior distribution (PD) represents the predictors, the PPD is linked to the outcome variable. -->
<!-- <!-- More specifically, the PPD is the models best guess of the expected value $\mu$. -->
<p>–&gt;
<!-- <!-- Once the posterior has been estimated, we can draw from it with any values of interest. --> –&gt;
<!-- <!-- As these can be combinations of values that have never been observed, we can truly speak of prediction. --> –&gt;
<!-- Regression engines usually provide easy means to simulate from a PD and generate predictions. -->
<!-- In the following example, we simulate some equidistant steps of sociophily for three participants with technophily scores from .1 to .9. --></p>
<!-- ```{r fit-AR-game-ppd, opts.label = "mcmc"} -->
<!-- D_2 <-  -->
<!--   mascutils::expand_grid(technophile = seq(.1, .9, by = .1), -->
<!--                          sociophile = seq(.3, .6, by = .1)) %>%  -->
<!--   arrange(technophile) -->
<!-- T_comb_pred <-  -->
<!--   post_pred(M_cmrm, newdata = D_2, thin =2) %>% -->
<!--   predict() -->
<!-- D_2 <-  -->
<!--   D_2 %>%  -->
<!--   mutate(intention = T_comb_pred$center) -->
<!-- ``` -->
<!-- ```{r ar-3, fig.cap = ""} -->
<!-- D_2 %>%  -->
<!--   mutate(technophile = as.factor(technophile)) %>%  -->
<!--   ggplot(aes(x = sociophile,  -->
<!--              col = technophile,  -->
<!--              y = intention)) + -->
<!--   geom_point() + -->
<!--   geom_line(aes(group = technophile)) -->
<!-- ``` -->
<!-- <!-- #72 -->
<p>–&gt;</p>
<!-- The effect is not stunning, but visible in Figure \@ref(fig:ar-3). -->
<!-- The lines diverge, which means they have different slopes. -->
<!-- With high technophily, every (tenth) unit of sociophily has a stronger effect on intention to play. -->
<!-- ```{r opts.label = "mcsync"} -->
<!-- sync_CE(AR_game, D_2) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- detach(AR_game) -->
<!-- ``` -->
<p>Saturation effects are about declining net effects, the more similar treatments pile up.
Amplification effects are more like two-component glue.
When using only one of the components, all one gets is a smear. You have to put them together for a strong hold.</p>
<p>Saturation and amplification also have parallels in formal logic (Table <a href="mpm.html#tab:boolean-OR">5.15</a>).
The logical <code>AND</code>, requires both operands to be <code>TRUE</code> for the result to become <code>TRUE</code>.
Instead, a saturation process can be imagined as logical <code>OR</code>.
If A is already <code>TRUE</code>, B no longer matters.</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="mpm.html#cb432-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb432-2"><a href="mpm.html#cb432-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">A =</span> <span class="fu">c</span>(F, F, T, T),</span>
<span id="cb432-3"><a href="mpm.html#cb432-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">B =</span> <span class="fu">c</span>(F, T, F, T)</span>
<span id="cb432-4"><a href="mpm.html#cb432-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb432-5"><a href="mpm.html#cb432-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span></span>
<span id="cb432-6"><a href="mpm.html#cb432-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="st">&quot;A OR B&quot;</span> <span class="ot">=</span> A <span class="sc">|</span> B) <span class="sc">%&gt;%</span></span>
<span id="cb432-7"><a href="mpm.html#cb432-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="st">&quot;A AND B&quot;</span> <span class="ot">=</span> A <span class="sc">&amp;</span> B) </span></code></pre></div>
<table>
<caption><span id="tab:boolean-OR">Table 5.15: </span>Saturation and amplification are like Bolean Or and AND</caption>
<thead>
<tr class="header">
<th align="left">A</th>
<th align="left">B</th>
<th align="left">A OR B</th>
<th align="left">A AND B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FALSE</td>
<td align="left">FALSE</td>
<td align="left">FALSE</td>
<td align="left">FALSE</td>
</tr>
<tr class="even">
<td align="left">FALSE</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">FALSE</td>
</tr>
<tr class="odd">
<td align="left">TRUE</td>
<td align="left">FALSE</td>
<td align="left">TRUE</td>
<td align="left">FALSE</td>
</tr>
<tr class="even">
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
</tr>
</tbody>
</table>
<p>For decision making in design research, the notion of saturation and amplification are equally important.
Saturation effects can happen with seemingly different design choices that act on the same cognitive (or other) processes.
That is good to know, because it allows the designer to compensate one design feature with the other, should there be a conflict between different requirements, such as aesthetics and readability of text.
Amplification effects are interesting, because they break barriers.
Only if the right ingredients are present, a system is adopted by users.
Many technology break-throughs can perhaps be attributed to adding the final necessary ingredient.</p>
<p>Sometimes we can see that on technology that first is a failure, just to take off years later.
For example, the first commercial smart phone (with touchscreen, data connectivity and apps) has been the IBM Simon Personal Communicator, introduced in 1993. Only a few thousands were made and it was discontinued after only six months on the market.
It lasted more than ten years before smartphones actually took off.
What were the magic ingredients added? My best guess is it was the combination of good battery time and still fitting in your pockets.</p>
<p>A feature that must be present for the users to be satisfied (in the mere sense of absence-of-annoyance) is commonly called a <em>necessary user requirements</em>.
That paints a more moderate picture of amplification in everyday design work: The peak, where all features work together usually is not the magic break-through; it is the strenuous path of user experience design, where user requirements whirl around you and not a single one must be left behind.</p>
</div>
<div id="cfx-theory" class="section level3" number="5.4.5">
<h3><span class="header-section-number">5.4.5</span> Conditional effects and design theory</h3>
<p>Explaining or predicting complex behaviour with psychological theory is a typical approach in design research.
Unfortunately, it is not an easy one.
While design is definitely multifactorial, with a variety of cognitive processes, individual differences and behavioural strategies, few psychological theories cover more than three associations between external or individual conditions and behaviour.
The design researcher is often forced to enter a rather narrow perspective or knit a patchwork model from multiple theories.
Such a model can either be loose, making few assumptions on how the impact factors interact which others.
A more tightened model frames multiple impact factors into a conditional network, where the impact of one factor can depend on the overall configuration.
A classic study will now serve to show how conditional effects can clarify theoretical considerations.</p>
<p>Vigilance is the ability to remain attentive for rarely occurring events.
Think of truck drivers on lonely night rides, where most of the time they spend keeping the truck on a straight 80km/h course.
Only every now and then is the driver required to react to an event, like when braking lights flare up ahead.
Vigilance tasks are among the hardest thing to ask from a human operator.
Yet, they are safety relevant in a number of domains.</p>
<p>Keeping up vigilance most people perceive as tiring, and vigilance deteriorates with tiredness.
Several studies have shown that reaction time at simple tasks increases when people are deprived of sleep.
The disturbing effect of loud noise has been documented as well.
A study by <span class="citation">(Corcoran 1962)</span> examined the simultaneous influence of sleep deprivation and noise on a rather simple reaction task.
They asked:</p>
<blockquote>
<p>will the effects of noise summate with those of loss of sleep to induce an even greater performance decrement or will noise subtract from the performance decrement caused by loss of sleep?</p>
</blockquote>
<p>The theoretical argument is that sleep deprivation deteriorates the central nervous arousal system.
In consequence, sleep deprived persons cannot maintain the necessary level of energy that goes with the task.
Noise is a source of irritation and therefore usually reduces performance.
At the same time, loud noise has an agitating effect, which may compensate for the loss of arousal due to sleep deprivation.</p>
<p>The Sleep case study is a simplified simulation of Corcoran’s results.
Participants were divided into 2x2 groups (quiet/noisy, rested/deprived) and had to react to five signal lamps in a succession of trials.
In the original study, performance measure gaps were counted, which is the number of delayed reactions (<span class="math inline">\(&gt;1500ms\)</span>).
Here we just go with (simulated) reaction times, assuming that declining vigilance manifests itself in overall slower reactions (Figure <a href="mpm.html#fig:sleep-1">5.16</a>).</p>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb433-1"><a href="mpm.html#cb433-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Sleep)</span></code></pre></div>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="mpm.html#cb434-1" aria-hidden="true" tabindex="-1"></a>D_1 <span class="sc">%&gt;%</span></span>
<span id="cb434-2"><a href="mpm.html#cb434-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(</span>
<span id="cb434-3"><a href="mpm.html#cb434-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> Environment,</span>
<span id="cb434-4"><a href="mpm.html#cb434-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> Sleep,</span>
<span id="cb434-5"><a href="mpm.html#cb434-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> RT</span>
<span id="cb434-6"><a href="mpm.html#cb434-6" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb434-7"><a href="mpm.html#cb434-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>()</span></code></pre></div>
<div class="figure"><span id="fig:sleep-1"></span>
<img src="Classic_linear_models_files/figure-html/sleep-1-1.png" alt="Conditional effects of environmental noise and sleep deprivation" width="90%" />
<p class="caption">
Figure 5.16: Conditional effects of environmental noise and sleep deprivation
</p>
</div>
<p>Using a 2x2 model including a conditional effect, we examine the conditional association between noise and sleepiness. Note that the <code>*</code> operator in the model formula is an abbreviation for a fully factorial model <code>1 + Environment + Sleep + Environment:Sleep</code>. The results are shown in Table <a href="mpm.html#tab:sleep-2">5.16</a></p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb435-1"><a href="mpm.html#cb435-1" aria-hidden="true" tabindex="-1"></a>M_1 <span class="ot">&lt;-</span></span>
<span id="cb435-2"><a href="mpm.html#cb435-2" aria-hidden="true" tabindex="-1"></a>  D_1 <span class="sc">%&gt;%</span></span>
<span id="cb435-3"><a href="mpm.html#cb435-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(RT <span class="sc">~</span> Environment <span class="sc">*</span> Sleep, <span class="at">data =</span> .)</span></code></pre></div>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="mpm.html#cb436-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_1)</span></code></pre></div>
<table>
<caption><span id="tab:sleep-2">Table 5.16: </span>Coefficient estimates with 95% credibility limits</caption>
<colgroup>
<col width="36%" />
<col width="36%" />
<col width="10%" />
<col width="8%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">97.68</td>
<td align="right">63.1</td>
<td align="right">130.7</td>
</tr>
<tr class="even">
<td align="left">EnvironmentNoisy</td>
<td align="left">EnvironmentNoisy</td>
<td align="right">2.32</td>
<td align="right">-47.7</td>
<td align="right">53.9</td>
</tr>
<tr class="odd">
<td align="left">SleepSleepy</td>
<td align="left">SleepSleepy</td>
<td align="right">163.58</td>
<td align="right">114.2</td>
<td align="right">212.4</td>
</tr>
<tr class="even">
<td align="left">EnvironmentNoisy:SleepSleepy</td>
<td align="left">EnvironmentNoisy:SleepSleepy</td>
<td align="right">-103.50</td>
<td align="right">-173.4</td>
<td align="right">-32.9</td>
</tr>
</tbody>
</table>
<p>Recall, that treatment contrasts were used, where all effects are given relative to the reference group quiet-rested (intercept).
The results confirm the deteriorating effect of sleepiness, although its exact impact is blurred by pronounced uncertainty <span class="math inline">\(163.58 [114.21, 212.36]_{CI95}\)</span>.
Somewhat surprisingly, noise did not affect well-rested persons by much <span class="math inline">\(2.32 [-47.67, 53.91]_{CI95}\)</span>.
Note however, that we cannot conclude a null effect, as the credibility limits are wide.
Maybe the lack of a clear effect is because steady white noise was used, not a disturbing tumult.
The effect of sleepiness on RT is partly reduced in a noisy environment <span class="math inline">\(-103.5 [-173.38, -32.88]_{CI95}\)</span>.
This suggests that the arousal system is involved in the deteriorating effect of sleep deprivation, which has interesting consequences for the design of vigilance tasks in the real world.</p>
<p>These findings reverb with a well known law in Psychology of Human Factors, the Yerkes-Dodson law, which states that human performance at cognitive tasks is influenced by arousal.
The influence is not linear, but better approximated with a curve as shown in the Figure <a href="mpm.html#fig:Yerkes-Dodson-1">5.17</a>.
Performance is highest at a moderate level of arousal.
If we assume that sleepy participants in Corcona’s study showed low performance due to under-arousal, the noise perhaps has increased the arousal level, resulting in better performance.
If we accept that noise has an arousing effect, the null effect of noise on rested participants stands in opposition to the Yerkes-Dodson law: if rested participants were on an optimal arousal level, additional arousal would usually have a negative effect on performance.
There is the slight possibility, that Corcona has hit a sweet spot: if we assume that calm/rested participants were still below an optimal arousal level, noise could have pushed them right to the opposite point.</p>
<!-- #76 -->
<div class="figure"><span id="fig:Yerkes-Dodson-1"></span>
<img src="Classic_linear_models_files/figure-html/Yerkes-Dodson-1-1.png" alt="Conditional effects explained by the Yerkes-Dodson law" width="90%" />
<p class="caption">
Figure 5.17: Conditional effects explained by the Yerkes-Dodson law
</p>
</div>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<hr />
<p>To sum it up, saturation and amplification effects have in common that performance is related to design features in a monotonous increasing manner, albeit not linearly.
Such effects can be interpreted in a straight-forward manner: when saturation occurs with multiple factors, it can be inferred that they all impact the same underlying cognitive mechanism and are therefore interchangeable to some extent, like compensating letter size with stronger contrast.</p>
<p>In contrast, amplification effects indicate that multiple cognitive mechanisms (or attitudes) are necessarily involved. Whereas saturation effects point us at options for compensation, amplification narrows it down to a particular configuration that works.</p>
<p>The Sleep study demonstrated that conditional effects can also occur in situations with <em>non monotonously increasing</em> relationships between design features and performance.
When such a relationship takes the form of a parabole, like the Yerkes-Dodson law, the designer (or researcher) is faced with the complex problem of finding the sweet spot.</p>
<p>In the next section we will see how paraboles, but also more wildly curved relationships can be modeled using linear models with polynomial. And we will see how sweet spots or, to be more accurate, a catastrophic spot can be identified.</p>
<!-- ### Exercises -->
<!-- 1. In the IPump case (data set D_agg), two infusion pumps have been tested against each other (L = legacy, N = novel). Nurses from two professional groups (intensive and general care) completed a sequence of tasks with both devices. In order to account for learning effects, this procedure was repeated in three sessions. Three performance measures were taken: time-on-task, deviations from the optimal path and self-reported workload. Analyze the results using the models that have been introduced so far. Start with basic CGM and LRM, then use these as building blocks for more complex models. At least for the more basic models, always start by an exploratory plot. Then build and run the model. Extract the coefficients table and interpret magnitude and certainty. -->
<!-- 1. For the basic models of the previous exercise, extract and plot the residual distribution. Compare the residual distribution by groups. -->
<!-- 1. For the more complex models, extract the predicted values and residuals and plot them. Is their an association between the two? -->
</div>
</div>
<div id="prm" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Doing the rollercoaster: polynomial regression models</h2>
<p>In the preceding sections, we used linear models with conditional effects to render processes that are not linear, but somehow curved.
These non-linear processes fell into two classes: learning curves, saturation effects and amplification.
But, what can we do when a process follows more complex curves, with more ups-and downs?
In the following I will introduce polynomial regression models, which still uses a linear combination, but can describe a wild variety of shapes for the association between two variables.</p>
<p>Robots build our cars and sometimes drive them.
They mow the lawn and may soon also deliver parcels to far-off regions.
In prophecies robots will also enter social domains, such as care for children and the elderly.
One can imagine that in social settings emotional acceptance plays a significant role for technology adoption.
Next to our voices, our faces and mimic expressions are the main source of interpersonal messaging.
Since the dawn of the very idea of robots, anthropomorphic designs have been dominant.
Researchers and designers all around the globe are currently pushing the limits of human-likeness of robots. (Whereas I avoid Science Fiction movies with humonoid extraterrestrians.)
One could assume that emotional response improves with every small step towards perfection.
Unfortunately, this is not the case.
<span class="citation">(Mori 1970)</span> discovered a bizarre non-linearity in human response: people’s emotional response is proportional to human-likeness, but only at the lower end.
A <!-- #77 --> robot design with cartoon style facial features will always beat a robot vacuum cleaner.
But, an almost anatomically correct robot face may provoke a very negative emotional response, an intense feeling of eery, which is called the <em>Uncanny Valley</em> (Figure <a href="mpm.html#fig:uncanny-1">5.18</a>).</p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="mpm.html#cb438-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb438-2"><a href="mpm.html#cb438-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">hl =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">100</span>),</span>
<span id="cb438-3"><a href="mpm.html#cb438-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">emotional_valence =</span> <span class="sc">-</span>.<span class="dv">5</span> <span class="sc">*</span> hl <span class="sc">+</span> .<span class="dv">6</span> <span class="sc">*</span> hl<span class="sc">^</span><span class="dv">3</span> <span class="sc">+</span> .<span class="dv">2</span> <span class="sc">*</span> hl<span class="sc">^</span><span class="dv">4</span></span>
<span id="cb438-4"><a href="mpm.html#cb438-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb438-5"><a href="mpm.html#cb438-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">human_likeness =</span> (hl <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb438-6"><a href="mpm.html#cb438-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> human_likeness, <span class="at">y =</span> emotional_valence)) <span class="sc">+</span></span>
<span id="cb438-7"><a href="mpm.html#cb438-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>()</span></code></pre></div>
<div class="figure"><span id="fig:uncanny-1"></span>
<img src="Classic_linear_models_files/figure-html/uncanny-1-1.png" alt="The Uncanny Valley phenomenon is a non-linear emotional response to robot faces" width="90%" />
<p class="caption">
Figure 5.18: The Uncanny Valley phenomenon is a non-linear emotional response to robot faces
</p>
</div>
<p>In <span class="citation">(Mathur and Reichling 2016)</span>, the observation of Mori is put to a test: Is the relationship between human-likeness and emotional response really shaped like a valley?
They collected 60 pictures of robots and attached a score for human likeness to them.
Then they frankly asked their participants how much they liked the faces.
For the data analysis they calculated an average score of likeability per robot picture.
Owing to the curved shape of the uncanny valley, linear regression is not applicable to the problem.
Instead, Mathur et al. applied a third degree polynomial term.</p>
<p>A polynomial function of degree <span class="math inline">\(k\)</span> has the form: <!-- #78 --></p>
<p><span class="math display">\[
y_i = \beta_0 x_i^0 + \beta_1 x_i^1 + ... + \beta_{k}  x_i^{k}
\]</span></p>
<p>The degree of a polynomial is its largest exponent. In fact, you are already familiar with two polynomial models. The zero degree polynomial is the grand mean model, with <span class="math inline">\(x_i^0 = 1\)</span>, which makes <span class="math inline">\(\beta_0\)</span> a constant.
A first degree polynomial is simply the linear model: <span class="math inline">\(\beta_0 + \beta_1x_{1i}\)</span>
By adding higher degrees we can introduce more complex curvature to the association (Figure <a href="mpm.html#fig:uncanny-2">5.19</a>).</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb439-1"><a href="mpm.html#cb439-1" aria-hidden="true" tabindex="-1"></a>D_poly <span class="ot">&lt;-</span></span>
<span id="cb439-2"><a href="mpm.html#cb439-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb439-3"><a href="mpm.html#cb439-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="at">by =</span> .<span class="dv">1</span>),</span>
<span id="cb439-4"><a href="mpm.html#cb439-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">degree_0 =</span> <span class="dv">2</span>,</span>
<span id="cb439-5"><a href="mpm.html#cb439-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">degree_1 =</span> <span class="dv">1</span> <span class="sc">*</span> degree_0 <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> x,</span>
<span id="cb439-6"><a href="mpm.html#cb439-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">degree_2 =</span> <span class="fl">0.5</span> <span class="sc">*</span> (degree_1 <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> x<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb439-7"><a href="mpm.html#cb439-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">degree_3 =</span> <span class="fl">0.5</span> <span class="sc">*</span> (degree_2 <span class="sc">+</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">*</span> x<span class="sc">^</span><span class="dv">3</span>),</span>
<span id="cb439-8"><a href="mpm.html#cb439-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">degree_4 =</span> <span class="fl">0.4</span> <span class="sc">*</span> (degree_3 <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> x<span class="sc">^</span><span class="dv">4</span>),</span>
<span id="cb439-9"><a href="mpm.html#cb439-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">degree_5 =</span> <span class="fl">0.3</span> <span class="sc">*</span> (degree_4 <span class="sc">+</span> <span class="sc">-</span><span class="fl">0.3</span> <span class="sc">*</span> x<span class="sc">^</span><span class="dv">5</span>)</span>
<span id="cb439-10"><a href="mpm.html#cb439-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb439-11"><a href="mpm.html#cb439-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(polynomial, y, degree_0<span class="sc">:</span>degree_5) <span class="sc">%&gt;%</span></span>
<span id="cb439-12"><a href="mpm.html#cb439-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(polynomial, y, x)</span>
<span id="cb439-13"><a href="mpm.html#cb439-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb439-14"><a href="mpm.html#cb439-14" aria-hidden="true" tabindex="-1"></a>D_poly <span class="sc">%&gt;%</span></span>
<span id="cb439-15"><a href="mpm.html#cb439-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb439-16"><a href="mpm.html#cb439-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb439-17"><a href="mpm.html#cb439-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>polynomial)</span></code></pre></div>
<div class="figure"><span id="fig:uncanny-2"></span>
<img src="Classic_linear_models_files/figure-html/uncanny-2-1.png" alt="The higher the degree of a polynomial, the more complex the association." width="90%" />
<p class="caption">
Figure 5.19: The higher the degree of a polynomial, the more complex the association.
</p>
</div>
<p>Mathur et al. argue that the Uncanny Valley curve possesses two stationary points, with a slope of zero: the valley is a local minimum and represents the deepest point in the valley, the other is a local maximum and marks the shoulder left of the valley.
Such a curvature can be approximated with a polynomial of (at least) third degree, which has a constant term <span class="math inline">\(\beta_0\)</span>, a linear slope <span class="math inline">\(x\beta_1\)</span>, quadratic component <span class="math inline">\(x^2\beta_2\)</span> and a cubic component <span class="math inline">\(x^3\beta_3\)</span>.</p>
<p>While R provides high-level methods to deal with polynomial regression, it is instructive to build the regression manually.
The first step is to add variables to the data frame, which are the predictors taken to powers (<span class="math inline">\(x_k = x^k\)</span>).
These variables are then added to the model term, as if they were independent predictors.
For better clarity, we rename the intercept to be <span class="math inline">\(x_0\)</span>, before summarizing the fixed effects.
We extract the coefficients as usual. The four coefficients in Table <a href="mpm.html#tab:uncanny-3">5.17</a> specify the polynomial to approximate the average likability responses.</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="mpm.html#cb440-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Uncanny)</span></code></pre></div>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb441-1"><a href="mpm.html#cb441-1" aria-hidden="true" tabindex="-1"></a>M_poly_3 <span class="ot">&lt;-</span></span>
<span id="cb441-2"><a href="mpm.html#cb441-2" aria-hidden="true" tabindex="-1"></a>  RK_2 <span class="sc">%&gt;%</span></span>
<span id="cb441-3"><a href="mpm.html#cb441-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb441-4"><a href="mpm.html#cb441-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">huMech_0 =</span> <span class="dv">1</span>,</span>
<span id="cb441-5"><a href="mpm.html#cb441-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">huMech_1 =</span> huMech,</span>
<span id="cb441-6"><a href="mpm.html#cb441-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">huMech_2 =</span> huMech<span class="sc">^</span><span class="dv">2</span>,</span>
<span id="cb441-7"><a href="mpm.html#cb441-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">huMech_3 =</span> huMech<span class="sc">^</span><span class="dv">3</span></span>
<span id="cb441-8"><a href="mpm.html#cb441-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb441-9"><a href="mpm.html#cb441-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(avg_like <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> huMech_1 <span class="sc">+</span> huMech_2 <span class="sc">+</span> huMech_3,</span>
<span id="cb441-10"><a href="mpm.html#cb441-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ., <span class="at">iter =</span> <span class="dv">2500</span></span>
<span id="cb441-11"><a href="mpm.html#cb441-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb441-12"><a href="mpm.html#cb441-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb441-13"><a href="mpm.html#cb441-13" aria-hidden="true" tabindex="-1"></a>P_poly_3 <span class="ot">&lt;-</span> <span class="fu">posterior</span>(M_poly_3)</span></code></pre></div>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="mpm.html#cb442-1" aria-hidden="true" tabindex="-1"></a>T_coef <span class="ot">&lt;-</span> <span class="fu">coef</span>(P_poly_3)</span>
<span id="cb442-2"><a href="mpm.html#cb442-2" aria-hidden="true" tabindex="-1"></a>T_coef</span></code></pre></div>
<table>
<caption><span id="tab:uncanny-3">Table 5.17: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">-0.449</td>
<td align="right">-0.516</td>
<td align="right">-0.378</td>
</tr>
<tr class="even">
<td align="left">huMech_1</td>
<td align="left">huMech_1</td>
<td align="right">0.149</td>
<td align="right">-0.322</td>
<td align="right">0.588</td>
</tr>
<tr class="odd">
<td align="left">huMech_2</td>
<td align="left">huMech_2</td>
<td align="right">-1.092</td>
<td align="right">-2.006</td>
<td align="right">-0.086</td>
</tr>
<tr class="even">
<td align="left">huMech_3</td>
<td align="left">huMech_3</td>
<td align="right">0.919</td>
<td align="right">0.282</td>
<td align="right">1.499</td>
</tr>
</tbody>
</table>
<p>The thing is, coefficients of a polynomial model rarely have a useful interpretation.
Mathur and Reichling also presented a method to extract meaningful parameters from their model. If staying out of the Uncanny Valley is the only choice, it it is very important to know, where precisely it is. The trough of the Uncanny Valley is a local minimum of the curve and we can find this point with polynomial techniques.</p>
<p>Finding a local minimum is a two step procedure: first, we must find all <em>stationary points</em>, which includes all <em>local</em> minima and maxima.
Stationary points occur, where the curve bends from a rising to falling or vice versa.
At these points, the slope is zero, neither rising nor falling.
Therefore, stationary points are identified by the derivative of the polynomial, which is a second degree (cubic) polynomial:</p>
<p><span class="math display">\[
f&#39;(x) = \beta_1 + 2\beta_2x + 3\beta_2x^2
\]</span></p>
<p>The derivative <span class="math inline">\(f&#39;(x)\)</span> of a function <span class="math inline">\(f(x)\)</span> gives the slope of <span class="math inline">\(f(x)\)</span> at any given point <span class="math inline">\(x\)</span>.
When <span class="math inline">\(f&#39;(x) &gt; 0\)</span>, <span class="math inline">\(f(x)\)</span> is rising at <span class="math inline">\(x\)</span>, with <span class="math inline">\(f&#39;(x) &lt; 0\)</span> it is falling.
Stationary points are precisely those points, where <span class="math inline">\(f&#39;(x) = 0\)</span> and can be found by solving the equation.
The derivative of a third degree polynomial is of the second degree, which has a quadratic part.
This can produce a parabolic form, which hits point zero twice, once rising and once falling.
A rising encounter of point zero indicates that <span class="math inline">\(f(x)\)</span> has a local minimum at <span class="math inline">\(x\)</span>, a falling one indicates a local maximum.
In consequence, solving <span class="math inline">\(f&#39;(x) = 0\)</span> can result in two solutions, one minimum and one maximum, which need to be distinguished further.</p>
<p>If the stationary point is a local minimum, as the trough, slope switches from negative to positive; <span class="math inline">\(f&#39;(x)\)</span> crosses <span class="math inline">\(x = 0\)</span> in a rising manner, which is a positive slope of <span class="math inline">\(f&#39;(x)\)</span>.
Therefore, a stationary point is a local minimum, if <span class="math inline">\(f&#39;&#39;(x) &gt; 0\)</span>.</p>
<p>Mathur et al. followed these analytic steps to arrive at an estimate for the position of the trough.
However, they used frequentist estimation methods, which is why they could not attach a level of uncertainty to their estimate.
We will apply the polynomial operations on the posterior distribution which results in a new posterior for the position of the trough.</p>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="mpm.html#cb443-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(polynom)</span>
<span id="cb443-2"><a href="mpm.html#cb443-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb443-3"><a href="mpm.html#cb443-3" aria-hidden="true" tabindex="-1"></a>poly <span class="ot">&lt;-</span> <span class="fu">polynomial</span>(T_coef<span class="sc">$</span>center) <span class="co"># UC function on center</span></span>
<span id="cb443-4"><a href="mpm.html#cb443-4" aria-hidden="true" tabindex="-1"></a>dpoly <span class="ot">&lt;-</span> <span class="fu">deriv</span>(poly) <span class="co"># 1st derivative</span></span>
<span id="cb443-5"><a href="mpm.html#cb443-5" aria-hidden="true" tabindex="-1"></a>ddpoly <span class="ot">&lt;-</span> <span class="fu">deriv</span>(dpoly) <span class="co"># 2nd derivative</span></span>
<span id="cb443-6"><a href="mpm.html#cb443-6" aria-hidden="true" tabindex="-1"></a>stat_pts <span class="ot">&lt;-</span> <span class="fu">solve</span>(dpoly) <span class="co"># finding stat points</span></span>
<span id="cb443-7"><a href="mpm.html#cb443-7" aria-hidden="true" tabindex="-1"></a>slopes <span class="ot">&lt;-</span> <span class="fu">as.function</span>(ddpoly)(stat_pts) <span class="co"># slope at stat points</span></span>
<span id="cb443-8"><a href="mpm.html#cb443-8" aria-hidden="true" tabindex="-1"></a>trough <span class="ot">&lt;-</span> stat_pts[slopes <span class="sc">&gt;</span> <span class="dv">0</span>] <span class="co"># local minimum</span></span>
<span id="cb443-9"><a href="mpm.html#cb443-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb443-10"><a href="mpm.html#cb443-10" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The trough is most likely at a huMech score of &quot;</span>, <span class="fu">round</span>(trough, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## The trough is most likely at a huMech score of  0.72</code></pre>
<p>Note how the code uses high-level functions from package <code>polynom</code> to estimate the location of the trough, in particular the first and second derivative <code>d[d]poly</code>.</p>
<!-- The solution is that we let the trough calculation work on every single draw of the posterior distribution. -->
<!-- That will produce a posterior distribution of the derived trough position. -->
<p>Every step of the MCMC walk produces a simultaneous draw of the four parameters <code>huMech_[0:3]</code><!-- #79 -->, and therefore fully specifies a third degree polynomial.
If the position of the trough is computed for every step of the MCMC walk, the result is a posterior distribution of the trough position.
For the convenience, the R package Uncanny contains a function <code>trough(coef)</code> that includes all the above steps.
The following code creates a data frame with one row per MCMC draw and the four huMech variables, the function <code>trough</code> acts on this data frame as a matrix of coefficients and returns one trough point per row.
We have obtained the PD of the trough.</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb445-1"><a href="mpm.html#cb445-1" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&quot;schmettow/uncanny&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="mpm.html#cb446-1" aria-hidden="true" tabindex="-1"></a>P_trough <span class="ot">&lt;-</span></span>
<span id="cb446-2"><a href="mpm.html#cb446-2" aria-hidden="true" tabindex="-1"></a>  P_poly_3 <span class="sc">%&gt;%</span></span>
<span id="cb446-3"><a href="mpm.html#cb446-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(type <span class="sc">==</span> <span class="st">&quot;fixef&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb446-4"><a href="mpm.html#cb446-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(chain, iter, fixef, value) <span class="sc">%&gt;%</span></span>
<span id="cb446-5"><a href="mpm.html#cb446-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spread</span>(fixef, value) <span class="sc">%&gt;%</span></span>
<span id="cb446-6"><a href="mpm.html#cb446-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Intercept, <span class="fu">starts_with</span>(<span class="st">&quot;huMech&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb446-7"><a href="mpm.html#cb446-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trough =</span> uncanny<span class="sc">::</span><span class="fu">trough</span>(.)) <span class="sc">%&gt;%</span></span>
<span id="cb446-8"><a href="mpm.html#cb446-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="at">key =</span> parameter)</span></code></pre></div>
<p>This derived posterior distribution we can put it into a CLU form (Table <a href="mpm.html#tab:uncanny-4">5.18</a>) or plot it toegtehr with the estimated polynomial curve (Figure <a href="mpm.html#fig:uncanny-5">5.20</a>))</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="mpm.html#cb447-1" aria-hidden="true" tabindex="-1"></a>P_trough <span class="sc">%&gt;%</span></span>
<span id="cb447-2"><a href="mpm.html#cb447-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(parameter) <span class="sc">%&gt;%</span></span>
<span id="cb447-3"><a href="mpm.html#cb447-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb447-4"><a href="mpm.html#cb447-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">center =</span> <span class="fu">median</span>(value, <span class="at">na.rm =</span> T),</span>
<span id="cb447-5"><a href="mpm.html#cb447-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">lower =</span> <span class="fu">quantile</span>(value, .<span class="dv">025</span>, <span class="at">na.rm =</span> T),</span>
<span id="cb447-6"><a href="mpm.html#cb447-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">upper =</span> <span class="fu">quantile</span>(value, .<span class="dv">975</span>, <span class="at">na.rm =</span> T)</span>
<span id="cb447-7"><a href="mpm.html#cb447-7" aria-hidden="true" tabindex="-1"></a>  ) </span></code></pre></div>
<table>
<caption><span id="tab:uncanny-4">Table 5.18: </span>Polynomial coefficient table with 95 percent credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">huMech_1</td>
<td align="right">0.149</td>
<td align="right">-0.322</td>
<td align="right">0.588</td>
</tr>
<tr class="even">
<td align="left">huMech_2</td>
<td align="right">-1.092</td>
<td align="right">-2.006</td>
<td align="right">-0.086</td>
</tr>
<tr class="odd">
<td align="left">huMech_3</td>
<td align="right">0.919</td>
<td align="right">0.282</td>
<td align="right">1.499</td>
</tr>
<tr class="even">
<td align="left">Intercept</td>
<td align="right">-0.449</td>
<td align="right">-0.516</td>
<td align="right">-0.378</td>
</tr>
<tr class="odd">
<td align="left">trough</td>
<td align="right">0.715</td>
<td align="right">0.650</td>
<td align="right">0.801</td>
</tr>
</tbody>
</table>
<p>The 95% CI is a conventional measure of uncertainty and may be more or less irrelevant.
The most generous display on uncertainty is a density plot on the full posterior.
The density function just smooths over the frequency distribution of trough draws, but makes no arbitrary choices on where to cut it.</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="mpm.html#cb448-1" aria-hidden="true" tabindex="-1"></a>RK_2<span class="sc">$</span>M_poly_3 <span class="ot">&lt;-</span> <span class="fu">predict</span>(M_poly_3)<span class="sc">$</span>center</span>
<span id="cb448-2"><a href="mpm.html#cb448-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb448-3"><a href="mpm.html#cb448-3" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(</span>
<span id="cb448-4"><a href="mpm.html#cb448-4" aria-hidden="true" tabindex="-1"></a>  RK_2 <span class="sc">%&gt;%</span></span>
<span id="cb448-5"><a href="mpm.html#cb448-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> huMech, <span class="at">y =</span> avg_like)) <span class="sc">+</span></span>
<span id="cb448-6"><a href="mpm.html#cb448-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">size =</span> .<span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb448-7"><a href="mpm.html#cb448-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="fu">aes</span>(<span class="at">y =</span> M_poly_3), <span class="at">se =</span> F),</span>
<span id="cb448-8"><a href="mpm.html#cb448-8" aria-hidden="true" tabindex="-1"></a>  P_trough <span class="sc">%&gt;%</span></span>
<span id="cb448-9"><a href="mpm.html#cb448-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(parameter <span class="sc">==</span> <span class="st">&quot;trough&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb448-10"><a href="mpm.html#cb448-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value)) <span class="sc">+</span></span>
<span id="cb448-11"><a href="mpm.html#cb448-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb448-12"><a href="mpm.html#cb448-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb448-13"><a href="mpm.html#cb448-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">heights =</span> <span class="fu">c</span>(.<span class="dv">7</span>, .<span class="dv">3</span>)</span>
<span id="cb448-14"><a href="mpm.html#cb448-14" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="figure"><span id="fig:uncanny-5"></span>
<img src="Classic_linear_models_files/figure-html/uncanny-5-1.png" alt="Estimated Uncanny Valley curve and position of trough" width="90%" />
<p class="caption">
Figure 5.20: Estimated Uncanny Valley curve and position of trough
</p>
</div>
<p>With reasonable certainty, we can say that the trough is at approximately two-thirds of the huMech score range.
<!-- #80 --> In contrast, the illustration of the uncanny valley as they used to be perpetuated from the original source, place the trough at about four quarters of the scale.
The Uncanny Valley effect sets in “earlier” than I thought, at least.</p>
<p>A closer look at the scatterplot above reveals a problem with the data set: It seems that data is sparsest right where the valley is deepest.
Since there also is a lot of noise, the concern is that there actually is no trough.
This can be tested on the same posterior.
The <code>uncanny::trough</code> function returns a missing value, when no minimum stationary point could be found.
Hence, the proportion of non-<code>NA</code>s is the certainty we have that a trough exists:</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="mpm.html#cb449-1" aria-hidden="true" tabindex="-1"></a>cert_trough <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(<span class="fu">is.na</span>(P_trough))</span>
<span id="cb449-2"><a href="mpm.html#cb449-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Certainty that trough exists:&quot;</span>, cert_trough)</span></code></pre></div>
<pre><code>## Certainty that trough exists: 1</code></pre>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<div id="test-stat" class="section level3" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Make yourself a test statistic</h3>
<p>Generally, in design research we are interested in real world impact and this book takes a strictly quantitative stance.
Rather than testing the hypothesis whether any effect exists or not, we interpret coefficients by making statements on their magnitude and uncertainty.</p>
<p>We evaluated the position of the local minimum, the trough.
The theory goes that the Uncanny Valley effect is a disruption of a slow upwards trend, the disruption creates the shoulder and culminates in the trough.
But, there is no single coefficient telling us directly that there actually are a shoulder and a trough.</p>
<!-- Wouldn't it be very bold if we could just say: "With a probability of ... there is a trough and a shoulder". -->
<!-- Recall how we If it does not exist, we would see no mini  In the Uncanny Valley case, we have estimated a cubic polynomial and found the stationary points in the first (cubic) derivative and identified the valley in the second (straight) derivative <a href="mpm.html#prm">5.5</a>. Then, we applied the valley-finding procedure on the level of individual MCMC draws and added credibility limits  to population average trough. -->
<p>Polynomial theory tells us that a cubic function <em>can</em> have two stationary points, but it can also just have one or zero.
After all, straight line is a cubic, too, if we set the quadratic and cubic coefficients to zero. But that would mean that teh UNcanny Valley effect does not exist. If we run our MCMC chains long enough, they will visit spots in parameter space, where <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\beta_3\)</span> are close to zero, or: areas where the cubic coefficient dominates, and there is just is a saddle.</p>
<p>When a cubic model is estimated, the MCMC walk makes random visits in a four-dimensional coefficient space <a href="lm.html#random-walk">4.1.1</a> (five-dimensional, if we count the error variance).
These coordinates are stored <em>per iteration</em> in a posterior distribution object.
Every iteration represents one possible polynomial, as is illustrated in Figure <a href="mpm.html#fig:uncanny-6">5.21</a>.</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="mpm.html#cb452-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Uncanny)</span></code></pre></div>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="mpm.html#cb453-1" aria-hidden="true" tabindex="-1"></a><span class="fu">post_pred</span>(M_poly_3, <span class="at">thin =</span> <span class="dv">100</span>) <span class="sc">%&gt;%</span></span>
<span id="cb453-2"><a href="mpm.html#cb453-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(RK_2, <span class="at">by =</span> <span class="st">&quot;Obs&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb453-3"><a href="mpm.html#cb453-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> huMech, <span class="at">y =</span> value, <span class="at">group =</span> iter)) <span class="sc">+</span></span>
<span id="cb453-4"><a href="mpm.html#cb453-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_smooth</span>(<span class="at">geom =</span> <span class="st">&quot;line&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span id="fig:uncanny-6"></span>
<img src="Classic_linear_models_files/figure-html/uncanny-6-1.png" alt="MCMC estimation produces one polynomial per sample (40 shown)" width="90%" />
<p class="caption">
Figure 5.21: MCMC estimation produces one polynomial per sample (40 shown)
</p>
</div>
<!-- Note that -->
<!-- + the posterior predictive distribution is combined with the original data by a join operation. A simple `bind_rows` won't do it. The join uses `Obs` as key and this is the reason why I make these identifiers for all data tables and fitted reponses. -->
<!-- + although speaking of coefficients, I lazily use the fitted responses to draw the curves. -->
<p>All we have to do is count the number of MCMC visits, that have a trough and a shoulder.
The function <code>trough</code> in the Uncanny package (on Github) is designed to return the position, when it exists and returns <code>NA</code> otherwise.
The same goes for the function <code>shoulder</code>, which finds the local maximum.</p>
<p>With these two functions, we can create a test statistics, by counting how many of the MCMC draws represent a cubic polynomial <em>with</em> shoulder and trough.</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="mpm.html#cb454-1" aria-hidden="true" tabindex="-1"></a><span class="co"># devtools::install_github(&quot;schmettow/uncanny&quot;)</span></span>
<span id="cb454-2"><a href="mpm.html#cb454-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(uncanny)</span>
<span id="cb454-3"><a href="mpm.html#cb454-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb454-4"><a href="mpm.html#cb454-4" aria-hidden="true" tabindex="-1"></a>P_wide <span class="ot">&lt;-</span></span>
<span id="cb454-5"><a href="mpm.html#cb454-5" aria-hidden="true" tabindex="-1"></a>  P_poly_3 <span class="sc">%&gt;%</span></span>
<span id="cb454-6"><a href="mpm.html#cb454-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(type <span class="sc">==</span> <span class="st">&quot;fixef&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb454-7"><a href="mpm.html#cb454-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># as_tibble() %&gt;%</span></span>
<span id="cb454-8"><a href="mpm.html#cb454-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(iter, parameter, value) <span class="sc">%&gt;%</span></span>
<span id="cb454-9"><a href="mpm.html#cb454-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spread</span>(<span class="at">key =</span> parameter, <span class="at">value =</span> value) <span class="sc">%&gt;%</span></span>
<span id="cb454-10"><a href="mpm.html#cb454-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Intercept, <span class="fu">starts_with</span>(<span class="st">&quot;huMech&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb454-11"><a href="mpm.html#cb454-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb454-12"><a href="mpm.html#cb454-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">trough =</span> uncanny<span class="sc">::</span><span class="fu">trough</span>(.),</span>
<span id="cb454-13"><a href="mpm.html#cb454-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">shoulder =</span> uncanny<span class="sc">::</span><span class="fu">shoulder</span>(.),</span>
<span id="cb454-14"><a href="mpm.html#cb454-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">is_Uncanny =</span> <span class="sc">!</span><span class="fu">is.na</span>(trough) <span class="sc">&amp;</span> <span class="sc">!</span><span class="fu">is.na</span>(shoulder)</span>
<span id="cb454-15"><a href="mpm.html#cb454-15" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb454-16"><a href="mpm.html#cb454-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb454-17"><a href="mpm.html#cb454-17" aria-hidden="true" tabindex="-1"></a>cert_Uncanny <span class="ot">&lt;-</span> <span class="fu">mean</span>(P_wide<span class="sc">$</span>is_Uncanny)</span>
<span id="cb454-18"><a href="mpm.html#cb454-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb454-19"><a href="mpm.html#cb454-19" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The Uncanny Valley exists with a certainty of: &quot;</span>, cert_Uncanny)</span></code></pre></div>
<pre><code>## The Uncanny Valley exists with a certainty of:  0.999</code></pre>
<p>So, with the our data we can be pretty sure that the Uncanny Valley effect is present.
Probably, there is a very tiny chance that it does not exist, which we would only catch by increasing the resolution of the posterior, i.e. running more MCMC iterations.
This conclusion is remarkable also from a philosophy-of-science perspective.
It was in 1970, when Masahiro Mori published his theory on the relation between human likeness and emotional response.
Fifty years later this article is all but outdated in how lucidly it anticipates the emerge of human-like robots and virtual characters <span class="citation">(Mori 1970)</span> .
What can surprise the modern reader in Social Sciences is how the article abruptly stops, right where we would expect the experimental part confirming the theory.</p>
<p>It almost seems that Mori’s theory sprang just from his own feelings, and he just left it at that. Introspection as a scientific method is likely to give seasoned researcher another uncanny feeling. But that would be unjust! In the first place, Mori made on observation his own inner world and assumed that others would feel the same. Once the world was ready it, Mori’s theory turned out to provable and immensely useful for design.</p>
<p>Still, I argue that we have not yet fully confirmed Mori’s theory.
Strictly spoken, the data of Mathur &amp; Reichling only prove that <em>on average</em> the effect exists, because we aggregated scores over participants
It would be much stronger to state: <em>everyone</em> experiences the Uncanny Valley.
In essence, we could estimate the same cubic models, but <em>one per participant</em>.
That requires non-aggregated data, because the analysis of very participant requires the data from every participant.
The next chapter will introduce <em>multi-level models</em>, which can simultaneously estimate a model on population level and participant level.
At the end of the following chapter, we will return to the Uncanny Valley with more data to feed our chains.
Spoiler alert: the Uncanny Valley effect could be <em>universal</em>.</p>
</div>
</div>
<div id="further-readings-1" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Further readings</h2>
<ol style="list-style-type: decimal">
<li>The original paper on the IPump case illustrates how Bayesian analysis can be reported in scientific journals <span class="citation">(Schmettow, Schnittker, and Schraagen 2017)</span>.</li>
<li>Our Uncanny Valley experiment is described in detail in <span class="citation">(Koopman 2019)</span>.</li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mlm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsubsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
