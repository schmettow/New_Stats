<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Elements of Bayesian statistics | New statistics for the design researcher</title>
  <meta name="description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works hard to make this world a better place." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Elements of Bayesian statistics | New statistics for the design researcher" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works hard to make this world a better place." />
  <meta name="github-repo" content="schmettow/New_Stats" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Elements of Bayesian statistics | New statistics for the design researcher" />
  
  <meta name="twitter:description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works hard to make this world a better place." />
  

<meta name="author" content="Martin Schmettow" />


<meta name="date" content="2020-08-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="getting-started-r.html"/>
<link rel="next" href="linear-models.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="part"><span><b>I Preparations</b></span></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#whom-this-book-is-for"><i class="fa fa-check"></i><b>1.1</b> Whom this book is for</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#the-empirical-design-researcher"><i class="fa fa-check"></i><b>1.1.1</b> The empirical design researcher</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#the-experimentalist"><i class="fa fa-check"></i><b>1.1.2</b> The experimentalist</a></li>
<li class="chapter" data-level="1.1.3" data-path="index.html"><a href="index.html#the-applied-researcher"><i class="fa fa-check"></i><b>1.1.3</b> The applied researcher</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#assumptions"><i class="fa fa-check"></i><b>1.2</b> Assumptions</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i><b>1.3</b> How to read this book</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#quantitative-design-research"><i class="fa fa-check"></i><b>1.4</b> Quantitative design research</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#studies"><i class="fa fa-check"></i><b>1.5</b> Studies</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#observations-and-measurestbc"><i class="fa fa-check"></i><b>1.6</b> Observations and measures[TBC]</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#interaction-sequences"><i class="fa fa-check"></i><b>1.6.1</b> Interaction sequences</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#performance-measures"><i class="fa fa-check"></i><b>1.6.2</b> performance measures</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#experience-tbd"><i class="fa fa-check"></i><b>1.6.3</b> experience [TBD]</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#design-features-tbd"><i class="fa fa-check"></i><b>1.6.4</b> design features [TBD]</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#the-human-factor-tbd"><i class="fa fa-check"></i><b>1.6.5</b> the human factor [TBD]</a></li>
<li class="chapter" data-level="1.6.6" data-path="index.html"><a href="index.html#situations-tbd"><i class="fa fa-check"></i><b>1.6.6</b> situations [TBD]</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="elements-of-bayesian-statistics.html"><a href="elements-of-bayesian-statistics.html"><i class="fa fa-check"></i><b>2</b> Elements of Bayesian Statistics</a></li>
<li class="chapter" data-level="3" data-path="getting-started-r.html"><a href="getting-started-r.html"><i class="fa fa-check"></i><b>3</b> Getting started with R</a><ul>
<li class="chapter" data-level="3.1" data-path="getting-started-r.html"><a href="getting-started-r.html#setting-up-the-r-environment"><i class="fa fa-check"></i><b>3.1</b> Setting up the R environment</a><ul>
<li class="chapter" data-level="3.1.1" data-path="getting-started-r.html"><a href="getting-started-r.html#installing-cran-packages"><i class="fa fa-check"></i><b>3.1.1</b> Installing CRAN packages</a></li>
<li class="chapter" data-level="3.1.2" data-path="getting-started-r.html"><a href="getting-started-r.html#installing-packages-from-github"><i class="fa fa-check"></i><b>3.1.2</b> Installing packages from Github</a></li>
<li class="chapter" data-level="3.1.3" data-path="getting-started-r.html"><a href="getting-started-r.html#first_program"><i class="fa fa-check"></i><b>3.1.3</b> A first statistical program</a></li>
<li class="chapter" data-level="3.1.4" data-path="getting-started-r.html"><a href="getting-started-r.html#bibliographic-notes"><i class="fa fa-check"></i><b>3.1.4</b> Bibliographic notes</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="getting-started-r.html"><a href="getting-started-r.html#learning-r-a-primer"><i class="fa fa-check"></i><b>3.2</b> Learning R: a primer</a><ul>
<li class="chapter" data-level="3.2.1" data-path="getting-started-r.html"><a href="getting-started-r.html#assigning-and-calling-objects"><i class="fa fa-check"></i><b>3.2.1</b> Assigning and calling Objects</a></li>
<li class="chapter" data-level="3.2.2" data-path="getting-started-r.html"><a href="getting-started-r.html#vectors"><i class="fa fa-check"></i><b>3.2.2</b> Vectors</a></li>
<li class="chapter" data-level="3.2.3" data-path="getting-started-r.html"><a href="getting-started-r.html#basic-object-types"><i class="fa fa-check"></i><b>3.2.3</b> Basic object types</a></li>
<li class="chapter" data-level="3.2.4" data-path="getting-started-r.html"><a href="getting-started-r.html#operators-and-functions"><i class="fa fa-check"></i><b>3.2.4</b> Operators and functions</a></li>
<li class="chapter" data-level="3.2.5" data-path="getting-started-r.html"><a href="getting-started-r.html#storing-data-in-data-frames"><i class="fa fa-check"></i><b>3.2.5</b> Storing data in data frames</a></li>
<li class="chapter" data-level="3.2.6" data-path="getting-started-r.html"><a href="getting-started-r.html#import-export-and-archiving"><i class="fa fa-check"></i><b>3.2.6</b> Import, export and archiving</a></li>
<li class="chapter" data-level="3.2.7" data-path="getting-started-r.html"><a href="getting-started-r.html#case-environments"><i class="fa fa-check"></i><b>3.2.7</b> Case environments</a></li>
<li class="chapter" data-level="3.2.8" data-path="getting-started-r.html"><a href="getting-started-r.html#structuring-data"><i class="fa fa-check"></i><b>3.2.8</b> Structuring data</a></li>
<li class="chapter" data-level="3.2.9" data-path="getting-started-r.html"><a href="getting-started-r.html#data-transformation"><i class="fa fa-check"></i><b>3.2.9</b> Data transformation</a></li>
<li class="chapter" data-level="3.2.10" data-path="getting-started-r.html"><a href="getting-started-r.html#plotting-data"><i class="fa fa-check"></i><b>3.2.10</b> Plotting data</a></li>
<li class="chapter" data-level="3.2.11" data-path="getting-started-r.html"><a href="getting-started-r.html#fitting-regression-models"><i class="fa fa-check"></i><b>3.2.11</b> Fitting regression models</a></li>
<li class="chapter" data-level="3.2.12" data-path="getting-started-r.html"><a href="getting-started-r.html#knitting-statistical-reports"><i class="fa fa-check"></i><b>3.2.12</b> Knitting statistical reports</a></li>
<li class="chapter" data-level="3.2.13" data-path="getting-started-r.html"><a href="getting-started-r.html#exercises"><i class="fa fa-check"></i><b>3.2.13</b> Exercises</a></li>
<li class="chapter" data-level="3.2.14" data-path="getting-started-r.html"><a href="getting-started-r.html#bibliographic-notes-1"><i class="fa fa-check"></i><b>3.2.14</b> Bibliographic notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html"><i class="fa fa-check"></i><b>4</b> Elements of Bayesian statistics</a><ul>
<li class="chapter" data-level="4.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#decision_making"><i class="fa fa-check"></i><b>4.1</b> Rational decision making in design research</a><ul>
<li class="chapter" data-level="4.1.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#measuring-uncertainty"><i class="fa fa-check"></i><b>4.1.1</b> Measuring uncertainty</a></li>
<li class="chapter" data-level="4.1.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#benchmarking-designs"><i class="fa fa-check"></i><b>4.1.2</b> Benchmarking designs</a></li>
<li class="chapter" data-level="4.1.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#comparison-of-designs"><i class="fa fa-check"></i><b>4.1.3</b> Comparison of designs</a></li>
<li class="chapter" data-level="4.1.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#prior-knowledge"><i class="fa fa-check"></i><b>4.1.4</b> Prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#descriptive_stats"><i class="fa fa-check"></i><b>4.2</b> Descriptive statistics</a><ul>
<li class="chapter" data-level="4.2.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#frequencies"><i class="fa fa-check"></i><b>4.2.1</b> Frequencies</a></li>
<li class="chapter" data-level="4.2.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#central-tendency"><i class="fa fa-check"></i><b>4.2.2</b> Central tendency</a></li>
<li class="chapter" data-level="4.2.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#dispersion"><i class="fa fa-check"></i><b>4.2.3</b> Dispersion</a></li>
<li class="chapter" data-level="4.2.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#associations"><i class="fa fa-check"></i><b>4.2.4</b> Associations</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayesian-probability-theory"><i class="fa fa-check"></i><b>4.3</b> Bayesian probability theory</a><ul>
<li class="chapter" data-level="4.3.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#some-set-theory"><i class="fa fa-check"></i><b>4.3.1</b> Some set theory</a></li>
<li class="chapter" data-level="4.3.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#probability"><i class="fa fa-check"></i><b>4.3.2</b> Probability</a></li>
<li class="chapter" data-level="4.3.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#likelihood"><i class="fa fa-check"></i><b>4.3.3</b> Likelihood</a></li>
<li class="chapter" data-level="4.3.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayesian-and-frequentist-probability"><i class="fa fa-check"></i><b>4.3.4</b> Bayesian and frequentist probability</a></li>
<li class="chapter" data-level="4.3.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayes-theorem"><i class="fa fa-check"></i><b>4.3.5</b> Bayes theorem</a></li>
<li class="chapter" data-level="4.3.6" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#the-dynamics-of-belief"><i class="fa fa-check"></i><b>4.3.6</b> The dynamics of belief</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#statmod"><i class="fa fa-check"></i><b>4.4</b> Statistical models</a><ul>
<li class="chapter" data-level="4.4.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#predictions-and-likelihood"><i class="fa fa-check"></i><b>4.4.1</b> Predictions and likelihood</a></li>
<li class="chapter" data-level="4.4.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#distributions"><i class="fa fa-check"></i><b>4.4.2</b> Distributions: shapes of randomness</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayesian-estimation"><i class="fa fa-check"></i><b>4.5</b> Bayesian estimation</a></li>
</ul></li>
<li class="part"><span><b>II Models</b></span></li>
<li class="chapter" data-level="5" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>5</b> Linear models</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-models.html"><a href="linear-models.html#quantification-at-work-grand-mean-models"><i class="fa fa-check"></i><b>5.1</b> Quantification at work: grand mean models</a><ul>
<li class="chapter" data-level="5.1.1" data-path="linear-models.html"><a href="linear-models.html#likelihood-and-random-term"><i class="fa fa-check"></i><b>5.1.1</b> Likelihood and random term</a></li>
<li class="chapter" data-level="5.1.2" data-path="linear-models.html"><a href="linear-models.html#working-with-the-posterior-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Working with the posterior distribution</a></li>
<li class="chapter" data-level="5.1.3" data-path="linear-models.html"><a href="linear-models.html#center-and-interval-estimates"><i class="fa fa-check"></i><b>5.1.3</b> Center and interval estimates</a></li>
<li class="chapter" data-level="5.1.4" data-path="linear-models.html"><a href="linear-models.html#random_walk"><i class="fa fa-check"></i><b>5.1.4</b> Do the random walk: Markov Chain Monte Carlo sampling</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="linear-models.html"><a href="linear-models.html#linear-regression"><i class="fa fa-check"></i><b>5.2</b> Walk the line: linear regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="linear-models.html"><a href="linear-models.html#transforming-measures"><i class="fa fa-check"></i><b>5.2.1</b> Transforming measures</a></li>
<li class="chapter" data-level="5.2.2" data-path="linear-models.html"><a href="linear-models.html#correlations"><i class="fa fa-check"></i><b>5.2.2</b> Correlations</a></li>
<li class="chapter" data-level="5.2.3" data-path="linear-models.html"><a href="linear-models.html#endlessly-linear"><i class="fa fa-check"></i><b>5.2.3</b> Endlessly linear</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="linear-models.html"><a href="linear-models.html#factorial-models"><i class="fa fa-check"></i><b>5.3</b> Factorial Models</a><ul>
<li class="chapter" data-level="5.3.1" data-path="linear-models.html"><a href="linear-models.html#CGM"><i class="fa fa-check"></i><b>5.3.1</b> A versus B: Comparison of groups</a></li>
<li class="chapter" data-level="5.3.2" data-path="linear-models.html"><a href="linear-models.html#dummy_variables"><i class="fa fa-check"></i><b>5.3.2</b> Not stupid: dummy variables</a></li>
<li class="chapter" data-level="5.3.3" data-path="linear-models.html"><a href="linear-models.html#treament_contrasts"><i class="fa fa-check"></i><b>5.3.3</b> Treatment contrast</a></li>
<li class="chapter" data-level="5.3.4" data-path="linear-models.html"><a href="linear-models.html#AMM"><i class="fa fa-check"></i><b>5.3.4</b> Absolute Means Model</a></li>
<li class="chapter" data-level="5.3.5" data-path="linear-models.html"><a href="linear-models.html#OFM"><i class="fa fa-check"></i><b>5.3.5</b> Ordered Factors Models</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-models.html"><a href="linear-models.html#MPM"><i class="fa fa-check"></i><b>5.4</b> Putting it all together: multi predictor models</a><ul>
<li class="chapter" data-level="5.4.1" data-path="linear-models.html"><a href="linear-models.html#MRM"><i class="fa fa-check"></i><b>5.4.1</b> On surface: multiple regression models</a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-models.html"><a href="linear-models.html#MFM"><i class="fa fa-check"></i><b>5.4.2</b> Crossover: multifactorial models</a></li>
<li class="chapter" data-level="5.4.3" data-path="linear-models.html"><a href="linear-models.html#GRM"><i class="fa fa-check"></i><b>5.4.3</b> Line-by-line: grouped regression models</a></li>
<li class="chapter" data-level="5.4.4" data-path="linear-models.html"><a href="linear-models.html#empirical-versus-statistical-control"><i class="fa fa-check"></i><b>5.4.4</b> Empirical versus statistical control</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-models.html"><a href="linear-models.html#CFXM"><i class="fa fa-check"></i><b>5.5</b> Conditional effects models</a><ul>
<li class="chapter" data-level="5.5.1" data-path="linear-models.html"><a href="linear-models.html#CMRM"><i class="fa fa-check"></i><b>5.5.1</b> Conditional multiple regression</a></li>
<li class="chapter" data-level="5.5.2" data-path="linear-models.html"><a href="linear-models.html#CMFM"><i class="fa fa-check"></i><b>5.5.2</b> Conditional multifactorial models</a></li>
<li class="chapter" data-level="5.5.3" data-path="linear-models.html"><a href="linear-models.html#hitting-the-boundaries-of-saturation"><i class="fa fa-check"></i><b>5.5.3</b> Hitting the boundaries of saturation</a></li>
<li class="chapter" data-level="5.5.4" data-path="linear-models.html"><a href="linear-models.html#more-than-the-sum-amplification"><i class="fa fa-check"></i><b>5.5.4</b> More than the sum: amplification</a></li>
<li class="chapter" data-level="5.5.5" data-path="linear-models.html"><a href="linear-models.html#conditional-effects-and-theory-zap-me"><i class="fa fa-check"></i><b>5.5.5</b> Conditional effects and theory [ZAP ME]</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="linear-models.html"><a href="linear-models.html#PRM"><i class="fa fa-check"></i><b>5.6</b> Doing the rollercoaster: polynomial regression models</a><ul>
<li class="chapter" data-level="5.6.1" data-path="linear-models.html"><a href="linear-models.html#make-yourself-a-test-statistic"><i class="fa fa-check"></i><b>5.6.1</b> Make yourself a test statistic</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="MLM.html"><a href="MLM.html"><i class="fa fa-check"></i><b>6</b> Multilevel models</a><ul>
<li class="chapter" data-level="6.1" data-path="MLM.html"><a href="MLM.html#the-human-factor"><i class="fa fa-check"></i><b>6.1</b> The Human Factor: Intercept random effects</a></li>
<li class="chapter" data-level="6.2" data-path="MLM.html"><a href="MLM.html#slope-random-effects"><i class="fa fa-check"></i><b>6.2</b> Slope random effects: variance in change</a></li>
<li class="chapter" data-level="6.3" data-path="MLM.html"><a href="MLM.html#thinking-multi-level"><i class="fa fa-check"></i><b>6.3</b> Thinking multi-level</a></li>
<li class="chapter" data-level="6.4" data-path="MLM.html"><a href="MLM.html#testing-universality-of-theories"><i class="fa fa-check"></i><b>6.4</b> Testing universality of theories</a></li>
<li class="chapter" data-level="6.5" data-path="MLM.html"><a href="MLM.html#non-human-populations"><i class="fa fa-check"></i><b>6.5</b> Non-human populations and cross-overs</a></li>
<li class="chapter" data-level="6.6" data-path="MLM.html"><a href="MLM.html#nested-random-effects"><i class="fa fa-check"></i><b>6.6</b> Nested random effects</a></li>
<li class="chapter" data-level="6.7" data-path="MLM.html"><a href="MLM.html#what-are-random-effects-on-pooling-and-shrinkage"><i class="fa fa-check"></i><b>6.7</b> What are random effects? On pooling and shrinkage</a></li>
<li class="chapter" data-level="6.8" data-path="MLM.html"><a href="MLM.html#psychometrics-and-designometric-models"><i class="fa fa-check"></i><b>6.8</b> Psychometrics and designometric models</a><ul>
<li class="chapter" data-level="6.8.1" data-path="MLM.html"><a href="MLM.html#coverage"><i class="fa fa-check"></i><b>6.8.1</b> Coverage</a></li>
<li class="chapter" data-level="6.8.2" data-path="MLM.html"><a href="MLM.html#reliability"><i class="fa fa-check"></i><b>6.8.2</b> Reliability</a></li>
<li class="chapter" data-level="6.8.3" data-path="MLM.html"><a href="MLM.html#validity"><i class="fa fa-check"></i><b>6.8.3</b> Validity</a></li>
<li class="chapter" data-level="6.8.4" data-path="MLM.html"><a href="MLM.html#design-o-metrix"><i class="fa fa-check"></i><b>6.8.4</b> Design-o-metrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="GLM.html"><a href="GLM.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="GLM.html"><a href="GLM.html#glm_concepts"><i class="fa fa-check"></i><b>7.1</b> Elements of Generalized Linear Models</a><ul>
<li class="chapter" data-level="7.1.1" data-path="GLM.html"><a href="GLM.html#re-linking-linearity-relinking_linearity-tbc"><i class="fa fa-check"></i><b>7.1.1</b> Re-linking linearity (#relinking_linearity) [TBC]</a></li>
<li class="chapter" data-level="7.1.2" data-path="GLM.html"><a href="GLM.html#choosing-patterns-of-randomness-choosing_randomness"><i class="fa fa-check"></i><b>7.1.2</b> Choosing patterns of randomness (#choosing_randomness)</a></li>
<li class="chapter" data-level="7.1.3" data-path="GLM.html"><a href="GLM.html#mean-variance-relationship"><i class="fa fa-check"></i><b>7.1.3</b> Mean-variance relationship</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="GLM.html"><a href="GLM.html#count_data"><i class="fa fa-check"></i><b>7.2</b> Count data</a><ul>
<li class="chapter" data-level="7.2.1" data-path="GLM.html"><a href="GLM.html#poisson-regression"><i class="fa fa-check"></i><b>7.2.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="GLM.html"><a href="GLM.html#logistic-aka-binomial-regression-logistic_regression"><i class="fa fa-check"></i><b>7.2.2</b> Logistic (aka Binomial) regression (#logistic_regression)</a></li>
<li class="chapter" data-level="7.2.3" data-path="GLM.html"><a href="GLM.html#modelling-overdispersion"><i class="fa fa-check"></i><b>7.2.3</b> Modelling overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="GLM.html"><a href="GLM.html#measures_of_time"><i class="fa fa-check"></i><b>7.3</b> Measures of time</a><ul>
<li class="chapter" data-level="7.3.1" data-path="GLM.html"><a href="GLM.html#exponential-and-gamma-regression"><i class="fa fa-check"></i><b>7.3.1</b> Exponential and Gamma regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="GLM.html"><a href="GLM.html#exgaussian-regression"><i class="fa fa-check"></i><b>7.3.2</b> ExGaussian regression</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="GLM.html"><a href="GLM.html#rating-scales"><i class="fa fa-check"></i><b>7.4</b> Rating scales</a><ul>
<li class="chapter" data-level="7.4.1" data-path="GLM.html"><a href="GLM.html#ordered-logistic-regression"><i class="fa fa-check"></i><b>7.4.1</b> Ordered logistic regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="GLM.html"><a href="GLM.html#beta-regression"><i class="fa fa-check"></i><b>7.4.2</b> Beta regression</a></li>
<li class="chapter" data-level="7.4.3" data-path="GLM.html"><a href="GLM.html#distributional-models"><i class="fa fa-check"></i><b>7.4.3</b> Distributional models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ME.html"><a href="ME.html"><i class="fa fa-check"></i><b>8</b> Working with models</a><ul>
<li class="chapter" data-level="8.1" data-path="ME.html"><a href="ME.html#model-criticism"><i class="fa fa-check"></i><b>8.1</b> Model criticism</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ME.html"><a href="ME.html#residual-analysis"><i class="fa fa-check"></i><b>8.1.1</b> Residual analysis</a></li>
<li class="chapter" data-level="8.1.2" data-path="ME.html"><a href="ME.html#fitted-responses-analysis"><i class="fa fa-check"></i><b>8.1.2</b> Fitted responses analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ME.html"><a href="ME.html#model-selection"><i class="fa fa-check"></i><b>8.2</b> Model selection</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ME.html"><a href="ME.html#the-problem-of-over-fitting"><i class="fa fa-check"></i><b>8.2.1</b> The problem of over-fitting</a></li>
<li class="chapter" data-level="8.2.2" data-path="ME.html"><a href="ME.html#cross-validation-and-loo"><i class="fa fa-check"></i><b>8.2.2</b> Cross validation and LOO</a></li>
<li class="chapter" data-level="8.2.3" data-path="ME.html"><a href="ME.html#information-criteria"><i class="fa fa-check"></i><b>8.2.3</b> Information Criteria</a></li>
<li class="chapter" data-level="8.2.4" data-path="ME.html"><a href="ME.html#choosing-response-distributions"><i class="fa fa-check"></i><b>8.2.4</b> Choosing response distributions</a></li>
<li class="chapter" data-level="8.2.5" data-path="ME.html"><a href="ME.html#testing-theories"><i class="fa fa-check"></i><b>8.2.5</b> Testing theories</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="reflections.html"><a href="reflections.html"><i class="fa fa-check"></i><b>9</b> Reflections</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">New statistics for the design researcher</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian_statistics" class="section level1">
<h1><span class="header-section-number">4</span> Elements of Bayesian statistics</h1>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" title="1">knitr<span class="op">::</span>opts_chunk<span class="op">$</span><span class="kw">set</span>(<span class="dt">include =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb129-2" title="2"><span class="kw">source</span>(<span class="st">&quot;RMDR.R&quot;</span>)</a></code></pre></div>
<pre><code>## Warning: package &#39;lme4&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## Warning: package &#39;brms&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Loading required package: Rcpp</code></pre>
<pre><code>## Warning: package &#39;Rcpp&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Loading &#39;brms&#39; package (version 2.12.0). Useful instructions
## can be found by typing help(&#39;brms&#39;). A more detailed introduction
## to the package is available through vignette(&#39;brms_overview&#39;).</code></pre>
<pre><code>## 
## Attaching package: &#39;brms&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:lme4&#39;:
## 
##     ngrps</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     ar</code></pre>
<pre><code>## Warning: package &#39;rstanarm&#39; was built under R version 3.5.3</code></pre>
<pre><code>## rstanarm (Version 2.19.3, packaged: 2020-02-11 05:16:41 UTC)</code></pre>
<pre><code>## - Do not expect the default priors to remain the same in future rstanarm versions.</code></pre>
<pre><code>## Thus, R scripts should specify priors explicitly, even if they are just the defaults.</code></pre>
<pre><code>## - For execution on a local, multicore CPU with excess RAM we recommend calling</code></pre>
<pre><code>## options(mc.cores = parallel::detectCores())</code></pre>
<pre><code>## - bayesplot theme set to bayesplot::theme_default()</code></pre>
<pre><code>##    * Does _not_ affect other ggplot2 plots</code></pre>
<pre><code>##    * See ?bayesplot_theme_set for details on theme setting</code></pre>
<pre><code>## 
## Attaching package: &#39;rstanarm&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:brms&#39;:
## 
##     dirichlet, exponential, get_y, lasso, loo_R2, ngrps</code></pre>
<pre><code>## Warning: package &#39;polynom&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Warning: package &#39;knitr&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Warning: package &#39;printr&#39; was built under R version 3.5.1</code></pre>
<pre><code>## Warning: package &#39;GGally&#39; was built under R version 3.5.1</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Warning: package &#39;DiagrammeR&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Warning: package &#39;plyr&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Warning: package &#39;tidyverse&#39; was built under R version 3.5.3</code></pre>
<pre><code>## -- Attaching packages -------------------------------------- tidyverse 1.3.0 --</code></pre>
<pre><code>## v tibble  2.1.3     v dplyr   0.8.3
## v tidyr   1.0.2     v stringr 1.4.0
## v readr   1.3.1     v forcats 0.4.0
## v purrr   0.3.3</code></pre>
<pre><code>## Warning: package &#39;tibble&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Warning: package &#39;tidyr&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Warning: package &#39;readr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;purrr&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Warning: package &#39;stringr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;forcats&#39; was built under R version 3.5.2</code></pre>
<pre><code>## -- Conflicts ----------------------------------------- tidyverse_conflicts() --
## x dplyr::arrange()   masks plyr::arrange()
## x dplyr::combine()   masks gridExtra::combine()
## x purrr::compact()   masks plyr::compact()
## x dplyr::count()     masks plyr::count()
## x tidyr::expand()    masks Matrix::expand()
## x dplyr::failwith()  masks plyr::failwith()
## x dplyr::filter()    masks stats::filter()
## x dplyr::id()        masks plyr::id()
## x dplyr::lag()       masks stats::lag()
## x dplyr::mutate()    masks plyr::mutate()
## x tidyr::pack()      masks Matrix::pack()
## x dplyr::rename()    masks plyr::rename()
## x dplyr::summarise() masks plyr::summarise()
## x dplyr::summarize() masks plyr::summarize()
## x tidyr::unpack()    masks Matrix::unpack()</code></pre>
<pre><code>## Warning: package &#39;haven&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Warning: package &#39;openxlsx&#39; was built under R version 3.5.3</code></pre>
<pre><code>## 
## Attaching package: &#39;mascutils&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     expand_grid</code></pre>
<pre><code>## The following object is masked from &#39;package:base&#39;:
## 
##     mode</code></pre>
<pre><code>## 
## Attaching package: &#39;bayr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:rstanarm&#39;:
## 
##     fixef, ranef</code></pre>
<pre><code>## The following objects are masked from &#39;package:brms&#39;:
## 
##     fixef, ranef</code></pre>
<pre><code>## The following objects are masked from &#39;package:lme4&#39;:
## 
##     fixef, ranef</code></pre>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" title="1">CE =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Rainfall&quot;</span>, <span class="st">&quot;Sec99&quot;</span>, <span class="st">&quot;IPump&quot;</span>, <span class="st">&quot;Rational&quot;</span>)</a>
<a class="sourceLine" id="cb178-2" title="2"><span class="kw">load_CE_</span>(CE)</a></code></pre></div>
<pre><code>## Loading case environment Rainfall</code></pre>
<pre><code>## Loading case environment Sec99</code></pre>
<pre><code>## Loading case environment IPump</code></pre>
<pre><code>## Loading case environment Rational</code></pre>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" title="1">purp.mcmc &lt;-<span class="st"> </span>T</a></code></pre></div>
<p>As human beings we make our decisions on what has happened to us in the past. For example, we trust a person or a company more, when we can look back at a series of successful transactions. And we have remarkable capability to recall what has just happened, but also what happened yesterday or years ago. By integrating over all the evidence, we form a view of the world we forage. When evidence is abundant, we vigorously experience a feeling of certainty, or lack of doubt. That is not to deny, that in a variety of situations, the boundedness of the human mind kicks in and we become terrible decision makers. This is for a variety of psychological reasons, to name just a few:</p>
<ul>
<li>forgetting evidence</li>
<li>the primacy effect: recent events get more weight</li>
<li>confirmation bias: evidence that supports a belief is actively sought for, counter-evidence gets ignored.</li>
<li>the hindsight bias: once a situation has taken a certain outcome, we believe that it had to happen that way.</li>
</ul>
<p>The very aim of scientific research is to avoid the pitfalls of our minds and act as rational as possible by translating our theory into a formal model, gathering evidence in an unbiased way and weigh the evidence by formal procedures. This weighing of evidence using data essentially is <em>statistical modeling</em> and statistical models in this book all produces two sorts of numbers: <em>magnitude of effects</em> and <em>level of certainty</em>. When a statistic is reported together with the strength of evidence, this is conventionally called an <em>inferential statistic</em>. In applied research, real world decisions depend on the evidence, which has two aspects: first, the strength of effects and the level of certainty we have reached.</p>
<p>Bayesian inferential statistics grounds on the idea of accumulating evidence along research. <em>Certainty</em> (or belief or credibility or credence) in Bayesian statistics is formalized as a <em>probability scale (0 = impossible, 1 = certain)</em>. Certainty is reached by looking at evidence and such evidence comes from two sources: everything that we already know about the subject and the data we just gathered. These sources are only seemingly different, because when new data is analyzed, a transition occurs from what you new before, <em>prior belief</em>, to what you know after seeing the data, <em>posterior belief</em>. In other words: by data the current belief gets an update.</p>
<p>Updating our beliefs is essential for acting in rational ways. The first section of this chapter is intended to tell the Big Picture. It puts statistics into the context of decision-making in design research. For those readers with a background in statistics, this section may be a sufficient introduction all by itself.</p>
<p>In the remainder of this chapter the essential concepts of statistics and Bayesian analysis will be introduced from ground up. First we will look at descriptive statistics, introducing analysis tools, such as summary statistics and statistical graphs. Descriptive statistics can be used effectively to explore data and prepare the statistical modelling, but they lack one important ingredient: information about uncertainty. For that, a sound understanding is required of what certainty is. In Bayesian thinking, certainty is a sort of probability and section [#BPT] first derives probability from set theory and relative frequencies. Then it goes on to explain basic concepts of statistical modeling, such as the likelihood and finishes with Bayes theorem, which does the calculation of the posterior certainty from prior knowledge and data. Despite the matter, I will make minimal use of mathematical formalism. Instead, I use R code as much as possible to illustrate the concepts. If you are not yet familiar with R, you may want to read chapter [#IntroR] first, or alongside.</p>
<p>Section [#] goes into the practical details of modelling. A statistical model is introduced by its two components: the structural part, which typically carries the research question or theory, is only briefly introduced, followed by a rather deep account of the second component of statistical models: the random part.</p>
<div id="decision_making" class="section level2">
<h2><span class="header-section-number">4.1</span> Rational decision making in design research</h2>
<ul>
<li>I see clouds. Should I take my umbrella?</li>
<li>Should I do this bungee jump? How many people came to death by jumping? (More or less than alpine skiing?) And how much fun is it really?</li>
<li>Overhauling our company website will cost us EUR 100.000. Is it worth it?</li>
</ul>
<p>All the above cases are examples of decision making under uncertainty. The actors aim for maximizing their outcome, be it well being, fun or money. But, they are uncertain about what will really happen. And their uncertainty occurs on two levels:</p>
<ol style="list-style-type: decimal">
<li>One cannot precisely foresee the exact outcome of one’s chosen action:</li>
</ol>
<ul>
<li>Taking the umbrella with you can have two consequences: if it rains, you have the benefit of staying dry. If it does not rain, you have the inconvenience of carrying it with you.</li>
<li>You don’t know if you will be the rare unlucky one, who’s bungee rope breaks.</li>
<li>You don’t know by how much the new design will attract more visitors and how much the income will raise.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>It can be difficult to precisely determine the benefits or losses of potential outcomes:</li>
</ol>
<ul>
<li>How much worse is your day when carrying a useless object with you? How much do you hate moisture? In order to compare the two, they must be assessed on the same scale.</li>
<li>How much fun (or other sources of reward, like social acknowledgments) is it to jump a 120 meter canyon? And how much worth is your own life to you?</li>
<li>What is the average revenue generated per visit? What is an increase of recurrence rate of, say, 50% worth?</li>
</ul>
<p>Once you know the probabilities of all outcomes and the respective losses, <em>decision theory</em> provides an intuitive framework to estimate these values. <em>Expected utility</em> <span class="math inline">\(U\)</span> is the sum product of <em>outcome probabilities</em> <span class="math inline">\(P\)</span> and the involved <em>losses</em>. In the case of the umbrella, the decision is between two options: taking an umbrella versus taking no umbrella, when it is cloudy. We calculate and compare the expected utilities <code>U</code> as follows (<span class="math inline">\(P(\text{outcome})\)</span> is the probability of an outcome):</p>
<p><span class="math display">\[\begin{aligned}
P(\text{rain}) = 0.6 \\
P(\text{no rain}) = 1 - P(\text{rain}) = 0.4 \\
L(\text{carry}) = 2 \\
L(\text{wet}) = 4 \\
U(\text{umbrella}) = P(\text{rain}) L(\text{carry}) + P(\text{no rain}) L(\text{carry})  = L(\text{carry}) = 2\\
U(\text{no umbrella}) = P(\text{rain}) L(\text{wet}) = 2.4
\end{aligned}
\]</span></p>
<p>We conclude that, given the high chance for rain, and the conditional losses, the expected loss is larger for not taking an umbrella with you. It is rational to take an umbrella when it is cloudy.</p>
<div id="measuring-uncertainty" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Measuring uncertainty</h3>
<p>As we have seen above, a decision requires two investigations: outcomes and their probabilities, and the assigned loss. Assigning loss to decisions is highly context dependent and often requires domain-specific expertise. The issues of probabilistic processes and the uncertainty that arises from them is basically what the idea of <em>New Statistics</em> represents. We encounter uncertainty in two forms: first, we usually have just a limited set of observations to draw inference from, this is <em>uncertainty of parameter estimates</em>. From just 20 days of observation, we cannot be absolutely certain about the true chance of rain. It can be 60%, but also 62% or 56%. Second, even if we precisely knew the chance of rain, it does not mean we could make a certain statement of the future weather conditions, which is <em>predictive uncertainty</em>. For a perfect forecast, we had to have a complete and exact figure of the physical properties of the atmosphere, and a fully valid model to predict future states from it. For all non-trivial systems (which excludes living organisms and weather), this is impossible.</p>
<p>Review the rainfall example: the strategy of taking an umbrella with you has proven to be superior under the very assumption of <em>predictive uncertainty</em>. As long as you are interested in long-term benefit (i.e. optimizing the average loss on a long series of days), this is the best strategy. This may sound obvious, but it is not. In many cases, where we make decisions under uncertainty, the decision is not part of a homogeneous series. If you are member of a startup team, you only have this one chance to make a fortune. There is not much opportunity to average out a single failure at future occasions. In contrast, the investor, who lends you the money for your endeavor, probably has a series. You and the investor are playing to very different rules. For the investor it is rational to optimize his strategy towards a minimum average loss. The entrepreneur is best advised to keep the maximum possible loss at a minimum.</p>
<p>As we have seen, predictive uncertainty is already embedded in the framework of rational decision making. Some concepts in statistics can be of help here: the uncertainty regarding future events can be quantified (for example, with posterior predictive distributions [#post_pred]) and the process of model selection can assist in finding the model that provides the best predictive power.</p>
<p>Still, in our formalization of the Rainfall case, what magically appears are the estimates for the chance of rain. Having these estimates is crucial for finding an optimal decision, but they are created outside of the framework. Furthermore, we pretended to know the chance of rain exactly, which is unrealistic. Estimating parameters from observations is the reign of statistics. From naive calculations, statistical reasoning differs by also regarding uncertainty of estimates. Generally, we aim for making statements of the following form:</p>
<p>“With probability <span class="math inline">\(p\)</span>, the attribute <span class="math inline">\(A\)</span> is of magnitude <span class="math inline">\(X\)</span>.”</p>
<p>In the umbrella example above, the magnitude of interest is the chance of rain. It was assumed to be 60%. This appears extremely high for an average day. A more realistic assumption would be that the probability of rainfall is 60% <em>given the observation of a cloudy sky</em>. How could we have come to the belief that with 60% chance, it will rain when the sky is cloudy? We have several options, here:</p>
<ol style="list-style-type: decimal">
<li>Supposed, you know that, on average, it rains 60% of all days, it is a matter of common sense, that the probability of rain must be equal or larger than that, when it’s cloudy.</li>
<li>You could go and ask a number of experts about the association of clouds and rain.</li>
<li>You could do some systematic observations yourself.</li>
</ol>
<p>Imagine, you have recorded the coincidences of clouds and rainfall over a period, of, let’s say, 20 days, with the following observations:</p>
<p>Intuitively, you would use the average to estimate the probability of rain under every condition.</p>
<p>These probabilities we can feed into the decision framework as outlined above. The problem is, that we obtained just a few observations to infer the magnitude of the parameter <span class="math inline">\(P(rain|cloudy) = 60\)</span>%. Imagine, you would repeat the observation series on another 20 days. Due to random fluctuations, you would get a more or less different series and different estimates for the probability of rain. More generally, the <em>true</em> parameter is only imperfectly represented by any sample, it is not unlikely, that it is close to the estimate, but it could be somewhere else, for example, <span class="math inline">\(P(rain|cloudy) = 64.93\)</span>%.</p>
<p>The trust you put in your estimation is called <em>level of certainty</em> or <em>belief</em> or <em>confidence</em>. It is the primary aim of statistics to rationally deal with uncertainty, which involves to <em>measure the level of certainty</em> associated with any statement derived from teh data. So, what would be a good way to determine certainty? Think for a moment. If you were asking an expert, how would you do that to learn about magnitude and uncertainty regarding <span class="math inline">\(P(rain|cloudy)\)</span>?</p>
<p>Maybe, the conversation would be as follows:</p>
<blockquote>
<p>YOU: What is the chance of rain, when it’s cloudy.</p>
</blockquote>
<blockquote>
<p>EXPERT: Wow, difficult question. I don’t have a definite answer.</p>
</blockquote>
<blockquote>
<p>YOU: Oh, c’mon. I just need a rough answer. Is it more like 50%-ish, or rather 70%-ish.</p>
</blockquote>
<blockquote>
<p>EXPERT: Hmm, maybe somewhere between 50 and 70%.</p>
</blockquote>
<blockquote>
<p>YOU: Then, I guess, taking an umbrella with me is the rational choice of action.</p>
</blockquote>
<p>Note how the expert gave two endpoints for the parameter in question, to indicate the location and the level of uncertainty. If she had been more certain, she had said "between 55 and 65%. While this is better than nothing, it remains unclear, which level of uncertainty is enclosed. Is the expert 100%, 90% or just 50% sure the true chance is in the interval? Next time, you could ask as follows:</p>
<blockquote>
<p>…</p>
</blockquote>
<blockquote>
<p>EXPERT: Hmm, maybe somewhere between 70-90%</p>
</blockquote>
<blockquote>
<p>YOU: What do you bet? I’m betting 5 EUR that the true parameter is outside the range you just gave.</p>
</blockquote>
<blockquote>
<p>EXPERT: I dare you! 95 EUR it’s inside!</p>
</blockquote>
<p>The expert feels 95% certain, that the parameter in question is in the interval. However, for many questions of interest, we have no expert at hand (or we may not even trust them altogether). Then we proceed with option 3: making our own observations.</p>
</div>
<div id="benchmarking-designs" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Benchmarking designs</h3>
<p>The most basic decision in practical design research is whether a design fulfills an external criterion. External criteria for human performace in a human-machine system are most common, albeit not abundant, in safety-critical domains.</p>
<p>Consider Jane: she is user experience researcher at the mega-large rent-a-car company smartr.car. Jane was responsible for a overhaul of the customer interface for mobile users. Goal of the redesign was to streamline the user interface, which had grown wild over the years. Early customer studies indicated that the app needed a serious visual de-cluttering and stronger funneling of tasks. 300 person months went into the re-development and the team did well: a recent A/B study had shown that users learned the smartr.car v2.0 fast and could use its functionality very efficiently. Jane’s team is prepared for the roll-out, when Marketing comes up with the following request:</p>
<blockquote>
<p>Marketing: We want to support market introduction with the following slogan: “rent a car in 99 seconds”.</p>
</blockquote>
<blockquote>
<p>Jane: Not all users manage a full transaction in that short time. That could be a lie.</p>
</blockquote>
<blockquote>
<p>Marketing: Legally, the claim is fine if it holds on average.</p>
</blockquote>
<blockquote>
<p>Jane: That I can find out for you.</p>
</blockquote>
<p>Jane takes another look at the performance of users in the smartr car v2.0 condition. As she understands it, she has to find out whether the average of all recorded time-on-tasks with smartr.car 2.0 is 99 seconds, or better. Here is how the data looks like:</p>
<p>The performance is not completely off the 99 seconds, many users are even faster. Jane figures out that she has to ask a more precise question, first, as teh slogan can mean different things, like:</p>
<ul>
<li>all users can do it within 99 seconds</li>
<li>at least one user can do it</li>
<li>half of the users can do it</li>
</ul>
<p>Jane decides to go the middle way and chooses the population average, hence the average ToT must not be more than 99 seconds. Unfortunately, she had only tested a small minority of users and therefore cannot be certain about the true average:</p>
<p>Because the sample average is an uncertain, Jane is afraid, Marketing could use this as an argument to ignore the data and go with the claim. Jane sees no better way as quantifying the chance of being wrong using a statistical model, which will later become known as the <em>Grand Mean Model</em>)</p>
<p>Let’s see what the GMM reports about the population average and its uncertainty: The table above is called a <em>CLU table</em>, because it reports three estimates per coefficient:</p>
<ul>
<li>Center, which (approximately) is the most likely position of the true value</li>
<li>Lower, which is the lower 95% credibility limit. There is a 2.5% chance that the true value is lower</li>
<li>Upper, the 95% upper limit. The true value is larger than this with a chance of 2.5%.</li>
</ul>
<p>This tell Jane that most likely the average time-on-task is <span class="math inline">\(Intercept\)</span>. That is not very promising, and it is worse: 99 is even below the lower 95% credibility limit. So, Jane can send a strong message: The probability that this claim is justified, is smaller than 2.5%.</p>
<p>Luckily, Jane had the idea that the slogan could be changed to <em>“rent a card in 1-1-1 seconds”</em>. The 95% credibility limits are in her favor, since 111 is at the upper end of the credibility limit. It would be allowed to say that the probability to err is not much smaller than 2.5%. But Jane desires to make an accurate statement. But what precisely is the chance that the true population average is 111 or lower? In Bayesian analysis there is a solution to that. When estimating such a model, we get the complete distribution of certainty, called the posterior distribution. In fact, a CLU table with 95% credibility limits is just a summary on the posterior distribution. This distribution is not given as a function, but has been generated by a (finite) random walk algorith, known as Markov-Chain Monte Carlo. At every step (or most, to be precise), this algorithm jumps to another set of coordinates in parameter space and a frequency distribution arises that can be used to approximate levels of certainty. The following illustration shows the posterior frequency distribution of the coefficient, divided into the two possible outcomes.</p>
<p>In the present case, we can derive the chance that the true average of the customer population is lower than the target of 111:</p>
<p>In a similar manner to how the graph above was produced, a precise certainty level can be estimated from the MCMC frequency distribution contained in the posterior object. The certainty that the 111 seconds slogan holds is much better:</p>
<p>The story of Jane is about decision making under risk and under uncertainty. We have seen how easily precise statements on uncertainty can be derived from a statistical model. But regarding rational decision making, this is not an ideal story: What is missing is a systematic analysis of losses (and wins). The benefit of going with the slogan has never been quantified. How many new customers it will really attract and how much they will spend cannot really be known upfront. Let alone, predicting the chance to loose in court and what this costs are almost unintelligeable. The question must be allowed, what good is the formula for utility, when it is practically impossible to determine the losses. And if we cannot estimate utilities, what are the certainties good for?</p>
<p>Sometimes, one possible outcome is just so bad, that the only thing that practically matters, is to avoid it at any costs. Loosing a legal battle often falls into this category and the strategy of Marketing/Jane effectively reduced this risk: they dismissed a risky action, the 99 seconds statement, and replaced it with a slogan that they can prove is true with good certainty.</p>
<p>In general, we can be sure that there is at least some implicit calculation of utilities going on in the minds of Marketing. Perhaps, that is a truly intuitive process, which is felt as an emotional struggle between the fear of telling the untruth and love for the slogan. This utility analysis probably is inaccurate, but that does not mean it is completely misleading. A rough guess always beats complete ignorance, especially when you know about the attached uncertainty. Decision-makers tend to be pre-judiced, but even then probabilities can help find out to what extent this is the case: Just tell the probabilities and see who listens.</p>
</div>
<div id="comparison-of-designs" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Comparison of designs</h3>
<p>The design of systems can be conceived as a choice between design options. For example, when designing an informational website, you have the choice of making the navigation structure flat or deep. Your choice will change usability of the website, hence your customer satisfaction, rate of recurrence, revenue etc. Much practical design research aims at making good choices and from a statistical perspective that means to compare the outcomes of two (or more) design option. A typical situation is to compare a redesign with its predecessor, which will now be illustrated by a hypothetical case:</p>
<p>Violet is a manager of an e-commerce website and at present, a major overhaul of the website is under debate. The management team agrees that this overhaul is about time and will most likely increase the revenue per existing customer. Still, there are considerable development costs involved and the question arises whether the update will pay itself in a reasonable time frame. To answer this question, it is not enough to know that revenues increase, but an more accurate prediction of <em>how much precisely</em> is gained. In order to return the investment, the increase must be in the ballpark of 10% increase in revenue.
For this purpose, Violet carries out a user study to compare the two designs. Essentially, she observes the transactions of 50 random customers using the current system with 50 transactions with a prototype of the new design. The measured variable is the money every user spends during the visit. The research question is: <em>By how much do revenues increase in the prototype condition?</em>. The figure below shows the distribution of measured revenue in the experiment.</p>
<p>There seems to be a slight benefit for the prototype condition. But, is it a 10% increase? The following calculation shows Violet that it could be the case:</p>
<p>Like in the previous case, testing only a sample of 100 users out of the whole population leaves room for uncertainty. So, how certain can Violet be? A statistical model can give a more complete answer, covering the magnitude of the improvement, as well as a level of certianty. Violet estimates a model that compares the means of the two conditions [#CGM], assuming that the randomness in follows a Gamma distribution [#Gamma, #temporal GLM].</p>
<p>The coefficients of the Gamma model are on a logartithmic scale, but when exponentiated, they can directly be interpreted as multiplicators [#link_function]. That precisely matches the reseach question, which is stated as percentage increase, rather than a difference.</p>
<p>The results tell Violet that, most likely, the average user spends $</p>
<table>
<thead>
<tr class="header">
<th align="right">center</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">49.9</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="right">center</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1.13</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="right">lower</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1.03</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1.23</td>
</tr>
</tbody>
</table>
<p>The risk of failure is just below 30%. With this information in mind Violet now has several options:</p>
<ol style="list-style-type: decimal">
<li>deciding that</li>
</ol>
<table>
<thead>
<tr class="header">
<th align="right">risk_of_failure</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">28.9</td>
</tr>
</tbody>
</table>
<ol start="2" style="list-style-type: decimal">
<li><em>continue testing</em> more users to reach a higher level of certainty</li>
<li>take into account sources of evidence from the past, or <em>prior knowledge</em></li>
</ol>
<!-- The first option represents an unspecific tolerance towards risk. Extreme risk tolerance of managers became is infamous as one cause for large scale economic crises [BLACK_SWANs]. Her motives could be manifold, still: maybe she was nursed to take risks in her life; or she is confident that consequences in case of failure will be bearable. -->
<!-- The second and third options have one thing in common: they make use of *external knowledge*. This is explained in the coming section. The fourth option is another way of making *cumulative use of knowledge*, namely *[adaptive testing: find correct term]*.  -->
</div>
<div id="prior-knowledge" class="section level3">
<h3><span class="header-section-number">4.1.4</span> Prior knowledge</h3>
<p>It is rarely the case that we encounter a situation <em>tabula rasa</em>. Whether we are correct or not, when we look at the sky in the morning, we have some expectations on how likely there will be rain. We also take into account the season and the region and even the very planet is sometimes taken into account: the Pathfinder probe carried a bag of high-tech gadgets, including an umbrella, but this was for safe landing only, not to cover from precipation, as Mars is a dry planet.</p>
<p>In behavioral research it has been general standard that every experiment had to be judged on the produced data alone. For the sake of objectivity, researchers were not allowed to take into account previous results, let alone their personal opinion. In Bayesian statistics, you have the choice. You can make use of external knowledge, but you don’t have to.</p>
<p>Violet, the rational design researcher has been designing and testing e-commerce systems for many years and has supervised several large scale roll-outs. So the current project is not a completely new situation at all. From the top of her head, Violet produces the following table to capture her past projects and the increase in revenue that had been recorded afterwards (on the whole population).</p>
<p>On this small data set, Violet estimates another grand mean model that essentially captures prior knowledge about revenue increases after redesign:</p>
<p>The following CLU table shows the results. The mean increase was Intercept and without any further information, this is the best guess for the population mean of all present or future projects (of Violet). A statistical model that is based on such a small number of observations usually produces very uncertain estimates. In this case, we have a very low level of certainty, as the 95% credibility limits are wide. There even remains a considerable risk that a project results in a decrease of revenue, although that has never been recorded. (Intercept <span class="math inline">\(&lt;1\)</span>, recall that these coefficients are multiplicative).</p>
<p>The population average (of projects) is less favorable than what Violet saw in her experiment, which is worrying. If the estimated revenue on teh experimental data is correct, it would be a rather extreme outcome. In the following step, she uses the (posterior) certainty from M_prior and employs it as prior information (by means of a Gaussian distribution). Model M_2 has the same formula as M_1 before, but combines the information of both sources, data and prior certainty.</p>
<p>Finally, Violet computes the risk for the true value to be below 110%. As it turns out, the probability of failure increases by around 10%, when prior information is taken into account. That is not so much as to turn around any decisions on the redesign. But, that also contains a positive message: The current data is not extraordinary as to collide with prior knowledge.</p>
<div id="finish" class="section level4">
<h4><span class="header-section-number">4.1.4.1</span> FINISH</h4>
<!-- ## What is wrong with classic statistics? [TBC] -->
<!-- The short version of the story is this: in face of typically noisy data, there is always the risk that  experimental data supports a certain hypothesis, seemingly. If the researcher calls the results supportive, although they were just due to randomness, this is called a *false alarm*. Many scientific disciplines have therefore committed to keep the overall number of false alarms under a threshold, for example 5%.  -->
<!-- The even shorter version is that voluntary commitment failed. The true rate of false alarms is about a magnitude higher. -->
<!-- The p-value [...] Imagine giant replication study that showed that 35% of published psychology studies are not replicable. In fact, such a study exists and it has far-reaching consequences. [...]  [REF] shows that rejection bias is one reason: studies that have not reached the *magic .05 level* of significance, have a lower chance of publication. [REF] sees as a reason that author's implicitly trick the system by *the garden of forking paths*  strategy. The research project collects an array of variables, followed by a fishing expenditure. Theory is conveniently considered last, but written up in  a pseudo-a prior way. -->
</div>
</div>
</div>
<div id="descriptive_stats" class="section level2">
<h2><span class="header-section-number">4.2</span> Descriptive statistics</h2>
<p>In empirical research we systematically gather observations. Observations of the same kind are usually subsumed as variables. A set of variables that have been gathered on the same sample are called a data set, which typically is a table with variables in columns. In the most general meaning, <em>a statistic</em> is a single number that somehow represents relevant features of a data set, such as:</p>
<ul>
<li>frequency: how many measures of a certain kind can be found in the data set?</li>
<li>central tendency: do measures tend to be located left (weak) or right (strong) on a scale?</li>
<li>dispersion: are measures close together or widely distributed along the scale?</li>
<li>association: does one variable X tend to change when another variable Y changes?</li>
</ul>
<div id="frequencies" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Frequencies</h3>
<p>The most basic statistics of all probably is the number of observations on a variable <span class="math inline">\(x\)</span>, usually denoted by <span class="math inline">\(n_{x}\)</span>. The number of observations is a rough indicator for the amount of data that has been gathered. In turn, more data usually results in better accuracy of statistics and higher levels of certainty can be reached.</p>
<p>The number of observations is not as trivial as it may appear at first. In particular, it is usually not the same as the sample size, for two reasons: First, most studies employ repeated measures to some extent. You may have invited <span class="math inline">\(n_{Part}\)</span> participants to your lab, but on each participant you have obtained several measures of the same kind. When every participant is tested on, let’s say, five tasks, the number of observations is <span class="math inline">\(n_{Part}\)</span> times five. Second, taking a valid measure can always fail for a variety of reasons, resulting in <em>missing values</em>. For example, in the 99 seconds study, it has happened, that a few participants missed to fill in their age on the intake form. The researcher is left fewer measures of age <span class="math inline">\(n_{age}\)</span> than there were participants.</p>
<p>Another important issue is the distribution of observations across groups. Again, the number of observations in a group is linked to the certainty we can gain on statistics of that group. Furthermore, it is sometimes important to have the distribution match the proportions in the population, as otherwise biases may occur.</p>
<p>The table above shows so called absolute frequencies. When comparing frequencies by groups, it often is more appropriate to report <em>relative frequencies</em> or <em>proportions</em>:</p>
<p>Summarizing frequencies of metric measures, such as time-on-task (ToT) or number of errors is useful, too. However, a complication arises by the fact that continuous measures do not naturally fall into groups. Especially in duration measures no two measures are exactly the same.</p>
<p>The answer to this problem is <em>binning</em>: the scale of measurement is divided into a number of adjacent sections, called bins, and all measures that fall into one bin are counted. For example, we could use bins of 10 seconds and assess whether the bin with values larger than 90 and smaller or equal to 100 is representative in that it contains a large proportion of values. If we put such a binned summary of frequencies into a graph, that is called a <em>histogram</em>.</p>
<p>Strictly spoken, grouped and binned frequencies are not one statistic, but a vector of statistics. It approximates what we will later get to know more closely as a <em>distribution</em>.</p>
</div>
<div id="central-tendency" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Central tendency</h3>
<p>Reconsider Jane <a href="#case99"><strong>??</strong></a>. When asked about whether users can complete a transaction within 99, she looked at the population average of her measures. The population average is what we call the <em>(arithmetic) mean</em>. The mean is computed by summing over all measures and divide by the number of observations. The mean is probably the most often used measure of central tendency, but two more are being used and have their own advantages: <em>median</em> and <em>mode</em>.</p>
<p>Imagine a competitor of the car rental company goes to court to fight the 99-seconds claim. Not an expert in juridical matters, my humble opinion is that one of the first questions to be regarded probably is: what does “rent a car in 99 seconds” actually promise?
One way would be the mean (“on average users can rent a car in 99 seconds”), but here are some other ways to interpret the same slogan:</p>
<p>“50% (or more) of users can …”. This is called the <em>median</em>. The median is computed by ordering all measures and identify the the element right in the center. If the number of observations is even, there is no one center value, and the mean of the center pair is used, instead.</p>
<p>Actually, the median is a special case of so called <em>quantiles</em>. Generally, an quantiles are based on the order of measures and an X% quantile is that value where X% of measures are equal to or smaller. The court could decide that 50% of users is too lenient as a criterion and could demand that 75% percent of users must complete the task within 99 seconds for the slogan to be considered valid.</p>
<p>A common pattern to be found in distributions of measures is that a majority of observations accumulate in the center region. The point of highest density of a distribution is called the <em>mode</em>. In other words: the mode is the region (or point) that is most likely to occur. For continuous measures this once again poses the problem that every value is unique. Sophisticated procedures exist to smooth over this inconvenience, but by binning we can construct an approximation of the mode: just choose the center of the bin with highest frequency.</p>
<p>The table above shows the three statistics for central tendency side-by-side. Mean and median are close together. This is frequently the case, but not always. When the distribution of measures is completely symmetric mean and median perfectly coincide. In section @(distributions) we will encounter distributions that are not symmetric. The more a distribution is skewed, the stronger the difference between mean and median increases.</p>
<p>To be more precise: for left skewed distributions the mean is strongly influenced by few, but extreme, values in the left tail of the distribution. The median only counts the number of observations to both sides and is not influenced by how extreme these values are. Therefore, it is located more to the right. The mode does not regard any values other than those in the densest region and just marks that peak. The same principles hold for right-skewed distributions.</p>
<p>To summarize, the mean is the most frequently used measure of central tendency, one reason being that it is a so called <em>sufficient statistic</em>, meaning that it exploits the full information present in the data. The median is frequently used when extreme measures are a concern. The mode is the point in the distribution that is most typical.</p>
</div>
<div id="dispersion" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Dispersion</h3>
<p>In a symmetric distribution with exactly one peak, mean and mode coincide and the mean represents the most typical value. For a value being more typical does not mean it is very typical. That depends on how the measures are dispersed over the whole range. In the figure below, the center value of the narrow distribution contains 60% of all measures, as compared to 40% in the wide distribution, and is therefore more representative.</p>
<p>A very basic way to describe dispersion of a distribution is to report the <em>range</em> between the two extreme values, <em>minimum</em> and <em>maximum</em>. These are easily computed by sorting all values and selecting the first and the last element. Coincidentally, they are also special cases of quantiles, namely the 0% and 100% quantiles.</p>
<p>A <em>boxplot</em> is a commonly used geometry to examine the shape of dispersion. Like histograms, boxplots use a binning mechanisms and are therefore useful for continuous measures. Whereas histograms use equidistant bins on the scale of measurement, boxplots create four bins based on 25% quantile steps. These are also called <em>quartiles</em>.</p>
<p>The min/max statistics only uses just these two values and therefore does not fully represent the amount of dispersion. A statistic for dispersion that does so is the <em>variance</em>, which is the mean of squared deviations from the mean. Squaring the deviations always produces a positive value, but makes variance difficult to interpret. The <em>standard deviation</em> is the square root of variance. By reversing the square the standard deviation is on the same scale as the original measures and their mean.</p>
</div>
<div id="associations" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Associations</h3>
<ul>
<li>Are elderly users slower at navigating websites?</li>
<li>How does reading speed depend on font size?</li>
<li>Is the result of an intelligence test independent from gender?</li>
</ul>
<p>In the previous section we have seen how all individual variables can be described by location and dispersion. A majority of research deals with associations between measures and the present section introduces some statistics to describe them. Variables represent properties of the objects of research and fall into two categories: <em>Metric variables</em> represent a measured property, such as speed, height, money or perceived satisfaction. <em>Categorical variables</em> put observations (or objects of research) into non-overlapping groups, such as experimental conditions, persons who can program or cannot, type of education etc. Consequently, associations between any two variables fall into precisely one of three cases, as shown in the table. In the following I will explain these types of associations.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>categorical</th>
<th>metric</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>categorical</td>
<td>frequency cross tables</td>
<td>differences in mean</td>
</tr>
<tr class="even">
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>metric</td>
<td>classification</td>
<td>covariance, correlation</td>
</tr>
<tr class="even">
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<div id="categorical-associations" class="section level4">
<h4><span class="header-section-number">4.2.4.1</span> Categorical associations</h4>
<p>Categorical variables group observations, and when they are <em>both categorical</em>, the result is just another categorical case and the only way to compare them is by size, which is done by <em>frequency tables</em>. To illustrate the categorical-categorical case, consider a study to assess the safety of two syringe infusion pump designs, called Legacy and Novel. All participants of the study are asked to perform a typical sequence of operation on both devices (categorical variable Design) and it is recorded whether the sequence was completed correctly or not (categorical variable Correctness).</p>
<p>Besides the troubling result that incorrect completion is the rule, not the exception, there is almost no difference between the two designs. Note that in this study, both professional groups were even in number. If that is not the case, absolute frequencies are difficult to compare and we better report <em>relative frequencies</em>. Note how every row sums up to <span class="math inline">\(1\)</span>.</p>
<p>The absolute or relative frequencies can be shown in a stacked <em>bar plot</em>:</p>
</div>
<div id="categorical-metric-associations" class="section level4">
<h4><span class="header-section-number">4.2.4.2</span> Categorical-metric associations</h4>
<p>Associations between <em>categorical and metric</em> variables are reported by <em>grouped location statistics</em>. In the case of the two infusion pump designs, the time spent to complete the sequence is compared by the following table. And as you can see, adding a comparison of variance (or any other statistic) is not a hassle.</p>
<p>For the illustration of categorical-metric associations case, <em>boxplots</em> haven proven useful. Boxplots show differences in central tendency (median) and dispersion (other quartiles) simultaneously. Here we can observe that Novel design produces shorter ToT and seems to have less dispersion.</p>
</div>
<div id="covariance-and-correlation" class="section level4">
<h4><span class="header-section-number">4.2.4.3</span> Covariance and correlation</h4>
<p>For associations between <em>both metric</em> variables, <em>covariance</em> and <em>correlations</em> are commonly employed statistics. As correlations derive from covariances, I describe them first:</p>
<ol style="list-style-type: decimal">
<li>A covariance is a real number that is <em>zero</em> when there really is <em>no association</em> between two
variables.</li>
<li>When two variables move into the <em>same direction</em>, covariance gets the <em>positive</em>.</li>
<li>When they move in <em>opposite directions</em>, covariance is <em>negative</em></li>
</ol>
<p>For an illustration, consider the following hypothetical example of study on the relationship between mental ability test scores and performance in real tasks. The real task could be a buzz wire task or any other complex motor task that has a strong spatial component. As predictors for ToT on the buzz-wire task, mental rotation speed was taken. It was even taken twice (MRS_1, MRS_2), because the researchers are also interested in how stable over time the scores are. The second test is for measuring visual-spatial working memory, using the Corsi task.</p>
<p>The following function computes the covariance of two variables. The covariance between the two MRS scores is positive, indicating that they move into the same direction.</p>
<p>The problem with covariance is that it usually is hard to interpret. This is why later, we will transformk covariances into correlations, but for understanding the stapes to go there, we have to understand the link between variance and covariance. The formula for covariance is (with <span class="math inline">\(E(X)\)</span> the mean of <span class="math inline">\(X\)</span>):</p>
<p><span class="math display">\[
\textrm{cov}_{XY} = \frac{1}{n} \sum_{i=1}^n (x_i - E(X)) (y_i - E(Y))
\]</span></p>
<p>Covariance essentially arises by the multiplication of deviations from the mean, <span class="math inline">\((x_i - E(X)) (y_i - E(Y)\)</span>. When for one observation both factors go into the same direction, be it positive or negative, this term gets positive. If that happens a lot, the whole sum gets largely positive. When the deviations systematically move in opposite direction, such that one factor is always positive and the other negative, we get a large negative cvariance. When the picture is mixed, i.e. no clear tendenvcy, covariance will stay close to zero. The following illustration uses a geometric interpretation of the multiplication as the area of rectangles. Rectangles with equal directions (blue) are in the upper-right and lower-left quadrant. They overwhelm the opposite direction rectangles (red), which speaks for a strong positive association. The associations between MRT_1 and Corsi, as well as between Corsi and ToT seem to have a slight overhead in same direction, so the covariance is positive, but less strong. A clear negative association exists between MRS_1 and Corsi. It seems these two tests have some common ground.</p>
<p>If we now look at the defintion of variance, it is apparent that variance is just covariance of a variable with itself (, because <span class="math inline">\((x_i - E(X))^2 = (x_i - E(X))(x_i - E(X))\)</span>:</p>
<p><span class="math display">\[
\textrm{var}_{X} = \frac{1}{n} \sum_{i=1}^n (x_i - E(X))^2)
\]</span></p>
<p>That gives rise to a compact form to show all covariances and variances between a bunch of variables at once. The following table is a <em>variance-covariance matrix</em>. It shows the variance of every variable in the diagonale and the mutual covariances in the off-diagonal cells.</p>
<p>As intuitive the idea of covariance is, as unintelligible is the statistic itself for reporting results. Th problem is that covariance is not a pure measure of association, but is contaminated by the dispersion of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. For that reason, two covariances can only be compared if the variables have the same variance. The <em>Pearson correlation coefficient</em> <span class="math inline">\(r\)</span> solves the problem by rescaling covariances by the product of the two standard deviations:</p>
<p><span class="math display">\[
r_XY = \frac{\textrm{cov}_{XY}}{\textrm{sd}_X \textrm{sd}_Y}
\]</span></p>
<p>Due to the standardization of dispersion, Pearson correlation <span class="math inline">\(r\)</span> can be interpreted as strength of association independent of scale of measurement. More precisely, <span class="math inline">\(r\)</span> will always be in the interval <span class="math inline">\([-1,1]\)</span>. That makes it the perfect choice when associations are being compared to each other or to an external standard. In the field of psychometrics, correlations are ubiquitously employed to represent <em>reliability</em> and <em>validity</em> of psychological tests. <em>Test-retest stability</em> is one form to measure reliability and it is just the correlation of the same test taken on different days. For example, we could ask whether mental rotation speed as measured by the mental rotation task (MRT) is stable over time, such that we can use it for long-term predictions, such as how likely someone will become a good surgeon. Validity of a test means that it represents what it was intended for, and that requires an external criterion that is known to be valid. For example, we could ask how well the ability of a person to become a minimally invasive surgeon depends on spatial cognitive abilities, like mental rotation speed. Validity could be assessed by taking performance scores from exercises in a surgery simulator and do the correlation with mental rotation speed. A correlation of <span class="math inline">\(r = .5\)</span> would indicate that mental rotation speed as measured by the task has rather limited validity. Another form is called <em>discriminant validity</em> and is about how specific a measure is. Imagine another test as part of the surgery assessment suite. This test aims to measure another aspect of spatial cognition, namely the capacity of the visual-spatial working memory (e.g., the Corsi block tapping task). If both tests are as specific as they claim to be, we would expect a particularly low correlation.</p>
<p>And similar to covariances, correlations between a set of variables can be put into a correlation table. This time, the diagonal is the correlation of a variable with itself, which is perfect correlation and therefore equals 1.</p>
<p>Another way to illustrate a bunch of correlations is produced by the following command, combining scatterplots, density plots and correlation coefficients</p>
<!-- #28 -->
<p>Correlations give psychometricians a comparable standard for the quality of measures, irrespectively on what scale they are. In exploratory analysis, one often seeks to get a broad overview of how a bunch of variables is associated. Creating a correlation table of all variables is no hassle and allows to get a broad picture of the situation. Correlations are ubiquitous in data analysis, but have limitations: First, a correlation only uncovers linear trends, whereas the association between two variables can take any conceivable form. The validity of correlations depends on how salient the feature of linear trend is. In the example below, <span class="math inline">\(Y_1\)</span> reveals a strong parabolic form, which results in zero correlation. The curvature of an exponentially rising function is only captured insufficiently. For that reason, I recommend that correlations are always cross-checked by a scatterplot.</p>
<p>Another situation where covariances and correlations fail is when there simply is no variance. It is almost trivial, but for observing how a variable <span class="math inline">\(Y\)</span> changes when <span class="math inline">\(X\)</span> moves is that both variables <em>vary</em>, there is no co-variance without variance.</p>
<p>As we have seen, for every combination of two categorical and metric variables, we can produce summary statistics for the association, as well as graphs. The second part of this book really is about statistical models that extend association statistics in two ways: first, we will see that linear models [#LM] can capture the associations between more than just two variables. Second, by Bayesian estimation, we can derive statements of uncertainty.</p>
<p>While it could be tempting to primarily use summary statistics and rather omit statistical graphs, the last example makes clear that some statistics like correlation are making assumptions on the shape the association. The different graphs we have seen are much less presupposing and can therefore be used to check the assumptions of statistics and models.</p>
<!-- 31
With covariance and correlations we covered linear trends between metric variables. But, how would we represent associations between groups and metric variables?
-->
<!--


+ There are at least some users who can (which clearly is the case). That would be the *minimum value*.
+ Half of the users or more can, which is a statistic called the *median*.
+ Most users can in around 99s, which is called the *mode*.

All the above, including the  mean, are numbers that summarize the observations. A single number that summarizes a set of observations is called *a statistic*, formally it is just

$s = f_s(D)$

with $s$ being a statistic on data $D$, defined by the function $f_s$. Characterizing a data set by appropriate statistics is called *descriptive statistics*. 

Most commonly used statistics fall in three classes: cardinality, central tendency (or location) of data and data dispersion. Cardinality statistics give the number of units of a class in a data set. The most common cardinalities are  the number of observed performance measures $n_{Obs}$, the number of individual participants $n_{Part}$ in the study. 

Reporting the calculated mean of time-on-task in two designs is descriptive. It summarizes the data set in a reasonable way. It is informative in so far as one learns which design to prefer, but coarsely. Consider a case where two designs were just slight variations, come at precisely the same costs, and the researcher has collected performance measures on five subjects per group, without the option of inviting more participants.
-->
</div>
</div>
</div>
<div id="bayesian-probability-theory" class="section level2">
<h2><span class="header-section-number">4.3</span> Bayesian probability theory</h2>
<!--Probability is an elusive concept and a source of mental suffering and heated debates, especially when people rely to much  intuition. The reason is that in the human mind, probability can be rooted in two different ways: in frequentist thinking, probability is represented by relative frequencies, whereas in Bayesian school of thought it is the level of certainty for some event to happen.-->
<!-- This is a Bayesian book and the following considerations are meant to convince the reader that the Bayesian use of probability has a wider domain of application than the frequentist. I also see this as one of the more rare situations where introducing some formalism is supportive in that it dissolves the subtle preoccupations that often accompany intuitive (or, I'd rather say: exemplified) understanding. In addition, the mathematical definition is bare of any real world notions, like observed frequencies or experienced certainties and  (figuratively speaking) makes the different perspectives converge. The algebraic definition of probability is given by the three Kolmogorov axioms. Before we come to that, let me undertake one intermediate step: introducing some set-theoretic concepts, which at the same time is meant as a deep bow to the discipline of thought that is so awe inspiring on the one hand a source of mental suffering for many on the other hand. -->
<p>Mathematics is emptiness. In its purest form, it does not require or have any link to the real world. That makes it so difficult to comprehend. Sometimes a mathematical theory describes real world phenomena, but we have no intuition about it. A classic example is Einstein’s General Relativity Theory, which assumes a curved space, rather than straight space as Newtonian Mechanics does. Our minds are Newtonian and the closest to intuitive understanding I got was the imagination of the universe as a four-dimensional mollusk, thanks to Einstein.</p>
<p>Math can become easy, if mathematical system directly translate into real world ideas and sensations. I recall how my primary school teacher introduced the sum of two numbers as removing elements from one stack and place it on second (with an obvious stop rule). Later, as a student, I was taught the Peano axiomatic theory of Natural Numbers. As it turned out, I somehow knew them already, because they just formalized, what my teacher had told me. The formal proof for <span class="math inline">\(1 + 1 = 2\)</span> is using just the same elements as me shifting blocks between towers.</p>
<!-- ####  MAKE FIGURE -->
<!-- ```{r include = F} -->
<!-- Theories <- tribble(~Theory, ~intuitive, -->
<!--                     "Natural Numbers", .1, -->
<!--                     "Basic set theory", .15, -->
<!--                     "Probability theory", .2, -->
<!--                     "Curved spacetime", .9, -->
<!--                     "Quantum mechanics", .95) -->
<!-- Theories %>%  -->
<!--   ggplot(aes(x = intuitive, y = 0, yend = 0, xend = intuitive, label = Theory)) + -->
<!--   geom_segment() + -->
<!--   geom_point() + -->
<!--   geom_text(angle = 45) + -->
<!--   geom_hline(yintercept=0,size=1,color='purple') + -->
<!--   scale_y_continuous(limits = c(-0.1,0.1), breaks = 0, labels = NULL, name = NULL) + -->
<!--   scale_x_continuous(limits = c(0,1), breaks = 1, labels = NULL, name = "Intuition") -->
<!-- ``` -->
<!-- intuitive --------------------------------- a total math -->
<!-- |   |  | |                                 |    | -->
<!-- Peano   -->
<!-- Set_theory  -->
<!-- probability -->
<!-- Curved space time -->
<!-- Quantum mechanics -->
<!-- #### HERE -->
<p>As we will see, the formal definition of probability is based on set theory, which put it more on the intuitive side of things. The formal theory of probability, the Kolmogorov axioms may be somewhat disappointing from an ontological perspective, as it just defines rules for when a set of numbers can be regarded probabilities. But calculating actual probabilities is rather easy and a few R commands suffice to start playing with set theory and probability. The most tangible interpretation of probabilities is that the probability of an event to happen, say getting a Six when rolling a dice, coincides with the relative frequency of Six in a (very long) sequence of throws. This is called the <em>frequentist interpretation</em> of probability and this is how probability will be introduced in the following. While thinking in terms of relative frequency in long running sequences is rather intuitive, it has limitations. Not all events we want to assign a probability can readily be imagined as a long running sequence, for example:</p>
<ul>
<li>the probability that your house burns down (you only have this one)</li>
<li>the probability that a space ship will safely reach Mars (there’s only this one attempt)</li>
<li>the probability that a theory is more true than another (there’s only this pair)</li>
</ul>
<p>The <em>Bayesian interpretation</em> of probability is essentially the same as the frequentist, but is mnore relaxed as it does <em>not require that all</em> probabilities are measured through relative frequencies in long running sequences. Bayesian thinking includes the idea that probabilities can also be a <em>degree of belief</em>, which can, but doesn’t have to be grounded in long-running series. In the following I will present in broad strokes how the theory of probability emerges from set theory and can be set into motion by computing relative frequencies of sets and subsets. Then I will introduce the <em>likelihood</em>, which is a concept equally used in classic and Bayesian statistics. After clarifying the differences between frequentist and Bayesian ideas of measuring probability, <em>Bayes theorem</em> is introduced as the formal underpinning of all Bayesian statistics. We will see how the likelihood and idea of <em>certainty as probability</em> combines to a scientific framework that emphasizes the incremental updating of our knowledge about the world we measure.</p>
<div id="some-set-theory" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Some set theory</h3>
<p>The mathematical concept of probability can most intuitively be approached by thinking in terms of relative frequency in long-running sequences. Actually, it is not even required to think of a sequence (where events have an order). It suffices to assume a set of events that emerge from one experiment.</p>
<p>A mathematical <em>set</em> is a collection of elements taken from a domain (or universe, more dramatically). These can either be defined by stating all the elements, like <span class="math inline">\(S = \{\textrm{red}, \textrm{yellow}, \textrm{green}, \textrm{off}\}\)</span> or by a characterizing statement, like:</p>
<p><span class="math inline">\(S := \textrm{possible states of a Dutch traffic light}\)</span></p>
<p>The elements should be clearly identified, but need not have a particular order. (If they do, this is called an <em>ordered set</em>, the set of natural numbers is an example). Sets can have all possible sizes, which is called the <em>cardinality</em> of a set:</p>
<ul>
<li>finite (and countable) like the states of a traffic light</li>
<li>empty like “all opponents who can defeat Chuck Norris”, <span class="math inline">\(\{\}\)</span> or <span class="math inline">\(\oslash\)</span></li>
<li>infinite, but countable, like the natural numbers <span class="math inline">\(N\)</span></li>
<li>infinite, uncountable, like the real numbers <span class="math inline">\(R\)</span></li>
</ul>
<p>You may wonder now, whether you would ever need such a strange concept as uncountable infinite sets in your down-to-earth design research. Well, the set of primary interest in every design study is the possible outcomes. Sometimes, these are finite, like <span class="math inline">\(\{\textrm{success}, \textrm{failure}\}\)</span>, but when you measure durations or distances, you enter the realm of real numbers. We will set this issue aside for the moment and return to it later in the context of continuous distributions of randomness.</p>
<p>In order to introduce the mathematical concept of probability, we first have to understand some basic operations on sets. For an illustration, imagine a validation study for a medical infusion pump, where participants were given a task and the outcome was classified by the following three criteria:</p>
<ul>
<li>was the task goal achieved successfully?</li>
<li>was the task completed timely (e.g., one minute or below)?</li>
<li>were there any operation errors along the way with potentially harmful consequences?</li>
</ul>
<p>Note how the data table makes use of logical values to assign each observation a membership (or not) to each of the three sets. We can use the filter command to create all kinds of subsets and actually that would carry us pretty far into set theory. In the following I will introduce set theory thze Programming way, but use the package Sets, as it most closely resembles the mathematical formalism, it replaces. We begin with loading the package, which unfortunately uses the <code>%&gt;%</code> operator for its own purpose.</p>
<!-- 33 -->
<!-- 34 add set notation-->
<p>Once there is more than one set in the game, set operators can be used to create all kinds of new sets. We begin with the <em>set difference</em>, which removes elements of one set from another (if they exist), for example the set of all successful tasks that were not completed in time. Note how the Sets package uses the minus operator to remove elements of one set (Timely) from another (Success).</p>
<p>Using the set difference, we can produce <em>complementary set</em> which include all elements that are not included in a set.</p>
<p>In probability theory this corresponds with the probability of an event (Success) and its <em>counter-event</em> (Failure). A set and its complementary set taken together produce the <em>universal set</em>, which in probability theory is the <em>sure event</em> with a probability of One. To show that we can use <em>set union</em>, which collects the elements of two separate sets into one new set, for example re-uniting a set with its complementary,</p>
<p>or creating the set of all observations that were failure or delayed (or both):</p>
<p>Another commonly used set operator is the <em>intersect</em>, which produces a set that contains only those elements present in both original sets, like the set of timely and successful task completions.</p>
<p>Turns out all successful observations are also harmless. But not all harmless observations were successful. In set theory Success is therefore a <em>subset</em> of Harmless. The subset operator differs from those discussed so far, in that it does not produce a new set, but a truth value (also called logical or Boolean). Per definition, two <em>equal</em> sets are also subsets of each other. The <code>&lt;</code> operator is more strict and it means a <em>proper subsets</em>, where being a subset has just one direction.</p>
<p>The example above demonstrates the, figuratively, smallest concept of set theory. The <em>empty set</em> has the special property of being a subset of all other set:</p>
<p>The empty set is important for the intersect operator to work properly. It may happen that two sets do not share any elements at all. It would be problematic, if the intersect operator only worked if common elements truly existed. In such a case, the intersection of two sets is the empty set. Sets that have an empty intersection are called <em>disjunct sets</em> (with complementary sets as a special case). The package Sets, which defines all operators on sets so far is lacking a dedicated function for disjunctness, but this is easily defined using the intersect function:</p>
<p>So far, we have only seen sets of atomic elements, where all elements are atomic, i.e. they are not sets themselves.</p>
<p>With a little more abstraction, we can also conceive a set that has other sets as its elements. The set of sets that are defined by the three performance criteria and their complementary sets is an obvious example:</p>
<p>For the formal introduction of probability, we need two concepts related to sets of sets: First, a <em>partition of a set</em> is a set of non-empty subsets such that every element is assigned to exactly one subset. The subsets of successes and its complementary set, all failures, is such a partition. Second, the <em>power set</em> is the set of all possible subsets in a set. Even with a rather small set of 20 elements, this is getting incredibly large, so let’s see it on a smaller example:</p>
<p>The power set is tantamount for the definition of probability that follows, because it has two properties: first, for every subset of <code>S</code> it also contains the complementary set. That is called <em>closed under complementarity</em>. Second, for every pair of subsets of <code>S</code>, <code>P</code> it also contains the union, it is <em>closed under union</em>. In the same way, power sets are also <em>closed under intersection</em>. Generally, all sets of subsets that fulfill these three requirements are called <em><span class="math inline">\(\Sigma\)</span> algebras</em>. The mathematical theory of <span class="math inline">\(\Sigma\)</span> algebras is central for the mathematical definition of all measures.</p>
<p>Without going into to much depth on measurement theory, a measure is a mapping from the domain of empirical observations to the domain of numbers, such that certain operations in the domain of measurement work conistently with numerical operations. One example is the following: if you have to towers of blocks, L and R, next to each other and you look at them from one side, then the following rule applies for translating between the world of sensations and the world of sets:</p>
<p><code>L L   R    &lt;-- Observer L   R L   R</code>
If you can see the top of tower L, when looking from the right side, then tower L is larger than tower R is build with <em>more</em> blocks than tower R. Probabilities are measures and in the next section we will see how numerical operations on probabilities relate to set operations in a <span class="math inline">\(\Sigma\)</span> algebra. We will also see that relative frequencies are measures of probability.</p>
</div>
<div id="probability" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Probability</h3>
<p>In the following I will outline the formal theory of probability and use the same fictional validation study to illustrate the relevant concepts. introduced in the previous section. Performance of participants was classified by the three two-level criteria, success, harm and timeliness. Every recorded outcome therefore falls into one of eight possible sets and a purposeful way to summarize the results of the study would be relative frequencies (<span class="math inline">\(\pi\)</span>, <code>pi</code>):</p>
<p>Let’s examine on an abstract level, what has happened here:</p>
<ol style="list-style-type: decimal">
<li>The set of events has been partitioned into eight non-overlapping categories made by three-way intersections. The first row, for example, is the intersect of the three sets Failure, Harmless and Delayed (see previous section).</li>
<li>All subsets got a real number assigned, by the operation of relative frequencies which produces numbers between (and including) Zero and One.</li>
<li>A hidden property is that, if we unite all sets, we get the universal set and, not coincidentally, if we sum over the frequencies the result is One:</li>
</ol>
<p>Back to formal: The mathematical theory of probability departs from a set of <em>outcomes</em> <span class="math inline">\(\Omega\)</span> and a <span class="math inline">\(\Sigma\)</span> algebra <span class="math inline">\(F\)</span> defined on <span class="math inline">\(\Omega\)</span>. An element <span class="math inline">\(E\)</span> of <span class="math inline">\(F\)</span> therefore is a set of outcomes, which is called an <em>event</em>.</p>
<p>The eight threeway interaction sets above are a partition of <span class="math inline">\(\Omega\)</span>, but not a <span class="math inline">\(\Sigma\)</span>-algebra. As disjunct sets they are closed under intersection for trivial reasons, but they are not closed under union. For that we had to add a lot of possible outcomes, all counter-sets to start with. The point is that we can construct all these subsets using filter commands and produce relative frequencies, like above.</p>
<p>Probability as an axiomatic theory is defined by the three <em>Kolmogorov axioms</em>:</p>
<p>The <em>first Kolmogorov axiom</em> states that a probability is a non-negative real number assigned to every event. The computation of relative frequencies satisfies this condition hands down.</p>
<p>The first axiom defines a lower border of Zero for a probability measure, the <em>second Kolmogorov axiom</em> is taking care of an upper limit of One. This happens indirectly by stating that the set of all observations <span class="math inline">\(\Omega\)</span> (which is an element of <span class="math inline">\(F\)</span>) is assigned a probability of One. In the table of relative frequencies that is not yet covered, but we can easily do so:</p>
<p>So far, the theory only cared for assigning numbers to events (subsets), but provides no means to operate on probabilities. The <em>third Kolmogorov axiom</em> establishes a relation between the union operator on sets and the sum operator on probabilities by stating that the probability of a union of disjunct events is the sum of the individual probabilities. We can approve this to be true for the relative frequencies. For example, the question could be: Is the set of all successful observations the union of successful timely observations. Indeed, the relative frequency of all successful events is the sum of the two and satisfies the third axiom:</p>
<p>The Kolmogorov axioms establish a probability measure and lets us do calculations on disjunct subsets. That would be a meager toolbox to do calculations with probabilities. What about all the other set operators and their possible counterparts in the realm of numbers? It is one of greatest wonders of the human mind that the rich field of reasoning about probabilities spawns from just these three axioms and a few set theoretic underpinnings. To just give one very simple example, we can derive that the probability of the complement of a set <span class="math inline">\(A\)</span> is <span class="math inline">\(P(\Omega/A) = 1 - P(A)\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>From set theory follows that a set <span class="math inline">\(A\)</span> and its complement <span class="math inline">\(\Omega/A\)</span> are disjunct, hence axiom 3 is applicable:
<span class="math inline">\(P(A \cup \Omega/A) = P(A) + P(\Omega/A)\)</span></p></li>
<li><p>From set theory follows that a set <span class="math inline">\(A\)</span> and its complement <span class="math inline">\(\Omega/A\)</span> form a partition on <span class="math inline">\(\Omega\)</span>. Using axiom 2, we can infer:
<span class="math display">\[\begin{aligned}
A \cup \Omega/A = \Omega\\ 
\Rightarrow P(A) + P(\Omega/A) = P(\Omega) = 1\\ 
\Rightarrow P(\Omega/A) = 1 - P(A)
\end{aligned}\]</span></p></li>
</ol>
<!-- However, we may want to calculate the probability of events that are not disjunct, speaking of sets that is they overlap. If we just add -->
<p>The third axiom tells us how to deal with probabilities, when events are disjunct. As we have seen, it applies for defining more general events. How about the opposite direction, calculating probabilities of more special events? In our example, two rather general events are Success and Timely, whereas the intersection event Success <em>and</em> Timely is more special. The probability of two events occurring together is called <em>joint probability</em> <span class="math inline">\(P(\textrm{Timely} \cap \textrm{Success})\)</span>. The four joint probabilities on the two sets and their complements are shown in the following table.</p>
<p>As joint probability asks for simultaneous occurrence it treats both involved sets symmetrically: <span class="math inline">\(P(\textrm{Timely} \cap \textrm{Success}) = P(\textrm{Successes} \cap \textrm{Timely})\)</span>. What if you are given one piece of information first, such as “this was a successful outcome” and you have to guess the other “Was it harmful?”. That is called <em>conditional probability</em> and in this case, it is Zero. But what is the conditional probability that, when you know an event was a failure, it really caused harm?</p>
<p><span class="math inline">\(P(\textrm{Harmful}|\textrm{Success})\)</span></p>
<p>In the manner of a speed-accuracy trade-off, there could be a relationship between Timely and Harm. Participants who rush through the task are likely to make more harmful errors. We would then expect a different distribution of probability of harm by whether or not task completion was timely.</p>
<p>See how conditional probabilities sum up to one <em>within</em> their condition. In this case, the conditional probabilities for harm are the same for successes and failures. As a consequence, it is also the same as the overall probability, hence:</p>
<p><span class="math display">\[\begin{aligned}
P(\textrm{Harm} | \textrm{Timely}) = P(\textrm{No harm} | \textrm{Timely}) =
P(\textrm{Timely})
\end{aligned}\]</span></p>
<p>This situation is called <em>independence of events</em> and it means that knowing about one variable does not help in guessing the other. In Statistics, <em>conditional probability</em> and <em>independence of events</em> are tightly linked to <em>likelihoods</em> and <em>Bayes theorem</em> [#Bayes_theorem].</p>
<!-- <!-- #39 elaborate the role of independence -> -->
<!-- #### COMPLETE ME -->
<!-- + joint probability -->
<!-- + conditional probability (events in a sequence -> knowledge of A is knowledge of B) -->
<!-- + independence -->
<!-- + Bayes theorem -->
<!-- Other definitions and basic numerical operations on probabilities can be inferred in similar ways.  -->
<!-- <!-- 35 -> -->
<!-- The basic operations on probability then give rise to more advanced laws of probability: -->
<!-- <!-- 36 -> -->
<!--The two endpoints of the probability scale we can call the *impossible event*, $P(A) = 0$ and the *inevitable event* $P(A) = 1$. For example, it is impossible that any task a researcher throws at a participant is completed in a negative time: $P(ToT < 0) = 0$. At the same time, it is certain that the participant either correctly completes the task or not, $P(\textrm{correct } or \textrm{ incorrect}) = 1$. You may find the latter example trivial, but, in fact it points to another element of probability, namely, *the probability of all possible mutually exclusive events sums to 1*, for example:

+ tomorrow it rains or is doesn't
+ all people carrying zero or more Y chromosomes
+ the duration is a positive number

Have you noticed the frequent occurrence of the word *or* in the above examples? In probability space, this little word precisely denotes an event 

On the other hand, *and* means that two events occur together. 


+ 
+ probabilities of independent events multiply


What are those events in design research and how do we assign them those numbers called probabilities? -->
<!--32-->
<!--To some confusion, but also underlining the emptiness of math, several types of measures are true probabilities themselves. Consider a validation study for a medical device. A sample of nurses is asked to complete a series of similar tasks. The performance measure of interest is the *relative frequency $\pi$* of failures. Generally, classifications of outcomes are probabilities, when the classification is unambiguous and exhaustive, such that every event falls into exactly one class. When the criterion for success is crisp enough, these requirements are fulfilled. They are also fulfilled by the relative frequencies by which you see green, yellow, red or dark when approaching a Dutch traffic light.

Especially for readers with some theoretical grounding in frequentist statistics it may be helpful to see that also continuous variables can be probabilities, namely proportions. Imagine a novel behavioral screening test on alcohol influence. The client is asked to walk 10 meters on a curved path. Using a set of smart sensors, that continuously assess whether the clients center of gravity is inside or outside the path. The *proportions* of relative distances inside or outside the path are probabilities (without being countable events).-->
</div>
<div id="likelihood" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Likelihood</h3>
<p>In the previous section we have seen how to create probabilities from relative frequencies in data how to do basic calculations with those probabilities. Using these concept, we will see in teh following how data can be used for inference. Inference means to draw conclusions about theoretical models. In the following this will be illustrated at the example of rolling dices, where the default theory is that the dice is fair, hence the probability of Six is assumed to be <code>\pi_{Six}</code>. For the matter here, we take the theory of the dice being fair into the equantion, as a condittional probability, in words: “Conditional on this dice being fair, the chance of rolling Six is <span class="math inline">\({1/6}\)</span>”.</p>
<p><span class="math display">\[P(y = \textrm{Six}|\pi_\textrm{Six} = {1 \over 6})\]</span></p>
<p>When such a standard standard dice is rolled twice, how likely is an outcome of two times Six? We use probability theory: It is one dice but the results of the rolls are otherwise independent; for example, the probability to roll a Six does not wear off over time or anything like that.</p>
<p><span class="math inline">\(P(y_1 = \textrm{Six}|\pi = {1 \over 6}) = P(y_2 = \textrm{Six} = {1 \over 6})\)</span>.</p>
<p>Because of independence, the joint probability of rolling two times Six is just the product:</p>
<p><span class="math display">\[
P(y_1 = \textrm{Six} \textrm{ and } y_2 = \textrm{Six}|\pi = {1 \over 6}) \\= 
P(y_1 = \textrm{Six}) \times P(y_2 = \textrm{Six}|\pi = {1 \over 6}) \\= {1 \over 36}
\]</span></p>
<p>The joint probability of all data points is called the <em>Likelihood</em> and generalizes to situations where the probabilities of events are not the same, for example: What is the probability of the first dice being an Four, the second a five and the third a Three or a Six?</p>
<p><span class="math display">\[
P(y_1 = \textrm{Four} \textrm{ and } y_2 = \textrm{Five} \textrm{ and } y_3 = \textrm{Three} \textrm{ or } \textrm{Six})\\
=  {1 \over 6}
\times \frac 1 6 \times \frac 1 3 = \frac 1 {108}
\]</span></p>
<p>Notice how the likelihood gets smaller in the second example. In fact, likelihoods are products of numbers between zero and one and therefore become smaller with every observation that is added. In most empirical studies, the number of observations is much larger than two or three and the likelihood becomes inexpressibly small. Consider the following results from 16 rolls.</p>
<p>The likelihood of this result, given that it is a fair dice, is <span class="math inline">\(\frac 1 6^{16} = 3.545\times 10^{-13}\)</span>. Therefore, one usually reports the <em>logarithm of the likelihood (log-likelihood)</em>. This results in “reasonable” negative numbers. Why negative? Because all Likelihoods are fractions of One (the identity element of multiplication), which results in a negative logarithm.</p>
<!-- The negative log likelihood is central concept in Bayesian and classic model estimation, as we will see. When estimating a model, a likelihood is computed a myriad of times. With a little mathematical trick, the negative log-likelihood can be calculated as sum of logarithms, instead of a logarithm of products. -->
<!-- $$ -->
<!-- \log(a \times b) = \log(a) + log(b) -->
<!-- $$ -->
<!-- ```{r} -->
<!-- -sum(rep(log(1/6), 16)) -->
<!-- ``` -->
<!-- For the human mind, summing over numbers is much easier than products. And the same goes for computers. The following code measures the computing time (ms) for a likelihood of ten thousand dice rolls. On my computer, summing over logarithms almost cuts the computing time in half. -->
<!-- ```{r} -->
<!-- Rolls <- rep(1/6, 10000) -->
<!-- bind_rows( -->
<!--   microbenchmark::microbenchmark(-prod(Rolls)), -->
<!--   microbenchmark::microbenchmark(-sum(log(Rolls))) -->
<!-- ) -->
<!-- ``` -->
<p>The dice rolling example above has a twist. assumed that we may enter <span class="math inline">\(\pi = {1 \over 6}\)</span>, because we <em>believe</em> that it is a fair dice, without further notice. In other words, we needed no data, because of overwhelming prior knowledge (or theory, if you will). And now we will come to see, why I took the effort to write the probabilities above as conditional probabilities. A likelihood is the probability of the data, given a parameter value. The basic idea of likelihoods is to consider data constant, and vary the parameter. In such a way, we can see how the likelihood changes when we assume different values for <span class="math inline">\(\pi_\textrm{Six}\)</span>.</p>
<p>Imagine we have been called in to uncover fraud with biased dices in a casino. There is suspicion, that the chance of rolling a Six is lower than <span class="math inline">\(1 \over 6\)</span>. So, what is the most likely chance of rolling a Six? In the following simulation, 6000 rolls have been recorded:</p>
<p>The result is shown in the figure above and just for simplicity we just focus on the events of rolling a Six. If we have no prior suspicion about the dice, the estimated probability is simply the relative frequency of Six.</p>
<p>In this case, we can simply note down that the <em>most likely value</em> is <span class="math inline">\(\pi_{Six} =.147\)</span>, which is lower than the fair 0.167. But, note the slight ruggedness of the bar chart. Not a single bar is read as exactly <span class="math inline">\(1 \over 6\)</span>, so the deviation of Six could have happened by chance. One way to approach this question is comparing the likelihoods <span class="math inline">\(P(\text{Result = Six}|\pi = {1 \over 6})\)</span> and <span class="math inline">\(P(\text{Result = Six}|\pi = {.147})\)</span>.
For that purpose, we create a new event variable <code>Six</code>, that indicates whether a roll is a Six (<code>TRUE</code>) or not (<code>FALSE</code>). Further, a <em>distribution function</em> is required that assigns these events their probabilities. Distribution functions can take very complicated forms [#statistical models], but in the case here it is the rather simple <em>Bernoulli distribution</em>.</p>
<!-- $$ -->
<!-- d_\text{Bern}(p) =  -->
<!--   \begin{cases} -->
<!--     \text{Six} ,& p\\ -->
<!--     \text{not Six},& 1 - p -->
<!--   \end{cases} -->
<!-- $$ -->
<p>The log-likelihood function of the Bernoulli distribution is just the sum of log-probabilities of any roll <span class="math inline">\(y_i\)</span>.</p>
<p><span class="math display">\[
LL_\text{Bern}(Y|\pi) = \sum_i{\log(d_\text{Bern}(y_i, \pi))}
\]</span></p>
<p>Now, we can determine the <em>ratio of likelihoods LR</em> with different values for <span class="math inline">\(\pi\)</span>. Note that on the logarithmic scale, what was a ratio, becomes a difference.</p>
<p><span class="math display">\[
LR = \exp(LL_\text{Bern}(\text{Rolls}, .147) - LL_\text{Bern}(\text{Rolls}, \frac 1 6)
\]</span></p>
<p>Now, recall what a likelihood is: the probability of the observed data, under a certain model. Here, the data is almost 5000 times more likely with <span class="math inline">\(p = .147\)</span>. In classic statistics, such likelihood ratios are routinely been used for comparison of models.</p>
<p>Previously, I have indicated that the relative frequency gives us the most likely value for parameter <span class="math inline">\(\pi\)</span> (the case of a Bernoulli distributed variable), the <em>maximum likelihood estimate (MLE)</em>. The MLE is that point in the parameter range (here <span class="math inline">\([0;1]\)</span>), which maximizes the likelihood. It is the point, where the data is most likely. In a similar way, the mean of Gaussian distributed measures is the maximum likelihood estimate for the distribution parameter <span class="math inline">\(\mu\)</span>. But, more advanced models do not have such a closed form, i.e., a formula that you can solve. Parameter estimation in classic statistics heavily grounds on numerical procedures to find <em>maximum likelihood estimates</em>, which I will outline now:</p>
<p>The probability function <span class="math inline">\(d_\text{Bern}(y_i, \pi)\)</span> has two parameters that vary, the result of a roll <span class="math inline">\(y_i\)</span> and the assumed chance <span class="math inline">\(pi\)</span>. The likelihood function, as we use it here, in contrast, takes the data as fixed and only varies on parameter <span class="math inline">\(\pi\)</span>. By varying the parameter and reading the resulting likelihood of data, we can numerically interpolate the MLE. The most basic numerical interpolation method is a grid search, which starts at the left boundary of parameter range, zero in this case, and walks in small steps along a grid to the right boundary (one). By convention, maximum likelihood estimation is performed by <em>minimizing the negative log-likelihood</em>. For the convenience, the following likelihood function has been vectorized, to make it work smoothly in a tidy processing chain.</p>
<p>Because we use the <em>negative</em> log-likelihood, the value for <span class="math inline">\(\pi\)</span> with maximum likelihood is the minimum of the likelihood curve. Here, <span class="math inline">\(\pi_\text{MLE} = 0.15\)</span>, which is very close to the relative frequency we obtained above. The slight deviation is due to the limited resolution of the grid, but it is always possible to be more accurate by using a finer grid.</p>
<p>In classic statistics MLE is one of the most common methods for estimating parameters from models. However, most of the time, data is more abundant and there is more than one parameter. It is possible to extend the grid method to as many parameters as the model contains by just creating multi-dimensional grids and walk through them by the likelihood function. However, already a two-dimensional grid of rather coarse <span class="math inline">\(100 \times 100\)</span> would require the computation of 10.000 likelihoods. Classic statisticians have therefore developed optimization methods to identify the MLE more efficiently. Soon, we will turn our attention to Bayesian estimation, where the Likelihood plays a central role in estimation, too [#Bayesian_est].</p>
</div>
<div id="bayesian-and-frequentist-probability" class="section level3">
<h3><span class="header-section-number">4.3.4</span> Bayesian and frequentist probability</h3>
<!-- Inferential statistics serves rational decision making under uncertainty by attaching information on the *level of certainty* to a parameter of interest. A central difference between frequentist and Bayesian statistical theory is how the elusive concept of *certainty* emerges. -->
<p>Let us for a moment return to how probability was axiomatically defined by Kolmogorov’s axioms. The axioms themselves only speak of relations between sets and probability. There is not a trace of any algorithm that would let you construct a set of numbers that actually <em>qualifies</em> as probabilities. There is one theory everyone agrees with: relative frequencies satisfy the three Kolmogorov axioms. That can be demonstrated, as I did in section [#Probability], and there is mathematical proof for that, more precisely: if you repeat a random experiment an infinite number of times, relative frequencies are measures for probability. And since not all mathematical theory has such an intuitive interpretation, we can call ourselves lucky. And at the same time it is obvious, because Kolmogorov, like Peano, sat down to formalize something most people intuitively understand.</p>
<p>All statisticians believe that relative frequencies satisfy Kolmogorov’s axioms, but not everyone thinks that this is the only way. And that is where frequentist and Bayesian statistics diverge: Frequentest believe that <em>only relative frequencies</em> are valid measures for probability, whereas Bayesians believe that <em>certainty</em> is a measure of probability, too. As subtle this may sound, it has remarkable consequences in context with Bayes theorem.</p>
<blockquote>
<blockquote>
<p>QUESTION: What is the chance that this dice will fall on Six?
FREQUENTIST: I can only answer this question after a long-running series of dice rolls.
BAYESIAN: I am 50% certain that this is a trick question, so I’d say 50% it never falls on Six. If this is not a trick question, this seems to be a dice from a well-known manufacturer, so I’d say 1/6.
FREQUENTIST (frowning): And how does the manufacturer know?
BAYESIAN (rolls eyes): … by a long-running series of experiments.</p>
</blockquote>
</blockquote>
<p>The frequentist in the caricature is someone sticking to the principles: every probability must be produced by relative frequencies, which must be produced in repetitions of the same experiment. The Bayesian has no problem with long running sequences, but feels confident to use other sources of information. In a strictly frequentist view, this is like pulling numbers, just in the unit interval, out of the thin air. But, as we will see, Bayes theorem is amazingly useful, when we allow for probabilities that don’t derive from long-running sequences. These are called <em>prior probabilities</em> and they can be used to factor in prior knowledge about the world.</p>
<!-- It is our intuition that makes it almost inevitable to think of probability of a set $A$ as relative frequencies, i.e. the ratio of the cardinality of A divided by the cardinality of the universal set $\Omega$. Relative frequency precisely is the frequentist definition of probability: -->
<!-- $$ -->
<!-- P(A) = {|A| \over |\Omega|} -->
<!-- $$ -->
<p>Consequently, a Bayesian also accepts that levels of certainty after the experiment can be reported as probabilities. In contrast, a frequentist must insist that certainty can be interpreted as long-running series. In frequentist statistics, a common way to express ones level of certainty is the infamous p-value. And this is its definition: <em>A result is called statistically significant on level <span class="math inline">\(\alpha = .05\)</span>, if drawing from the null distribution (an infinite number of times) will produce the observed result or a larger result in no more than 5% of cases.</em></p>
<p>Another, and more preferable way of expressing ones certainty about a parameter (say, the population mean) is the <em>95% confidence interval</em>, which is expressed as two endpoints:</p>
<p>It is common to say: “we can be 95% certain that the true values is between these bounds”, but the 95% confidence interval really is defined by assuming an (infinite) set of replications of the very same experiment and using relative frequencies: <em>The 95% confidence interval is constructed in such a way that, if the same experiment were repeated an infinite number of times, in 95% of these repetitions the true value is contained in the interval.</em></p>
<p>You are not alone when you lack intuition of what the definition says and when you feel at unease about where all these experiments are supposed to come from. It seems as if a a true frequentist cannot imagine the future other than by a series of long-running experiments. In Bayesian statistics, the level of certainty is expressed as a <em>proposition about one’s state of knowledge</em>, like : <em>Based on my data I am 95% sure that there is a difference</em>. Equating level of certainty with probability directly, without taking the detour via relative frequencies, may be a little lax, but it leads to remarkably intuitive statements on uncertainty. In Bayesian statistics the <em>credibility interval</em> is defined as: <em>With a probability of 95%, the true value is contained.</em> Since there seems to be no external criterion (such as a series of experiments, imagined or not), Bayesian statistics often faced the criticism of being subjective. In fact, if we imagine a certainty of 95% as some number in the researchers mind, that might be true. But, it is quite easy to grasp certainty as an objective quantity, when we assume that there is something at stake for the researcher and that she aims for a rational decision. In the previous chapter I have illustrated this idea by the example of carrying an umbrella with you (or not) and the 99 seconds claim. Generally, it helps to imagine any such situation as a gamble: if you bet 1 EUR that the true population mean is outside the 95% credibility interval, as a rational person I would put 19 EUR against.</p>
<p>In effect, the Bayesian certainty is a probability in mathematical terms, without the necessity to implement it as a relative frequency. That liberates our reasoning from the requirement to think of long-running series. In particular, it allows us to enter non-frequentist probabilities into Bayes theorem, resulting a statistical framework that captures the dynamics of belief in science [#dynamics]. Before we come to that, I will explain Bayes theorem in another context of updating knowledge, medical diagnostics.</p>
</div>
<div id="bayes-theorem" class="section level3">
<h3><span class="header-section-number">4.3.5</span> Bayes theorem</h3>
<p>Bayes’ theorem emerges from formal probability theory and therefore is neither Bayesian nor frequentist. Essentially, the theorem shows how to calculate a conditional probability of interest <span class="math inline">\(P(A|B)\)</span> from a known conditional probability <span class="math inline">\(P(B|A)\)</span> and two marginal probabilities <span class="math inline">\(P(A)\)</span> and <span class="math inline">\(P(B)\)</span>:</p>
<p><span class="math display">\[
P(A|B) = { P(A)P(B|A) \over P(B)}
\]</span></p>
<p>The proof of the theorem is rather simple and does not require repetitiuon here. But, Bayes’ theorem can lead to rather surprising conclusions, which can be illustrated by example of a medical screening test, as follows:</p>
<p>In the 1980, the human immunodeficiency virus (HIV) was discovered and since then has become a scourge for humanity. Given that the first outbursts of the disease raged among homosexual men, it is not really surprising that some conservative politicians quickly called for action an proposed a mandatory test for every one, with the results to be registered in a central data base. Without much of a stretch it may seem justifiable to store (and use) the information that someone is carrying such a dangerous disease. The problem is with those people who do not carry it, but could be mis-diagnosed. These are called <em>false-positives</em>. Let#s assume the power of a screening test has been assessed by examining samples of participants where it is fully known whether someone is a carrier of the virus <span class="math inline">\(C+\)</span> or not <span class="math inline">\(C-\)</span>. The result is a <em>specificity</em> of 95%, meaning that 95% of <span class="math inline">\(C-\)</span> are diagnosed correctly (<span class="math inline">\(P(T-|C-)\)</span>), and a <em>sensitivity</em> of 99%, meaning that 99% with <span class="math inline">\(C+\)</span> are diagnosed correctly (<span class="math inline">\(P(T+|C+)\)</span>). The question that Bayes’ theorem can answer in such a situation is <em>How many citizens would be registered as HIV carrying, although they are not?</em>. For this to work, we must also know the probability that someone randomly chosen from the population is a carrier (<span class="math inline">\(P(C+)\)</span>) and the proportion of positive test results <span class="math inline">\(P(T+)\)</span>.</p>
<!-- # Explain marginal probability p(A) = p(B)p(A|B) + p(-B)p(A|-B), weighted average of probabilities -->
<p><span class="math display">\[
\begin{aligned}
P(C+) &amp;&amp;= .0001\\
P(C-) &amp;= 1 - P(C+) &amp;= .9999\\
P(T+|C+) &amp;&amp;= .99\\
P(T-|C+) &amp;= 1 - P(T-|C+) &amp;= .01\\
P(T-|C-) &amp;&amp;= .95\\
P(T+|C-) &amp;= 1 - P(T-|C-) &amp;= .05\\
P(T+) &amp;= P(C+)P(T+|C+) + P(C-)P(T+|C-) &amp;\approx .05\\
P(T-) &amp;= 1 - P(T+) &amp;\approx .95 
\end{aligned}
\]</span></p>
<p>How do these numbers arise? The first, <span class="math inline">\(P(C+)\)</span>) is the proportions of HIV carriers in the whole population. If you have no test at all, that is your best guess for whether a random person has the virus, your <em>prior knowledge</em>. Then, the validation study of the test provides us with more <em>data</em>. The study examined the outcome of the test (<span class="math inline">\(T+\)</span> or <span class="math inline">\(T-\)</span>) in two groups of participants, those that were knowingly carriers <span class="math inline">\(C+\)</span> and those that were not <span class="math inline">\(C-\)</span>. This is where the four conditional probabilities come from. Finally, we need the expected proportion of positive test results <span class="math inline">\(P(T+)\)</span>, which we compute as a marginal probability over the two conditions. Because non-carriers <span class="math inline">\(C-\)</span> dominate the population by so much, the marginal probability for a positive test is almost the same as the probability for a positive test amomg non-carriers <span class="math inline">\(P(T+|C-)\)</span>.</p>
<p>All conditional probabilities here emerge from a validation study, where it is known upfront whether someone is a carrier or not. What matters for the application of the screening test is the reverse: what is learned by test about carrier status? In the following the test is being characterized by two types of errors that can occur: false alarms and misses.</p>
<ul>
<li>False alarms: What is the probability that someone will be registered as carrier, although being a non-carrier? That is: <span class="math inline">\(P(C-|T+)\)</span>. When the false alarm rate is low, the test is called to have good <em>specificity</em>.</li>
<li>Misses: Which proportion of the tested population are carriers, but have a negative test result (and act accordingly)? That is: <span class="math inline">\(P(C+|T-)\)</span>. When the probability of misses is low, the test is called to have good <em>sensitivity</em>.</li>
</ul>
<p>Using Bayes’ theorem, we obtain the following probability for false alarms:</p>
<p><span class="math display">\[
\begin{aligned}
P(C-|T+) &amp;= {P(C-)P(T+|C-) \over P(T+)}\\
&amp;\approx {{.9999 \times .05} \over .05}\\
&amp;\approx .9999
\end{aligned}
\]</span></p>
<p>Obviously, testing the whole population with the screening test would result in disaster. Practically everyone who is registered as carrier in the data base is really a non-carrier. In turn, the probability that a person has the virus despite a negative test result is:</p>
<p><span class="math display">\[
\begin{aligned}
P(C+|T-) &amp;= {P(C+)P(T-|C+) \over P(T-)}\\
&amp;\approx {.0001 \times .01 \over .95}\\
&amp;\approx .000001
\end{aligned}
\]</span></p>
<p>We see a strong asymmetry in how useful the test is in the two situations. Specificity of the test is rather low and stands no chance against the over-whelming prevalence of non-carriers. In the second use case, prevalence and high test sensitivity work in the same direction, which results in a fantastically low risk to err. That is what Bayes’ theorem essentially does: it combines prior knowledge (prevalence, <span class="math inline">\(P(C+)\)</span>) against obtained evidence and produces <em>posterior</em> knowledge. In the following section we will see how this makes Bayes’ theorem a very useful tool in a context that is all about updating knowledge, like in science. In the case of medical diagnostics, the probabilty <span class="math inline">\(P(C+|T+)\)</span> can be called a posterior probability, but it becomes prior knowledge in teh very moment that new data arrives and we can update our knowledge once again. A reasonable strategy arises from the asymmetry of certainties: a negative test almost certainly coincides with being a non-Carrier, whereas identifying true carriers remains problematic. Follow-up research should therefore capitalize on reducing false alarms and use a test with extreme specificity (= absence of false alarms) at the expense of sensitivity.</p>
<!-- It is well-known that common people have problems to really grasp the mechanism of Bayes' theorem. As Gerd Gigerenzer argues from an evolutionary perspective, our mind is much better at understanding probability laws such as Bayes' theorem if it sees  *frequencies* and not bare probabilities. [#REF] (Gigerenzer argues that our brains might even be Bayesian machines that operate on frequencies in our personal history with encounters of this and that type.) In fact, the probabilities above once were frequencies in the first place, when the evaluation study was conducted. Specificity and sensitivity are statistics that arise when relative frequencies are presented , i.e. summaries of *2x2 frequency tables*.  -->
<!-- We simulate the situation with a population of 100.000.000: -->
<!-- ```{r} -->
<!-- n_HIV <- 100000000 -->
<!-- set.seed(42) -->
<!-- D_HIV_pop  <-  -->
<!--   tibble(Carrier = as.logical(rbinom(n_HIV, size = 1, prob = .0001))) %>%  -->
<!--   mutate(prob = if_else(Carrier, .99, .05), -->
<!--          Test = as.logical(rbinom(n_HIV, size = 1, prob = prob))) %>%  -->
<!--   group_by(Carrier, Test) %>%  -->
<!--   summarize(N = n()) %>%  -->
<!--   print() -->
<!-- ``` -->
<!-- #### [HERE] -->
<!-- The table below comes in long format and computes the posterior probability in just two steps. First, it takes the frequencies of all positive and negative tests. These are the *margin sums* of Test. -->
<!-- ```{r} -->
<!-- D_HIV_pop %>%  -->
<!--   group_by(Test) %>%  -->
<!--   mutate(margin_Test = sum(N)) %>%  -->
<!--   ungroup() %>% -->
<!--   group_by(Carrier) %>%  -->
<!--   mutate(margin_Carrier = sum(N)) %>%  -->
<!--   ungroup() %>% -->
<!--   mutate(posterior_prob = N /margin_Test, -->
<!--          log_posterior_prob = log(posterior_prob)) %>%  -->
<!--   print() -->
<!-- ``` -->
<!-- <!-- # correct numbers in formulas -> -->
<!-- Now, we clearly see how it happens: Despite to the gigantic sample size, only 108 persons were actually carriers, due to the low prevalence of the virus. Only 1 in 949906 is a carrier with a negative test result. At the same time, almost everybody who has a positive test is not a carrier, just because almost everybody is a non-carrier. To be fair, part of the magic is an assumption we made: the evaluation study is fully representative as it reflects the true distribution of carriers in the full population. Probably, a real study would be conducted with a biased sample, say 100 carriers and 100 non-carriers. That is not a problem, if we first adjust for prevalence and then carry out the same procedure: -->
<!-- ```{r} -->
<!-- TestEval <-  -->
<!--   tribble(~Carrier,   ~Test,  ~prevalence, ~N, -->
<!--           T,          T,      .0001,       99,       -->
<!--           T,          F,      .0001,        1, -->
<!--           F,          T,      .9999,        5, -->
<!--           F,          F,      .9999,       95) %>%  -->
<!--   group_by(Test) %>%  -->
<!--   mutate(margin_Test = sum(N)) %>%  -->
<!--   ungroup() %>%  -->
<!--   mutate(prob = (N /margin_Test)) %>%  -->
<!--   print() -->
<!-- ``` -->
<!-- ```{r opts.label = "future"} -->
<!-- M_TestEval <- -->
<!--   stan_glm(cbind(freq,sum_Test) ~ 0 + Carrier : Test,  -->
<!--            family = "binomial", -->
<!--            data = TestEval) -->
<!-- ``` -->
<!-- ```{r opts.label = "future"} -->
<!-- M_TestEval -->
<!-- ``` -->
<!-- <!-- find correct term for testAbundnc -> -->
</div>
<div id="the-dynamics-of-belief" class="section level3">
<h3><span class="header-section-number">4.3.6</span> The dynamics of belief</h3>
<p>In the previous section we have seen an application of Bayes theorem where it served to debunk a diagnostic procedure. As the procedure of testing everyone for HIV may seem reasonable at first, it turned out to be impractical. That is an example of how a principally intuitive theories like Probability Theory can result in counter-intuitive, but correct, answers. These are the most valuable answers, actually.</p>
<!-- In Bayesian statistics -->
<!-- $$\text{posterior}\ \propto \text{prior}\times\text{likelihood}$$ -->
<!-- <!-- #24 -> -->
<!-- Note that $\propto$ here means *proportional to*. In very plain words this is: -->
<!-- > what you believe now is a combination of what you knew before and what you have just seen in the data. -->
<p>In Bayesian statistics, certainty (or strength of belief, state of knowledge, credibility) is assumed to follow all rules of probability theory. We may still think of it in terms of frequencies or illustrate it as a gamble, whatever fits the context better. The point is that we may enter degrees of belief (instead of relative frequencies) into Bayes rule, as follows:</p>
<p><span class="math display">\[
P(\theta|\text{Data}) = 
{ P(\theta) P(\text{Data}|\theta) \over P(\text{Data})}
\]</span></p>
<p>In this equation, the only probability that has been measured by frequencies is the <em>likelihood</em> <span class="math inline">\(P(\theta|\text{Data})\)</span>, where <span class="math inline">\(\theta\)</span> is the parameters to be estimated (e.g. <span class="math inline">\(\pi\)</span> of Bernoulli distributions). This likelihood carries <em>evidence from data</em>. <span class="math inline">\(P(\theta|\text{Data})\)</span> is the <em>posterior certainty</em>, or how much we believe in a parameter position, after seeing the data. Posterior certainty turns out to be proportional to the product of <em>prior certainty</em> <span class="math inline">\(P(\theta)\)</span> and likelihood. This is the place where prior knowledge and strength of evidence are weighed against each other, for example how strongly we belive that a dice is fair. <span class="math inline">\(P(\text{Data})\)</span> is called the <em>marginal likelihood</em>.</p>
<p>In section [#likelihood] it was explained how parameters can be estimated by maximum likelihood estimation. This procedure compared the likelihood over a range of parameter values to determine the position that makes the observed data most likely. In the context of parameter estimation, a model is just such a position in parameter space. Bayesian estimation does it very similar, but operates on posterior probability, which includes prior knowledge.</p>
<p>As it turns out the marginal likelihood <span class="math inline">\(P(Data)\)</span> cannot be known or easily estimated. The reason why we can still use Bayes rule for estimation is that this term does not depend on the parameter vector <span class="math inline">\(\theta\)</span>, which is our search space, meaning that it is practically a constant. For parameter estimation, Bayes rule can be reduced to saying that the posterior certainty is <em>proportional to</em> the product of prior certainty and evidence:</p>
<p><span class="math display">\[
\text{posterior}\ \propto \text{prior}\times\text{likelihood}
\]</span></p>
<!-- The second possible meaning of Model is that it denotes a particular likelihood function. As we will see in the second part of this book, the likelihood function is where we express our theories as mathematical relations between measured variables. For example, we can imagine that time-on-task (ToT) in a design evaluation study is related to age, which we would express by the following linear models ($M_\text{LRM}$), or it is not ($M_\text{GMM}$) -->
<!-- $$ -->
<!-- M_\text{GMM}: y_i = \text{Gaus}(\beta_0, \sigma_\epsilon)\\ -->
<!-- M_\text{LRM}: y_i = \text{Gaus}(\beta_0 + \beta_1 x_\text{age}, \sigma_\epsilon) -->
<!-- $$ -->
<!-- If we can solve Bayes rule for these two models, we would be able to compare them by which is called the *Bayes Factor*: -->
<!-- $$ -->
<!-- BF(M_1, M_2) =  -->
<!-- {P(M_1|\text{Data}) P(M_2)  \over P(M_2|\text{Data}) P(M_1)} -->
<!-- $$ -->
<!-- A Bayes Factor larger than 1 indicates that model $M_1$ is the one to be believe in more.  -->
<!-- Bayes Factor is a great tool to assign certanties, meaning real probabilities,  to models, or even theories, but there is one problem: in most cases, Bayes rule cannot be solved, because  -->
<!-- #24 -->
<!-- Note that $\propto$ here means *proportional to*. In very plain words this is: -->
<!-- or in plain words: -->
<!-- > what you believe now is a combination of what you knew before and what you have just seen in the data. -->
<p>This mechanism of updating states of beliefs is what sets Bayesian statistics apart and it reverbs with what happens in the real world and in science:</p>
<ul>
<li>Someone skeptical of climate change may change their view after a couple of very hot summers.</li>
<li>When almost everyone believed in Newton’s Mechanics as ground truth, some people got curious when it turned out that light speed is constant for every observer.</li>
</ul>
<p>Note how these examples all have the same dynamics: a prior belief is updated when new data arrives. These are precisely the three elements of Bayesian statistics:</p>
<ol style="list-style-type: decimal">
<li>the <em>prior belief</em> is what you believe before (climate change is a myth)</li>
<li>the <em>likelihood</em> is what you learn by observation (hot summer)</li>
<li>the <em>posterior belief</em> is your adjusted believe after seeing the data (maybe not such a myth)</li>
</ol>
<p>One could almost say that updating knowledge is the primary purpose of any central nervous system, where the attached sensors provide the data. All animals with such a system cannot just react to their environment by innate patterns, but <em>learn</em> and over a life’s course it is not such a stretch to call this a long-running experiment.</p>
<p>Systematic research is what only some central nervous systems do, but the principle is the same: we have a theory which we are unsure about; data is collected and after that we may find this theory more likely (or not). But, that does not exclude that prior knowledge grounds on data, too. In experimental Psychology researchers almost seem to entertain themselves with repetitions of the very same experimental paradigm, with slight variations maybe. For example, in the famous Stroop effect, participants have to name the ink color of a word. When the word is itself a color word and refers to a different color, response times typically increase. This effect has been replicated in many dozens of published studies and, probably, thousands of student experiments. Cumulative evidence is so strong that, would someone repeat the experiment another time and find the reverse effect, no one would seriously take this as a full debunk of decades of research. The previous section closed with the remark that more research could solve the dilemma of testing diseases with a very low prevalence, and the same goes here: Even just one negative result on teh Stroop effect can raise some mild suspicions about the effect’s universalality and someone may feel that another replication is required. One principle of Bayesian thinking is:</p>
<blockquote>
<p>Today’s posterior is tomorrow’s prior.</p>
</blockquote>
<p>There is no principled difference between prior and posterior knowledge. They are just degrees of belief at different points in time, expressed as probabilities. Both can differ in strength: prior knowledge can be firm when it rests on an abundance of past evidence. But, prior knowledge can also be over-ruled when a lot of data disproves it. Evidence in data varies in strength: for example, the more observations, the larger the likelihood becomes, which means stronger evidence. If measures are more accurate (and less disturbed by randomness), smaller sample sizes suffice to reach conclusions of similar strength. This is why larger sample sizes are preferred and why researchers try to improve their methods. Prior belief and evidence by data can be congruent or contradict each other. When they are congruent, prior belief is just strengthened by the data. When they are contradicting each other, prior belief is over-ruled to some degree, depending on the strength of (counter-) evidence. Updating our knowledge is essential for individual decision makers, but in particular applies for scientific progress.</p>
<!--#25-->
<p>The problem with frequentist statistics is that it has not developed general methods to incorprorate prior knowledge in the analysis. It is even worse: When practicing null hypothesis significance testing, which is very common among frequentists, you strictly <em>must not update</em> your degree of belief during the study and act accordingly. Neither is there a way to express one’s prior belief when doing a t-test, nor may you adjust sample size ad hoc until satisfactory certainty is reached. Classic data analysis pretends as if no one has ever done any such an experiment before. Also, it is strictly forbidden to invite further participants to the lab, when the evidence is still to weak. If you planned the study with, say, <span class="math inline">\(N = 20\)</span>, this is what you must do, no less no more. If you reach your goal with less participants, you must continue testing. If you are unsatisfied with the level of certainty, the only permissible action is dump your data and start over from zero. The frequentist denial of incremental knowledge is not just counter-intuitive in all science, it is a millstone around the neck of every researcher.</p>
<!-- To get to a practical example, reconsider Jane and Andrew [#decision_making]. What did they know about the current state of affairs when running a particular session? Using some time-on-task measures they disproved the claim "rent a car in 99 seconds". Recall how the question was phrased, precisely: on average, users had to be able to complete the transaction in 99 seconds. The statistic of interest is the mean. This was debunked with almost no effort, by calculating: -->
<!-- $$\hat M_{ToT} = 1 \over n * \sum{ToT} = 105.975$$ -->
<!-- But how about the updated slogan: "rent a car in 111 seconds".  Can we be sure it holds, when someone repeats the study? We can only to a degree. It could still happen, that the belligerent  competitor comes to a different result, just because they have a different sample of participants. Even if they would test a fully matching sample of participants, the measures will differ, simply because an array of smaller and larger impact factors is continuously  hitting the central and peripheral nervous systems of your participants. The result is randomness in the data. Fortunately, randomness is often found to be well-behaved in that recurrent patterns emerge. These patterns are called distributions of randomness and I will introduce a whole bunch of them later in this chapter @\ref().  -->
<!--


So, if everybody would just do descriptive statistics, we were done, here: Identify the quantitative summary that suits your situation and compute it. However, what happens the field of statistics strongest contribution is that the level of uncertainty can also be quantified. 


Inferential statistics is required, whenever there is imperfect evidence and when there is skin-in-the-game.


This uncertainty arises from incomplete knowledge: only a small fraction of potential users have been tested, whereas the claim is about the whole population. Notice that I have not written the sample mean as $M$, but put a "hat" on it. That is to denote that the sample mean is an *estimate* for the population mean. The term estimate usually denotes that it is reasonable (intuitive, as well strictly mathematical) to assume that the statistic obtained from the sample is useful for making claims about the whole population. At the same time, every sample of users carries only partial information on the whole population, such that the estimate is imperfect.

A consequence of imperfection is that when another sample is drawn, one usually does not obtain the precise same estimate. The fluctuation of samples from an unknown population is what classic frequentist statistics draws upon. In the case of Jane and Andrew, a frequentist statistician would ask the question:

> How certain can you be that in the population of users $M_{ToT} <= 99$?

For frequentist thinkers the idea of the sample is central and the mathematical underpinning rests on an experiment of thought: how would all other possible samples look like? Here, following the Bayesian approach, and the fluctuation in sample statistics we consider a consequence of uncertainty. All of them carrying incomplete information, and inferential Bayesian statistics centers around full quantification of uncertainty. As we have seen, uncertainty about a future ivent to occur, is crucial for decision making on rational grounds.


-->
</div>
</div>
<div id="statmod" class="section level2">
<h2><span class="header-section-number">4.4</span> Statistical models</h2>
<p>It is a scientific, maybe even naturalistic, principle that every event to happen has its causes (from the same universe). The better these causes are understood, the better will be all predictions of what is going to happen the next moment, given that one knows the laws of physics. <em>Laplace demon</em> is a classic experiment of thought on the issue: the demon is said to have perfect knowledge of laws of physics and about the universe’s current state. Within naturalistic thinking, the demon should be able to perfectly predict what is going to happen in the next moment. Of course, such an entity could never exist, because it were actually a computer that matches the universe in size (and energy consumption). In addition, there are limits to how precisely we can measure the current state, although physicist and engineers have pushed the limits very far.</p>
<p>When Violet did her experiment to prove the superiority of design B, the only two things she knew about the state of affairs was that the participant sitting in front of her is member of a very loose group of people called the “typical user” and the design her or she was exposed to. That is painstakingly little information on what’s currently going on in the patricipant’s central nervous system. Her lack of knowledge is profound but still not a problem as the research question was on a gross scale itself, too. Not what happens to individual users needed to be described, but just the difference in <em>average</em> duration. Instead, imagine Violet and a colleague had invented a small game where they both guess the time-on-task of individual participants as they enter the lab. Who comes closest wins. As both players are smart people, they do not just randomly announce numbers, but let themselves guide by data of previous sessions. A very simple but reasonable approach would be to always guess what the average ToT in all previous sessions has been. In [#GMM] we will call this a grand mean model.</p>
<p>Of course, Violet would never expect her grand mean model to predict the accurate outcome of a session. Still, imagine a device that has perfect knowledge of the website, the complete current neural state of a the participant and the physical environment both are in. As an all-knowing device it would also have complete knowledge on how neural states change. With this device, Jane could always make a perfect prediction of the outcome. Unfortunately, real design researchers are far from Laplace demonism. Routinely borrowing instruments from social sciences, precision of measurement is humble and the understanding of neural processes during web navigation is highly incomplete. Participants vary in many complex ways in their neural state and this makes a myriad of <em>small unrelated forces (SMURF)</em> that steers an individual users in a unique situation away from the average. Laplace demon has perfect knowledge of all SMURF trajectories and therefore can produce a perfect prediction. Violet, in contrast, is completely ignorant of any SMURFs and her predictions will be way off many times. A common way to conceive this situation is that any measure is composed of a <em>structural part</em> and a <em>random part</em>, <span class="math inline">\(\epsilon_i\)</span>.</p>
<p><span class="math display">\[\text{Measure}_i = \text{structural part} + \text{random part}_i\]</span></p>
<p>Note that Measure and Random part both have an observation-level index <span class="math inline">\(i\)</span>, which means they are <em>unique</em> per observation <span class="math inline">\(i\)</span>. The structural part does not have such an index, because it makes a <em>universal</em> statement, a proposition that holds for all the observations. And for the scope of this book, a <em>statistical model</em> is defined as a <em>mechanism that separates the structural from the random part</em>, to some extent. The population average is the most simple of a class of structural parts called <em>linear models</em> [#lm] and we will see throughout the whole second Part of this book, how the structural part encodes increasingly complex research questions or theories of ours. After a minimally sufficient introduction to the matter, the remainder of this section will focus on the random part, which is not so random as one may think.</p>
<p>Generally, statistical models consist of these two parts: the <em>likelihood</em> to describe the association between predictors and expected values and the random part, which describes the overall influence of the unexplained SMURFs.</p>
<div id="finish-1" class="section level4">
<h4><span class="header-section-number">4.4.0.1</span> FINISH</h4>
</div>
<div id="predictions-and-likelihood" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Predictions and likelihood</h3>
<p>The structural part of statistical models</p>
<div id="edit" class="section level4">
<h4><span class="header-section-number">4.4.1.1</span> EDIT</h4>
<p>The likelihood function states the dependency of outcome on the predictor variables. The dependency can be a complex mathematical function of multiple predictors, or as simple as the population average. A common likelihood function is the linear function. For example, in their guessing game, Violet could try to improve her population model, by also taking age of participants into account. Older people tend to be slower. Violet creates a plot from past records. The ellipsoid form of the point cloud indicates that ToT is somehow depending on age. Violet draws a straight line with an upward slope to approximate the relationship. It seems that 30 year old persons have an average ToT of around 90 seconds, which increases to around 120 seconds for 50 year old. Arithmetically, this is an increase of around 1.5 seconds per year of age.</p>
<p>Violet can use this information to improve her gambling. Instead of stoically calling the population mean, she uses a linear function as predictor: $90 + ( - 30) 1.5 $. In Bayesian statistics, this is called a <em>likelihood function</em> and the general form for a single linear likelihood function is:</p>
<p><span class="math display">\[\mu_i = \beta_0 + \beta_1x_{1i}\\\]</span></p>
<p>Likelihood functions connect the <em>expected value</em> <span class="math inline">\(\mu\)</span> with <em>observed variables</em> <span class="math inline">\(x_{i1}, x_{i2}, ..., x_{ik}\)</span>, and (to be estimated) parameters, e.g. <span class="math inline">\(\beta_0, \beta_1\)</span>. The likelihood function is often called the <em>deterministic part</em> of a model, because its prediction strictly depends on the observed values and the predictors, but nothing else. For example, two persons of age 30 will always be predicted to use up 90 seconds. Apparently, this is not the case for real data.</p>
<p>The linear model is very common in statistical modeling, but likelihoods can basically take all mathematical forms. For example:</p>
<ul>
<li>the grand mean model, Violet used before: <span class="math inline">\(\mu_i = \beta_0\)</span></li>
<li>two predictors with a linear relationship: <span class="math inline">\(\mu_i = \beta_0 + \beta_1x_{1i} + \beta_1x_{2i}\\\)</span></li>
<li>a parabolic relationship: <span class="math inline">\(\mu_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i}^2\)</span></li>
<li>a nonlinear learning curve: <span class="math inline">\(\mu_i = \beta_\textrm{asym} (1 + \exp(-\beta_\textrm{rate}(x_\textrm{training} + \beta_\textrm{pexp})))\)</span></li>
<li>the difference between groups A and B, where <span class="math inline">\(x_1\)</span> is a membership (dummy) variable coded as <span class="math inline">\(A\rightarrow x_1:=0, B\rightarrow x_1:=1\)</span>: <span class="math inline">\(\mu_i = \beta_0 + \beta_1x_{1i}\)</span></li>
</ul>
<p>In the vast majority of cases, the likelihood function is the interesting part of the model, where researchers transform their theoretical considerations or practical questions into a mathematical form. The parameters of the likelihood function are being estimated and answer the urging questions, such as:</p>
<ul>
<li>Is the design efficient enough? (<span class="math inline">\(\beta_0\)</span>)</li>
<li>By how much does performance depend on age? (<span class="math inline">\(\beta_1\)</span>)</li>
<li>Under which level of arousal does performance peak? (determining the stationary point of the parabola)</li>
<li>How fast people learn by training (<span class="math inline">\(\beta_\textrm{rate}\)</span>)</li>
<li>By how much design B is better than A (<span class="math inline">\(\beta_1\)</span>)</li>
</ul>
<p>A subtle, but noteworthy feature of likelihood functions is that <span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(x_i\)</span> have indicators <span class="math inline">\(i\)</span>. Potentially, every observation <span class="math inline">\(i\)</span> has their own realization of predictors and gets a unique expected value, whereas the parameters <span class="math inline">\(\beta_0, \beta_1\)</span> asf. are single values that apply for all observations at once. In fact, we can conceive statistical models as operating on multiple levels, where there is always the two: the observation level and the population level. When introducing multi-level models, we will see how this principle extends to more than these two levels. Another related idea is that parameters summarizes patterns found in data. Any summary implies repetition and that is what the likelihood expresses: the pattern that repeats across observations and is therefore predictable.</p>
</div>
</div>
<div id="distributions" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Distributions: shapes of randomness</h3>
<!--
+ spell out R code hidden in figures and formulas to teach Rs distribution commands along the way
+ change trials to attempts for binomial distributions to not confuse with experimental trails
- discrete or continuous
- cumulative probability
- sums to one
- density intervals
- probability and density
+ range of support
+ parameters
- location and dispersion
+ in R
-->
<p>Review the formula <span class="math display">\[\text{Measure}_i = \text{structural part} + \text{random part}_i\]</span>. What do we actually have? It turns out that the only part we know for sure is the measure <span class="math inline">\(i\)</span>. The structural part contains the answer to our research question, so this is what we want to estimate. Unfortunately, there is another unknown, the <span class="math inline">\(random part\)</span>, which we literally need to subtract to get to our answers. Obviously, this would never work if the random part were random in the sense of being completely arbitrary. Fortunately, randomness is not arbitrary, but has a shape, which is described as a probability (or density) distribution. These distributions typically belong to a certain family of distributions, e.g. Poisson disributions or Gaussian distributions. By assuming a certain shape of randomness, we know a lot more about the random component, which will make it easier to flesh out the structure. Choosing the most appropriate distribution therefore is a crucial step in creating a valid statistical model.</p>
<div id="probability-and-density-distributions" class="section level4">
<h4><span class="header-section-number">4.4.2.1</span> Probability and density distributions</h4>
<p>The random part of a statistical model is what changes between observation and is not fully predictable. When using the grand mean model, the only information we are using is that the person is from the target population. Everything else is left to the unobserved SMURFs and that goes into the random part of the model. Fortunately, SMURFs don’t work completely arbitrary and in practice there is just a small number of recognizable shapes randomness can take. These patterns can be formulated mathematically as probability and density distributions. A probability distribution is given by a probability mass function (PMF) that assigns <em>probabilities to outcomes</em>, such as:</p>
<ul>
<li>probability of <em>task success</em> is <span class="math inline">\(.81\)</span></li>
<li>probability of <em>99 seconds or better</em> is <span class="math inline">\(.22\)</span></li>
<li>probability of all SMURFs together pushing a persons <em>IQ beyond 115</em> is around <span class="math inline">\(.33\)</span></li>
</ul>
</div>
<div id="redact" class="section level4">
<h4><span class="header-section-number">4.4.2.2</span> REDACT</h4>
<p>Probability mass functions are mathematical functions that assign probabilities to the outcome of a measured variable <span class="math inline">\(Y\)</span>. Let’s see this at a small example: Consider a participant who is asked to complete three tasks of constant difficulty, with a chance of <span class="math inline">\(.3\)</span> for each one to be solved. The outcome variable of interest is the number of correct completions, which can take the values 0, 1, 2 or 3. Under idealized conditions (but not removing randomness), the following probability distribution assigns every possible outcome a probability to occur.</p>
<p>We see that the most likely outcome is one correct task, which occurs with a probability of <span class="math inline">\(P(y = 1) = 0.441\)</span>. At the same time, it is surprisingly likely to fail at all tasks, <span class="math inline">\(P(y = 0) = 0.343\)</span>. We may also look at <em>combined events</em>, say the probability for less than two correct. That is precisely the sum <span class="math inline">\(P(y \leq 1) = P(y = 0) + P(y = 1) = 0.784\)</span>. We can bundle basic events by adding up the probabilities. An extreme case of that is the universal event that includes all possible outcomes. You can say with absolute certainty that the outcome is, indeed, within zero and three and certainty means the probability is 1, or: <span class="math inline">\(P(0 \leq y \leq 3) = 1\)</span>. Simply in order to comply with the third Kolmogorov axiom, all probability (and density) distributions have <em>probability mass of One</em>. More precisely, the area covered by the function must be exactly One.</p>
<p>Outcomes like task successes have a countable number of possible outcomes, and are therefore called <em>discrete</em>. If we want to obtain the probability of combined events, we just have to sum probabilities over all the included events. This approach fails, if the outcome is not discret, but continuous. On a continuous scale, every outcome is a point in the strictly geometrical sense of having no extension. That makes the probability masses of any such point extensionless, too, i.e. exactly Zero.</p>
<p>Another way to characterize a random distribution is <em>cumulative distribution distribution (CDF)</em>, which is the renders the <em>increase in probability mass</em> when moving from left to right over the outcome scale. As shown below for discrete outcomes this function starts at Zero, then moves upwards in increments and reaches One at the left end of the scale.</p>
<p>Using the CDF, the probability of an event, that is a <em>range of outcomes</em>, can be written as the difference between the upper and the lower limit of cumulative probability mass. For example, the probability of having two or three tasks correct is. Note that the lower limit must be included and strictly spoken, the second term in the difference is the <em>largest outcome value not included</em>. Owing to extensionlessness of outcomes this does not matter for continuous outcomes. Note that the lower limit must be included and strictly spoken, the second term in the difference is the <em>largest outcome value not included</em>. Owing to extensionlessness of outcomes this does not matter for continuous outcomes.</p>
<p>when the outcome measure is <em>continuous</em>, rather than discrete, we cannot assign non-zero probabilities to outcomes. But, we can give non-zero probabilities to ranges of outcoms. Consider the distribution of intelligence quotients (IQ) scores. Strictly spoken, the IQ is <em>not</em> continuous, as one usually only measures and reports whole number scores. Still, for instructional purposes, assume that the IQ is given in arbitrary precision, for example <span class="math inline">\(115.0\)</span>, <span class="math inline">\(100.00010...\)</span> or <span class="math inline">\(\pi * 20\)</span>. IQ scores are <em>designed to follow the Gaussian</em> distribution with a mean of 100 and a standard distribution of 15.</p>
</div>
<div id="here" class="section level4">
<h4><span class="header-section-number">4.4.2.3</span> HERE</h4>
<p>That means, we can use this distribution functions to predict</p>
<p>We observe that the most likely IQ is 100 and that almost nobody reaches scores higher than 150 or lower than 50. But, how likely is it to have an IQ of exactly 100? Less than you might think! With continuous measures, we can no longer think in blocks that have a certain area. In fact, the probability of having an IQ of <em>exactly</em> <span class="math inline">\(100.00...0\)</span> is exactly zero. The block of IQ = 100 is infinitely narrow and therefore has an area of zero. Generally, with continuous outcome variables, We can no longer read probabilities directly. Therefore, probability mass distributions don’t apply, but the association between outcome and probability is given by what is called <em>probability density functions</em>. Density is not confined to the probability unit interval, but what density distributions share with PDFs is that the area under the curve is always exactly one and that probabilities still arise when taking intervals of values.</p>
<p>Practically, nobody is really interested in infinite precision. When asking <em>“what is the probability of IQ = 100?”</em>, the answer is <em>“zero”</em>, but what was really meant was: <em>“what is the probability of an IQ in a close interval around 100?”</em>. Once we speak of intervals, we clearly have areas larger than zero. The graph below shows the area in the range of 85 to 115.</p>
<p>But, how large is this area exactly? As the distribution is curved, we can no longer simply count virtual blocks. Recall that the CDF gives the probability mass, i.e. the area under the curve, for outcomes up to a chosen point. Continuous distributions have CDFs, too, and the the graph below shows the CDF for the IQs. We observe how the curve starts to rise from zero at around 50, has its steepest point at 100, just to slow down and run against 1.</p>
<p>Take a look at the following graph. It shows the two areas <span class="math inline">\(IQ \leq 85\)</span> and <span class="math inline">\(IQ \leq 115\)</span>. The magic patch in the center is just the desired interval.</p>
<p>And here the CDF comes into the play. To any point of the PDF, the CDF yields the area up to this point and we can compute the area of the interval by simple subtraction:</p>
<p><span class="math display">\[
\begin{aligned}
P(IQ \leq 115) &amp;&amp;= 0.159
P(IQ \leq 85) &amp;&amp;= 0.841
P(85 \leq IQ \leq 115) &amp;= P(IQ \leq 115) - P(IQ \leq 85) &amp;= 0.683
\end{aligned}
\]</span></p>
<p>Probability and density distributions usually take on one of a few forms, which can be expressed as mathematical functions. For example, the function for the case of task completion is the binomial distribution, which gives the probability for <span class="math inline">\(y\)</span> successes in <span class="math inline">\(k\)</span> trials when the success rate is <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
Pr(y|p,k) = {k  \choose y}p^y(1-p)^{k-y}
\]</span></p>
<p>In most cases where the binomial distribution applies, base probability <span class="math inline">\(p\)</span> is the parameter of interest, whereas the number of trials is known beforehand and therefore does not require estimation. For that reason, the binomial distribution is commonly taken as a one-parameter distribution. When discussing the binomial in more detail, we will learn that <span class="math inline">\(p\)</span> determines the location of the distribution, as well as how widely it is dispersed (preview Figure XY). The distribution that approximated the IQs is called the Gaussian distribution (or Normal). The Gaussian distribution function takes two parameters, <span class="math inline">\(mu\)</span> determines the location of the distribution, say average IQ being 98, 100 or 102 and <span class="math inline">\(sigma\)</span> which gives the dispersion, independently (preview Figure XY).</p>
</div>
<div id="location-and-dispersion" class="section level4">
<h4><span class="header-section-number">4.4.2.4</span> Location and dispersion</h4>
</div>
<div id="redact-and-merge" class="section level4">
<h4><span class="header-section-number">4.4.2.5</span> REDACT AND MERGE</h4>
<p>As we will see, random distributions can take a variety of basic shapes, but two immediate properties are <em>location</em> and <em>dispersion</em>. Location of a distribution usually reflects where the most typical values come to lie (100 in the IQ example). When an experimenter asks for the difference of two designs in ToT, this is purely about location. Dispersion can either represent <em>uncertainty</em> or <em>variation</em> in a population, depending on the research design and statistical model. The most common interpretation is uncertainty. The basic problem with dispersion is that spreading out a distribution influences <em>how typical</em> the most typical values are. The fake IQ data basically is a perfect Gaussian distribution with a mean of 100 and a standard deviation of 15. The density of this distribution at an IQ of 100 is 0.027. If IQs had a standard deviation of 30, the density at 100 would fall to 0.013. If you were in game to guess an unfamiliar persons IQ, in both cases 100 would be the best guess, but you had a considerable higher chance of being right, when dispersion is low.</p>
<p>The perspective of uncertainty routinely occurs in the experimental comparison of conditions, e.g. design A compared to design B. What causes experimenters worry is when the <em>residual distributions</em> in their models is widely spread. Roughly speaking, residuals are the variation that is not predicted by the model. The source of this variation is unknown and usually called <em>measurement error</em>. It resides in the realm of the SMURFs. With stronger measurement error dispersion, the two estimated locations get less certainty assigned, which blurs the difference between the two.</p>
<p>The second perspective on dispersion is that it indicates <em>variation by a known source</em>. Frequently, this source is differences between persons. The IQ is an extreme example of this, as these tests are purposefully designed to have the desired distribution. In chapter [LMM] we will encounter several sources of variation, but I am really concerned about human variation, mostly. Commonly, experimental researchers are obsessed by differences in location, which to my mind confuses “the most typical” with “in general”. Only when variation by participants is low, this gradually becomes the same. We will re-encounter this idea when turning to multi-level models.</p>
<p>Most distributions routinely used in statistics have one or two parameters. Generally, if there is one parameter this determines both, location and dispersion, whereas two-parameter distributions can vary location and dispersion independently, to some extent. The Gaussian distribution is a special case as <span class="math inline">\(\mu\)</span> purely does location, whereas <span class="math inline">\(\sigma\)</span> is just dispersion. With common two-parametric distributions, both parameters influence location and dispersion in more or less twisted ways. For example, mean and variance of a two-parametric binomial distributions both depend on chance of success <span class="math inline">\(p\)</span> and number of trials <span class="math inline">\(k\)</span>, as <span class="math inline">\(\textrm{M} = pk\)</span> and <span class="math inline">\(\textrm{Var} = kp(1-p)\)</span>.</p>
</div>
<div id="range-of-support-and-skew" class="section level4">
<h4><span class="header-section-number">4.4.2.6</span> Range of support and skew [### ]</h4>
</div>
<div id="complete-and-merge" class="section level4">
<h4><span class="header-section-number">4.4.2.7</span> COMPLETE AND MERGE</h4>
<p>In this book I advocate the thoughtful choice of distributions rather than doing batteries of goodness-of-fit to confirm that one of them, the Gaussian, is an adequate approximation. It usual is trivial to determine whether a measure is discrete (like everything that is counted) or (quasi)continuous and that is the most salient feature of distributions. A second, nearly as obvious, feature of any measure is its range. Practically all physical measures, such as duration, size or temperature have natural lower bounds, which typically results in scales of measurement which are non-negative. Counts have a lower boundary, too (zero), but there can be a known upper bound, such as the number of trials. Statistical distributions can be classified the same way: having no bounds (Gaussian, t), one bound (usually the lower, Poisson, exponential) or two bounds (binomial, beta).</p>
</div>
<div id="data-generating-process" class="section level4">
<h4><span class="header-section-number">4.4.2.8</span> Data generating process</h4>
</div>
<div id="zap" class="section level4">
<h4><span class="header-section-number">4.4.2.9</span> ZAP</h4>
<p>Many dozens of PMFs and PDFs are known in statistical science and are candidates to choose from. First orientation grounds on superficial characteristics of measures, such as discrete/continuous or range, but that is sometimes not sufficient. For example, the pattern of randomness in three-tasks falls into a binomial distribution only, when all trials have the same chance of success. If the tasks are very similar in content and structure, learning is likely to happen and the chance of success differs between trials. Using the binomial distribution when chances are not constant leads to severely mistaken statistical models.</p>
<p>For most distributions, strict mathematical definitions exist for under which circumstances randomness takes this particular pattern. Frequently, there is one or more natural phenomena that accurately fall into this pattern, such as the number of radioactive isotope cores decaying in a certain interval (Poisson distributed) or … . This is particularly the case for the canonical four random distributions that follow. Why are these canonical? The pragmatic answer is: they cover the basic types of measures: chance of success in a number of trials (binomial), counting (Poisson) and continuous measures (exponential, Gaussian).</p>
<!--
However, 


Imagine, standing slightly elevated in front of a moving crowd and choose one pedestrian to follow with your eyes. The task is easier when the target person is tall, moves straight on while the rest of the crowd is moving in random curly patterns. This method also works to identify the waiter in a crowded bar. 


The theorists answer is twofold: first, they are exponential .... second, they are lowest entropy.




Random distributions: 

+ discrete or continuous
+ probability and density
+ range of support
+ parameters
+ location and dispersion
+ data generating process
-->
</div>
<div id="binomial_dist" class="section level4">
<h4><span class="header-section-number">4.4.2.10</span> Binomial distributions</h4>
</div>
<div id="redact-1" class="section level4">
<h4><span class="header-section-number">4.4.2.11</span> REDACT</h4>
<p>A very basic performance variable in design research is task success. Think of devices in high risk situations such as medical infusion pumps in surgery. These devices are remarkably simple, giving a medication at a certain rate into the bloodstream for a given time. Yet, they are operated by humans under high pressure and must therefore be extremely error proof in handling. Imagine, the European government would set up a law that manufacturers of medical infusion pump must prove a 90% error-free operation in routine tasks. A possible validation study could be as follows: a sample of <span class="math inline">\(N = 30\)</span> experienced nurses are invited to a testing lab and asked to complete ten standard tasks with the device. The number of error-free task completions per nurse is the recorded performance variable to validate the 90% claim. Under somewhat idealized conditions, namely that all nurses have the same proficiency with the device and all tasks have the success chance of 90%, the outcome follows a <em>Binomial distribution</em> and the results could look like the following:</p>
<p>Speaking about the Binomial distribution in terms of <em>successes in a number of attempts</em> is common. As a matter of fact, <em>any</em> binary classification of outcomes is amenable for Binomial modeling, like on/off, red/blue, male/female. Imagine, Jane’s big boss needs a catchy phrase for an investor meeting. Together they decide that the return rate of customers could be a good measure, translating into a statement such as <em>eighty percent of customers come back</em>. To prove (or disprove) the claim, Jane uses the customer data base and divides all individuals into two groups: those who have precisely one record and those who returned (no matter how many times). This process results in a distribution, that has two possible outcomes: : <span class="math inline">\(0\)</span> for one-timers and <span class="math inline">\(1\)</span> for returners. This is in fact, a special case of the Binomial distribution with <span class="math inline">\(k = 1\)</span> attempts (the first visit is not counted). Examples are given in the first row of the figure.</p>
<p>A Binomial distributions has two parameters: <span class="math inline">\(p\)</span> is the chance of success and <span class="math inline">\(k\)</span> is the number of attempts. <span class="math inline">\(p\)</span> is a probability and therefore can take values in the range from zero to one. With larger <span class="math inline">\(p\)</span> the distribution moves to the right. The mean of Binomial distributions is the probability scaled by number of attempts, <span class="math inline">\(M = kp\)</span>. Logically, there cannot be more successes then <span class="math inline">\(k\)</span>, but with larger <span class="math inline">\(k\)</span> the distribution gets wider. The variance is the odds scaled by number of attempts, <span class="math inline">\(\textrm{Var} = kp(1-p)\)</span>. As mean and variance depend on the exact same parameters, they cannot be set independently. In fact, the relation <span class="math inline">\(Var = M(1-p)\)</span> is parabolic, so that variance is largest at <span class="math inline">\(p = .5\)</span>, but decreases towards both boundaries. A Binomial distribution with, say <span class="math inline">\(k=10\)</span> and <span class="math inline">\(p = .4\)</span> always has mean <span class="math inline">\(4\)</span> and variance <span class="math inline">\(2.4\)</span>. This means, in turn, that an outcome with a mean of <span class="math inline">\(4\)</span> and a variance of <span class="math inline">\(3\)</span> is not Binomially distributed. This occurs frequently, when the success rate is not identical across trials. A common solution is to use plugin distributions, where the parameter <span class="math inline">\(p\)</span> itself is distributed, rather than fixed. A common distribution for <span class="math inline">\(p\)</span> is the <em>beta</em> distribution and the <em>logit-normal</em> distribution is an alternative.</p>
<p>The Binomial distribution has two boundaries, zero below and number of attempts <span class="math inline">\(k\)</span> above. While a lower boundary of zero is often natural, one cannot always speak of a number of attempts. For example, the number of times a customer returns to the car rental website does not yield a natural interpretation of number of attempts. Rather, one could imagine the situation as that any moment is an opportunity to hire a car. At the same time, every single moment has a very, very small chance that a car is hired, indeed. Under these conditions, an infinite (or painstakingly large) number of opportunities and a very low rate, the random pattern is neatly summarized by <em>Poisson distributions</em>.</p>
</div>
<div id="poisson_dist" class="section level4">
<h4><span class="header-section-number">4.4.2.12</span> Poisson distributions</h4>
<p>Some counting processes have no natural upper limit like the number of trials in a test. In design research, a number of measures are such <em>unbound counts</em>:</p>
<ul>
<li>number of erroneous actions</li>
<li>frequency of returns</li>
<li>behavioral events, e.g. showing exploratory behavior</li>
<li>physiological events, such as number of peaks in galvanic skin response</li>
</ul>
<p>These measures can often be modeled as <em>Poisson distributed</em>. A useful way to think of unbound counts, is that they can happen at every moment, but with a very small chance. Think of a longer interaction sequence of a user with a system, where errors are recorded. It can be conceived as an almost infinite number of opportunities to err, with a very small chance of something to happen. The Poisson distribution is a so called limiting case of the binomial distributions, with infinite <span class="math inline">\(k\)</span> and very small <span class="math inline">\(p\)</span>. Of course, such a situation is completely ideal. Yet, Poisson distributions fit such situations well enough.</p>
<p>Poisson distributions possess only one parameter <span class="math inline">\(\lambda\)</span> (lambda), that is strictly positive and determines mean and variance of the distribution alike: <span class="math inline">\(\lambda = M = \textrm{Var}\)</span>. As a matter of fact, there cannot be massively dispersed distributions close to zero, nor narrow ones in the far. Owing to the lower boundary, Poisson distributions are <em>asymmetric</em>, with the left tail always being steeper. Higher <span class="math inline">\(\lambda\)</span>s push the distribution away from the boundary and the skew diminishes. It is commonly practiced to approximate counts in the high numbers by <em>Gaussian distributions</em>.</p>
<p>The linkage between mean and variance is very strict for Poison distributions. Only a certain amount of randomness can be contained at a location. If there is more randomness, and that is almost certainly so, Poisson distributions are not appropriate. One speaks of <em>over-dispersion</em> in such a case.</p>
</div>
<div id="transition" class="section level4">
<h4><span class="header-section-number">4.4.2.13</span> TRANSITION</h4>
<p>Consider a very simple video game, <em>subway smurfer</em>, where the player jumps and runs a little blue avatar on the roof of a train and catches items passing by. Many items have been placed into the game, but catching a single one is very difficult. The developers are aware that a too low success rate would demotivate players as much as when the game is made to easy. In this experiment, only one player is recorded, and in wonderful ways this player never suffers from fatigue, or is getting better with training. The player plays a 100 times and records the catches after every run. In this idealized situation, the distribution of catches would, approximately, follow a Poisson distribution, as in the figure below.</p>
<p>Consider a variation of the experiment with 100 players doing one game and less restrictive rules. Players come differently equipped to perform visual search tasks and coordinate actions at high speeds. They are tested at different times of the day and by chance feel a bit groggy or energized. The chance of catching varies between players, which violates the assumption that was borrowed from the Binomial, a constant chance <span class="math inline">\(p\)</span>. The extra variation is seen in the wider of the two distributions.</p>
<p>Poisson distributions’ lower boundary can cause trouble: the measure at hand is truly required to include the lower bound. A person can perform a sequence with no errors, catch zero items or have no friends on social media. But, you cannot complete an interaction sequence in zero steps or have a conversation with less than two statements. Fortunately, once a count measure has a lower boundary right of zero, the offset is often available, such as the minimum necessary steps to complete a task. In such a case, the number of erroneous steps can be derived and used as a measure, instead:</p>
<p><span class="math display">\[
\textrm{\#errors} = \textrm{\#steps} - \textrm{\#neccessary steps}
\]</span></p>
</div>
<div id="zap-1" class="section level4">
<h4><span class="header-section-number">4.4.2.14</span> ZAP</h4>
<p>Another lower bound problem arises, when there first is a hurdle before . In traffic research, the frequency of use public transport certainly is an interesting variable. A straight-forward assessment would be to ask bus passengers “How many times have you taken the bus the last five days?”. This clearly is a count measure, but it cannot be zero, because the person is sitting in the bus right now. This could be solved by a more inclusive form of inquiry, such as approaching random households. But, the problem is deeper: actually, the whole population is of two classes, those who use public transport and those who don’t.</p>
</div>
<div id="exponential-distribution-tbc" class="section level4">
<h4><span class="header-section-number">4.4.2.15</span> Exponential distribution [TBC]</h4>
<p>Exponential distributions apply for measures of duration. Exponential distributions have the same generating process as Poisson distributions, except, that the <em>duration for an event to happen</em> is the variable of interest, rather than number of events in a given time. The same idealized conditions of a completely unaffected subway smurfer player and constant catchability of items, the duration between any two catches is exponentially distributed. In more general, the chance for the event to happen is the same at any moment, completely independent of how long one has been waiting for it. For this property, the exponential distribution is called <em>memoryless</em>.</p>
<p>Durations are common measures in design research, most importantly, time-on-task and reaction time. Unfortunately, the exponential distribution is a poor approximation of the random pattern found in duration measures. That is for two reasons: first, the exponential distribution shares with Poisson, that it does not allow variance between participants. Second, the distribution always starts at zero, whereas human reactions always include some basic processing, and be this just the velocity of signals passing nerve cells, which is far below speed of sound (in air).</p>
</div>
<div id="finish-2" class="section level4">
<h4><span class="header-section-number">4.4.2.16</span> FINISH</h4>
</div>
<div id="gamma-distribution-tbd" class="section level4">
<h4><span class="header-section-number">4.4.2.17</span> Gamma distribution [TBD]</h4>
</div>
<div id="write" class="section level4">
<h4><span class="header-section-number">4.4.2.18</span> WRITE</h4>
</div>
<div id="gaussian-distributions" class="section level4">
<h4><span class="header-section-number">4.4.2.19</span> Gaussian distributions</h4>
</div>
<div id="edit-1" class="section level4">
<h4><span class="header-section-number">4.4.2.20</span> EDIT</h4>
<p>The best known distributions are <em>normal distributions</em> or <em>Gaussian distributions</em>. These distributions arise mathematically under the assumption of a myriad of small unrelated forces (SMURF) pushing performance (or any other outcome) up or down. As SMURFs work in all directions independently, their effects often average out and the majority of observations stays clumped together in the center, more or less.</p>
<p>Normal distributions have two parameters: <span class="math inline">\(\mu\)</span> marks the center and mean of the distribution. The linear models introduced later are aiming at predicting <span class="math inline">\(\mu\)</span>. The second parameter <span class="math inline">\(\sigma\)</span> represents the dispersion of the random pattern. When randomness is pronounced, the center of the distribution gets less mass assigned, as the tails get wider. Different to Poisson and Binomial distributions, mean and variance of the distribution can be set independently and over-dispersion is never an issue.</p>
<p>Normal distributions have the compelling interpretation of summarizing the effect of SMURFs. They serve to capture randomness in a broad class of regression models and other statistical approaches. The problem with normal distributions is that they only capture the pattern of randomness under two assumption. The first assumption is that the outcome is continuous. While that holds for duration as a measure of performance, it would not hold for counting the errors a user makes. The second assumption is that the SMURFs are truly additive, like the forces add up, when two pool balls collide. This appears subtle at first, but it has the far reaching consequence that the outcome variable must have an infinite range in both directions, which is impossible.</p>
<p>The normal distribution is called “normal”, because people normally use it. Of course not. It got its name for a deeper reason, commonly known (and held with awe) as the <em>central limit theorem</em>. Basically, this theorem proves what we have passingly observed at binomial and Poisson distributions: the more they move to the right, the more symmetric they get. The central limit theorem proves that, in the long run, a wide range of distributions are indistinguishable from the normal distribution. In practice, infinity is relative. In some cases, it is reasonable to trade in some fidelity for convenience and good approximations make effective statisticians. As a general rule, the normal distribution approximates other distributions well, when the majority of measures stay far from the natural boundaries. That is the case in experiments with very many attempts and moderate chances(e.g. signal detection experiments), when counts are in the high numbers (number of clicks in a complex task) or with long durations. However, these rules are no guarantee and careful model criticism is essential.</p>
<p>Measurement is prime and specialized (non-central-limit) distributions remain the first recommendation for capturing measurement errors. The true salvation of normal distributions is their application in <em>multi-level models</em>. While last century statistics was reigned by questions of location, and variance considered nuisance, new statistics care for variation. Most notably, amount of variation in a population is added as a central idea in multi-level modeling, which is commonly referred to as <em>random effects</em>. These models can become highly complex and convenience is needed more than ever. Normal distributions tie things together in multi-level models, as they keep location and dispersion apart, tidy.</p>
<p>The dilemma is then solved with the introduction of <em>generalized linear models</em>, which is a framework for using linear models with appropriate error distributions. Fortunately, MLM and GLM work seamlessly together. With MLM we can conveniently build graceful likelihood models, using normal distributions for populations. The GLM part is a thin layer to get the measurement scale right and choose the right error distribution, just like a looking glass.</p>
<!--
#### Plug-in distributions [TBD]

The five canonical random distributions match the basic type of measurements, they make strict assumptions on the data generating process  A majority of data does not meet these conditions. A routine problem is that binomial and Poisson assume that the chance for an event to occur is strictly constant. Take the number of successful tasks in . A Poisson distribution with $\lambda = M = \textrm{Var}$ emerges only, if all users in the study have the same chance of errors. Could that be the case?

In GLMM is can happen that a normal distribution sits underneath a Poisson distribution. That is not a plugin distribution, strictly, but a related concept.



-->
<!-- #### Exercises:  -->
<!-- 1. Google it: speed of sound and signal velocity in nerve cells. -->
</div>
</div>
</div>
<div id="bayesian-estimation" class="section level2">
<h2><span class="header-section-number">4.5</span> Bayesian estimation</h2>
<div id="complete-me-1" class="section level4">
<h4><span class="header-section-number">4.5.0.1</span> COMPLETE ME</h4>
<p>Frequentist statistics falls short on recognizing that research is incremental. Bayesian statistics embraces the idea of gradual increase in certainty when new data arrives. Why has Bayesian statistics not been broadly adopted earlier? The reason is that Bayesian estimation was computationally unfeasible for many decades. In practice, the seemingly innocent multiplication of prior and likelihood results in a complex integral, which in most cases has no analytic solution. If you have enjoyed a classic statistics education, you may remember how the computation of sum of squares (explained and residual) can be done by paper and pencil in reasonable time (e.g. during an exam). And that is precisely how statistical computations have been performed before the advent of electronic computing machinery. In the frequentist statistical framework, ingenious mathematicians have developed procedures that were rather efficient to compute. That made statistical data analysis possible in those times. It came at costs, though:</p>
<ol style="list-style-type: decimal">
<li>procedures make more or less strong assumptions, limiting their applicability.</li>
<li>procedures are asymptotically accurate with inference being accurate at large sample sizes only</li>
<li>common researchers do not understand crucial elements, for example how the F distribution is derived</li>
</ol>
<p>Expensive computation is in the past. Modern computers can simulate realistic worlds in real time and the complex integrals in Bayesian statistics they solve hands down. When analytic solutions do not exist, the integrals can still be solved using numerical procedures. Numerical procedures have been used in frequentist statistics, too, for example the iterative least squares algorithm applies for Generalized Linear Models, Newton-Rapson optimizer can be used to find the maximum likelihood estimate and boot-strapping produces accurate confidence limits. However, these procedures are too limited as they fail for highly multidimensional problems as they are common in advanced regression models.</p>
<p>Most Bayesian estimation engines these days ground on a numerical procedure called <em>Markov-Chain Monte-Carlo (MCMC)</em> sampling. This method differs from the earlier mentioned in that it basically is a random walk. The closest frequentist counterpart to MCMC is the bootstrapping algorithm, which draws many samples from data and computes the estimates many times. In some way, MCMC turns this upside down, by randomly drawing possible parameter values and computing the posterior probability many times. Similar to boot strapping, the basic MCMC algorithm is so simple, it can be explained on half a page and implemented with 25 lines of code. Despite its simplicity, the MCMC algorithm is applicable to practically all statistical problems one can imagine. Being so simple and generic at the same time must come at some costs. The downside of MCMC sampling still is computing time. Models with little data and few variables, like the examples in [#rational], are estimated within a few minutes. Multi-level models, which we will encounter later in this book, can take hours and large psychometric models can take up to a few days of processing time.</p>
<p>The particular merit of the MCMC algorithm is that it not only delivers accurate point estimates in almost any situation, it produces the full <em>posterior probability distribution</em>. This lets us characterize a parameters magnitude and degree of (un-)certainty. Let’s run an analysis on the 20 rainfall observations to see how this happens.</p>
<p>What the estimation does, is to calculate the <em>posterior distribution</em> from the observations. The <em>posterior distribution</em> contains the probability (more precisely: the <em>density</em>) for all possible values of the parameter in question. The following density plot represents our belief about the parameter <span class="math inline">\(P(rain|cloudy)\)</span> after we have observed twenty days:</p>
<p>From the posterior distribution, we can deduct all kinds of summary statistics, such as:</p>
<ol style="list-style-type: decimal">
<li>the most likely value for a parameter in question is called the <em>posterior mode</em> and is the same as the <em>maximum likelihood estimate</em> when prior knowledge is absent.</li>
<li>the average of parameter values, weighted by their probability is called the <em>posterior mean</em></li>
<li>a defined range to express 95% (or any other level of) certainty is the <em>95% credibility interval</em></li>
</ol>
<p>We can also make non-standard evaluations on the posterior distribution, for example: How certain is it that <span class="math inline">\(P(rain|cloudy) &lt; 0.7\)</span>? We’ll demonstrate the use of this in the next section.</p>
<p>Coming back to MCMC: how is this distribution actually produced. In plain words, MCMC makes a random walk through parameter space. Regions where the true value is more likely to be are just visited more often. The posterior distribution plots above are really just marginal frequencies. Note how the gray connecting lines show the jumps in the MCMC random walk.</p>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="getting-started-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
