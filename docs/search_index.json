[
["index.html", "New statistics for the design researcher A Bayesian course in tidy R 1 Introduction 1.1 Whom this book is for 1.2 Assumptions 1.3 How to read this book 1.4 Quantitative design research 1.5 Studies 1.6 Observations and measures[TBC]", " New statistics for the design researcher A Bayesian course in tidy R Martin Schmettow 2020-08-12 1 Introduction 1.1 Whom this book is for 1.1.1 The empirical design researcher If you are not a designer, chances are good that you are a design researcher very well. Are you doing studies with people and is your work about ways to achieve or improve products, services, trainings or business? Welcome to the not-so-special club of people I call design researchers. Of course, I have no idea who you are, personally, but I figure you as one of the following personas: You are an industrial design researcher, thirty-three years old, leading a small team in the center for research of a car maker. Your task is to evaluate emerging technologies, like augmented reality, car-to-car communication and smart light. Downstream research relies on your competent assessment of what is feasible. For example, you have just been asked to evaluate the following: Blue light makes people wake up easier, could it also be used to let car drivers not fall asleep? Several engineers and one industrial engineering student are involved in putting a light color stimulator for a premium car of the brand and stuff two workstations in its back, hooked to the recording systems and driving performance and physiological measures. Yes, this is as expensive as it sounds, and this why you have skin in the game, when you do a study. You are a young academic and just got a cum laude master’s degree in computer science. For your thesis you developed and implemented an animated avatar for a digital assistant. A professor from the social psychology department has read your thesis. He just got a funding for research on mass emergency communication using projected virtual characters. He fears that a young psychologist would not be up to the technical part of the project and found you. But, you ask yourself, am I up to the task of running experimental studies and do the statistics? 1.1.2 The experimentalist If you are doing research with people, but the answers you seek are far from being applicable to anything in sight, don’t put this book away. Chances are good that you will be able to read through the ephemeral details of the cases I present and recognize your own research situations, for example that you are comparing two groups. As you can see in the table of contents, you don’t even have to read more than half the book to get there. It may be true that strictly experimental data does not require more than group comparison. Still, I please you: also read the chapter on multilevel models. So much contemporary experimental research mistakens average for universal. It makes a difference to ask: “Are responses in the Stroop task responses delayed in the incongruent condition?” or to ask: “Every person responds delayed in the incongruent condition?” If your work is about revealing universal laws of behaviour, you have to search for the answer on an individual level. Technically, that means a rather simple multi-level model and a spaghetti plot will do the rest. But note that such a model is composed of many little participant-level models and all of them need their data. For multi-level models you need a within-subject design and repeated measures. On the second thought that makes full sense, as what really ask is: “Do all people shift into a slower mode in the incongruent condition?” This is not about groups of people, but mode transitions in individual brains. These transitions can only be observed by putting one-and-the-same person into all the conditions. If you fear that the order of modes makes a difference, why not put order under statistical control? Record what you cannot control and check the interaction effects. Maybe it is not so bad. Just another candy for you: Response times! Have you been struggling with them forever, because they are not Normal distributed? Have you resorted to non-parametric tests so many times that what you say became “response times are non-parametric”? You shouldn’t say that. Furthermore, your non-parametric sacrifices can be history with Generalized Linear Models. Then again, you may soon notice a disturbing lack of stars. Is this book like a very dark night? Consider the opposite possibility: most times when you don’t see the stars is at daylight. But let me hand you a torch with a chapter on model selection. The following paragraph may help you pushing your work through the review process: In order to confirm that the Stroop effect exists, we compared the predictive power of two models by information crieria. M_0 is an intercept-only model that denies the Stroop effect, M_1 allows for it. In order to evaluate universality of the Stroop effect, the models got participant-level random effects. As response times tend to have an offset and are left-skewed we chose exponential-Gaussian error distributions. 1.1.3 The applied researcher Tongue-in-cheek, applied researchers take real problems to get their questions, but rarely solve them. Why not? It is legitimate, almost natural, to ask what causes the Uncanny Valley effect, for instance. You do a series of experimental study and also throw personality scales into the game. Maybe, the effect is not unversal. Why? Just that. 1.2 Assumptions This book makes the following assumptions: Design research is for decision making, where one accounts for expected utility of design options. This requires quantitative statements statements of (un)certainty Bayesian statistics is intuitive. Everybody has a good intuition about probability, value, decision making and gambling. It requires little effort to get to a more formal level of understanding. Data arrives through data generating processes. Premise of statistical modelling is to neatly align to the generating process’ anatomy. Applied research data is multivariate and correlated. There is nothing such as a nuisance variable. Everything that helps understanding advantages and drawbacks of a design matters. Average? Neverage! People differ, and diversity matters in design. The universe is endless, but everything in it is finite, which strictly is at odds with linearity assumptions. The human eye is a highly capable pattern evaluator. It would be a waste to not use visuals for exploring data and communicating results. The best way to anticipate and plan data analysis is to simulate data upfront. R is the statisticians preferred toolbox. 80% of statistical analysis can be done with 20% of R’s full capabilities. 1.3 How to read this book Chapter @ref(design_research) introduces a framework for quantitative design research. It carves out the basic elements of empirical design research, such as users, designs and performance and links them to typical research problems. Then the idea of design as decision making under uncertainty is developed at the example of two case studies. Chapter @ref(bayesian_statistics) introduces the basic idea of Bayesian statistics, which boils down to three remarkably intuitive conjectures: uncertainty is the rule you gain more certainty by observations your present knowledge about the world is composed of what you learned from data and what you knew before. The chapter goes on with introducing basic terms of statistical thinking. Finally, an overview on common statistical distributions serve as a vehicle to make the reader familiar with data generazing processes. I am confident that this chapter serves newbies and classically trained researchers with all tools they need to understand Bayesian statistics. The more formally trained reader may want to take this as a bedtime reading. Chapter @ref(getting_started_r) is a minimal introduction to this marvelous programming language. Readers with some programming experience can work through this in just one day and they will get everything they need to get started with the book. Readers with prior R experience may get a lot out of this chapter, too, as it introduces the tidy paradigm of R programming. Tidy R is best be thought of as a set standard libraries 2.0. New tidy packages are arriving at an accelerating pace in the R world, and coding tidy is usually much briefer, easier to read and less error prone. It has at least the same significance as turning from procedural programming to object orientation. While this chapter serves as a stepping stone, the reader will encounter countless code examples throughout the book, working through these examples is a The second part of the book consists of three chapters that strictly built on each other. The first chapter @ref(linear_models) introduces basic elements of linear models, which are factors, covariates and interaction effects. A number of additional sections cover under-the-hood concepts, such as dummy variables or contrasts, as well as basic model criticism (residual analysis). Working through this chapter fully is essential as develops the jargon to a good deal. However, for most readers, the first chapter is not sufficient to get to work, as it does not cover models for repeated measures. This is added extensively in chapter @ref(multilevel_models). It proceeds from simple repeated measures designs to complex designs, where human and multiple non-human populations encounter each other. After working through this chapter, the reader is prepared to design highly effective studies. As long as the patterns of randomness are not off by to much from the linear model assumptions, it also suffices to get to work, seriously. The third chapter @ref(generalized_linear:models) opens up a plethora of possibilities to analyze all kinds of performance variables. Standard models are introduces to deal with counts, chances, temporal variables and rating scales. It is also meant as an eye opener for reseachers who routinely resorted to non-parametric procedures. After working through this chapter, I would consider anyone a sincere data analyst. The fourth chapter @ref(nonlinear_models) re-iterates on the truth that in an endless universe everything is finite. We leave our fond of high-level regression engine alone and take a closer look at the nuts and bolts it is based upon. By the example of a logistic growth process and efefcts of training, the reader gains a preview on the almighty model specification language Stan. The third part of the book is primarily dedicated to researchers who deal with complex and high-stake problems, routinely. Elememts of the statistical workflow are spread out in much detail, covering data manipulation, advanced graphics, simulation and model evaluation. 1.4 Quantitative design research 1.4.0.1 BEGIN 1.4.0.2 REDUCE 1.4.0.3 STRUCTURE 1.4.0.4 COMPLETE A design is the structure of an artifact or process that people make for a purpose. This definition appears extremely broad as it covers everything, tea kettles, software, cockpits, training programs and complex human-machine systems. In fact, this definition is even redundant in two points: first, artifacts are usually made for a purpose and some call them just “tools”. Second, who else than people have purposes? Well, many wild animals reportedly create artificial structures for purposes such as shelter, mating and hunting. And when I have chosen such a broad definition for design, it is that I have absolutely no objections against using this book to compare the stability of birds nestings, for example. (In such a case, everyone please, read “design” as “part of the extended phenotype” and “purpose” as “emerged by natural selection”. This book may apply to everything anyone likes to call a design; but only so for a specific set of research questions that hinge on the word “purpose”. We all have purposes and much of the time it is quite a job to get there. Getting-there typically requires resources and often enough we find ourselves right there only to some extent. So, the two basic questions in quantitative design research are: to what amount is a purpose fulfilled? how easy is it to get there? To the apt reader this may sound as i would next reduce all quantitative design research to the concept of usability and in a way i will be doing just that below (even discarding some subconcepts). The point here is that statistician actually don’t care much about definitions from a domain, but think about research problems in a more abstract manner. Here is a number of things, a statistician would inquire: On what a scale is the measurement of purpose fulfillment? What is the expected precision of measures? Is a lot of noise to be expected? How many observations are available? Is the research concerned with how purpose fulfillment compares under various conditions? How are observations grouped? In this book, quantitative design research is rather defined by a set of typical research problems, which includes the structure of the research question, as well as the empirical circumstances. In the following I will break this down one by one, and will also point out why the statistical framework of this book Bayesian regression models will do the job. 1.4.0.4.1 Introduce magnitude, as this is used when explaining uncertainty. One obvious element is quantification, but that carries some ambiguity, as we can speak of measures or research questions. In much research, quantification happens during the process of measuring. Recording reaction times or counting errors certainly is quantitative. But, what matters is to really ask quantitative research questions and try to answer them. In the real world, decisions are (or should be) based on benefits and rational allocation of resources. Changing the background color of a website might just be a switch (and can have undesired effects as users hate change), but restructuring an intranet site can cost a million. In industrial design research, there usually is someone who wants to know whether this or that redesign is worth it, and that requires to ask research questions like the following: By how much does reaction ability degrade with age and it is safe that people beyond 80 still drive? Does design B reduce the required number of steps by a third, at least? What proportion of users prefers red over yellow? All websites better go red? Sadly, in much existing research, precise quantification is frequently lost along the way and conclusions read more like: Older people have longer reaction times in traffic situations (\\(p \\leq .05\\)). People find information more easily with design A, compared to B (\\(p \\leq .05\\)). Chinese people on average prefer red color schemes over yellow (\\(p \\leq .001\\)). There simply is no numbers in these statements (except for the notorious p-values, which I will briefly discuss in chapter @ref(bayesian_statistics)). Regression models are just right for measures and they are terrific at drawing quantitative conclusions. Every regression model features a so called outcome, which must be a measure (rather than a category). At the same time, regression models can deal with a broad class of measures and their peculiarities. Modern designs tend to be very complex and so are research questions, potentially. The options for designing just your personal homepage are myriad and there is considerable uncertainty about which options, or rather which configuration works best. Consider every option, say font type, font size, color, background color, position of menu, a potential impact factor on how pleasant the page is to read. Perhaps, someone should once and for all figure out the optimal configuration. It is recommended in such a case, that as many as possible impact factors are represented in a single study and evaluated by a single comprehensive statistical model. The primary reason for this recommendation is given in chapter @ref(interaction_effects): impact factors have the nasty tendency to not act out independent of each other. Regression models handle such complexity with grace. There is theoretically no limit for the number of impact factors or predicors and interaction effects. The central peculiarity in all behavioural research is that measures are extremely noisy. In the next chapter, the concept of measurement errors will be elaborated upon, but for now it suffices to understand that all measures approximate the measured property, only and that leaves room for uncertainty. In a simple experiment where participants respond to a signal and time is recorded, the first three trials will more likely have values that are widely scattered, like 900ms, 1080ms, 1110ms. Imagine this were an ability test in a training for, say, astronauts. To be admissioned to the space program the applicant needs a score of less than 1000ms. Would you dare to decide on the career of a young person based on these three observations? Hardly so. On the other side, consider measuring a persons waste length for the purpose of tayloring a suit. By using a meter the taylor measures 990mm, and would be perfectly fine with that. Why did the taylor not take a second and a third measure? Well, experience tells that meters are pretty precise measures and waste length shows relatively little variation (under constant habits). Say the two measures were 995mm and 989mm. Such small deviations have practically no influence on cutting the linen. “Our minds are not run as top - down dictatorships ; they are rambunctious parliaments, populated by squabbling factions and caucuses, with much more going on beneath the surface than our conscious awareness ever accesses.” Carroll, Sean. The Big Picture (Kindle Locations 5029-5031). Oneworld Publications. Kindle Edition. Vast fluctuations of measures are common in design research, simply for the fact that human behaviour is involved. Every magnitude we derive from a study is uncertain to some degree. Uncertainty makes that at any moment, we can rely on a quantitative result only to some extent, which influences how we take risks. Statistics is called inferential, when every derived magnitude comes with a degree of certainty attached. Section @ref(decision_making) gives some reasoning and examples, how to operate rationally under uncertainty, and drives at right into the arms of Bayesian statistics. In an admission test for astronaut training, a decision is raised on very few individuals. Down on earth, everyday designs affect many people at once. Consider your personal homepage. If you decide, for aesthetic reasons, to shrink the font size, I promise you, that you just start loosing all visitors from the e-senior generation. Or, if your content is really good, they start using looking glasses on their o-pads. As a researcher, you can approach this in two ways: do a user study to compare the new design to the current design, or be the one who finds out, once and for all, what the optimal trade-off is between readability and aesthetic pleasure. Your method of choice would be an experiment. When you have no greater goal in mind than proving your design is of quality, user studies are effective and quick. In the easiest case, you want to put your design against a fixed benchmark. For example, in the design of automotives, media devices in the cockpit may not distract the driver for more than 1.5 seconds at times [REF]. To prove that, you will have to plug some advanced eye tracking gear into a prototype car and send people on the test drive. But once the precise data is in, things get really simple. The saccade measures directly represent what you were out for: the length of episodes of visual attention on the media display. You can take the average value. IN web design, it is common to compare two or more designs in order to make the best choice. An e-commerce company can put a potential future design on the test drive, delivering it to a customer sample. Performance is measured as hard currency, which is as close to the purpose as it can get. A user studies solves the momentary problem of comparing a local design to a benchmark (which can be another design). In the long run, design configurations are too manyfold to be compared in a one-by-one manner. It is inevitable that we try to trace some general patterns and apply our knowledge to a whole class of designs at once. Our research design just got one step more complex. Instead of just checking whether a smaller font size creates problems on a single website, the reseacher reaches out to comparing the combined effect of aging and font size on reading time, in general. This is what I call a design experiment. Design experiments allow for much broader conclusions, if done right, and there are a some issues: The design features under scrunity must be under control of the researcher. It does not suffice to collect some websites with varying font sizes, but every website needs to undergo the test at various font sizes. The design feature must undergo a full range of manipulations. You will not find the laws of readability by just comparing 10pt versus 12pt. Design features usually do not stand on their own. Readability is influenced by other factors, such as contrast and comprehensibility. Deriving a model from just one design will only generalize to this one design. Hence, the researcher must run the experiment on a sample of designs, with one of two strategies (or a mix of both): the randomization strategy takes a representative sample of designs, hoping that other impact factors average out. As we will see in @ref(interaction_effects), this is a daring assumption. Therefore, the preferred way is statistical control, where potential impact factors are recorded and added as control variables to the regression model. 1.4.0.4.2 TODO Designing is wicked Evaluative design research Decision problems in design research Design research as exploration Mapping multidimensional impact factors Quantification for decision making Minimax decision making on designs Measures and psychometrics Emergent design theories 1.5 Studies user studies experimental/fundamental studies qualitative vs quantitative 1.6 Observations and measures[TBC] 1.6.1 Interaction sequences Behavioural records not necessarily require one-way mirrors and the nights of video coding. Log files of web servers provide sequences of how users navigate a web site. Plugin software is available that records keystrokes and mouse actions on computers. The difficult part is the following: When observing 50 users while doing a non-trivial task, no two interaction sequences are exactly the same (if i had to bet on it). By itself, there is little value without further means of interpretation and this can go two ways up: The diligent and skilled qualitative researchers are able to extract meaningful patterns from such sequences. For example, in usability tests, the following patterns are frequently observed and interpreted. a user jumping back to the main screen possibly went into a wrong direction users randomly probing around either have no clue or no motivation users in a longitudinal study who only interact at the begin and the end of the period, have no use for the system, really. The quantitative researcher will aim at deriving measures from the interaction sequence. Formally, to measure means assigning numbers to observed sequences, so that these can be brought into an order, at least. Obviously, we need some sort of transformation that gives us a single performance score. This we call here the performance function and we have many choices, for example: the number of all exploratory events their relative frequency their longest uninterrupted sequence total time spent in exploration Sometimes, you have to go a long way up the qualitative route, before you can derive useful measures. In [Schnittker, Schmettow &amp; Schraagen, 2016], we coded sequences from nurses using an infusion pump. Individual sequences were compared to a optimal reference path, the similar the better. But how to measure similarity? For eample, Levensthein distance counts the number of edits to transform one sequence into the other and we used it as the performance score: deviation from optimal path. Even more interpreting was another study we did on the active user paradox. We recorded interaction sequences of users doing some repetitive tasks with a graphics software. The sessions were taped and analysed using a behavioural coding scheme. First, events were classified on a low level (e.g. “reading the handbook”, “first time trying a function”) and later aggregated to broader classes (e.g. “exploratory behavior”). The number of events in a given time frame that were classified as such were finally taken as a measure, representing the exploratory tendencies. 1.6.2 performance measures A good point to start with is the classic concept of “usability”, which the ISO 9142-11 defines by the following three high-level criteria: effectiveness: can users accomplish their tasks? efficiency: what resources have to be spend for a task, e.g. time. satisfaction: how did the user like it? While these criteria originated in the field of Human-Computer Interaction, they can easily be adapted to compare everything that people do their work with. Even within the field, it has been adapted to hundreds of studies and a hundred ways are reported of assessing these criteria. Effectiveness is often measured by completion rate (CR). A classic waterfall approach would be to consult the user requirements documents and identify the, let’s say eight, crucial tasks the system must support. User test might then show that most users fail at the two tasks, and a completion rate of 75% is recorded for the system. Completion rate is only a valid effectiveness measure with distinct tasks. Strictly, the set of tasks also had to be complete, covering the whole system. When completion rate is taken from in series of repetitive tasks, it depends on whether it is effectiveness or efficiency. It is effectiveness, when a failed operation is unrepairable, such as a traffic accident, data loss on a computer or medical errors. But, who cares for a single number between 0 and 1, when the user test provides such a wealth of information on why users failed? Effectiveness, in the sense of task completion, is primarily a qualitative issue and we shall rarely encounter it in this book. A more subtle notion of effectiveness is the quality of outcome, and despite the very term, is a measure. Perhaps, it should better be called task perfection. Reconsider the AUP study, where participants had to modify a given graphic, e.g. change some colors and erase parts of it. Of course, some participants worked neatly, whereas others used broader strokes (literally). There certainly is a reasonable objective way to rank all results by quality. Efficiency is where it really gets quantitative as with efficiency, we ask about resources: time, attention, strain, distraction and Euros. Other than that, efficiency can be measured in a vast variety of ways: ToT, clicks, mental workload or time spent watching the traffic while driving. Counting the number of clicks before someone has found the desired piece of informnation is a coarse, but easy to acquire and intuitive measure of efficiency, and so is time-on-task (ToT), which is just the sum . Still, these It differs from ToT in that it only counts real action, not the time a participant read the manual or watched the sky. In the downside, this is a very limited definition of action. While errors are usually best analyzed qualitatively, error rate can be efficiency, too, when failures are non-critical, but can be repaired in some time, such as correcting a typing error. ToT and other performance measures can be very noisy, and RT even more so. In order to arrive at sufficiently certain estimates, it is recommended to always plan for repetition. In [LMM] we will make heavy use of repeated measures on users, tasks and designs. Above, I did not mean to say that design research always requires the sometimes painful recording of interaction sequences. The majority of studies jumps over them and proceed to performance measures, directly, by counting attempts or pressing stop watches. In many situations, such direct measures are right spot-on: When controlling a car in dense city traffic, a few hundred milliseconds is what makes huge a difference and therefore reaction time is a valid performance measure. In experimental cognitive research reaction times are a dominant tool to reveal the structure of the mind. The Stroop tasks is a golden evergreen cognitive psychology and serves as an example. Participants are shown a words drawn in one of three ink colors and are asked to name the ink as quick as possible. The Stroop effect occurs: in the incongruent condition, participants respond much slower than in the congruent and neutral (a few hundred ms). There is ample sweet replication of this effect and an ecosystem of theories surround it. In [Schmettow, Noordzij, Mundt] we employ the Stroop task to read the minds of participants. We showed them pictures with computers on them, followed by a printed word (e.g., “explore”) on which to perform the Stroop task. It was conceived that reaction time is increased when a participant experiences a strong association, like: black tower PC + “explore” –&gt; “can someone hand me a screwdriver” CONDITION WORD : Ink congruent [GREEN ]: green incongr [YELLOW]: green neutral [CHAIR ]: green 1.6.2.1 COMPLETE ME 1.6.2.2 –&gt;Satisfaction 1.6.3 experience [TBD] 1.6.4 design features [TBD] 1.6.5 the human factor [TBD] 1.6.6 situations [TBD] "],
["elements-of-bayesian-statistics.html", "2 Elements of Bayesian Statistics", " 2 Elements of Bayesian Statistics "],
["getting-started-r.html", "3 Getting started with R 3.1 Setting up the R environment 3.2 Learning R: a primer", " 3 Getting started with R In this book, we will be using the statistical computing environment R. R at its core is a programming language that specializes on statistics and data analysis. Like all modern programming languages, R is more than just a compiler or interpreter that translates human-writeable formal statements into something a computer understands and can execute. R comes with a complete set of standard libraries that cover the usual basic stuff, as well as a collection of common statistical routines, for example the t-test or functions to compute mean and variance. Even regression analysis and plotting is included in R. Finally, packaged with R comes a rudimentary programming environment, including a console, a simple editor and a help system. Most of what comes packaged with R, we will set aside. R is basically a brilliantly designed programming language for the task, but many of the standard libraries are an inconsistent and outdated mess. For example, R comes with a set of commands to import data from various formats (SPSS, CSV, etc.). Some of these commands produce objects called data.frames, whereas others return lists of lists. Although one can convert lists into dataframes, it is easier and prevents confusion if all import functions simply create dataframes. As another example, to sort a data frame by participant number, one has to write the following syntax soup: D[order(D$Part),] In the past few years, a single person, Hadley Wickham, has almost single-handedly started an initiative known as the tidyverse. The tidyverse is a growing collection of libraries that ground on a coherent and powerful set of principles for data management. One of the core packages is dplyr and it introduces a rich, yet rather generic, set of commands for data manipulation. The sorting of a data frame mentioned above would be written as D %&gt;% arrange(Part) One of Wickham’s first and well-known contributions is the ggplot system for graphics. Think of R’s legacy graphics system as a zoo of individual routines, one for boxplots, another one for scatterplots asf. Like animals in a zoo, they live in different habitats with practically no interaction. ggplot implements a rather abstract framework for data plots, where all pieces can be combined in a myriad of ways, using a simple and consistent syntax. Where to get these gems? R is an open source system and has spawned an open ecosystem for statistical computing. Thousands of extensions for R have been made available by data scientists for data scientists. The majority of these packages is available through the comprehensive R archive network (CRAN). For the common user it suffices to think of CRAN as an internet catalogue of packages, that can be searched and where desired packages can be downloaded and installed in an instance. Finally, R comes with a very rudimentary programming environment that carries the questionable charm of early 1990s. Whereas several alternatives exist, most R users will feel most comfortable with the R programming environment Rstudio. At the time of writing, it is the most user-friendly and feature rich software to program in R. The next sections describe how you can set up a fully functional environment and verify that it works. Subsequently, we will get to know the basics of programming in R. 3.1 Setting up the R environment First, we have to make sure that you have the two essential applications R and Rstudio downloaded and installed on your computer. The two programs can be retrieved from the addresses below. Make sure to select the version fit for your operating system. R Rstudio If you fully own the computer you are working with, meaning that you have administrator rights, just do the usual downloading and running of the setup. If everything is fine, you’ll find R and Rstudio installed under c\\Programs\\ and both are in your computers start menu. You can directly proceed to the installation of packages. In corporate environments, two issues can arise with the installation: first a user may not have administrator rights to install programs to the common path c:\\programs\\. Second, the home directory may reside on a network drive, which is likely to cause trouble when installing packages. If you have no administrator rights, you must choose your home directory during the setup. If that is a local directory, (c:/Users/YourName/), this should work fine and you can proceed with the installation of packages. If your home directory (i.e. My Documents) is located on a network drive, this is likely to cause trouble. In such a case, you must install R and Rstudio to a local directory (on your computers hard drive), where you have full read/write access. In the following, it is assumed that this directory is D:/Users/YourName/: create a directory D:/Users/YourName/R/. This is where both programs, as well as packages will reside. create a sub directory Rlibrary where all additional packages reside (R comes with a pack of standard packages, which are in a read-only system directory). start Rstudio create a regular text file File -&gt; New File -&gt; Text file copy and paste code from the box below save the file as .Rprofile in D:/Users/YourName/R/ open the menu and go to Tools -&gt; Global options -&gt; General -&gt; Default working directory. Select D:/Users/YourName/R/. ## .Rprofile options(stringsAsFactors = FALSE) .First &lt;- function(){ RHOME &lt;&lt;- getwd() cat(&quot;\\nLoading .Rprofile in&quot;, getwd(), &quot;\\n&quot;) .libPaths(c(paste0(RHOME,&quot;Rlibrary&quot;), .libPaths())) } .Last &lt;- function(){ cat(&quot;\\nGoodbye at &quot;, date(), &quot;\\n&quot;) } With the above steps you have created a customized start-up profile for R. The profile primarily sets the library path to point to a directory on the computers drive. As you are owning this directory, R can install the packages without admin rights. In the second part, you configure Rstudio’s default path, which is where R, invoked from Rstudio, searches for the .Rprofile. After closing and reopening Rstudio, you should see a message in the console window saying: Loading .Rprofile in D:/Users/YourName/R/ That means that R has found the .Rprofile file and loaded it at start-up. The .Rprofile file primarily sets the path of the library, which is the collection of packages you install yourself. Whether this was successful can be checked by entering the console window in Rstudio, type the command below and hit Enter. .libPaths() If your installation went fine, you should see an output like the following. If the output lacks the first entry, your installation was not successful and you need to check all the above steps. [1] “D:/Users/YourName/R/Rlibrary” “C:/Program Files/R/R-3.3.0/library” 3.1.1 Installing CRAN packages While R comes with a set of standard packages, thousands of packages are available to enhance functionality for every purpose you can think of. Most packages are available from the Comprehensive R Network Archive (CRAN). For example, the package foreign is delivered with R and provides functions to read data files in various formats, e.g. SPSS files. The package haven is a rather new package, with enhanced functionality and usability. It is not delivered with R, hence, so we have to fetch it. Generally, packages need to be installed once on your system and to be loaded everytime you need them. Installation is fairly straight-forward once your R environment has been setup correctly and you have an internet connection. In this book we will use a number of additional packages from CRAN. The listed packages below are all required packages, all can be loaded using the library(package) command. The package tidyverse is a metapackage that installs and loads a number of modern packages. The ones being used in this book are: dplyr and tidyr for data manipulation ggplot for graphics haven for reading and writing files from other statistical packages readr for reading and writing text-based data files (e.g., CSV) readxl for reading Excel files stringr for string matching and manipulation ## tidyverse library(tidyverse) ## data manipulation library(openxlsx) ## plotting library(gridExtra) ## regression models library(rstanarm) ## other library(devtools) ## only needed for installing from Github library(knitr) ## non-CRAN packages library(mascutils) library(bayr) We start by checking the packages: create a new R file by File --&gt; New file --&gt; R script copy and paste the above code to that file and run it. By repeatedly pressing Ctrl-Return you run every line one-by-one. As a first time user with a fresh installation, you will now see error messages like: Error in library(tidyverse) : there is no package called ‘tidyverse’ This means that the respective package is not yet present in your R library. Before you can use the package tidyverse you have to install it from CRAN. For doing so, use the built-in package management in RStudio, which fetches the package from the web and is to be found in the tab Packages. At the first time, you may have to select a repository and refresh the package list, before you can find and install packages. Then click Install, enter the names of the missing package(s) and install. On the R console the following command downloads and installs the package tidyverse. install.packages(&quot;tidyverse&quot;) CRAN is like a giant stream of software pebbles, shaped over time in a growing tide. Typically, a package gets better with every version, be it in reliability or versatility, so you want to be up-to-date. Rstudio has a nice dialogue to update packages and there is the R command: update.packages(&quot;tidyverse&quot;) Finally, run the complete code block at once by selecting it and Ctrl-Enter. You will see some output to the console, which you should check once again. Unless the output contains any error messages (like above), you have successfully installed and loaded all packages. Note that R has a somewhat idiosyncratic jargon: many languages, such as Java or Python, call “libraries” what R calls “packages”. The library in R is strictly the set of packages installed on your computer and the library command loads a package from the library. 3.1.2 Installing packages from Github Two packages, mascutils and bayr are written by the author of this book. They have not yet been committed to CRAN, but they are available on Github which is a general purpose versioning system for software developers and few authors. Fortunately, with the help of the devtools package it is rather easy to install these packages, too. Just enter the Rstudio console and type: library(devtools) install_github(&quot;schmettow/mascutils&quot;) install_github(&quot;schmettow/bayr&quot;) Again, you have to do that only once after installing R and you can afterwards load the packages with the library command. Only if the package gets an update to add functionality or remove bugs, you need to run these commands again. 3.1.3 A first statistical program After you have set up your R environment, you are ready to run your first R program (you will not yet understand all the code, but as you proceed with this book, all will become clear): Stay in the file where you have inserted and run the above code for loading the packages. Find the environment tab in Rstudio. It should be empty. Copy-and-paste the code below into your first file, right after library commands. Run the code lines one-by-one and observe what happens (in RStudio: Ctrl-Enter) ## Simulation of a data set with 100 participants ## in two between-subject conditions N &lt;- 100 levels &lt;- c(&quot;control&quot;,&quot;experimental&quot;) Group &lt;- rep(levels, N/2) age &lt;- round(runif(N, 18, 35),0) outcome &lt;- rnorm(N, 42 * 5, 10) + (Group == &quot;experimental&quot;) * 42 Experiment &lt;- data_frame(Group, age, outcome) Experiment %&gt;% sample_n(8) Group age outcome control 22 210 control 21 200 control 18 201 experimental 25 243 control 34 199 experimental 26 268 control 28 225 experimental 30 270 ## Plotting the distribution of outcome Experiment %&gt;% ggplot( aes(x = outcome) ) + geom_density(fill = 1) ## ... outcome by group Experiment %&gt;% ggplot( aes(fill = Group, x = outcome) ) + geom_density() ## ... statistical model comparing the groups model &lt;- stan_glm(formula = outcome ~ Group, data = Experiment) fixef(model) fixef center lower upper Intercept 209.9 207 213.0 Groupexperimental 41.4 37 45.6 Observe Console, Environment and Plots. Did you see how the Environment window is populated with new variables (Values and Data)? a table appears in the Console, when executing the summary(Experiment) command? how the “camel-against-the-light” in Plots tab morphed into “two-piles-of-colored-sugar”? Congratulations! You have just simulated data of a virtual experiment with two groups summarized the data plotted the data and estimated a Bayesian regression model that compares the two groups. Isn’t it amazing, that in less than 20 simple statements we have just reached the level of a second-year bachelor student? Still, you may find the R output a little inconvenient, as you may want to save the output of your data analysis. Not long ago, that really was an issue, but in the past few years R has become a great tool for reproducable research. The most simple procedure of saving your analysis for print or sharing is to: save the R file your have created by hitting CTRL-S and selecting a directory and name. in RStudio open File --&gt; Compile notebook and select Word as a format. Hit Compile A new Word document should appear that shows all code and the output. Now, you can copy-and-paste the graphics into another document or a presentation. 3.1.4 Bibliographic notes Getting started with Rstudio (presentation) Getting started with Rstudio (ebook) rstudio cheat sheets is a collection of beautifully crafted cheat sheets for ggplot, dplyr and more. I suggest you print the data mangling and ggplot cheat sheets and always keep them on your desk. The tidyverse is a meta package that loads all core tidyverse packages by Hadley Wickham. 3.2 Learning R: a primer cRazy as in ‘idiosyncrasy’ This book is for applied researchers in design sciences, whose frequent task is to analyze data and report it to stakeholders. Consequently, the way I use R in this book capitalizes on interactive data analysis and reporting. As it turns out, a small fraction of R, mostly from the tidyverse, is sufficient to write R code that is effective and fully transparent. In most cases, a short chain of simple data transformations tidies the raw data which can then be pushed into a modelling or graphics engine that will do the hard work. We will not bother (ourselves and others) with usual programming concepts such as conditionals, loops or the somewhat eccentric approaches to functional programming. At the same time, we can almost ignore all the clever and advanced routines that underlay statistical inference and production of graphics, as others have done the hard work for us. R mainly serves three purposes, from easy to advanced: 1. interactive data analysis 2. creating data analysis reports 3. developing new statistical routines. It turns out that creating multimedia reports in R has become very easy by the knitr/markdown framework that is neatly integrated into the Rstudio environment. I With R one typically works interactively through a data analysis. The analysis often is a rather routine series of steps, like: load the data make a scatter plot run a regression create a coefficient table A program in R is usually developed iteratively: once you’ve loaded and checked your data, you progress to the next step step of your analysis, test it and proceed. At every step, one or more new objects are created in the environment, capturing intermediate and final results of the analysis: a data frame holding the data a graphics object holding a scatterplot a model object holding the results of a regression analysis a data frame for the coefficient table As R is an interpreter language, meaning there is no tedious compile-and-run cycles in everyday R programming. You develop the analysis as it happens. It is even normal to jump back and forth in an R program, while building it. R is a way to report and archive what precisely you have been doing with your data. In statistics, mathematical formulas are the common form of unambiguously describing a statistical model. For example, the following equation defines a linear regression model between the observed outcome \\(y\\) and the predictor \\(x\\): \\[ \\mu_i = \\beta_0 + \\beta_1x_i\\\\ y_i \\sim N(\\mu_i, \\sigma) \\] As we will later see [CLM], in Rs formula language the same model is unambiguously specified as: y ~ x R is currently the lingua franca of statistical computing. As a programming language, R has the same precision as math, but is more expressive. You can specify complex models, but also graphics and the steps of data checking and preparation. As an example, consider an outlier removal rule of: An observation is valid if it does not exceed the tenfold of the observation mean. We just applied our own rule of outlier removal to the data. Others may consider this rule invalid or arbitrary. Disagreement is virtue in science and one can only disagree with what one actually sees. In R, the researcher formally reports what precisely has been done with the data. For example, the same outlier removal rule is unambiguously specified by the following code (the first line just simulates some data). D &lt;- data_frame(score = c(1, 2, 4, 3, 5, 6, 50, 800)) D %&gt;% filter(score &lt; mean(score)) score 1 2 4 3 5 6 50 Finally, R is a way to develop and share statistical programs. Thousands of packages in the R ecosystem cover almost all statistical problems you can imagine. As a programming language, R has been designed for that particular purpose. Under the hood of R, a bunch of generic, yet powerful, principles purr to make it a convenient language for typical problems in statistical computation. Readers with programming experience can fly over the R basics that follow. But, as a specific purpose language R has a few idiosyncracies you should know about: Almost all programming languages the first element of a list has the index zero. We got used to it, but for beginners it is a another hurdle that is unnecessary. Mathematicians, catholiques, software developers in bars and everyone, young or old, counts “one”, “two” ,“three”. And so does R: c(1:3)[1:3] ## [1] 1 2 3 Counting from one is perhaps the most lovable idiosyncracy of R. But, lets also welcome people who have experience with other programming languages: The first thing one has to know about R is that it is a functional programming language. A function simply is a programmed procedure that takes data as input, applies some transformation and returns data as output. That sounds trivial, but there is an important difference to most other languages: Different to procedures in Pascal or object oriented methods (in Java or Python), functions are forbidden to modify any external object. A certain function is a black box, but one can be sure that the only thing it does is return a new object. At the same time, functions are first-class citizens in R and can be called everywhere, even as an argument to another function. The plyr package is famous for functions that call functions, also called high-level functions. The following three liner makes heavy use of high level functions. First, a list of binary matrices is generated by repeatedly calling a random number generator, then the row sum of all matrices is computed and returned as a (new) list. make_binary_matrix &lt;- function(size) as.matrix(rbernoulli(size^2, p = 1/4)) LoM &lt;- llply(c(5:7), make_binary_matrix) llply(LoM, rowSums) ## [[1]] ## [1] 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 ## ## [[2]] ## [1] 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 ## ## [[3]] ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 ## [39] 0 0 0 0 1 0 0 0 0 0 0 What we have seen is a routine application of higher level functions: apply a transformation to a sequence of data sets. In a majority of programming languages you had to write a loop instead and this is why experienced programmers can easily fall for the active user paradox when they learn R, by sticking to loops. Believe me one thing: Once you have wrapped your head around functional programming, you will program the same procedure in a quarter of time with half the code and your program will run significantly faster. My general advice is: Whenever you think you need a loop, you don’t. For me it helped to imagine the following: loops carry the notion of chain of data that moves over a fixed transformation device. In functional programing functions the can work in a hierarchy, a high-level device moves along a string of data. It carries an exchangeable low-level device that applies the desired transformation to every position. If you come from relational data bases you you have something in common with the statistician: you both think in transformation of tables. Not coincidently, the features in dplyr, the tidy data transformation engine, are clearly borrowed from SQL. You will also feel at home with the idea of reports powered by functional chunks embedded in a templating system. For object orientation folks, R is a good choice, but you have to get used to it. First, it gives you the choice of several object orientation systems, which sometimes requires to install a package. The so-called S3 system is the original. It is rather limited and some even call it informal. The approach is as simple as it is unusal. S3 mainly is a raw method dispatcher that can handles overloading of functions. S3 puts methods first, then objects, whereas in traditional object orientation, the method belongs to the class, making the object “knows” its methods. In S3, the object and the method both know their class. When calling an S3 method, you actually call a generic method that finds the matching method and applies it. Beginners are at peace with all of this. You can count like you do. Functional programming is intuitive for working on research data. And because of S3 the function summary always does something useful. 3.2.1 Assigning and calling Objects Any statistical analysis can be thought of as a production chain. You take the raw data and process it into a neat data table, which you feed into graphics and regression engines or summarize by other means. At almost every step there is an input and an output object. Objects are a basic feature of R. They are temporary storage places in the computer’s memory. Objects always have a name chosen by the programmer. By its name, a stored object can be found back at any time. Two basic operations apply for all objects: an object is stored by assigning it a name and it is retrieved by calling its name. If you wanted to store the number of observations in your data set under the name N_obs, you use the assignment operator &lt;-. The name of the variable is left of the operator, the assigned value is right of it. N_obs &lt;- 100 Now, that the value is stored, you can call it any time by simply calling its name: N_obs ## [1] 100 Just calling the name prints the value to Console. In typical interactive programming sessions with R, this is already quite useful. But, you can do much more with this mechanism. Often, what you want is to do calculations with the value. For example, you have a repeated measures study and want to calculate the average number of observations per participant. For this you need the number of observations, and the number of participants. The below code creates both objects, does the calculation (right of &lt;-) and stores it in another object avg_N_Obs N_Obs &lt;- 100 N_Part &lt;- 25 avg_N_Obs &lt;- N_Obs/N_Part avg_N_Obs ## [1] 4 Objects can exist without a name, but are volatile, then. They cannot be used any further. The following arithmetic operation does create an object, a single number. For a moment or so this number exists somewhere in your computers memory, but once it is printed to the screen, it is gone. Of course, the same expression can be called again, resulting in the same number. But, strictly, it is a different object. N_Obs/N_Part ## gone after printing ## [1] 4 N_Obs/N_Part ## same value, different object ## [1] 4 There even is a formal way to show that the two numbers, although having the same value assigned, are located at different addresses. This is just for the purpose of demonstration and you will rarely use it in everyday programming tasks: tracemem(N_Obs/N_Part) ## [1] &quot;&lt;000000003B9618B0&gt;&quot; tracemem(N_Obs/N_Part) ## [1] &quot;&lt;000000003B9CE888&gt;&quot; 3.2.2 Vectors Notice the [1] that R put in front of the single value when printing it? This is an index. Different to other programming languages, all basic data types are vectors in R. Vectors are containers for storing many values of the same type. The values are addressed by the index, N_Obs[1] calls the first element. In R, indices start counting with 1, which is different to most other languages that start at zero. And if you have a single value only, this is just a vector of length one. For statistics programming having vectors as basic data types makes perfect sense. Any statistical data is a collection of values. What holds for data is also true for functions applied to data. Practically all frequently used mathematical functions work on vectors, take the following example: X &lt;- rnorm(100, 2, 1) mean(X) ## [1] 1.96 The rnorm command produces a vector of length 100 from three values. More precisely, it does 100 random draws from a normal distribution with mean 2 and an SD of 1. The mean command takes the collection and reduces it to a single value. By the way, this is precisely, what we call a statistic: a single quantity that characterizes a collection of quantities. 3.2.3 Basic object types Objects can be of various classes. In R the common basic classes are: logical, factor, character, integer and numeric. Besides that, programmers can define their own complex classes, for example, to store the results of regression models. Objects of type logical store the two levels TRUE, FALSE, like presence or absence of a treatment, or passed and failed test items. With Boolean operators one can compute new logical values, for example Apple &lt;- TRUE Pear &lt;- FALSE Apple &amp; Pear ## and ## [1] FALSE Apple | Pear ## or ## [1] TRUE More generally, logical values can be used for categorization, when their are only two categories, which is called a dichotomous variable. For example, gender is usually coded as a vector of characters (\"m\", \"f\", \"f\"), but one can always do: is_female &lt;- c(FALSE, TRUE, TRUE) is_male &lt;- c(T, F, F) Programmers are lazy folks when it comes to typing, therefore R allows you to abbreviate TRUE and FALSE as shown above. As a consequence, one should never assign objects the name reserved for logical values, so don’t do one of the following: ## never do this TRUE &lt;- &quot;All philosophers have beards&quot; ## a character object, see below F &lt;- &quot;All gods existed before the big bang&quot; 42 * F The class numeric stores real numbers and is therefore abundant in statistical programming. All the usual arithmetic operations apply: a &lt;- 1.0 b &lt;- a + 1 sqrt(b) ## [1] 1.41 Objects of class integer are more specific as they store natural numbers, only. This often occurs as counts, ranks or indices. friends &lt;- c(anton = 1, berta = 3, carlo = 2) order(friends) ## [1] 1 3 2 The usual arithmetic operations apply, although the result of operation may no longer be integer, but numeric N &lt;- 3 sum_of_scores &lt;- 32 mean_score &lt;- 32/3 mean_score ## [1] 10.7 class(mean_score) ## [1] &quot;numeric&quot; Surprisingly, logical values can be used in arithmetic expressions, too. When R encounters value TRUE in an arithmetic context, it replaces it with 1, zero otherwise. Used with multiplication, this acts like an on/off switch, which we will use in [LM: dummy variables]. TRUE + TRUE ## [1] 2 sqrt(3 + TRUE) ## [1] 2 is_car &lt;- c(TRUE, TRUE, FALSE) ## bicycle otherwise wheels &lt;- 2 + is_car * 2 wheels ## [1] 4 4 2 Data sets usually contain variables that are not numeric, but partition the data into groups. For example, we frequently group observations by the following Part: participant Condition: experimental condition Design: one of several designs Education: level of education (e.g., low, middle or high) Two object types apply for grouping observations: factor and character. While factors specialize on grouping observations, character objects can also be used to store longer text, say the description of a usability problem. The following identifies two conditions in a study, say a comparison of designs A and B. Note how the factor identifies its levels when called to the screen: design_char &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;A&quot;) design_char ## [1] &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;A&quot; design_fact &lt;- factor(c(&quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;A&quot;)) design_fact ## [1] A B B A ## Levels: A B Statistical analyses deal with real world data which ever so often is messy. Frequently, a planned observation could not be recorded, because the participant decided to quit or the equipment did not work properly or the internet collapsed. Users of certain legacy statistics packages got used to coding missing observations as -999 and then declared this a missing value. In R missing values are first-class citizens. Every vector of a certain class can contain missing values, which are identified as NA. Most basic statistics functions, like mean(), sd() or median() are act conservatively when the data contains missing values. If there is a single NA in the variable to be summarized, the result is NA. While this is good in the sense of transparency, much of the time what the researcher wants is to have the summary statistic with NA values being removed, first. clicks &lt;- c(3, NA, 2) mean(clicks) ## [1] NA mean(clicks, na.rm = T) ## [1] 2.5 This book is about programming and statistics at the same time. Unfortunately, there are a few terms that have a particular meaning in both domains. One of those is a “variable”. In statistics, a variable usually is a property we have recorded, say the body length of persons, or their gender. In general programming, a variable is a space in the computers memories, where results can be stored and recalled. Fortunately, R avoids any confusion and calls objects what is usually called a programming variable. 3.2.4 Operators and functions R comes with a full bunch of functions for creating and summarizing data. Let me first introduce you to functions that produce exactly one number to characterize a vector. The following functions do that with to the vector X, every in their own way: length(X) sum(X) mean(X) var(X) sd(X) min(X) max(X) median(X) These functions are a transformation of data. The input to these transformations is X and is given as an argument to the function. Other functions require more than one argument. The quantile function is routinely used to summarize a variable. Recall that X has been drawn from a Normal distribution of \\(\\mu=2\\) and standard deviation \\(\\sigma = 1\\). All Normal distributions have the property that about 66% of the mass is within the range of \\(\\mu-\\sigma\\) and \\(\\mu+\\sigma\\) . That means in turn: 17% are below \\(\\mu-\\sigma\\) and 66 + 17 = 83% are below \\(\\mu+\\sigma\\). The number of observations in a certain range of values is called a quantile. The quantile function operates on X, but takes an (optional) vector of quantiles as second argument: quantile(X, c(.17, .83)) ## 17% 83% ## 1.17 2.71 Most functions in R have optional arguments that let you change how the function performs. The basic mathematical functions all have the optional argument na.rm. This is a switch that determines how the function deals with missing values NA. Many optional arguments have defaults. The default of na.rm is FALSE (“return NA in case of NAs in the vector”). By setting it to TRUE, they are removed before operation. B &lt;- c(1, 2, NA, 3) mean(B) ## [1] NA mean(B, na.rm = TRUE) ## [1] 2 Most more complex routines in R have an abundance of parameters, most of which have reasonable defaults, fortunately. To give a more complex example, the first call of stan_glm performs a Bayesian estimation of the grand mean model. The second does a Poisson grand mean model with 5000 iterations per MCMC chain. As seed has been fixed, every subsequent run will produce the exact same chains. My apologies for the jargon! D_1 = data_frame(X = rnorm(20, 2, 1)) M_1 = stan_glm(X ~ 1, data = D_1) D_2 = data_frame(X = rpois(20, 2)) M_2 = stan_glm(X ~ 1, family = poisson, seed = 42, iter = 5000, data = D_1) R brings the usual set of arithmetic operators, like +, -, *, / and more. In fact, an operator is just a function. The sum of two numbers can, indeed, be written in these two ways: 1 + 2 ## [1] 3 `+`(1,2) ## [1] 3 The second term is a function that takes two numbers as input and returns a third. It is just a different syntax, and this one is called the polish notation. I will never use it throughout the rest of this book. Another set of commonly used operators are logical, they implement Boolean algebra. Some common Boolean operators are shown in the truth table: A B !A A &amp; B A | B A == B not and or equals T T F T T T T F F F T F F T T F T F F F T F F T Be careful not to confuse Boolean “and” and “or” with their common natural language use. If you ask: “Can you buy apples or pears on the market?”, the natural answer would be: “both”. The Boolean answer is: TRUE. In a requirements document you could state “This app is for children and adults”. In Boolean the answer would be FALSE, because no one can be a child and an adult at the same time, strictly. A correct Boolean statement would be: “The envisioned users can be adult or child”. Further Boolean operators exist, but can be derived from the three above. For example, the exclusive OR, “either A or B” can be written as: (A | B) &amp; !(A &amp; B). This term only gets TRUE when A or B is TRUE, but not both. In the data analysis workflow, Boolean logic is frequently used for filtering data and we re-encounter them in data transformation. Finally, it sometimes is convenient or necessary to program own functions. A full coverage of developing functions is beyond the scope of this introduction, so I show just one simple example. If one desires a more convenient function to compute the mean that ignore missing values by default, this can be constructed as follows: mean_conv &lt;- function(x) { mean(x = B, na.rm = TRUE) } mean_conv(B) ## [1] 2 Notice that: the function() function creates new functions the arguments given to function(x) will be the arguments expected by the function mean_conv(x). code enclosed by More examples of creating basic functions can be found in section @ref(descriptive_stats). As R is a functional programming language, it offers very elaborate ways of programming functions, way beyond what is found in common languages, such as Python or Java. An advanced example is given in section @ref(forecasting_LOO). 3.2.5 Storing data in data frames Most behavioral research collects real data to work with. As behavior researchers are obsessed about finding associations between variables, real data usually contains several. If you have a sample of observations (e.g. participants) and every case has the same variables (measures or groups), data is stored in a table structure, where columns are variables and rows are observations. R knows the data.frame objects to store variable-by-observation tables. Data frames are tables, where columns represent statistical variables. Variables have names and can be of different data types, as they usually appear in empirical research. In many cases data frames are imported to R, as they represent real data. Here, we first see how to create data frames by simulation. First, we usually want some initial inspection of a freshly harvested data frame. Data frames are objects. By just calling a data frame it gets printed to the screen. This is a good moment to reflect on one idiosyncrasy in R, a feature, of course. R is primarily used interactively, which has the immense advantage that the programmer can incrementally build the data analysis. This implies, that the programmer often wants to quickly check what currently is in a variable. Now, observe what happens when we call Experiment: Experiment Group age outcome control 30 207 experimental 26 252 control 21 207 experimental 30 255 control 23 195 experimental 34 269 control 23 214 experimental 20 245 control 30 208 experimental 21 263 control 31 204 experimental 30 249 control 18 219 experimental 30 270 control 19 207 experimental 33 254 control 23 193 experimental 23 250 control 34 235 experimental 21 256 control 19 204 experimental 24 262 control 22 234 experimental 29 249 control 19 199 experimental 34 254 control 24 203 experimental 31 259 control 34 223 experimental 25 243 control 22 210 experimental 31 267 control 34 211 experimental 26 250 control 19 209 experimental 20 252 control 24 197 experimental 33 255 control 32 221 experimental 21 253 control 27 212 experimental 34 248 control 20 215 experimental 28 232 control 27 218 experimental 33 264 control 27 213 experimental 26 240 control 29 221 experimental 26 268 control 26 214 experimental 32 272 control 20 207 experimental 28 255 control 21 217 experimental 31 256 control 34 193 experimental 27 250 control 21 207 experimental 29 246 control 28 216 experimental 33 240 control 26 213 experimental 31 256 control 31 219 experimental 27 241 control 18 201 experimental 27 251 control 24 223 experimental 21 255 control 21 200 experimental 24 247 control 33 197 experimental 35 257 control 31 221 experimental 24 232 control 20 196 experimental 26 241 control 32 189 experimental 25 255 control 31 221 experimental 27 241 control 29 208 experimental 31 246 control 24 203 experimental 24 255 control 28 225 experimental 28 244 control 34 199 experimental 21 248 control 30 209 experimental 32 237 control 31 212 experimental 31 267 control 20 208 experimental 27 248 control 34 207 experimental 26 237 control 22 231 experimental 25 249 This is useful information printed to the screen, we see sample size, names of objects and their classes, and the first ten observations as examples. Obviously, this is not the data itself, but some sort of summary. It would be a complete disaster, if R would pass this information on when the call is part of an assignment or other operation on the data, for example: NewExp &lt;- Experiment. Apparently, R is aware of whether a called object is part of an operation, or purely for the programmers eyes. For any object, if called to the console in an intercative session, R silently uses the print function on the object. The following would give precisely the same results as above. print(Experiment) Such print functions exist for dozens of classes of objects. They represent what the developers thought would be the most salient or logical thing to print in an interactive session. By virtue of the object system, R finds the right print function for the object at hand. However, when working interactively, most of the time you do a data transformation and assign it to a new variable, thus you check it immediately. As the assignment is always silent (unless there occurs an error), you are forced to type a print statement afterwards. There is shortcut for that: if you embrace the assignment statement in brackets, the result will be printed to the screen. As convenient as this is, especially when creating longer data transformation chains, it introduces some visual clutter. In this book I use it sparingly, at most. The dedicated print function does what way back a developer thought would be the most useful information in most cases. Depending on the situation, it can provide too much, too little or just the wrong information. For example: when simulating a data set, like in [First Exercise], we want to check whether the resulting data frame has the desired length and whether the variables have the correct class. The standard implementation of the print command, dumps the whole data frame to the screen, which can be much more than ever needed. The newer dplyr implementation data_frame (from the tidyverse, also called tibble) you have seen in action already, is much better suited for this situation, as it displays dimensions and variable classes. In contrast, if the job is to spot whether there are many missing values (NA), the classic print performs better. Whatever the question is, several commands are available to look into a data frame from different perspectives. Another command that is implemented for a variety of classes is summary. For all data frames, it produces an overview with descriptive statistics for all variables (i.e. columns). Particularly useful for data initial screening, is that missing values are listed per variable. print(summary(Experiment)) ## Group age outcome ## Length:100 Min. :18.0 Min. :189 ## Class :character 1st Qu.:23.0 1st Qu.:209 ## Mode :character Median :27.0 Median :233 ## Mean :26.8 Mean :231 ## 3rd Qu.:31.0 3rd Qu.:251 ## Max. :35.0 Max. :272 The str (structure) command works on any R object and displays the hierarchical structure (if there is one): str(Experiment) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 100 obs. of 3 variables: ## $ Group : chr &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; ... ## $ age : num 30 26 21 30 23 34 23 20 30 21 ... ## $ outcome: num 207 252 207 255 195 ... Data frames store variables, but statistical procedures operate on variables. We need ways of accessing and manipulating statistical variables and we will have plenty. First, recall that in R the basic object types are all vectors. You can store as many elements as you want in an object, as long as they are of the same class. Internally, data frames are a collection of “vertical” vectors that are equally long. Being a collection of vectors, the variables of a data frame can be of different classes, like character, factor or numeric. In the most basic case, you want to calculate a statistic for a single variable out of a data frame. The $ operator pulls the variable out as a vector: mean(Experiment$outcome) ## [1] 231 As data frames are rectangular structures, you can also access individual values by their addresses. The following commands call the first outcome measure the first to third elements of Group the complete first row Experiment[ 1, 3] outcome 207 Experiment[1:3, 2] age 30 26 21 Experiment[ 1, ] Group age outcome control 30 207 Addressing one or more elements in square brackets, always requires two elements, first the row, second the column. As odd as it looks, one or both elements can be empty, which just means: get all rows (or all columns). Even the expression Experiment[,] is fully valid and will just the return the whole data frame. There is an important difference, however, when using R’s classic data.frame as compared to dplyr’s data_frameimplementation: When using single square brackets on dplyr data frames one always gets a data frame back. That is a very predictable behavior, and very much unlike the classic: with data.frame, when the addressed elements expand over multiple columns, like Experiment[, 1:2], the result will be a data.frame object, too. However, when slicing a single column, the result is a vector: Exp_classic &lt;- as.data.frame(Experiment) Exp_classic[1:2,1:2] ## data.frame Group age control 30 experimental 26 Exp_classic[1,] ## data.frame Group age outcome control 30 207 Exp_classic[,1] ## vector ## [1] &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; ## [6] &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; ## [11] &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; ## [16] &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; ## [21] &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; ## [26] &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; ## [31] &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; ## [36] &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; ## [41] &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; ## [46] &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; ## [51] &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; ## [56] &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; ## [61] &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; ## [66] &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; ## [71] &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; ## [76] &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; ## [81] &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; ## [86] &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; ## [91] &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; ## [96] &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; &quot;control&quot; &quot;experimental&quot; Predictability and a few other useful tweaks made me prefer data_frame over data.frame. But, many third-party packages continue to produce classic data.frame objects. For example, there is an alternative to package `readxl, openxlsx, which reads (and writes) Excel files. It returns classic data.frames, which can easily be converted as follows: D_foo &lt;- read.xlsx(&quot;foo.xlsx&quot;) %&gt;% as_data_frame() While data_frame[] behaves perfectly predictable, in that it always returns a data frame, even when this is just a single column or cell. Sometimes, one wants to truly extract a vector. With a data_frame a single column can be extracted as a vector, using double square brackets, or using the $ operator. Experiment[[1]] ## vector Experiment$Group ## the same Sometimes, it may be necessary to change values in a data frame. For example, a few outliers have been discovered during data screening, and the researcher decides to mark them as missing values. The syntax for indexing elements in a data frame can be used in conjunction with the assignment operator &lt;-. In the example below, we make the simulated experiment more realistic by injecting a few outliers. Then we discard these outliers by setting them all to NA. ## injecting Experiment[2, &quot;outcome&quot;] &lt;- 660 Experiment[6, &quot;outcome&quot;] &lt;- 987 Experiment[c(1,3), &quot;age&quot;] &lt;- -99 ## printing first few observations head(Experiment) Group age outcome control -99 207 experimental 26 660 control -99 207 experimental 30 255 control 23 195 experimental 34 987 ## setting to NA Experiment[c(2, 6),&quot;outcome&quot;] &lt;- NA Experiment[c(1, 3),&quot;age&quot;] &lt;- NA Besides the injection, note two more features of addressing data frame elements. The first is, that vectors can be used to address multiple rows, e.g. 2 and 6. In fact, the range operator 1:3 we used above is just a convenient way of creating a vector c(1,2,3). Although not shown in the example, this works for columns alike. The careful reader may also have noted another oddity in the above example. With Experiment[c(2, 6),\"outcome\"] we addressed two elements, but right-hand side of &lt;- is only one value. That is a basic mechanism of R, called reuse. When the left-hand side is longer than the right-hand side, the right-hand side is reused as many times as needed. Many basic functions in R work like this, and it can be quite useful. For example, you may want to create a vector of 20 random numbers, where one half has a different mean as the second half of observations. rnorm(20, mean = c(1,2), sd = 1) The above example reuses the two mean values 50 times, creating an alternating pattern. Strictly speaking, the sd = 1 parameter is reused, too, a 100 times. While reuse often comes in convenient, it can also lead to difficult programming errors. So, it is a good advice to be aware of this mechanism and always carefully check the input to vectorized functions. 3.2.6 Import, export and archiving R lets you import data from almost every conceivable source, given that you have installed and loaded the appropriate packages (foreign, haven or openxlsx for Excel files). Besides that R has its own file format for storing data, which is .Rda files. With these files you can save data frames (and any other object in R), using the save(data, file = \"some_file.Rda\") and load(file = \"some_file.Rda\") commands. Few people create their data tables directly in R, but have legacy data sets in Excel (.xslx) and SPSS files (.sav). Moreover, the data can be produced by electronic measurement devices (e.g. electrodermal response measures) or programmed experiments can provide data in different forms, for example as .csv (comma-separated-values) files. All these files can be opened by the following commands: ## Text files Experiment &lt;- read_csv(&quot;Data/Experiment.csv&quot;) ## Excel Experiment &lt;- read_excel(&quot;Data/Experiment.xlsx&quot;, sheet = 1) ## SPSS (haven) Experiment &lt;- read_sav(&quot;Data/Experiment.sav&quot;) ## SPSS (foreign) Experiment &lt;- read.spss(&quot;Data/Experiment.sav&quot;, to.data.frame = TRUE) Note that I gave two options for reading SPSS files. The first (with an underscore) is from the newer haven package (part of tidyverse). With some SPSS files, I experienced problems with this command, as it does not convert SPSS’s data type labelled (which is almost the same as an R factor). The alternative is the classic read.spss command which works almost always, but as a default it creates a list of lists, which is not what you typically want. With the extra argument, as shown, it behaves as expected. Remember, data frames are objects and volatile as such. If you leave your session, they are gone. Once you have you data frame imported and cleaned, you may want to store it to a file. Like for reading, many commands are available for writing all kinds of data formats. If you are lucky to have a complete R-based workflow, you can conveniently use R’s own format for storing data, Rdata files. For storing a data frame and then reading it back in (in your next session), simply do: save(Experiment, file = &quot;Data/Experiment.Rda&quot;) load(file = &quot;Data/Experiment.Rda&quot;) Note that with save and load all objects are restored by their original names, without using any assignments. Take care, as this will not overwrite any object with the same name. Another issue is that for the save command you have to explicitly refer to the file argument and provide the file path as a character object. In Rstudio, begin typing file=\"\", put the cursor between the quotation marks and hit Tab, which opens a small dialogue for navigation to the desired directory. Once you have loaded, prepared and started to analyze a data frame in R, there is little reason to go back to any legacy program. Still, the haven and foreign packages contain commands to write to various file formats. I’ll keep that as an exercise to the reader. 3.2.7 Case environments This book features more than a dozen case studies. Every case will be encountered several times and multiple objects are created along the way: data sets, regressions, graphics, tables, you name it. That posed the problem of naming the objects, so that they are unique. I could have chosen object names, like: BrowsingAB_M_1, AUP_M_1, etc. But, this is not what you normally would do, when working on one study at a time. Moreover, every letter you add to a line of code makes it more prone to errors and less likely that you, dear reader, are typing it in and trying it out yourself. For these reasons, all cases are enclosed in case environments and provided with this book. For getting a case environment to work in your session, it has to be loaded from the respective R data file first: load(&quot;BrowsingAB.Rda&quot;) In R, environments are containers for collections of objects. If an object BAB1 is placed in an environment BrowsingAB, it can be called as BrowsingAB$BAB1. This way, no brevity is gained. Another way to assess objects in an environment is to attach the environment first as: attach(BrowsingAB) BAB1 %&gt;% as_tbl_obs() detach(BrowsingAB) attach(Reading) D_1 %&gt;% as_tbl_obs() Calling attach gets you into the namespace of the environment (formally correct: the namespace gets imported to your working environment). All objects in that namespace become immediately visible by their mere name. The detach command leaves the environment, again. When working with the case environments, I strongly recommend to detach before attaching another environment. All case environments provided with this book contain one or more data sets. Many of the cases are synthetic data which has been generated by a simulation function. This function, routinely called simulate, is provided with the case environment, too. That gives you the freedom to produce your own data sets with the same structure, but different effects. Generally, calling the simulation function without any further arguments, exactly reproduces the synthetic data set provided with the case environment. simulate() %&gt;% as_tbl_obs() ## reproduces the data frame D_1 simulate(N = 6) %&gt;% as_tbl_obs() ## simulates first 6 participants only Furthermore, once you delve deeper into R, you can critically inspect the simulation function’s code for its behavioral and psychological assumptions (working through the later chapters on data management and simulation will help). simulate ## calling a function without parantheses prints its code ## function(N = 40, ## beta = c(Intercpt = 60, ## fnt_size_12 = -12, ## fnt_color_blk = -10, ## ia_blk_12 = 8), ## sigma = 5, ## seed = 42) ## { ## set.seed(seed) ## out &lt;- ## tibble(Part = 1:N, ## font_size = factor(rep(c(1, 2), N/2), ## levels = c(1,2), ## labels = c(&quot;10pt&quot;, &quot;12pt&quot;)), ## font_color = factor(c(rep(1, N/2), rep(2, N/2)), ## levels = c(1,2), ## labels = c(&quot;gray&quot;, &quot;black&quot;))) %&gt;% ## mutate( mu = beta[1] + ## beta[2] * (font_size == &quot;12pt&quot;) + ## beta[3] * (font_color == &quot;black&quot;) + ## beta[4] * (font_color == &quot;black&quot;) * (font_size == &quot;12pt&quot;), ## ToT = rnorm(N, mu, sigma)) %&gt;% ## as_tbl_obs() ## ## #class(out) &lt;- append(class(out), &quot;sim_tbl&quot;) ## attr(out, &quot;coef&quot;) &lt;- list(beta = beta, ## sigma = sigma) ## attr(out, &quot;seed&quot;) &lt;- seed ## ## out %&gt;% as_tbl_obs() ## } ## &lt;bytecode: 0x000000002c0b9500&gt; detach(Reading) Finally, the case environments contain all objects that have been created throughout this book. This is especially useful for the regression models, as fitting these can take from a few seconds to hours. Note that working with environments is a tricky business. Creating these case environments in an efficient way was more difficult than you may think. Therefore, I do not recommend using environments in everday data analysis, with one exception: at any moment the current working environment contains all objects that have been created, so far. That is precisely the set of objects shown in the Environment pane of Rstudio (or call ls() for a listing). Saving all objects and retrieving them when returning from a break is as easy as: save(file = &quot;my_data_analysis.Rda&quot;) ## have a break load(&quot;my_data_analysis.Rda&quot;) Next to that, Rstudio can be configured to save the current workspace on exit and reload it on the next start. When working on just one data analysis for a longer period of time, this can be a good choice. 3.2.8 Structuring data In the whole book, data sets are structured according to the rule one-row-per-observation of the dependent variable (the ORPO rule). Many researchers still organize their data tables as one-row-per-participant, as is requested by some legacy statistics programs. This is fine in research with non-repeated measures, but will not function properly with modern regression models, like linear mixed-effects models. Consider a study where two designs were evaluated by three participants using a self-report item, like “how easy to use is the interface?” Then, the wrong way of structuring the data would be: ORPO %&gt;% filter(Task == 1, Item == 1) %&gt;% mascutils::discard_redundant() %&gt;% spread(Design, response) %&gt;% as_tbl_obs() The correct way of setting up the data frame is: ORPO %&gt;% filter(Task == 1, Item == 1) %&gt;% mascutils::discard_redundant() Part Design response 1 A 4 2 A 4 3 A 6 1 B 1 2 B 4 3 B 3 The ORPO rule dictates another principle: every row should have a unique identifier, which can be a combination of values. In the example above, every observation is uniquely identified by the participant identifier and the design condition. If we extend the example slightly, it is immediatly apparent, why the ORPO rule is justified. Imagine, the study actually asked three partcipants to rate two different tasks on two different designs by three self-report items. By the ORPO rule, we can easily extend the data frame as below (showing a random selection of rows). I leave it up to the reader to figure out how to press such a data set in the wide legacy format. Using identifiers is good practice for several reasons. First, it reduces problems during manual data entry. Second, it allows to efficiently record data in multi-method experiments and join them automatically. This is shown in chapter [DM]. Lastly, the identifiers will become statistically interesting by themselves when we turn to linear mixed-effects models and the notion of members of a population. Throughout the book I will use standard names for recurring identifier variables in design research: Part Design Item Task Note that usually these entities get numerical identifiers, but these numbers are just labels. Throughout, variables are written Uppercase when they are entities, but not real numbers. An exception is the trial order in experiments with massive repeated measures. These get a numerical type to allow exploring effects over time such as learning, training and fatigue. ORPO %&gt;% sample_n(6) %&gt;% arrange(Part, Task, Design, Item) Part Task Design Item response 1 1 B 1 1 1 2 B 1 6 2 1 B 3 5 2 2 B 2 3 3 2 A 2 6 3 2 B 2 4 3.2.9 Data transformation Do you wonder about the strange use of %&gt;% in my code above? This is the tidy way of programming data transformations in R. The so-called magritte operator %&gt;% is part of the dplyr/tidyr framework for data manipulation. It chains steps of data manipulation by connecting transformation functions, also called piping. In the following, we will first see a few basic examples. Later, we will proceed to longer transformation chains and see how graceful dplyr piping is, compared to the classic data transformation syntax in R. Importing data from any of the possible resources, will typically give a data frame. However, often the researcher wants to select or rename variables in the data frame. Say, you want the variable Group to be called Condition, omit the variable age and store the new data frame as Exp. The select command does all this. In the following code the data frame Experiment is piped into select. The variable Condition is renamed to Group, and the variable outcome is taken as-is. All other variables are discarded. Exp &lt;- Experiment %&gt;% select(Condition = Group, outcome) %&gt;% as_tbl_obs() Another frequent step in data analysis is cleaning the data from missing values and outliers. In the following code example, we first “inject” a few missing values for age (which were coded as -99) and outliers (&gt;500) in the outcome variable. Note that I am using some R commands that you don’t need to understand by now. Then we reuse the above code for renaming (this time keeping age on board) and add some filtering steps: ## rename, then filtering Exp &lt;- Experiment %&gt;% select(Condition = Group, age, outcome) %&gt;% filter(outcome &lt; 500) %&gt;% filter(age != -99) Exp %&gt;% as_tbl_obs() During data preparation and analysis, new variables are created routinely. For example, the covariate is often shifted to the center before using linear regression: mean_age = mean(Exp$age) Exp &lt;- Exp %&gt;% mutate(age_cntrd = age - mean_age) Exp %&gt;% as_tbl_obs() Finally, for the descriptive statistics part of your report, you probably want to summarize the outcome variable per experimental condition. The following chain of commands first groups the data frame, then computes means and standard deviations. At every step, a data frame is piped into another command, which processes the data frame and outputs a data frame. Exp %&gt;% group_by(Condition) %&gt;% summarize(mean = mean(outcome), sd = sd(outcome) ) Condition mean sd control 210 10.83 experimental 251 9.47 3.2.10 Plotting data Good statistical graphics can vastly improve your and your readers understanding of data and results. This book exclusively introduces the modern ggplot2 graphics system of R, which is based on the grammar of graphics. Every plot starts with piping a data frame into the ggplot(aes(...)) command. The aes(...) argument of ggplot creates the aesthetics, which is a mapping between variables and features of the plot (and only remotely has something to do with beauty). Review the code once again that produces the piles-of-sugar: the aesthetics map the variable Group on the fill color, whereas outcome is mapped to the x axis. For a plot to be valid, there must at least one layer with a geometry. The above example uses the density geometry, which calculates the density and maps it to the y axis. The ggplot2 plotting system knows a full set of geometries, like: scatter plots with geom_point() smooth line plots with geom_smooth() histograms with geom_histogram() box plots with geom_boxplot() and my personal favorite: horizontal density diagrams with geom_violin() For a brief demonstration of ggplots basic functionality, we use the BAB1 data set of the BrowsingAB case. We attach the case environment and use the str command to take a first look at the data: attach(BrowsingAB) BAB1 %&gt;% as_tbl_obs() The BrowsingAB case is a virtual series of studies, where two websites were compared by how long it takes users to complete a given task, time-on-task (ToT). Besides the design factor, a number of additional variables exist, that could possibly play a role for ToT, too. We explore the data set with ggplot: We begin with a plot that shows the association between age of the participant and ToT. Both variables are metric and suggest themselves to be put on a 2D plane, with coordinates x and y, a scatter plot. BAB1 %&gt;% ggplot(aes(x = age, y = ToT)) + geom_point() Let’s take a look at the elements of the command chain: The first two lines pipe the data frame into the ggplot engine. BAB1 %&gt;% ggplot(...) At that moment, the ggplot engine “knows” which variables the data frame contains and hence are available for the plot. It does not yet know, which variables are being used, and how. The next step is, usually, to consider a basic (there exist more than 30) geometry and put it on a layer. The scatter plot geometry of ggplot is geom_point: BAB1 %&gt;% ggplot(...) + geom_point() The last step is the aesthetic mapping, which tells ggplot the variables to use and how to map them to aesthetic properties of the geometry. The basic properties of points in a coordinate system are the x and y-positions: BAB1 %&gt;% ggplot(aes(x = age, y = ToT)) + geom_point() The function aes creates a mapping where the aesthetics per variable are given. When call aes directly, we see that it is just a table. aes(x = age, y = ToT) ## Aesthetic mapping: ## * `x` -&gt; `age` ## * `y` -&gt; `ToT` One tiny detail in the above chain has not yet been explained: the +. When choosing the geometry, you actually add a layer to the plot. This is, of course, not the literal mathematical sum. Technically, what the author of the ggplot2 package did, was to overload the + operator. A large set of ggplot functions can be combined in a myriad of ways, just using +. The overloaded + in ggplot is a brilliant analogy: you can infinitely chain ggplot functions, like you can create long sums. You can store ggplot object and later modify it by adding functions. The analogy has its limits, though: other than sums, order matters in ggplot combinations: the first in the chain is always ggplot and layers are drawn upon each other. Let’s move on with a slightly different situation that will result in a different geometry. Say, we are interested in the distribution of the time-on-task measures under the two designs. We need a geometry, that visualizes the distribution of quantitative variables split by a grouping variable, factor. The box plot does the job: BAB1 %&gt;% ggplot(aes(x = Design, y = ToT)) + geom_boxplot() The box plot maps ToT to y (again). The factor Design is represented as a split on the x-axis. Interestingly, the box plot does not represent the data as raw as in the scatter plot example. The geometry actually performs an analysis on ToT, which produces five statistics: min, first quartile, median, third quartile and max. These statistics define the vertical positions of bars and end points. Now, we combine all three variables in one plot: how does the association between ToT and age differ by design? As we have two quantitative variables, we stay with the scatter plot for now. As we intend to separate the groups, we need a property of points to distinguish them. Points offer several additional aesthetics, such as color, size and shape. We choose color, and add it to the aesthetic mapping by aes. Note, that it does not matter whether you use the British or American way of writing (colour vs. color). BAB1 %&gt;% ggplot(aes(x = age, y = ToT, color = Design)) + geom_point() Now, we can distinguish the groups visually, but there is too much clutter to discover any relation. With the box plot we saw that some geometries do not represent the raw data, but summaries (statistics) of data. For scatter plots, a geometry that does the job of summarizing the trend is geom_smooth. This geometry summarizes a cloud of points by drawing a LOESS-smooth line through it. Note how the color mapping is applied to all geometry layers. BAB1 %&gt;% ggplot(aes(x = age, y = ToT, color = Design)) + geom_point() + geom_smooth() We see a highly interesting pattern: the association between age and ToT follows two slightly different mirrored sigmoid curves. Now that we have represented three variables with properties of geometries, what if we wanted to add a fourth one, say education level? Formally, we could use another aesthetic, say shape of points, to represent it. You can easily imagine that this would no longer result in a clear visual figure. For situations, where there are many factors, or factors with many levels, it is impossible to reasonably represent them in one plot. The alternative is to use facetting. A facet splits the data by a grouping variable and creates one single plot for every group: BAB1 %&gt;% ggplot(aes(x = age, y = ToT, color = Design)) + geom_point() + geom_smooth() + facet_grid(Education ~ .) See, how the facet_grid command takes a formula, instead of just a variable name. This makes faceting the primary choice for highly-dimensional situations. For example, we may also choose to represent both factors, Design and education by facets: BAB1 %&gt;% ggplot(aes(x = age, y = ToT)) + geom_point() + geom_smooth() + facet_grid(Education ~ Design) Note how the color aesthetic, although unnecessary, is kept. It is possible to map several aesthetics (or facets) to one variable, but not vice versa. 3.2.11 Fitting regression models Above we have seen examples of functions that boil down a vector to a single statistic, like the mean. R has several functions that summarize data in a more complex way. One function with a wide range of applications is the lm command, that applies regression models to data (provided as data frames). In the following, we will use another simulated data frame Exp to demonstrate linear models. To make this more interesting, we simulate Exp in a slightly advanced way, with quantitative associations between variables. Note how the expected value \\(\\mu\\) is created by drawing on the variables Condition and age. The last step adds (somewhat) realistic noise to the measures, by drawing from the normal distribution with a mean of \\(mu\\). N_Obs &lt;- 20 set.seed(42) Exp &lt;- data_frame(Obs = 1:N_Obs, Condition = rep(c(&quot;Experimental&quot;, &quot;Control&quot;), N_Obs/2), age = runif(N_Obs, 18, 35), mu = 200 + (Condition == &quot;Control&quot;) * 50 + age * 1, outcome = rnorm(N_Obs, mu, 10)) The experiment involves two groups, which in classic statistics would clearly point to what is commonly referred to as ANOVA. As it will turn out in [LM], old-fashioned ANOVA can be replaced by a rather simple regression model, that I call comparison of groups model (CGM). The estimation of regression models is done by a regression engine, which basically is a (very powerful) R command. The specification for any regression model is given in R’s formula language. Learning this formula language is key to unleashing the power of regression models in R. We can perform a CGM on the data frame Exp using the regression engine stan_glm. The desired model estimates the effect of Condition on outcome. This produces a regression object that contains an abundance of information, much of it is of little interest for now. (A piece of information, that it does not contain is F-statistics and p-values; and that is why it is not an ANOVA, strictly speaking!) The foremost question is how strong the difference between the groups is. The fixef command extracts the parameter estimates from the model to answer the question. M_1 &lt;- stan_glm(outcome ~ Condition, data = Exp) fixef(M_1) fixef center lower upper Intercept 275.4 264.0 286.2 ConditionExperimental -47.5 -62.6 -31.9 Another classic model is linear regression, where outcome is predicted by a metric variable, say age. The stan_glm regression engine is truly multi-purpose and does the job with grace: M_2 &lt;- stan_glm(outcome ~ age, data = Exp) fixef(M_2) fixef center lower upper Intercept 219.02 133.51 308.60 age 1.16 -1.98 4.18 If you are interested in both at the same time, you can combine that in one model by the following formula: M_3 &lt;- stan_glm(outcome ~ Condition + age, data = Exp) fixef(M_3) fixef center lower upper Intercept 219.72 177.280 261.59 ConditionExperimental -50.35 -63.763 -36.38 age 2.01 0.557 3.52 A statistical model has several components, for example the coefficients and residuals. Models are complex objects, from which a variety of inferences can be made. For example, the coefficient estimates can be extracted and used for prediction. This is what fixef() does in the above code. A number of functions can be used to extract certain aspects of the model. For example: fixef(model) extracts the linear effects residuals(model) extracts the measurement errors predict(model) extracts the expected values These will all be covered in later chapters. 3.2.12 Knitting statistical reports As you have seen throughout this chapter, with R you can effectively manage data, create impressively expressive graphics and conveniently estimate statistical models. Then usually comes the painful moment where all this needs to be assembled into a neat report. With R and Rstudio it has never been easier than that. In fact, complete books have been written in R, like the one you are reading. A minimal statistical report contains four elements: a recap of the research question description of how the statistical model relates to the research question a few figures or tables that answers the research question an explanation of the results Of these four elements, three are pure text. For a minimal report it is a fairly convenient to use a word processor software for the text, craft the figure in R and copy it. One problem with this approach is that a scrunitable statistical report contains at least the following additional elements: procedures of data preparation (sources, transformations, variable names, outlier removal) data exploration (ranges of variables, outlier discovery, visualizing associations, etc.) model estimation (formula specification, convergence checks) model criticism (normality of residuals, etc.) In advanced statistical workflows this is then multiplied by the number of models, an iterative selection process. Because it is easy to lie with statistics, these elements are needed as to build a fundament of credibility. Full transparency is achieved, when another researcher can exactly reproduce all steps of the original analysis. It is obvious that the easiest way to achieve this, is to hand over the full R script. The most user-friendly way to achieve both, a good looking report and full transparency, is to write a document that contains all before mentioned: text, graphics, tables and R code. In the R environment such mixed documents can be written in the markdown/knitr framework. Markdown implements a simple markup language, with that you can typset simple, structured texts in a plain ASCII editor. Later in the workflow, such a markup document is transformed into one of various output formats, that are rendered by the respective programs, such as Microsoft Word or an HTML browser. A minimal statistical report in markdown The above text is an alternation of markup text and chunks, those weirdly enclosed pieces of R code. While the text is static, the chunks are processed by the knitr engine, evaluating the enclosed R code and knitting the output into a document. Very conveniently, when the output is a figure, it will be inserted into the document right away. The kable command from the knitr package, in turn, produces neatly rendered tables from data frame objects. By default, the R code is shown, too, but that can be customized. The minimal workflow for statistical reporting with knitr is as follows: Use markdown right away, covering all steps of your data analysis, i.e. a scrutable report. You may even start writing when only one part of your data gathering is completed, because due to the dynamic chunks, updating the report when new data arrives is just a button click away. When the data analysis is complete, compile the scrutable report to Word format Extract the passages, figures and tables for a minimal statistical report. This is your results section. provide the scrutable report as appendix or supplementary material In the notion of this chapter, this is just to get you started and knitr is so tightly integrated with the Rstudio environment that I don’t even bother to explain the commands for knitting a document. Once acquainted with the basics, markdown provides a few additional markup tokens, like footnotes, hyperlinks or including images. The customization options and addons for knitr are almost endless and various interesting addons are available, just to mention two: The bookdown package provides an infrastructure for writing and publishing longer reports and books. With the shiny package one can add dynamic widgets to HTML reports. Think of a case, where your statistical model is more complicated than a linear regression line or a few group means, say you are estimating a polynomial model or a learning curve. Then, with a simple shiny app, you can enable your readers to understand the model by playful exploration. 3.2.13 Exercises In the book package (directory /Data) you will find the data set of the (virtual) study BAB1, which we will be using in coming chapters. This data comes as comma-separated value file with the file ending .csv. Load this file into R using the read_csv command and check the dataframe. Find the documentation of the packages haven and readr. Find two import functions. The one that is most useful for you and the one that you consider most exotic. We have seen how to extract a data frame column as a vector using the double square brackets. There seems to be no such option to extract an individual row as a vector. Why? (Think about object types). &lt;–! #14 –&gt; Use the world wide web to find geometries that are useful for plotting associations between grouping variables (factors) and a metric variables. Try them all on the BAB1 data frame. Compare the geometries on what properties of the data they convey. Like data frames and regression results, plots produced by ggplot are complex objects, too. Create an arbitrary plot, store it in a variable and inspect it using class, summary and str. In addition, what happens when you assign the plot to a variable and what happens when you call the variable? Revisit the python-swallowed-camel plot and check out how the aesthetic mapping is created. The plot uses a density geometry. Change it into a histogram. Then produce a box plot that shows the two conditions (think carefully about the mappings of x and y). Use the data set BAB5 in BrowsingAB. It contains a follow-up experiment, where participants had to do five different tasks on the website. Plot the association between age and ToT by task, using color. Then put Task on a facet grid, and use color to represent Design again. Use the data set BAB5 in BrowsingAB. Using a transformation chain, take the sum or average of participants’ ToT. Then run a few simple regression models. Use your own data. Drop an Excel and/or an SPSS file into the same directory as your current R file. Read the data into a data frame and summarize what is in the data frame. Use ggplot to create one or more exploratory graphs. Then use dplyr summarize to create summary statistics. Do a full exploratory analysis of the dataframe D_agg in case environment IPump. It has the predictors Design, Group, Education and experience. It has the outcome variables ToT, deviations and workload. Get the data frame into R and produce a summary. Plot a histogram for all dependent variables. Produce a table that counts the number of observations per Education(al level). Produce a table that displays minimum, maximum, median, mean and standard deviation of experience. Exclude participants with less than four years of experience. Produce a table with group means per Design and Session. For every outcome variable, produce a plot for Design by session. Explore graphically, what other relations may exist between outcome variables and Group, Education and experience. Run a regression model for ToT with Design and Session as predictors. Produce a coefficient table. Plot the residuals. 3.2.14 Bibliographic notes R for Data Science is a book co-authored by Hadley “Tidy” Wickham. ggplot2 Version of Figures in “25 Recipes for Getting Started with R” for readers who are familiar with the legacy plotting commands in R. Introduction to dplyr for Faster Data Manipulation in R introduces dplyr, the next generation R interface for data manipulation, which is used extensively in this book. Quick-R is a comprehensive introduction to many common statistical techniques with R. Code as manuscript features a small set of lessons with code examples, assignments and further resources. For if you are in a haste. bookdown: Authoring Books and Technical Documents with R Markdown fully unleashes the power of knitr for writing and publishing longer reports and books. "],
["bayesian-statistics.html", "4 Elements of Bayesian statistics 4.1 Rational decision making in design research 4.2 Descriptive statistics 4.3 Bayesian probability theory 4.4 Statistical models 4.5 Bayesian estimation", " 4 Elements of Bayesian statistics knitr::opts_chunk$set(include = FALSE) source(&quot;RMDR.R&quot;) ## Warning: package &#39;lme4&#39; was built under R version 3.5.3 ## Loading required package: Matrix ## Warning: package &#39;brms&#39; was built under R version 3.5.3 ## Loading required package: Rcpp ## Warning: package &#39;Rcpp&#39; was built under R version 3.5.3 ## Loading &#39;brms&#39; package (version 2.12.0). Useful instructions ## can be found by typing help(&#39;brms&#39;). A more detailed introduction ## to the package is available through vignette(&#39;brms_overview&#39;). ## ## Attaching package: &#39;brms&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## ngrps ## The following object is masked from &#39;package:stats&#39;: ## ## ar ## Warning: package &#39;rstanarm&#39; was built under R version 3.5.3 ## rstanarm (Version 2.19.3, packaged: 2020-02-11 05:16:41 UTC) ## - Do not expect the default priors to remain the same in future rstanarm versions. ## Thus, R scripts should specify priors explicitly, even if they are just the defaults. ## - For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()) ## - bayesplot theme set to bayesplot::theme_default() ## * Does _not_ affect other ggplot2 plots ## * See ?bayesplot_theme_set for details on theme setting ## ## Attaching package: &#39;rstanarm&#39; ## The following objects are masked from &#39;package:brms&#39;: ## ## dirichlet, exponential, get_y, lasso, loo_R2, ngrps ## Warning: package &#39;polynom&#39; was built under R version 3.5.3 ## Warning: package &#39;knitr&#39; was built under R version 3.5.3 ## Warning: package &#39;printr&#39; was built under R version 3.5.1 ## Warning: package &#39;GGally&#39; was built under R version 3.5.1 ## Loading required package: ggplot2 ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.3 ## Warning: package &#39;DiagrammeR&#39; was built under R version 3.5.3 ## Warning: package &#39;plyr&#39; was built under R version 3.5.3 ## Warning: package &#39;tidyverse&#39; was built under R version 3.5.3 ## -- Attaching packages -------------------------------------- tidyverse 1.3.0 -- ## v tibble 2.1.3 v dplyr 0.8.3 ## v tidyr 1.0.2 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.4.0 ## v purrr 0.3.3 ## Warning: package &#39;tibble&#39; was built under R version 3.5.3 ## Warning: package &#39;tidyr&#39; was built under R version 3.5.3 ## Warning: package &#39;readr&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.3 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.3 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 ## Warning: package &#39;forcats&#39; was built under R version 3.5.2 ## -- Conflicts ----------------------------------------- tidyverse_conflicts() -- ## x dplyr::arrange() masks plyr::arrange() ## x dplyr::combine() masks gridExtra::combine() ## x purrr::compact() masks plyr::compact() ## x dplyr::count() masks plyr::count() ## x tidyr::expand() masks Matrix::expand() ## x dplyr::failwith() masks plyr::failwith() ## x dplyr::filter() masks stats::filter() ## x dplyr::id() masks plyr::id() ## x dplyr::lag() masks stats::lag() ## x dplyr::mutate() masks plyr::mutate() ## x tidyr::pack() masks Matrix::pack() ## x dplyr::rename() masks plyr::rename() ## x dplyr::summarise() masks plyr::summarise() ## x dplyr::summarize() masks plyr::summarize() ## x tidyr::unpack() masks Matrix::unpack() ## Warning: package &#39;haven&#39; was built under R version 3.5.3 ## Warning: package &#39;openxlsx&#39; was built under R version 3.5.3 ## ## Attaching package: &#39;mascutils&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## expand_grid ## The following object is masked from &#39;package:base&#39;: ## ## mode ## ## Attaching package: &#39;bayr&#39; ## The following objects are masked from &#39;package:rstanarm&#39;: ## ## fixef, ranef ## The following objects are masked from &#39;package:brms&#39;: ## ## fixef, ranef ## The following objects are masked from &#39;package:lme4&#39;: ## ## fixef, ranef CE = c(&quot;Rainfall&quot;, &quot;Sec99&quot;, &quot;IPump&quot;, &quot;Rational&quot;) load_CE_(CE) ## Loading case environment Rainfall ## Loading case environment Sec99 ## Loading case environment IPump ## Loading case environment Rational purp.mcmc &lt;- T As human beings we make our decisions on what has happened to us in the past. For example, we trust a person or a company more, when we can look back at a series of successful transactions. And we have remarkable capability to recall what has just happened, but also what happened yesterday or years ago. By integrating over all the evidence, we form a view of the world we forage. When evidence is abundant, we vigorously experience a feeling of certainty, or lack of doubt. That is not to deny, that in a variety of situations, the boundedness of the human mind kicks in and we become terrible decision makers. This is for a variety of psychological reasons, to name just a few: forgetting evidence the primacy effect: recent events get more weight confirmation bias: evidence that supports a belief is actively sought for, counter-evidence gets ignored. the hindsight bias: once a situation has taken a certain outcome, we believe that it had to happen that way. The very aim of scientific research is to avoid the pitfalls of our minds and act as rational as possible by translating our theory into a formal model, gathering evidence in an unbiased way and weigh the evidence by formal procedures. This weighing of evidence using data essentially is statistical modeling and statistical models in this book all produces two sorts of numbers: magnitude of effects and level of certainty. When a statistic is reported together with the strength of evidence, this is conventionally called an inferential statistic. In applied research, real world decisions depend on the evidence, which has two aspects: first, the strength of effects and the level of certainty we have reached. Bayesian inferential statistics grounds on the idea of accumulating evidence along research. Certainty (or belief or credibility or credence) in Bayesian statistics is formalized as a probability scale (0 = impossible, 1 = certain). Certainty is reached by looking at evidence and such evidence comes from two sources: everything that we already know about the subject and the data we just gathered. These sources are only seemingly different, because when new data is analyzed, a transition occurs from what you new before, prior belief, to what you know after seeing the data, posterior belief. In other words: by data the current belief gets an update. Updating our beliefs is essential for acting in rational ways. The first section of this chapter is intended to tell the Big Picture. It puts statistics into the context of decision-making in design research. For those readers with a background in statistics, this section may be a sufficient introduction all by itself. In the remainder of this chapter the essential concepts of statistics and Bayesian analysis will be introduced from ground up. First we will look at descriptive statistics, introducing analysis tools, such as summary statistics and statistical graphs. Descriptive statistics can be used effectively to explore data and prepare the statistical modelling, but they lack one important ingredient: information about uncertainty. For that, a sound understanding is required of what certainty is. In Bayesian thinking, certainty is a sort of probability and section [#BPT] first derives probability from set theory and relative frequencies. Then it goes on to explain basic concepts of statistical modeling, such as the likelihood and finishes with Bayes theorem, which does the calculation of the posterior certainty from prior knowledge and data. Despite the matter, I will make minimal use of mathematical formalism. Instead, I use R code as much as possible to illustrate the concepts. If you are not yet familiar with R, you may want to read chapter [#IntroR] first, or alongside. Section [#] goes into the practical details of modelling. A statistical model is introduced by its two components: the structural part, which typically carries the research question or theory, is only briefly introduced, followed by a rather deep account of the second component of statistical models: the random part. 4.1 Rational decision making in design research I see clouds. Should I take my umbrella? Should I do this bungee jump? How many people came to death by jumping? (More or less than alpine skiing?) And how much fun is it really? Overhauling our company website will cost us EUR 100.000. Is it worth it? All the above cases are examples of decision making under uncertainty. The actors aim for maximizing their outcome, be it well being, fun or money. But, they are uncertain about what will really happen. And their uncertainty occurs on two levels: One cannot precisely foresee the exact outcome of one’s chosen action: Taking the umbrella with you can have two consequences: if it rains, you have the benefit of staying dry. If it does not rain, you have the inconvenience of carrying it with you. You don’t know if you will be the rare unlucky one, who’s bungee rope breaks. You don’t know by how much the new design will attract more visitors and how much the income will raise. It can be difficult to precisely determine the benefits or losses of potential outcomes: How much worse is your day when carrying a useless object with you? How much do you hate moisture? In order to compare the two, they must be assessed on the same scale. How much fun (or other sources of reward, like social acknowledgments) is it to jump a 120 meter canyon? And how much worth is your own life to you? What is the average revenue generated per visit? What is an increase of recurrence rate of, say, 50% worth? Once you know the probabilities of all outcomes and the respective losses, decision theory provides an intuitive framework to estimate these values. Expected utility \\(U\\) is the sum product of outcome probabilities \\(P\\) and the involved losses. In the case of the umbrella, the decision is between two options: taking an umbrella versus taking no umbrella, when it is cloudy. We calculate and compare the expected utilities U as follows (\\(P(\\text{outcome})\\) is the probability of an outcome): \\[\\begin{aligned} P(\\text{rain}) = 0.6 \\\\ P(\\text{no rain}) = 1 - P(\\text{rain}) = 0.4 \\\\ L(\\text{carry}) = 2 \\\\ L(\\text{wet}) = 4 \\\\ U(\\text{umbrella}) = P(\\text{rain}) L(\\text{carry}) + P(\\text{no rain}) L(\\text{carry}) = L(\\text{carry}) = 2\\\\ U(\\text{no umbrella}) = P(\\text{rain}) L(\\text{wet}) = 2.4 \\end{aligned} \\] We conclude that, given the high chance for rain, and the conditional losses, the expected loss is larger for not taking an umbrella with you. It is rational to take an umbrella when it is cloudy. 4.1.1 Measuring uncertainty As we have seen above, a decision requires two investigations: outcomes and their probabilities, and the assigned loss. Assigning loss to decisions is highly context dependent and often requires domain-specific expertise. The issues of probabilistic processes and the uncertainty that arises from them is basically what the idea of New Statistics represents. We encounter uncertainty in two forms: first, we usually have just a limited set of observations to draw inference from, this is uncertainty of parameter estimates. From just 20 days of observation, we cannot be absolutely certain about the true chance of rain. It can be 60%, but also 62% or 56%. Second, even if we precisely knew the chance of rain, it does not mean we could make a certain statement of the future weather conditions, which is predictive uncertainty. For a perfect forecast, we had to have a complete and exact figure of the physical properties of the atmosphere, and a fully valid model to predict future states from it. For all non-trivial systems (which excludes living organisms and weather), this is impossible. Review the rainfall example: the strategy of taking an umbrella with you has proven to be superior under the very assumption of predictive uncertainty. As long as you are interested in long-term benefit (i.e. optimizing the average loss on a long series of days), this is the best strategy. This may sound obvious, but it is not. In many cases, where we make decisions under uncertainty, the decision is not part of a homogeneous series. If you are member of a startup team, you only have this one chance to make a fortune. There is not much opportunity to average out a single failure at future occasions. In contrast, the investor, who lends you the money for your endeavor, probably has a series. You and the investor are playing to very different rules. For the investor it is rational to optimize his strategy towards a minimum average loss. The entrepreneur is best advised to keep the maximum possible loss at a minimum. As we have seen, predictive uncertainty is already embedded in the framework of rational decision making. Some concepts in statistics can be of help here: the uncertainty regarding future events can be quantified (for example, with posterior predictive distributions [#post_pred]) and the process of model selection can assist in finding the model that provides the best predictive power. Still, in our formalization of the Rainfall case, what magically appears are the estimates for the chance of rain. Having these estimates is crucial for finding an optimal decision, but they are created outside of the framework. Furthermore, we pretended to know the chance of rain exactly, which is unrealistic. Estimating parameters from observations is the reign of statistics. From naive calculations, statistical reasoning differs by also regarding uncertainty of estimates. Generally, we aim for making statements of the following form: “With probability \\(p\\), the attribute \\(A\\) is of magnitude \\(X\\).” In the umbrella example above, the magnitude of interest is the chance of rain. It was assumed to be 60%. This appears extremely high for an average day. A more realistic assumption would be that the probability of rainfall is 60% given the observation of a cloudy sky. How could we have come to the belief that with 60% chance, it will rain when the sky is cloudy? We have several options, here: Supposed, you know that, on average, it rains 60% of all days, it is a matter of common sense, that the probability of rain must be equal or larger than that, when it’s cloudy. You could go and ask a number of experts about the association of clouds and rain. You could do some systematic observations yourself. Imagine, you have recorded the coincidences of clouds and rainfall over a period, of, let’s say, 20 days, with the following observations: Intuitively, you would use the average to estimate the probability of rain under every condition. These probabilities we can feed into the decision framework as outlined above. The problem is, that we obtained just a few observations to infer the magnitude of the parameter \\(P(rain|cloudy) = 60\\)%. Imagine, you would repeat the observation series on another 20 days. Due to random fluctuations, you would get a more or less different series and different estimates for the probability of rain. More generally, the true parameter is only imperfectly represented by any sample, it is not unlikely, that it is close to the estimate, but it could be somewhere else, for example, \\(P(rain|cloudy) = 64.93\\)%. The trust you put in your estimation is called level of certainty or belief or confidence. It is the primary aim of statistics to rationally deal with uncertainty, which involves to measure the level of certainty associated with any statement derived from teh data. So, what would be a good way to determine certainty? Think for a moment. If you were asking an expert, how would you do that to learn about magnitude and uncertainty regarding \\(P(rain|cloudy)\\)? Maybe, the conversation would be as follows: YOU: What is the chance of rain, when it’s cloudy. EXPERT: Wow, difficult question. I don’t have a definite answer. YOU: Oh, c’mon. I just need a rough answer. Is it more like 50%-ish, or rather 70%-ish. EXPERT: Hmm, maybe somewhere between 50 and 70%. YOU: Then, I guess, taking an umbrella with me is the rational choice of action. Note how the expert gave two endpoints for the parameter in question, to indicate the location and the level of uncertainty. If she had been more certain, she had said \"between 55 and 65%. While this is better than nothing, it remains unclear, which level of uncertainty is enclosed. Is the expert 100%, 90% or just 50% sure the true chance is in the interval? Next time, you could ask as follows: … EXPERT: Hmm, maybe somewhere between 70-90% YOU: What do you bet? I’m betting 5 EUR that the true parameter is outside the range you just gave. EXPERT: I dare you! 95 EUR it’s inside! The expert feels 95% certain, that the parameter in question is in the interval. However, for many questions of interest, we have no expert at hand (or we may not even trust them altogether). Then we proceed with option 3: making our own observations. 4.1.2 Benchmarking designs The most basic decision in practical design research is whether a design fulfills an external criterion. External criteria for human performace in a human-machine system are most common, albeit not abundant, in safety-critical domains. Consider Jane: she is user experience researcher at the mega-large rent-a-car company smartr.car. Jane was responsible for a overhaul of the customer interface for mobile users. Goal of the redesign was to streamline the user interface, which had grown wild over the years. Early customer studies indicated that the app needed a serious visual de-cluttering and stronger funneling of tasks. 300 person months went into the re-development and the team did well: a recent A/B study had shown that users learned the smartr.car v2.0 fast and could use its functionality very efficiently. Jane’s team is prepared for the roll-out, when Marketing comes up with the following request: Marketing: We want to support market introduction with the following slogan: “rent a car in 99 seconds”. Jane: Not all users manage a full transaction in that short time. That could be a lie. Marketing: Legally, the claim is fine if it holds on average. Jane: That I can find out for you. Jane takes another look at the performance of users in the smartr car v2.0 condition. As she understands it, she has to find out whether the average of all recorded time-on-tasks with smartr.car 2.0 is 99 seconds, or better. Here is how the data looks like: The performance is not completely off the 99 seconds, many users are even faster. Jane figures out that she has to ask a more precise question, first, as teh slogan can mean different things, like: all users can do it within 99 seconds at least one user can do it half of the users can do it Jane decides to go the middle way and chooses the population average, hence the average ToT must not be more than 99 seconds. Unfortunately, she had only tested a small minority of users and therefore cannot be certain about the true average: Because the sample average is an uncertain, Jane is afraid, Marketing could use this as an argument to ignore the data and go with the claim. Jane sees no better way as quantifying the chance of being wrong using a statistical model, which will later become known as the Grand Mean Model) Let’s see what the GMM reports about the population average and its uncertainty: The table above is called a CLU table, because it reports three estimates per coefficient: Center, which (approximately) is the most likely position of the true value Lower, which is the lower 95% credibility limit. There is a 2.5% chance that the true value is lower Upper, the 95% upper limit. The true value is larger than this with a chance of 2.5%. This tell Jane that most likely the average time-on-task is \\(Intercept\\). That is not very promising, and it is worse: 99 is even below the lower 95% credibility limit. So, Jane can send a strong message: The probability that this claim is justified, is smaller than 2.5%. Luckily, Jane had the idea that the slogan could be changed to “rent a card in 1-1-1 seconds”. The 95% credibility limits are in her favor, since 111 is at the upper end of the credibility limit. It would be allowed to say that the probability to err is not much smaller than 2.5%. But Jane desires to make an accurate statement. But what precisely is the chance that the true population average is 111 or lower? In Bayesian analysis there is a solution to that. When estimating such a model, we get the complete distribution of certainty, called the posterior distribution. In fact, a CLU table with 95% credibility limits is just a summary on the posterior distribution. This distribution is not given as a function, but has been generated by a (finite) random walk algorith, known as Markov-Chain Monte Carlo. At every step (or most, to be precise), this algorithm jumps to another set of coordinates in parameter space and a frequency distribution arises that can be used to approximate levels of certainty. The following illustration shows the posterior frequency distribution of the coefficient, divided into the two possible outcomes. In the present case, we can derive the chance that the true average of the customer population is lower than the target of 111: In a similar manner to how the graph above was produced, a precise certainty level can be estimated from the MCMC frequency distribution contained in the posterior object. The certainty that the 111 seconds slogan holds is much better: The story of Jane is about decision making under risk and under uncertainty. We have seen how easily precise statements on uncertainty can be derived from a statistical model. But regarding rational decision making, this is not an ideal story: What is missing is a systematic analysis of losses (and wins). The benefit of going with the slogan has never been quantified. How many new customers it will really attract and how much they will spend cannot really be known upfront. Let alone, predicting the chance to loose in court and what this costs are almost unintelligeable. The question must be allowed, what good is the formula for utility, when it is practically impossible to determine the losses. And if we cannot estimate utilities, what are the certainties good for? Sometimes, one possible outcome is just so bad, that the only thing that practically matters, is to avoid it at any costs. Loosing a legal battle often falls into this category and the strategy of Marketing/Jane effectively reduced this risk: they dismissed a risky action, the 99 seconds statement, and replaced it with a slogan that they can prove is true with good certainty. In general, we can be sure that there is at least some implicit calculation of utilities going on in the minds of Marketing. Perhaps, that is a truly intuitive process, which is felt as an emotional struggle between the fear of telling the untruth and love for the slogan. This utility analysis probably is inaccurate, but that does not mean it is completely misleading. A rough guess always beats complete ignorance, especially when you know about the attached uncertainty. Decision-makers tend to be pre-judiced, but even then probabilities can help find out to what extent this is the case: Just tell the probabilities and see who listens. 4.1.3 Comparison of designs The design of systems can be conceived as a choice between design options. For example, when designing an informational website, you have the choice of making the navigation structure flat or deep. Your choice will change usability of the website, hence your customer satisfaction, rate of recurrence, revenue etc. Much practical design research aims at making good choices and from a statistical perspective that means to compare the outcomes of two (or more) design option. A typical situation is to compare a redesign with its predecessor, which will now be illustrated by a hypothetical case: Violet is a manager of an e-commerce website and at present, a major overhaul of the website is under debate. The management team agrees that this overhaul is about time and will most likely increase the revenue per existing customer. Still, there are considerable development costs involved and the question arises whether the update will pay itself in a reasonable time frame. To answer this question, it is not enough to know that revenues increase, but an more accurate prediction of how much precisely is gained. In order to return the investment, the increase must be in the ballpark of 10% increase in revenue. For this purpose, Violet carries out a user study to compare the two designs. Essentially, she observes the transactions of 50 random customers using the current system with 50 transactions with a prototype of the new design. The measured variable is the money every user spends during the visit. The research question is: By how much do revenues increase in the prototype condition?. The figure below shows the distribution of measured revenue in the experiment. There seems to be a slight benefit for the prototype condition. But, is it a 10% increase? The following calculation shows Violet that it could be the case: Like in the previous case, testing only a sample of 100 users out of the whole population leaves room for uncertainty. So, how certain can Violet be? A statistical model can give a more complete answer, covering the magnitude of the improvement, as well as a level of certianty. Violet estimates a model that compares the means of the two conditions [#CGM], assuming that the randomness in follows a Gamma distribution [#Gamma, #temporal GLM]. The coefficients of the Gamma model are on a logartithmic scale, but when exponentiated, they can directly be interpreted as multiplicators [#link_function]. That precisely matches the reseach question, which is stated as percentage increase, rather than a difference. The results tell Violet that, most likely, the average user spends $ center 49.9 center 1.13 lower 1.03 upper 1.23 The risk of failure is just below 30%. With this information in mind Violet now has several options: deciding that risk_of_failure 28.9 continue testing more users to reach a higher level of certainty take into account sources of evidence from the past, or prior knowledge 4.1.4 Prior knowledge It is rarely the case that we encounter a situation tabula rasa. Whether we are correct or not, when we look at the sky in the morning, we have some expectations on how likely there will be rain. We also take into account the season and the region and even the very planet is sometimes taken into account: the Pathfinder probe carried a bag of high-tech gadgets, including an umbrella, but this was for safe landing only, not to cover from precipation, as Mars is a dry planet. In behavioral research it has been general standard that every experiment had to be judged on the produced data alone. For the sake of objectivity, researchers were not allowed to take into account previous results, let alone their personal opinion. In Bayesian statistics, you have the choice. You can make use of external knowledge, but you don’t have to. Violet, the rational design researcher has been designing and testing e-commerce systems for many years and has supervised several large scale roll-outs. So the current project is not a completely new situation at all. From the top of her head, Violet produces the following table to capture her past projects and the increase in revenue that had been recorded afterwards (on the whole population). On this small data set, Violet estimates another grand mean model that essentially captures prior knowledge about revenue increases after redesign: The following CLU table shows the results. The mean increase was Intercept and without any further information, this is the best guess for the population mean of all present or future projects (of Violet). A statistical model that is based on such a small number of observations usually produces very uncertain estimates. In this case, we have a very low level of certainty, as the 95% credibility limits are wide. There even remains a considerable risk that a project results in a decrease of revenue, although that has never been recorded. (Intercept \\(&lt;1\\), recall that these coefficients are multiplicative). The population average (of projects) is less favorable than what Violet saw in her experiment, which is worrying. If the estimated revenue on teh experimental data is correct, it would be a rather extreme outcome. In the following step, she uses the (posterior) certainty from M_prior and employs it as prior information (by means of a Gaussian distribution). Model M_2 has the same formula as M_1 before, but combines the information of both sources, data and prior certainty. Finally, Violet computes the risk for the true value to be below 110%. As it turns out, the probability of failure increases by around 10%, when prior information is taken into account. That is not so much as to turn around any decisions on the redesign. But, that also contains a positive message: The current data is not extraordinary as to collide with prior knowledge. 4.1.4.1 FINISH 4.2 Descriptive statistics In empirical research we systematically gather observations. Observations of the same kind are usually subsumed as variables. A set of variables that have been gathered on the same sample are called a data set, which typically is a table with variables in columns. In the most general meaning, a statistic is a single number that somehow represents relevant features of a data set, such as: frequency: how many measures of a certain kind can be found in the data set? central tendency: do measures tend to be located left (weak) or right (strong) on a scale? dispersion: are measures close together or widely distributed along the scale? association: does one variable X tend to change when another variable Y changes? 4.2.1 Frequencies The most basic statistics of all probably is the number of observations on a variable \\(x\\), usually denoted by \\(n_{x}\\). The number of observations is a rough indicator for the amount of data that has been gathered. In turn, more data usually results in better accuracy of statistics and higher levels of certainty can be reached. The number of observations is not as trivial as it may appear at first. In particular, it is usually not the same as the sample size, for two reasons: First, most studies employ repeated measures to some extent. You may have invited \\(n_{Part}\\) participants to your lab, but on each participant you have obtained several measures of the same kind. When every participant is tested on, let’s say, five tasks, the number of observations is \\(n_{Part}\\) times five. Second, taking a valid measure can always fail for a variety of reasons, resulting in missing values. For example, in the 99 seconds study, it has happened, that a few participants missed to fill in their age on the intake form. The researcher is left fewer measures of age \\(n_{age}\\) than there were participants. Another important issue is the distribution of observations across groups. Again, the number of observations in a group is linked to the certainty we can gain on statistics of that group. Furthermore, it is sometimes important to have the distribution match the proportions in the population, as otherwise biases may occur. The table above shows so called absolute frequencies. When comparing frequencies by groups, it often is more appropriate to report relative frequencies or proportions: Summarizing frequencies of metric measures, such as time-on-task (ToT) or number of errors is useful, too. However, a complication arises by the fact that continuous measures do not naturally fall into groups. Especially in duration measures no two measures are exactly the same. The answer to this problem is binning: the scale of measurement is divided into a number of adjacent sections, called bins, and all measures that fall into one bin are counted. For example, we could use bins of 10 seconds and assess whether the bin with values larger than 90 and smaller or equal to 100 is representative in that it contains a large proportion of values. If we put such a binned summary of frequencies into a graph, that is called a histogram. Strictly spoken, grouped and binned frequencies are not one statistic, but a vector of statistics. It approximates what we will later get to know more closely as a distribution. 4.2.2 Central tendency Reconsider Jane ??. When asked about whether users can complete a transaction within 99, she looked at the population average of her measures. The population average is what we call the (arithmetic) mean. The mean is computed by summing over all measures and divide by the number of observations. The mean is probably the most often used measure of central tendency, but two more are being used and have their own advantages: median and mode. Imagine a competitor of the car rental company goes to court to fight the 99-seconds claim. Not an expert in juridical matters, my humble opinion is that one of the first questions to be regarded probably is: what does “rent a car in 99 seconds” actually promise? One way would be the mean (“on average users can rent a car in 99 seconds”), but here are some other ways to interpret the same slogan: “50% (or more) of users can …”. This is called the median. The median is computed by ordering all measures and identify the the element right in the center. If the number of observations is even, there is no one center value, and the mean of the center pair is used, instead. Actually, the median is a special case of so called quantiles. Generally, an quantiles are based on the order of measures and an X% quantile is that value where X% of measures are equal to or smaller. The court could decide that 50% of users is too lenient as a criterion and could demand that 75% percent of users must complete the task within 99 seconds for the slogan to be considered valid. A common pattern to be found in distributions of measures is that a majority of observations accumulate in the center region. The point of highest density of a distribution is called the mode. In other words: the mode is the region (or point) that is most likely to occur. For continuous measures this once again poses the problem that every value is unique. Sophisticated procedures exist to smooth over this inconvenience, but by binning we can construct an approximation of the mode: just choose the center of the bin with highest frequency. The table above shows the three statistics for central tendency side-by-side. Mean and median are close together. This is frequently the case, but not always. When the distribution of measures is completely symmetric mean and median perfectly coincide. In section @(distributions) we will encounter distributions that are not symmetric. The more a distribution is skewed, the stronger the difference between mean and median increases. To be more precise: for left skewed distributions the mean is strongly influenced by few, but extreme, values in the left tail of the distribution. The median only counts the number of observations to both sides and is not influenced by how extreme these values are. Therefore, it is located more to the right. The mode does not regard any values other than those in the densest region and just marks that peak. The same principles hold for right-skewed distributions. To summarize, the mean is the most frequently used measure of central tendency, one reason being that it is a so called sufficient statistic, meaning that it exploits the full information present in the data. The median is frequently used when extreme measures are a concern. The mode is the point in the distribution that is most typical. 4.2.3 Dispersion In a symmetric distribution with exactly one peak, mean and mode coincide and the mean represents the most typical value. For a value being more typical does not mean it is very typical. That depends on how the measures are dispersed over the whole range. In the figure below, the center value of the narrow distribution contains 60% of all measures, as compared to 40% in the wide distribution, and is therefore more representative. A very basic way to describe dispersion of a distribution is to report the range between the two extreme values, minimum and maximum. These are easily computed by sorting all values and selecting the first and the last element. Coincidentally, they are also special cases of quantiles, namely the 0% and 100% quantiles. A boxplot is a commonly used geometry to examine the shape of dispersion. Like histograms, boxplots use a binning mechanisms and are therefore useful for continuous measures. Whereas histograms use equidistant bins on the scale of measurement, boxplots create four bins based on 25% quantile steps. These are also called quartiles. The min/max statistics only uses just these two values and therefore does not fully represent the amount of dispersion. A statistic for dispersion that does so is the variance, which is the mean of squared deviations from the mean. Squaring the deviations always produces a positive value, but makes variance difficult to interpret. The standard deviation is the square root of variance. By reversing the square the standard deviation is on the same scale as the original measures and their mean. 4.2.4 Associations Are elderly users slower at navigating websites? How does reading speed depend on font size? Is the result of an intelligence test independent from gender? In the previous section we have seen how all individual variables can be described by location and dispersion. A majority of research deals with associations between measures and the present section introduces some statistics to describe them. Variables represent properties of the objects of research and fall into two categories: Metric variables represent a measured property, such as speed, height, money or perceived satisfaction. Categorical variables put observations (or objects of research) into non-overlapping groups, such as experimental conditions, persons who can program or cannot, type of education etc. Consequently, associations between any two variables fall into precisely one of three cases, as shown in the table. In the following I will explain these types of associations. categorical metric categorical frequency cross tables differences in mean - - - metric classification covariance, correlation - - - 4.2.4.1 Categorical associations Categorical variables group observations, and when they are both categorical, the result is just another categorical case and the only way to compare them is by size, which is done by frequency tables. To illustrate the categorical-categorical case, consider a study to assess the safety of two syringe infusion pump designs, called Legacy and Novel. All participants of the study are asked to perform a typical sequence of operation on both devices (categorical variable Design) and it is recorded whether the sequence was completed correctly or not (categorical variable Correctness). Besides the troubling result that incorrect completion is the rule, not the exception, there is almost no difference between the two designs. Note that in this study, both professional groups were even in number. If that is not the case, absolute frequencies are difficult to compare and we better report relative frequencies. Note how every row sums up to \\(1\\). The absolute or relative frequencies can be shown in a stacked bar plot: 4.2.4.2 Categorical-metric associations Associations between categorical and metric variables are reported by grouped location statistics. In the case of the two infusion pump designs, the time spent to complete the sequence is compared by the following table. And as you can see, adding a comparison of variance (or any other statistic) is not a hassle. For the illustration of categorical-metric associations case, boxplots haven proven useful. Boxplots show differences in central tendency (median) and dispersion (other quartiles) simultaneously. Here we can observe that Novel design produces shorter ToT and seems to have less dispersion. 4.2.4.3 Covariance and correlation For associations between both metric variables, covariance and correlations are commonly employed statistics. As correlations derive from covariances, I describe them first: A covariance is a real number that is zero when there really is no association between two variables. When two variables move into the same direction, covariance gets the positive. When they move in opposite directions, covariance is negative For an illustration, consider the following hypothetical example of study on the relationship between mental ability test scores and performance in real tasks. The real task could be a buzz wire task or any other complex motor task that has a strong spatial component. As predictors for ToT on the buzz-wire task, mental rotation speed was taken. It was even taken twice (MRS_1, MRS_2), because the researchers are also interested in how stable over time the scores are. The second test is for measuring visual-spatial working memory, using the Corsi task. The following function computes the covariance of two variables. The covariance between the two MRS scores is positive, indicating that they move into the same direction. The problem with covariance is that it usually is hard to interpret. This is why later, we will transformk covariances into correlations, but for understanding the stapes to go there, we have to understand the link between variance and covariance. The formula for covariance is (with \\(E(X)\\) the mean of \\(X\\)): \\[ \\textrm{cov}_{XY} = \\frac{1}{n} \\sum_{i=1}^n (x_i - E(X)) (y_i - E(Y)) \\] Covariance essentially arises by the multiplication of deviations from the mean, \\((x_i - E(X)) (y_i - E(Y)\\). When for one observation both factors go into the same direction, be it positive or negative, this term gets positive. If that happens a lot, the whole sum gets largely positive. When the deviations systematically move in opposite direction, such that one factor is always positive and the other negative, we get a large negative cvariance. When the picture is mixed, i.e. no clear tendenvcy, covariance will stay close to zero. The following illustration uses a geometric interpretation of the multiplication as the area of rectangles. Rectangles with equal directions (blue) are in the upper-right and lower-left quadrant. They overwhelm the opposite direction rectangles (red), which speaks for a strong positive association. The associations between MRT_1 and Corsi, as well as between Corsi and ToT seem to have a slight overhead in same direction, so the covariance is positive, but less strong. A clear negative association exists between MRS_1 and Corsi. It seems these two tests have some common ground. If we now look at the defintion of variance, it is apparent that variance is just covariance of a variable with itself (, because \\((x_i - E(X))^2 = (x_i - E(X))(x_i - E(X))\\): \\[ \\textrm{var}_{X} = \\frac{1}{n} \\sum_{i=1}^n (x_i - E(X))^2) \\] That gives rise to a compact form to show all covariances and variances between a bunch of variables at once. The following table is a variance-covariance matrix. It shows the variance of every variable in the diagonale and the mutual covariances in the off-diagonal cells. As intuitive the idea of covariance is, as unintelligible is the statistic itself for reporting results. Th problem is that covariance is not a pure measure of association, but is contaminated by the dispersion of \\(X\\) and \\(Y\\). For that reason, two covariances can only be compared if the variables have the same variance. The Pearson correlation coefficient \\(r\\) solves the problem by rescaling covariances by the product of the two standard deviations: \\[ r_XY = \\frac{\\textrm{cov}_{XY}}{\\textrm{sd}_X \\textrm{sd}_Y} \\] Due to the standardization of dispersion, Pearson correlation \\(r\\) can be interpreted as strength of association independent of scale of measurement. More precisely, \\(r\\) will always be in the interval \\([-1,1]\\). That makes it the perfect choice when associations are being compared to each other or to an external standard. In the field of psychometrics, correlations are ubiquitously employed to represent reliability and validity of psychological tests. Test-retest stability is one form to measure reliability and it is just the correlation of the same test taken on different days. For example, we could ask whether mental rotation speed as measured by the mental rotation task (MRT) is stable over time, such that we can use it for long-term predictions, such as how likely someone will become a good surgeon. Validity of a test means that it represents what it was intended for, and that requires an external criterion that is known to be valid. For example, we could ask how well the ability of a person to become a minimally invasive surgeon depends on spatial cognitive abilities, like mental rotation speed. Validity could be assessed by taking performance scores from exercises in a surgery simulator and do the correlation with mental rotation speed. A correlation of \\(r = .5\\) would indicate that mental rotation speed as measured by the task has rather limited validity. Another form is called discriminant validity and is about how specific a measure is. Imagine another test as part of the surgery assessment suite. This test aims to measure another aspect of spatial cognition, namely the capacity of the visual-spatial working memory (e.g., the Corsi block tapping task). If both tests are as specific as they claim to be, we would expect a particularly low correlation. And similar to covariances, correlations between a set of variables can be put into a correlation table. This time, the diagonal is the correlation of a variable with itself, which is perfect correlation and therefore equals 1. Another way to illustrate a bunch of correlations is produced by the following command, combining scatterplots, density plots and correlation coefficients Correlations give psychometricians a comparable standard for the quality of measures, irrespectively on what scale they are. In exploratory analysis, one often seeks to get a broad overview of how a bunch of variables is associated. Creating a correlation table of all variables is no hassle and allows to get a broad picture of the situation. Correlations are ubiquitous in data analysis, but have limitations: First, a correlation only uncovers linear trends, whereas the association between two variables can take any conceivable form. The validity of correlations depends on how salient the feature of linear trend is. In the example below, \\(Y_1\\) reveals a strong parabolic form, which results in zero correlation. The curvature of an exponentially rising function is only captured insufficiently. For that reason, I recommend that correlations are always cross-checked by a scatterplot. Another situation where covariances and correlations fail is when there simply is no variance. It is almost trivial, but for observing how a variable \\(Y\\) changes when \\(X\\) moves is that both variables vary, there is no co-variance without variance. As we have seen, for every combination of two categorical and metric variables, we can produce summary statistics for the association, as well as graphs. The second part of this book really is about statistical models that extend association statistics in two ways: first, we will see that linear models [#LM] can capture the associations between more than just two variables. Second, by Bayesian estimation, we can derive statements of uncertainty. While it could be tempting to primarily use summary statistics and rather omit statistical graphs, the last example makes clear that some statistics like correlation are making assumptions on the shape the association. The different graphs we have seen are much less presupposing and can therefore be used to check the assumptions of statistics and models. 4.3 Bayesian probability theory Mathematics is emptiness. In its purest form, it does not require or have any link to the real world. That makes it so difficult to comprehend. Sometimes a mathematical theory describes real world phenomena, but we have no intuition about it. A classic example is Einstein’s General Relativity Theory, which assumes a curved space, rather than straight space as Newtonian Mechanics does. Our minds are Newtonian and the closest to intuitive understanding I got was the imagination of the universe as a four-dimensional mollusk, thanks to Einstein. Math can become easy, if mathematical system directly translate into real world ideas and sensations. I recall how my primary school teacher introduced the sum of two numbers as removing elements from one stack and place it on second (with an obvious stop rule). Later, as a student, I was taught the Peano axiomatic theory of Natural Numbers. As it turned out, I somehow knew them already, because they just formalized, what my teacher had told me. The formal proof for \\(1 + 1 = 2\\) is using just the same elements as me shifting blocks between towers. As we will see, the formal definition of probability is based on set theory, which put it more on the intuitive side of things. The formal theory of probability, the Kolmogorov axioms may be somewhat disappointing from an ontological perspective, as it just defines rules for when a set of numbers can be regarded probabilities. But calculating actual probabilities is rather easy and a few R commands suffice to start playing with set theory and probability. The most tangible interpretation of probabilities is that the probability of an event to happen, say getting a Six when rolling a dice, coincides with the relative frequency of Six in a (very long) sequence of throws. This is called the frequentist interpretation of probability and this is how probability will be introduced in the following. While thinking in terms of relative frequency in long running sequences is rather intuitive, it has limitations. Not all events we want to assign a probability can readily be imagined as a long running sequence, for example: the probability that your house burns down (you only have this one) the probability that a space ship will safely reach Mars (there’s only this one attempt) the probability that a theory is more true than another (there’s only this pair) The Bayesian interpretation of probability is essentially the same as the frequentist, but is mnore relaxed as it does not require that all probabilities are measured through relative frequencies in long running sequences. Bayesian thinking includes the idea that probabilities can also be a degree of belief, which can, but doesn’t have to be grounded in long-running series. In the following I will present in broad strokes how the theory of probability emerges from set theory and can be set into motion by computing relative frequencies of sets and subsets. Then I will introduce the likelihood, which is a concept equally used in classic and Bayesian statistics. After clarifying the differences between frequentist and Bayesian ideas of measuring probability, Bayes theorem is introduced as the formal underpinning of all Bayesian statistics. We will see how the likelihood and idea of certainty as probability combines to a scientific framework that emphasizes the incremental updating of our knowledge about the world we measure. 4.3.1 Some set theory The mathematical concept of probability can most intuitively be approached by thinking in terms of relative frequency in long-running sequences. Actually, it is not even required to think of a sequence (where events have an order). It suffices to assume a set of events that emerge from one experiment. A mathematical set is a collection of elements taken from a domain (or universe, more dramatically). These can either be defined by stating all the elements, like \\(S = \\{\\textrm{red}, \\textrm{yellow}, \\textrm{green}, \\textrm{off}\\}\\) or by a characterizing statement, like: \\(S := \\textrm{possible states of a Dutch traffic light}\\) The elements should be clearly identified, but need not have a particular order. (If they do, this is called an ordered set, the set of natural numbers is an example). Sets can have all possible sizes, which is called the cardinality of a set: finite (and countable) like the states of a traffic light empty like “all opponents who can defeat Chuck Norris”, \\(\\{\\}\\) or \\(\\oslash\\) infinite, but countable, like the natural numbers \\(N\\) infinite, uncountable, like the real numbers \\(R\\) You may wonder now, whether you would ever need such a strange concept as uncountable infinite sets in your down-to-earth design research. Well, the set of primary interest in every design study is the possible outcomes. Sometimes, these are finite, like \\(\\{\\textrm{success}, \\textrm{failure}\\}\\), but when you measure durations or distances, you enter the realm of real numbers. We will set this issue aside for the moment and return to it later in the context of continuous distributions of randomness. In order to introduce the mathematical concept of probability, we first have to understand some basic operations on sets. For an illustration, imagine a validation study for a medical infusion pump, where participants were given a task and the outcome was classified by the following three criteria: was the task goal achieved successfully? was the task completed timely (e.g., one minute or below)? were there any operation errors along the way with potentially harmful consequences? Note how the data table makes use of logical values to assign each observation a membership (or not) to each of the three sets. We can use the filter command to create all kinds of subsets and actually that would carry us pretty far into set theory. In the following I will introduce set theory thze Programming way, but use the package Sets, as it most closely resembles the mathematical formalism, it replaces. We begin with loading the package, which unfortunately uses the %&gt;% operator for its own purpose. Once there is more than one set in the game, set operators can be used to create all kinds of new sets. We begin with the set difference, which removes elements of one set from another (if they exist), for example the set of all successful tasks that were not completed in time. Note how the Sets package uses the minus operator to remove elements of one set (Timely) from another (Success). Using the set difference, we can produce complementary set which include all elements that are not included in a set. In probability theory this corresponds with the probability of an event (Success) and its counter-event (Failure). A set and its complementary set taken together produce the universal set, which in probability theory is the sure event with a probability of One. To show that we can use set union, which collects the elements of two separate sets into one new set, for example re-uniting a set with its complementary, or creating the set of all observations that were failure or delayed (or both): Another commonly used set operator is the intersect, which produces a set that contains only those elements present in both original sets, like the set of timely and successful task completions. Turns out all successful observations are also harmless. But not all harmless observations were successful. In set theory Success is therefore a subset of Harmless. The subset operator differs from those discussed so far, in that it does not produce a new set, but a truth value (also called logical or Boolean). Per definition, two equal sets are also subsets of each other. The &lt; operator is more strict and it means a proper subsets, where being a subset has just one direction. The example above demonstrates the, figuratively, smallest concept of set theory. The empty set has the special property of being a subset of all other set: The empty set is important for the intersect operator to work properly. It may happen that two sets do not share any elements at all. It would be problematic, if the intersect operator only worked if common elements truly existed. In such a case, the intersection of two sets is the empty set. Sets that have an empty intersection are called disjunct sets (with complementary sets as a special case). The package Sets, which defines all operators on sets so far is lacking a dedicated function for disjunctness, but this is easily defined using the intersect function: So far, we have only seen sets of atomic elements, where all elements are atomic, i.e. they are not sets themselves. With a little more abstraction, we can also conceive a set that has other sets as its elements. The set of sets that are defined by the three performance criteria and their complementary sets is an obvious example: For the formal introduction of probability, we need two concepts related to sets of sets: First, a partition of a set is a set of non-empty subsets such that every element is assigned to exactly one subset. The subsets of successes and its complementary set, all failures, is such a partition. Second, the power set is the set of all possible subsets in a set. Even with a rather small set of 20 elements, this is getting incredibly large, so let’s see it on a smaller example: The power set is tantamount for the definition of probability that follows, because it has two properties: first, for every subset of S it also contains the complementary set. That is called closed under complementarity. Second, for every pair of subsets of S, P it also contains the union, it is closed under union. In the same way, power sets are also closed under intersection. Generally, all sets of subsets that fulfill these three requirements are called \\(\\Sigma\\) algebras. The mathematical theory of \\(\\Sigma\\) algebras is central for the mathematical definition of all measures. Without going into to much depth on measurement theory, a measure is a mapping from the domain of empirical observations to the domain of numbers, such that certain operations in the domain of measurement work conistently with numerical operations. One example is the following: if you have to towers of blocks, L and R, next to each other and you look at them from one side, then the following rule applies for translating between the world of sensations and the world of sets: L L R &lt;-- Observer L R L R If you can see the top of tower L, when looking from the right side, then tower L is larger than tower R is build with more blocks than tower R. Probabilities are measures and in the next section we will see how numerical operations on probabilities relate to set operations in a \\(\\Sigma\\) algebra. We will also see that relative frequencies are measures of probability. 4.3.2 Probability In the following I will outline the formal theory of probability and use the same fictional validation study to illustrate the relevant concepts. introduced in the previous section. Performance of participants was classified by the three two-level criteria, success, harm and timeliness. Every recorded outcome therefore falls into one of eight possible sets and a purposeful way to summarize the results of the study would be relative frequencies (\\(\\pi\\), pi): Let’s examine on an abstract level, what has happened here: The set of events has been partitioned into eight non-overlapping categories made by three-way intersections. The first row, for example, is the intersect of the three sets Failure, Harmless and Delayed (see previous section). All subsets got a real number assigned, by the operation of relative frequencies which produces numbers between (and including) Zero and One. A hidden property is that, if we unite all sets, we get the universal set and, not coincidentally, if we sum over the frequencies the result is One: Back to formal: The mathematical theory of probability departs from a set of outcomes \\(\\Omega\\) and a \\(\\Sigma\\) algebra \\(F\\) defined on \\(\\Omega\\). An element \\(E\\) of \\(F\\) therefore is a set of outcomes, which is called an event. The eight threeway interaction sets above are a partition of \\(\\Omega\\), but not a \\(\\Sigma\\)-algebra. As disjunct sets they are closed under intersection for trivial reasons, but they are not closed under union. For that we had to add a lot of possible outcomes, all counter-sets to start with. The point is that we can construct all these subsets using filter commands and produce relative frequencies, like above. Probability as an axiomatic theory is defined by the three Kolmogorov axioms: The first Kolmogorov axiom states that a probability is a non-negative real number assigned to every event. The computation of relative frequencies satisfies this condition hands down. The first axiom defines a lower border of Zero for a probability measure, the second Kolmogorov axiom is taking care of an upper limit of One. This happens indirectly by stating that the set of all observations \\(\\Omega\\) (which is an element of \\(F\\)) is assigned a probability of One. In the table of relative frequencies that is not yet covered, but we can easily do so: So far, the theory only cared for assigning numbers to events (subsets), but provides no means to operate on probabilities. The third Kolmogorov axiom establishes a relation between the union operator on sets and the sum operator on probabilities by stating that the probability of a union of disjunct events is the sum of the individual probabilities. We can approve this to be true for the relative frequencies. For example, the question could be: Is the set of all successful observations the union of successful timely observations. Indeed, the relative frequency of all successful events is the sum of the two and satisfies the third axiom: The Kolmogorov axioms establish a probability measure and lets us do calculations on disjunct subsets. That would be a meager toolbox to do calculations with probabilities. What about all the other set operators and their possible counterparts in the realm of numbers? It is one of greatest wonders of the human mind that the rich field of reasoning about probabilities spawns from just these three axioms and a few set theoretic underpinnings. To just give one very simple example, we can derive that the probability of the complement of a set \\(A\\) is \\(P(\\Omega/A) = 1 - P(A)\\): From set theory follows that a set \\(A\\) and its complement \\(\\Omega/A\\) are disjunct, hence axiom 3 is applicable: \\(P(A \\cup \\Omega/A) = P(A) + P(\\Omega/A)\\) From set theory follows that a set \\(A\\) and its complement \\(\\Omega/A\\) form a partition on \\(\\Omega\\). Using axiom 2, we can infer: \\[\\begin{aligned} A \\cup \\Omega/A = \\Omega\\\\ \\Rightarrow P(A) + P(\\Omega/A) = P(\\Omega) = 1\\\\ \\Rightarrow P(\\Omega/A) = 1 - P(A) \\end{aligned}\\] The third axiom tells us how to deal with probabilities, when events are disjunct. As we have seen, it applies for defining more general events. How about the opposite direction, calculating probabilities of more special events? In our example, two rather general events are Success and Timely, whereas the intersection event Success and Timely is more special. The probability of two events occurring together is called joint probability \\(P(\\textrm{Timely} \\cap \\textrm{Success})\\). The four joint probabilities on the two sets and their complements are shown in the following table. As joint probability asks for simultaneous occurrence it treats both involved sets symmetrically: \\(P(\\textrm{Timely} \\cap \\textrm{Success}) = P(\\textrm{Successes} \\cap \\textrm{Timely})\\). What if you are given one piece of information first, such as “this was a successful outcome” and you have to guess the other “Was it harmful?”. That is called conditional probability and in this case, it is Zero. But what is the conditional probability that, when you know an event was a failure, it really caused harm? \\(P(\\textrm{Harmful}|\\textrm{Success})\\) In the manner of a speed-accuracy trade-off, there could be a relationship between Timely and Harm. Participants who rush through the task are likely to make more harmful errors. We would then expect a different distribution of probability of harm by whether or not task completion was timely. See how conditional probabilities sum up to one within their condition. In this case, the conditional probabilities for harm are the same for successes and failures. As a consequence, it is also the same as the overall probability, hence: \\[\\begin{aligned} P(\\textrm{Harm} | \\textrm{Timely}) = P(\\textrm{No harm} | \\textrm{Timely}) = P(\\textrm{Timely}) \\end{aligned}\\] This situation is called independence of events and it means that knowing about one variable does not help in guessing the other. In Statistics, conditional probability and independence of events are tightly linked to likelihoods and Bayes theorem [#Bayes_theorem]. 4.3.3 Likelihood In the previous section we have seen how to create probabilities from relative frequencies in data how to do basic calculations with those probabilities. Using these concept, we will see in teh following how data can be used for inference. Inference means to draw conclusions about theoretical models. In the following this will be illustrated at the example of rolling dices, where the default theory is that the dice is fair, hence the probability of Six is assumed to be \\pi_{Six}. For the matter here, we take the theory of the dice being fair into the equantion, as a condittional probability, in words: “Conditional on this dice being fair, the chance of rolling Six is \\({1/6}\\)”. \\[P(y = \\textrm{Six}|\\pi_\\textrm{Six} = {1 \\over 6})\\] When such a standard standard dice is rolled twice, how likely is an outcome of two times Six? We use probability theory: It is one dice but the results of the rolls are otherwise independent; for example, the probability to roll a Six does not wear off over time or anything like that. \\(P(y_1 = \\textrm{Six}|\\pi = {1 \\over 6}) = P(y_2 = \\textrm{Six} = {1 \\over 6})\\). Because of independence, the joint probability of rolling two times Six is just the product: \\[ P(y_1 = \\textrm{Six} \\textrm{ and } y_2 = \\textrm{Six}|\\pi = {1 \\over 6}) \\\\= P(y_1 = \\textrm{Six}) \\times P(y_2 = \\textrm{Six}|\\pi = {1 \\over 6}) \\\\= {1 \\over 36} \\] The joint probability of all data points is called the Likelihood and generalizes to situations where the probabilities of events are not the same, for example: What is the probability of the first dice being an Four, the second a five and the third a Three or a Six? \\[ P(y_1 = \\textrm{Four} \\textrm{ and } y_2 = \\textrm{Five} \\textrm{ and } y_3 = \\textrm{Three} \\textrm{ or } \\textrm{Six})\\\\ = {1 \\over 6} \\times \\frac 1 6 \\times \\frac 1 3 = \\frac 1 {108} \\] Notice how the likelihood gets smaller in the second example. In fact, likelihoods are products of numbers between zero and one and therefore become smaller with every observation that is added. In most empirical studies, the number of observations is much larger than two or three and the likelihood becomes inexpressibly small. Consider the following results from 16 rolls. The likelihood of this result, given that it is a fair dice, is \\(\\frac 1 6^{16} = 3.545\\times 10^{-13}\\). Therefore, one usually reports the logarithm of the likelihood (log-likelihood). This results in “reasonable” negative numbers. Why negative? Because all Likelihoods are fractions of One (the identity element of multiplication), which results in a negative logarithm. The dice rolling example above has a twist. assumed that we may enter \\(\\pi = {1 \\over 6}\\), because we believe that it is a fair dice, without further notice. In other words, we needed no data, because of overwhelming prior knowledge (or theory, if you will). And now we will come to see, why I took the effort to write the probabilities above as conditional probabilities. A likelihood is the probability of the data, given a parameter value. The basic idea of likelihoods is to consider data constant, and vary the parameter. In such a way, we can see how the likelihood changes when we assume different values for \\(\\pi_\\textrm{Six}\\). Imagine we have been called in to uncover fraud with biased dices in a casino. There is suspicion, that the chance of rolling a Six is lower than \\(1 \\over 6\\). So, what is the most likely chance of rolling a Six? In the following simulation, 6000 rolls have been recorded: The result is shown in the figure above and just for simplicity we just focus on the events of rolling a Six. If we have no prior suspicion about the dice, the estimated probability is simply the relative frequency of Six. In this case, we can simply note down that the most likely value is \\(\\pi_{Six} =.147\\), which is lower than the fair 0.167. But, note the slight ruggedness of the bar chart. Not a single bar is read as exactly \\(1 \\over 6\\), so the deviation of Six could have happened by chance. One way to approach this question is comparing the likelihoods \\(P(\\text{Result = Six}|\\pi = {1 \\over 6})\\) and \\(P(\\text{Result = Six}|\\pi = {.147})\\). For that purpose, we create a new event variable Six, that indicates whether a roll is a Six (TRUE) or not (FALSE). Further, a distribution function is required that assigns these events their probabilities. Distribution functions can take very complicated forms [#statistical models], but in the case here it is the rather simple Bernoulli distribution. The log-likelihood function of the Bernoulli distribution is just the sum of log-probabilities of any roll \\(y_i\\). \\[ LL_\\text{Bern}(Y|\\pi) = \\sum_i{\\log(d_\\text{Bern}(y_i, \\pi))} \\] Now, we can determine the ratio of likelihoods LR with different values for \\(\\pi\\). Note that on the logarithmic scale, what was a ratio, becomes a difference. \\[ LR = \\exp(LL_\\text{Bern}(\\text{Rolls}, .147) - LL_\\text{Bern}(\\text{Rolls}, \\frac 1 6) \\] Now, recall what a likelihood is: the probability of the observed data, under a certain model. Here, the data is almost 5000 times more likely with \\(p = .147\\). In classic statistics, such likelihood ratios are routinely been used for comparison of models. Previously, I have indicated that the relative frequency gives us the most likely value for parameter \\(\\pi\\) (the case of a Bernoulli distributed variable), the maximum likelihood estimate (MLE). The MLE is that point in the parameter range (here \\([0;1]\\)), which maximizes the likelihood. It is the point, where the data is most likely. In a similar way, the mean of Gaussian distributed measures is the maximum likelihood estimate for the distribution parameter \\(\\mu\\). But, more advanced models do not have such a closed form, i.e., a formula that you can solve. Parameter estimation in classic statistics heavily grounds on numerical procedures to find maximum likelihood estimates, which I will outline now: The probability function \\(d_\\text{Bern}(y_i, \\pi)\\) has two parameters that vary, the result of a roll \\(y_i\\) and the assumed chance \\(pi\\). The likelihood function, as we use it here, in contrast, takes the data as fixed and only varies on parameter \\(\\pi\\). By varying the parameter and reading the resulting likelihood of data, we can numerically interpolate the MLE. The most basic numerical interpolation method is a grid search, which starts at the left boundary of parameter range, zero in this case, and walks in small steps along a grid to the right boundary (one). By convention, maximum likelihood estimation is performed by minimizing the negative log-likelihood. For the convenience, the following likelihood function has been vectorized, to make it work smoothly in a tidy processing chain. Because we use the negative log-likelihood, the value for \\(\\pi\\) with maximum likelihood is the minimum of the likelihood curve. Here, \\(\\pi_\\text{MLE} = 0.15\\), which is very close to the relative frequency we obtained above. The slight deviation is due to the limited resolution of the grid, but it is always possible to be more accurate by using a finer grid. In classic statistics MLE is one of the most common methods for estimating parameters from models. However, most of the time, data is more abundant and there is more than one parameter. It is possible to extend the grid method to as many parameters as the model contains by just creating multi-dimensional grids and walk through them by the likelihood function. However, already a two-dimensional grid of rather coarse \\(100 \\times 100\\) would require the computation of 10.000 likelihoods. Classic statisticians have therefore developed optimization methods to identify the MLE more efficiently. Soon, we will turn our attention to Bayesian estimation, where the Likelihood plays a central role in estimation, too [#Bayesian_est]. 4.3.4 Bayesian and frequentist probability Let us for a moment return to how probability was axiomatically defined by Kolmogorov’s axioms. The axioms themselves only speak of relations between sets and probability. There is not a trace of any algorithm that would let you construct a set of numbers that actually qualifies as probabilities. There is one theory everyone agrees with: relative frequencies satisfy the three Kolmogorov axioms. That can be demonstrated, as I did in section [#Probability], and there is mathematical proof for that, more precisely: if you repeat a random experiment an infinite number of times, relative frequencies are measures for probability. And since not all mathematical theory has such an intuitive interpretation, we can call ourselves lucky. And at the same time it is obvious, because Kolmogorov, like Peano, sat down to formalize something most people intuitively understand. All statisticians believe that relative frequencies satisfy Kolmogorov’s axioms, but not everyone thinks that this is the only way. And that is where frequentist and Bayesian statistics diverge: Frequentest believe that only relative frequencies are valid measures for probability, whereas Bayesians believe that certainty is a measure of probability, too. As subtle this may sound, it has remarkable consequences in context with Bayes theorem. QUESTION: What is the chance that this dice will fall on Six? FREQUENTIST: I can only answer this question after a long-running series of dice rolls. BAYESIAN: I am 50% certain that this is a trick question, so I’d say 50% it never falls on Six. If this is not a trick question, this seems to be a dice from a well-known manufacturer, so I’d say 1/6. FREQUENTIST (frowning): And how does the manufacturer know? BAYESIAN (rolls eyes): … by a long-running series of experiments. The frequentist in the caricature is someone sticking to the principles: every probability must be produced by relative frequencies, which must be produced in repetitions of the same experiment. The Bayesian has no problem with long running sequences, but feels confident to use other sources of information. In a strictly frequentist view, this is like pulling numbers, just in the unit interval, out of the thin air. But, as we will see, Bayes theorem is amazingly useful, when we allow for probabilities that don’t derive from long-running sequences. These are called prior probabilities and they can be used to factor in prior knowledge about the world. Consequently, a Bayesian also accepts that levels of certainty after the experiment can be reported as probabilities. In contrast, a frequentist must insist that certainty can be interpreted as long-running series. In frequentist statistics, a common way to express ones level of certainty is the infamous p-value. And this is its definition: A result is called statistically significant on level \\(\\alpha = .05\\), if drawing from the null distribution (an infinite number of times) will produce the observed result or a larger result in no more than 5% of cases. Another, and more preferable way of expressing ones certainty about a parameter (say, the population mean) is the 95% confidence interval, which is expressed as two endpoints: It is common to say: “we can be 95% certain that the true values is between these bounds”, but the 95% confidence interval really is defined by assuming an (infinite) set of replications of the very same experiment and using relative frequencies: The 95% confidence interval is constructed in such a way that, if the same experiment were repeated an infinite number of times, in 95% of these repetitions the true value is contained in the interval. You are not alone when you lack intuition of what the definition says and when you feel at unease about where all these experiments are supposed to come from. It seems as if a a true frequentist cannot imagine the future other than by a series of long-running experiments. In Bayesian statistics, the level of certainty is expressed as a proposition about one’s state of knowledge, like : Based on my data I am 95% sure that there is a difference. Equating level of certainty with probability directly, without taking the detour via relative frequencies, may be a little lax, but it leads to remarkably intuitive statements on uncertainty. In Bayesian statistics the credibility interval is defined as: With a probability of 95%, the true value is contained. Since there seems to be no external criterion (such as a series of experiments, imagined or not), Bayesian statistics often faced the criticism of being subjective. In fact, if we imagine a certainty of 95% as some number in the researchers mind, that might be true. But, it is quite easy to grasp certainty as an objective quantity, when we assume that there is something at stake for the researcher and that she aims for a rational decision. In the previous chapter I have illustrated this idea by the example of carrying an umbrella with you (or not) and the 99 seconds claim. Generally, it helps to imagine any such situation as a gamble: if you bet 1 EUR that the true population mean is outside the 95% credibility interval, as a rational person I would put 19 EUR against. In effect, the Bayesian certainty is a probability in mathematical terms, without the necessity to implement it as a relative frequency. That liberates our reasoning from the requirement to think of long-running series. In particular, it allows us to enter non-frequentist probabilities into Bayes theorem, resulting a statistical framework that captures the dynamics of belief in science [#dynamics]. Before we come to that, I will explain Bayes theorem in another context of updating knowledge, medical diagnostics. 4.3.5 Bayes theorem Bayes’ theorem emerges from formal probability theory and therefore is neither Bayesian nor frequentist. Essentially, the theorem shows how to calculate a conditional probability of interest \\(P(A|B)\\) from a known conditional probability \\(P(B|A)\\) and two marginal probabilities \\(P(A)\\) and \\(P(B)\\): \\[ P(A|B) = { P(A)P(B|A) \\over P(B)} \\] The proof of the theorem is rather simple and does not require repetitiuon here. But, Bayes’ theorem can lead to rather surprising conclusions, which can be illustrated by example of a medical screening test, as follows: In the 1980, the human immunodeficiency virus (HIV) was discovered and since then has become a scourge for humanity. Given that the first outbursts of the disease raged among homosexual men, it is not really surprising that some conservative politicians quickly called for action an proposed a mandatory test for every one, with the results to be registered in a central data base. Without much of a stretch it may seem justifiable to store (and use) the information that someone is carrying such a dangerous disease. The problem is with those people who do not carry it, but could be mis-diagnosed. These are called false-positives. Let#s assume the power of a screening test has been assessed by examining samples of participants where it is fully known whether someone is a carrier of the virus \\(C+\\) or not \\(C-\\). The result is a specificity of 95%, meaning that 95% of \\(C-\\) are diagnosed correctly (\\(P(T-|C-)\\)), and a sensitivity of 99%, meaning that 99% with \\(C+\\) are diagnosed correctly (\\(P(T+|C+)\\)). The question that Bayes’ theorem can answer in such a situation is How many citizens would be registered as HIV carrying, although they are not?. For this to work, we must also know the probability that someone randomly chosen from the population is a carrier (\\(P(C+)\\)) and the proportion of positive test results \\(P(T+)\\). \\[ \\begin{aligned} P(C+) &amp;&amp;= .0001\\\\ P(C-) &amp;= 1 - P(C+) &amp;= .9999\\\\ P(T+|C+) &amp;&amp;= .99\\\\ P(T-|C+) &amp;= 1 - P(T-|C+) &amp;= .01\\\\ P(T-|C-) &amp;&amp;= .95\\\\ P(T+|C-) &amp;= 1 - P(T-|C-) &amp;= .05\\\\ P(T+) &amp;= P(C+)P(T+|C+) + P(C-)P(T+|C-) &amp;\\approx .05\\\\ P(T-) &amp;= 1 - P(T+) &amp;\\approx .95 \\end{aligned} \\] How do these numbers arise? The first, \\(P(C+)\\)) is the proportions of HIV carriers in the whole population. If you have no test at all, that is your best guess for whether a random person has the virus, your prior knowledge. Then, the validation study of the test provides us with more data. The study examined the outcome of the test (\\(T+\\) or \\(T-\\)) in two groups of participants, those that were knowingly carriers \\(C+\\) and those that were not \\(C-\\). This is where the four conditional probabilities come from. Finally, we need the expected proportion of positive test results \\(P(T+)\\), which we compute as a marginal probability over the two conditions. Because non-carriers \\(C-\\) dominate the population by so much, the marginal probability for a positive test is almost the same as the probability for a positive test amomg non-carriers \\(P(T+|C-)\\). All conditional probabilities here emerge from a validation study, where it is known upfront whether someone is a carrier or not. What matters for the application of the screening test is the reverse: what is learned by test about carrier status? In the following the test is being characterized by two types of errors that can occur: false alarms and misses. False alarms: What is the probability that someone will be registered as carrier, although being a non-carrier? That is: \\(P(C-|T+)\\). When the false alarm rate is low, the test is called to have good specificity. Misses: Which proportion of the tested population are carriers, but have a negative test result (and act accordingly)? That is: \\(P(C+|T-)\\). When the probability of misses is low, the test is called to have good sensitivity. Using Bayes’ theorem, we obtain the following probability for false alarms: \\[ \\begin{aligned} P(C-|T+) &amp;= {P(C-)P(T+|C-) \\over P(T+)}\\\\ &amp;\\approx {{.9999 \\times .05} \\over .05}\\\\ &amp;\\approx .9999 \\end{aligned} \\] Obviously, testing the whole population with the screening test would result in disaster. Practically everyone who is registered as carrier in the data base is really a non-carrier. In turn, the probability that a person has the virus despite a negative test result is: \\[ \\begin{aligned} P(C+|T-) &amp;= {P(C+)P(T-|C+) \\over P(T-)}\\\\ &amp;\\approx {.0001 \\times .01 \\over .95}\\\\ &amp;\\approx .000001 \\end{aligned} \\] We see a strong asymmetry in how useful the test is in the two situations. Specificity of the test is rather low and stands no chance against the over-whelming prevalence of non-carriers. In the second use case, prevalence and high test sensitivity work in the same direction, which results in a fantastically low risk to err. That is what Bayes’ theorem essentially does: it combines prior knowledge (prevalence, \\(P(C+)\\)) against obtained evidence and produces posterior knowledge. In the following section we will see how this makes Bayes’ theorem a very useful tool in a context that is all about updating knowledge, like in science. In the case of medical diagnostics, the probabilty \\(P(C+|T+)\\) can be called a posterior probability, but it becomes prior knowledge in teh very moment that new data arrives and we can update our knowledge once again. A reasonable strategy arises from the asymmetry of certainties: a negative test almost certainly coincides with being a non-Carrier, whereas identifying true carriers remains problematic. Follow-up research should therefore capitalize on reducing false alarms and use a test with extreme specificity (= absence of false alarms) at the expense of sensitivity. 4.3.6 The dynamics of belief In the previous section we have seen an application of Bayes theorem where it served to debunk a diagnostic procedure. As the procedure of testing everyone for HIV may seem reasonable at first, it turned out to be impractical. That is an example of how a principally intuitive theories like Probability Theory can result in counter-intuitive, but correct, answers. These are the most valuable answers, actually. In Bayesian statistics, certainty (or strength of belief, state of knowledge, credibility) is assumed to follow all rules of probability theory. We may still think of it in terms of frequencies or illustrate it as a gamble, whatever fits the context better. The point is that we may enter degrees of belief (instead of relative frequencies) into Bayes rule, as follows: \\[ P(\\theta|\\text{Data}) = { P(\\theta) P(\\text{Data}|\\theta) \\over P(\\text{Data})} \\] In this equation, the only probability that has been measured by frequencies is the likelihood \\(P(\\theta|\\text{Data})\\), where \\(\\theta\\) is the parameters to be estimated (e.g. \\(\\pi\\) of Bernoulli distributions). This likelihood carries evidence from data. \\(P(\\theta|\\text{Data})\\) is the posterior certainty, or how much we believe in a parameter position, after seeing the data. Posterior certainty turns out to be proportional to the product of prior certainty \\(P(\\theta)\\) and likelihood. This is the place where prior knowledge and strength of evidence are weighed against each other, for example how strongly we belive that a dice is fair. \\(P(\\text{Data})\\) is called the marginal likelihood. In section [#likelihood] it was explained how parameters can be estimated by maximum likelihood estimation. This procedure compared the likelihood over a range of parameter values to determine the position that makes the observed data most likely. In the context of parameter estimation, a model is just such a position in parameter space. Bayesian estimation does it very similar, but operates on posterior probability, which includes prior knowledge. As it turns out the marginal likelihood \\(P(Data)\\) cannot be known or easily estimated. The reason why we can still use Bayes rule for estimation is that this term does not depend on the parameter vector \\(\\theta\\), which is our search space, meaning that it is practically a constant. For parameter estimation, Bayes rule can be reduced to saying that the posterior certainty is proportional to the product of prior certainty and evidence: \\[ \\text{posterior}\\ \\propto \\text{prior}\\times\\text{likelihood} \\] This mechanism of updating states of beliefs is what sets Bayesian statistics apart and it reverbs with what happens in the real world and in science: Someone skeptical of climate change may change their view after a couple of very hot summers. When almost everyone believed in Newton’s Mechanics as ground truth, some people got curious when it turned out that light speed is constant for every observer. Note how these examples all have the same dynamics: a prior belief is updated when new data arrives. These are precisely the three elements of Bayesian statistics: the prior belief is what you believe before (climate change is a myth) the likelihood is what you learn by observation (hot summer) the posterior belief is your adjusted believe after seeing the data (maybe not such a myth) One could almost say that updating knowledge is the primary purpose of any central nervous system, where the attached sensors provide the data. All animals with such a system cannot just react to their environment by innate patterns, but learn and over a life’s course it is not such a stretch to call this a long-running experiment. Systematic research is what only some central nervous systems do, but the principle is the same: we have a theory which we are unsure about; data is collected and after that we may find this theory more likely (or not). But, that does not exclude that prior knowledge grounds on data, too. In experimental Psychology researchers almost seem to entertain themselves with repetitions of the very same experimental paradigm, with slight variations maybe. For example, in the famous Stroop effect, participants have to name the ink color of a word. When the word is itself a color word and refers to a different color, response times typically increase. This effect has been replicated in many dozens of published studies and, probably, thousands of student experiments. Cumulative evidence is so strong that, would someone repeat the experiment another time and find the reverse effect, no one would seriously take this as a full debunk of decades of research. The previous section closed with the remark that more research could solve the dilemma of testing diseases with a very low prevalence, and the same goes here: Even just one negative result on teh Stroop effect can raise some mild suspicions about the effect’s universalality and someone may feel that another replication is required. One principle of Bayesian thinking is: Today’s posterior is tomorrow’s prior. There is no principled difference between prior and posterior knowledge. They are just degrees of belief at different points in time, expressed as probabilities. Both can differ in strength: prior knowledge can be firm when it rests on an abundance of past evidence. But, prior knowledge can also be over-ruled when a lot of data disproves it. Evidence in data varies in strength: for example, the more observations, the larger the likelihood becomes, which means stronger evidence. If measures are more accurate (and less disturbed by randomness), smaller sample sizes suffice to reach conclusions of similar strength. This is why larger sample sizes are preferred and why researchers try to improve their methods. Prior belief and evidence by data can be congruent or contradict each other. When they are congruent, prior belief is just strengthened by the data. When they are contradicting each other, prior belief is over-ruled to some degree, depending on the strength of (counter-) evidence. Updating our knowledge is essential for individual decision makers, but in particular applies for scientific progress. The problem with frequentist statistics is that it has not developed general methods to incorprorate prior knowledge in the analysis. It is even worse: When practicing null hypothesis significance testing, which is very common among frequentists, you strictly must not update your degree of belief during the study and act accordingly. Neither is there a way to express one’s prior belief when doing a t-test, nor may you adjust sample size ad hoc until satisfactory certainty is reached. Classic data analysis pretends as if no one has ever done any such an experiment before. Also, it is strictly forbidden to invite further participants to the lab, when the evidence is still to weak. If you planned the study with, say, \\(N = 20\\), this is what you must do, no less no more. If you reach your goal with less participants, you must continue testing. If you are unsatisfied with the level of certainty, the only permissible action is dump your data and start over from zero. The frequentist denial of incremental knowledge is not just counter-intuitive in all science, it is a millstone around the neck of every researcher. 4.4 Statistical models It is a scientific, maybe even naturalistic, principle that every event to happen has its causes (from the same universe). The better these causes are understood, the better will be all predictions of what is going to happen the next moment, given that one knows the laws of physics. Laplace demon is a classic experiment of thought on the issue: the demon is said to have perfect knowledge of laws of physics and about the universe’s current state. Within naturalistic thinking, the demon should be able to perfectly predict what is going to happen in the next moment. Of course, such an entity could never exist, because it were actually a computer that matches the universe in size (and energy consumption). In addition, there are limits to how precisely we can measure the current state, although physicist and engineers have pushed the limits very far. When Violet did her experiment to prove the superiority of design B, the only two things she knew about the state of affairs was that the participant sitting in front of her is member of a very loose group of people called the “typical user” and the design her or she was exposed to. That is painstakingly little information on what’s currently going on in the patricipant’s central nervous system. Her lack of knowledge is profound but still not a problem as the research question was on a gross scale itself, too. Not what happens to individual users needed to be described, but just the difference in average duration. Instead, imagine Violet and a colleague had invented a small game where they both guess the time-on-task of individual participants as they enter the lab. Who comes closest wins. As both players are smart people, they do not just randomly announce numbers, but let themselves guide by data of previous sessions. A very simple but reasonable approach would be to always guess what the average ToT in all previous sessions has been. In [#GMM] we will call this a grand mean model. Of course, Violet would never expect her grand mean model to predict the accurate outcome of a session. Still, imagine a device that has perfect knowledge of the website, the complete current neural state of a the participant and the physical environment both are in. As an all-knowing device it would also have complete knowledge on how neural states change. With this device, Jane could always make a perfect prediction of the outcome. Unfortunately, real design researchers are far from Laplace demonism. Routinely borrowing instruments from social sciences, precision of measurement is humble and the understanding of neural processes during web navigation is highly incomplete. Participants vary in many complex ways in their neural state and this makes a myriad of small unrelated forces (SMURF) that steers an individual users in a unique situation away from the average. Laplace demon has perfect knowledge of all SMURF trajectories and therefore can produce a perfect prediction. Violet, in contrast, is completely ignorant of any SMURFs and her predictions will be way off many times. A common way to conceive this situation is that any measure is composed of a structural part and a random part, \\(\\epsilon_i\\). \\[\\text{Measure}_i = \\text{structural part} + \\text{random part}_i\\] Note that Measure and Random part both have an observation-level index \\(i\\), which means they are unique per observation \\(i\\). The structural part does not have such an index, because it makes a universal statement, a proposition that holds for all the observations. And for the scope of this book, a statistical model is defined as a mechanism that separates the structural from the random part, to some extent. The population average is the most simple of a class of structural parts called linear models [#lm] and we will see throughout the whole second Part of this book, how the structural part encodes increasingly complex research questions or theories of ours. After a minimally sufficient introduction to the matter, the remainder of this section will focus on the random part, which is not so random as one may think. Generally, statistical models consist of these two parts: the likelihood to describe the association between predictors and expected values and the random part, which describes the overall influence of the unexplained SMURFs. 4.4.0.1 FINISH 4.4.1 Predictions and likelihood The structural part of statistical models 4.4.1.1 EDIT The likelihood function states the dependency of outcome on the predictor variables. The dependency can be a complex mathematical function of multiple predictors, or as simple as the population average. A common likelihood function is the linear function. For example, in their guessing game, Violet could try to improve her population model, by also taking age of participants into account. Older people tend to be slower. Violet creates a plot from past records. The ellipsoid form of the point cloud indicates that ToT is somehow depending on age. Violet draws a straight line with an upward slope to approximate the relationship. It seems that 30 year old persons have an average ToT of around 90 seconds, which increases to around 120 seconds for 50 year old. Arithmetically, this is an increase of around 1.5 seconds per year of age. Violet can use this information to improve her gambling. Instead of stoically calling the population mean, she uses a linear function as predictor: $90 + ( - 30) 1.5 $. In Bayesian statistics, this is called a likelihood function and the general form for a single linear likelihood function is: \\[\\mu_i = \\beta_0 + \\beta_1x_{1i}\\\\\\] Likelihood functions connect the expected value \\(\\mu\\) with observed variables \\(x_{i1}, x_{i2}, ..., x_{ik}\\), and (to be estimated) parameters, e.g. \\(\\beta_0, \\beta_1\\). The likelihood function is often called the deterministic part of a model, because its prediction strictly depends on the observed values and the predictors, but nothing else. For example, two persons of age 30 will always be predicted to use up 90 seconds. Apparently, this is not the case for real data. The linear model is very common in statistical modeling, but likelihoods can basically take all mathematical forms. For example: the grand mean model, Violet used before: \\(\\mu_i = \\beta_0\\) two predictors with a linear relationship: \\(\\mu_i = \\beta_0 + \\beta_1x_{1i} + \\beta_1x_{2i}\\\\\\) a parabolic relationship: \\(\\mu_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i}^2\\) a nonlinear learning curve: \\(\\mu_i = \\beta_\\textrm{asym} (1 + \\exp(-\\beta_\\textrm{rate}(x_\\textrm{training} + \\beta_\\textrm{pexp})))\\) the difference between groups A and B, where \\(x_1\\) is a membership (dummy) variable coded as \\(A\\rightarrow x_1:=0, B\\rightarrow x_1:=1\\): \\(\\mu_i = \\beta_0 + \\beta_1x_{1i}\\) In the vast majority of cases, the likelihood function is the interesting part of the model, where researchers transform their theoretical considerations or practical questions into a mathematical form. The parameters of the likelihood function are being estimated and answer the urging questions, such as: Is the design efficient enough? (\\(\\beta_0\\)) By how much does performance depend on age? (\\(\\beta_1\\)) Under which level of arousal does performance peak? (determining the stationary point of the parabola) How fast people learn by training (\\(\\beta_\\textrm{rate}\\)) By how much design B is better than A (\\(\\beta_1\\)) A subtle, but noteworthy feature of likelihood functions is that \\(\\mu_i\\) and \\(x_i\\) have indicators \\(i\\). Potentially, every observation \\(i\\) has their own realization of predictors and gets a unique expected value, whereas the parameters \\(\\beta_0, \\beta_1\\) asf. are single values that apply for all observations at once. In fact, we can conceive statistical models as operating on multiple levels, where there is always the two: the observation level and the population level. When introducing multi-level models, we will see how this principle extends to more than these two levels. Another related idea is that parameters summarizes patterns found in data. Any summary implies repetition and that is what the likelihood expresses: the pattern that repeats across observations and is therefore predictable. 4.4.2 Distributions: shapes of randomness Review the formula \\[\\text{Measure}_i = \\text{structural part} + \\text{random part}_i\\]. What do we actually have? It turns out that the only part we know for sure is the measure \\(i\\). The structural part contains the answer to our research question, so this is what we want to estimate. Unfortunately, there is another unknown, the \\(random part\\), which we literally need to subtract to get to our answers. Obviously, this would never work if the random part were random in the sense of being completely arbitrary. Fortunately, randomness is not arbitrary, but has a shape, which is described as a probability (or density) distribution. These distributions typically belong to a certain family of distributions, e.g. Poisson disributions or Gaussian distributions. By assuming a certain shape of randomness, we know a lot more about the random component, which will make it easier to flesh out the structure. Choosing the most appropriate distribution therefore is a crucial step in creating a valid statistical model. 4.4.2.1 Probability and density distributions The random part of a statistical model is what changes between observation and is not fully predictable. When using the grand mean model, the only information we are using is that the person is from the target population. Everything else is left to the unobserved SMURFs and that goes into the random part of the model. Fortunately, SMURFs don’t work completely arbitrary and in practice there is just a small number of recognizable shapes randomness can take. These patterns can be formulated mathematically as probability and density distributions. A probability distribution is given by a probability mass function (PMF) that assigns probabilities to outcomes, such as: probability of task success is \\(.81\\) probability of 99 seconds or better is \\(.22\\) probability of all SMURFs together pushing a persons IQ beyond 115 is around \\(.33\\) 4.4.2.2 REDACT Probability mass functions are mathematical functions that assign probabilities to the outcome of a measured variable \\(Y\\). Let’s see this at a small example: Consider a participant who is asked to complete three tasks of constant difficulty, with a chance of \\(.3\\) for each one to be solved. The outcome variable of interest is the number of correct completions, which can take the values 0, 1, 2 or 3. Under idealized conditions (but not removing randomness), the following probability distribution assigns every possible outcome a probability to occur. We see that the most likely outcome is one correct task, which occurs with a probability of \\(P(y = 1) = 0.441\\). At the same time, it is surprisingly likely to fail at all tasks, \\(P(y = 0) = 0.343\\). We may also look at combined events, say the probability for less than two correct. That is precisely the sum \\(P(y \\leq 1) = P(y = 0) + P(y = 1) = 0.784\\). We can bundle basic events by adding up the probabilities. An extreme case of that is the universal event that includes all possible outcomes. You can say with absolute certainty that the outcome is, indeed, within zero and three and certainty means the probability is 1, or: \\(P(0 \\leq y \\leq 3) = 1\\). Simply in order to comply with the third Kolmogorov axiom, all probability (and density) distributions have probability mass of One. More precisely, the area covered by the function must be exactly One. Outcomes like task successes have a countable number of possible outcomes, and are therefore called discrete. If we want to obtain the probability of combined events, we just have to sum probabilities over all the included events. This approach fails, if the outcome is not discret, but continuous. On a continuous scale, every outcome is a point in the strictly geometrical sense of having no extension. That makes the probability masses of any such point extensionless, too, i.e. exactly Zero. Another way to characterize a random distribution is cumulative distribution distribution (CDF), which is the renders the increase in probability mass when moving from left to right over the outcome scale. As shown below for discrete outcomes this function starts at Zero, then moves upwards in increments and reaches One at the left end of the scale. Using the CDF, the probability of an event, that is a range of outcomes, can be written as the difference between the upper and the lower limit of cumulative probability mass. For example, the probability of having two or three tasks correct is. Note that the lower limit must be included and strictly spoken, the second term in the difference is the largest outcome value not included. Owing to extensionlessness of outcomes this does not matter for continuous outcomes. Note that the lower limit must be included and strictly spoken, the second term in the difference is the largest outcome value not included. Owing to extensionlessness of outcomes this does not matter for continuous outcomes. when the outcome measure is continuous, rather than discrete, we cannot assign non-zero probabilities to outcomes. But, we can give non-zero probabilities to ranges of outcoms. Consider the distribution of intelligence quotients (IQ) scores. Strictly spoken, the IQ is not continuous, as one usually only measures and reports whole number scores. Still, for instructional purposes, assume that the IQ is given in arbitrary precision, for example \\(115.0\\), \\(100.00010...\\) or \\(\\pi * 20\\). IQ scores are designed to follow the Gaussian distribution with a mean of 100 and a standard distribution of 15. 4.4.2.3 HERE That means, we can use this distribution functions to predict We observe that the most likely IQ is 100 and that almost nobody reaches scores higher than 150 or lower than 50. But, how likely is it to have an IQ of exactly 100? Less than you might think! With continuous measures, we can no longer think in blocks that have a certain area. In fact, the probability of having an IQ of exactly \\(100.00...0\\) is exactly zero. The block of IQ = 100 is infinitely narrow and therefore has an area of zero. Generally, with continuous outcome variables, We can no longer read probabilities directly. Therefore, probability mass distributions don’t apply, but the association between outcome and probability is given by what is called probability density functions. Density is not confined to the probability unit interval, but what density distributions share with PDFs is that the area under the curve is always exactly one and that probabilities still arise when taking intervals of values. Practically, nobody is really interested in infinite precision. When asking “what is the probability of IQ = 100?”, the answer is “zero”, but what was really meant was: “what is the probability of an IQ in a close interval around 100?”. Once we speak of intervals, we clearly have areas larger than zero. The graph below shows the area in the range of 85 to 115. But, how large is this area exactly? As the distribution is curved, we can no longer simply count virtual blocks. Recall that the CDF gives the probability mass, i.e. the area under the curve, for outcomes up to a chosen point. Continuous distributions have CDFs, too, and the the graph below shows the CDF for the IQs. We observe how the curve starts to rise from zero at around 50, has its steepest point at 100, just to slow down and run against 1. Take a look at the following graph. It shows the two areas \\(IQ \\leq 85\\) and \\(IQ \\leq 115\\). The magic patch in the center is just the desired interval. And here the CDF comes into the play. To any point of the PDF, the CDF yields the area up to this point and we can compute the area of the interval by simple subtraction: \\[ \\begin{aligned} P(IQ \\leq 115) &amp;&amp;= 0.159 P(IQ \\leq 85) &amp;&amp;= 0.841 P(85 \\leq IQ \\leq 115) &amp;= P(IQ \\leq 115) - P(IQ \\leq 85) &amp;= 0.683 \\end{aligned} \\] Probability and density distributions usually take on one of a few forms, which can be expressed as mathematical functions. For example, the function for the case of task completion is the binomial distribution, which gives the probability for \\(y\\) successes in \\(k\\) trials when the success rate is \\(p\\): \\[ Pr(y|p,k) = {k \\choose y}p^y(1-p)^{k-y} \\] In most cases where the binomial distribution applies, base probability \\(p\\) is the parameter of interest, whereas the number of trials is known beforehand and therefore does not require estimation. For that reason, the binomial distribution is commonly taken as a one-parameter distribution. When discussing the binomial in more detail, we will learn that \\(p\\) determines the location of the distribution, as well as how widely it is dispersed (preview Figure XY). The distribution that approximated the IQs is called the Gaussian distribution (or Normal). The Gaussian distribution function takes two parameters, \\(mu\\) determines the location of the distribution, say average IQ being 98, 100 or 102 and \\(sigma\\) which gives the dispersion, independently (preview Figure XY). 4.4.2.4 Location and dispersion 4.4.2.5 REDACT AND MERGE As we will see, random distributions can take a variety of basic shapes, but two immediate properties are location and dispersion. Location of a distribution usually reflects where the most typical values come to lie (100 in the IQ example). When an experimenter asks for the difference of two designs in ToT, this is purely about location. Dispersion can either represent uncertainty or variation in a population, depending on the research design and statistical model. The most common interpretation is uncertainty. The basic problem with dispersion is that spreading out a distribution influences how typical the most typical values are. The fake IQ data basically is a perfect Gaussian distribution with a mean of 100 and a standard deviation of 15. The density of this distribution at an IQ of 100 is 0.027. If IQs had a standard deviation of 30, the density at 100 would fall to 0.013. If you were in game to guess an unfamiliar persons IQ, in both cases 100 would be the best guess, but you had a considerable higher chance of being right, when dispersion is low. The perspective of uncertainty routinely occurs in the experimental comparison of conditions, e.g. design A compared to design B. What causes experimenters worry is when the residual distributions in their models is widely spread. Roughly speaking, residuals are the variation that is not predicted by the model. The source of this variation is unknown and usually called measurement error. It resides in the realm of the SMURFs. With stronger measurement error dispersion, the two estimated locations get less certainty assigned, which blurs the difference between the two. The second perspective on dispersion is that it indicates variation by a known source. Frequently, this source is differences between persons. The IQ is an extreme example of this, as these tests are purposefully designed to have the desired distribution. In chapter [LMM] we will encounter several sources of variation, but I am really concerned about human variation, mostly. Commonly, experimental researchers are obsessed by differences in location, which to my mind confuses “the most typical” with “in general”. Only when variation by participants is low, this gradually becomes the same. We will re-encounter this idea when turning to multi-level models. Most distributions routinely used in statistics have one or two parameters. Generally, if there is one parameter this determines both, location and dispersion, whereas two-parameter distributions can vary location and dispersion independently, to some extent. The Gaussian distribution is a special case as \\(\\mu\\) purely does location, whereas \\(\\sigma\\) is just dispersion. With common two-parametric distributions, both parameters influence location and dispersion in more or less twisted ways. For example, mean and variance of a two-parametric binomial distributions both depend on chance of success \\(p\\) and number of trials \\(k\\), as \\(\\textrm{M} = pk\\) and \\(\\textrm{Var} = kp(1-p)\\). 4.4.2.6 Range of support and skew [### ] 4.4.2.7 COMPLETE AND MERGE In this book I advocate the thoughtful choice of distributions rather than doing batteries of goodness-of-fit to confirm that one of them, the Gaussian, is an adequate approximation. It usual is trivial to determine whether a measure is discrete (like everything that is counted) or (quasi)continuous and that is the most salient feature of distributions. A second, nearly as obvious, feature of any measure is its range. Practically all physical measures, such as duration, size or temperature have natural lower bounds, which typically results in scales of measurement which are non-negative. Counts have a lower boundary, too (zero), but there can be a known upper bound, such as the number of trials. Statistical distributions can be classified the same way: having no bounds (Gaussian, t), one bound (usually the lower, Poisson, exponential) or two bounds (binomial, beta). 4.4.2.8 Data generating process 4.4.2.9 ZAP Many dozens of PMFs and PDFs are known in statistical science and are candidates to choose from. First orientation grounds on superficial characteristics of measures, such as discrete/continuous or range, but that is sometimes not sufficient. For example, the pattern of randomness in three-tasks falls into a binomial distribution only, when all trials have the same chance of success. If the tasks are very similar in content and structure, learning is likely to happen and the chance of success differs between trials. Using the binomial distribution when chances are not constant leads to severely mistaken statistical models. For most distributions, strict mathematical definitions exist for under which circumstances randomness takes this particular pattern. Frequently, there is one or more natural phenomena that accurately fall into this pattern, such as the number of radioactive isotope cores decaying in a certain interval (Poisson distributed) or … . This is particularly the case for the canonical four random distributions that follow. Why are these canonical? The pragmatic answer is: they cover the basic types of measures: chance of success in a number of trials (binomial), counting (Poisson) and continuous measures (exponential, Gaussian). 4.4.2.10 Binomial distributions 4.4.2.11 REDACT A very basic performance variable in design research is task success. Think of devices in high risk situations such as medical infusion pumps in surgery. These devices are remarkably simple, giving a medication at a certain rate into the bloodstream for a given time. Yet, they are operated by humans under high pressure and must therefore be extremely error proof in handling. Imagine, the European government would set up a law that manufacturers of medical infusion pump must prove a 90% error-free operation in routine tasks. A possible validation study could be as follows: a sample of \\(N = 30\\) experienced nurses are invited to a testing lab and asked to complete ten standard tasks with the device. The number of error-free task completions per nurse is the recorded performance variable to validate the 90% claim. Under somewhat idealized conditions, namely that all nurses have the same proficiency with the device and all tasks have the success chance of 90%, the outcome follows a Binomial distribution and the results could look like the following: Speaking about the Binomial distribution in terms of successes in a number of attempts is common. As a matter of fact, any binary classification of outcomes is amenable for Binomial modeling, like on/off, red/blue, male/female. Imagine, Jane’s big boss needs a catchy phrase for an investor meeting. Together they decide that the return rate of customers could be a good measure, translating into a statement such as eighty percent of customers come back. To prove (or disprove) the claim, Jane uses the customer data base and divides all individuals into two groups: those who have precisely one record and those who returned (no matter how many times). This process results in a distribution, that has two possible outcomes: : \\(0\\) for one-timers and \\(1\\) for returners. This is in fact, a special case of the Binomial distribution with \\(k = 1\\) attempts (the first visit is not counted). Examples are given in the first row of the figure. A Binomial distributions has two parameters: \\(p\\) is the chance of success and \\(k\\) is the number of attempts. \\(p\\) is a probability and therefore can take values in the range from zero to one. With larger \\(p\\) the distribution moves to the right. The mean of Binomial distributions is the probability scaled by number of attempts, \\(M = kp\\). Logically, there cannot be more successes then \\(k\\), but with larger \\(k\\) the distribution gets wider. The variance is the odds scaled by number of attempts, \\(\\textrm{Var} = kp(1-p)\\). As mean and variance depend on the exact same parameters, they cannot be set independently. In fact, the relation \\(Var = M(1-p)\\) is parabolic, so that variance is largest at \\(p = .5\\), but decreases towards both boundaries. A Binomial distribution with, say \\(k=10\\) and \\(p = .4\\) always has mean \\(4\\) and variance \\(2.4\\). This means, in turn, that an outcome with a mean of \\(4\\) and a variance of \\(3\\) is not Binomially distributed. This occurs frequently, when the success rate is not identical across trials. A common solution is to use plugin distributions, where the parameter \\(p\\) itself is distributed, rather than fixed. A common distribution for \\(p\\) is the beta distribution and the logit-normal distribution is an alternative. The Binomial distribution has two boundaries, zero below and number of attempts \\(k\\) above. While a lower boundary of zero is often natural, one cannot always speak of a number of attempts. For example, the number of times a customer returns to the car rental website does not yield a natural interpretation of number of attempts. Rather, one could imagine the situation as that any moment is an opportunity to hire a car. At the same time, every single moment has a very, very small chance that a car is hired, indeed. Under these conditions, an infinite (or painstakingly large) number of opportunities and a very low rate, the random pattern is neatly summarized by Poisson distributions. 4.4.2.12 Poisson distributions Some counting processes have no natural upper limit like the number of trials in a test. In design research, a number of measures are such unbound counts: number of erroneous actions frequency of returns behavioral events, e.g. showing exploratory behavior physiological events, such as number of peaks in galvanic skin response These measures can often be modeled as Poisson distributed. A useful way to think of unbound counts, is that they can happen at every moment, but with a very small chance. Think of a longer interaction sequence of a user with a system, where errors are recorded. It can be conceived as an almost infinite number of opportunities to err, with a very small chance of something to happen. The Poisson distribution is a so called limiting case of the binomial distributions, with infinite \\(k\\) and very small \\(p\\). Of course, such a situation is completely ideal. Yet, Poisson distributions fit such situations well enough. Poisson distributions possess only one parameter \\(\\lambda\\) (lambda), that is strictly positive and determines mean and variance of the distribution alike: \\(\\lambda = M = \\textrm{Var}\\). As a matter of fact, there cannot be massively dispersed distributions close to zero, nor narrow ones in the far. Owing to the lower boundary, Poisson distributions are asymmetric, with the left tail always being steeper. Higher \\(\\lambda\\)s push the distribution away from the boundary and the skew diminishes. It is commonly practiced to approximate counts in the high numbers by Gaussian distributions. The linkage between mean and variance is very strict for Poison distributions. Only a certain amount of randomness can be contained at a location. If there is more randomness, and that is almost certainly so, Poisson distributions are not appropriate. One speaks of over-dispersion in such a case. 4.4.2.13 TRANSITION Consider a very simple video game, subway smurfer, where the player jumps and runs a little blue avatar on the roof of a train and catches items passing by. Many items have been placed into the game, but catching a single one is very difficult. The developers are aware that a too low success rate would demotivate players as much as when the game is made to easy. In this experiment, only one player is recorded, and in wonderful ways this player never suffers from fatigue, or is getting better with training. The player plays a 100 times and records the catches after every run. In this idealized situation, the distribution of catches would, approximately, follow a Poisson distribution, as in the figure below. Consider a variation of the experiment with 100 players doing one game and less restrictive rules. Players come differently equipped to perform visual search tasks and coordinate actions at high speeds. They are tested at different times of the day and by chance feel a bit groggy or energized. The chance of catching varies between players, which violates the assumption that was borrowed from the Binomial, a constant chance \\(p\\). The extra variation is seen in the wider of the two distributions. Poisson distributions’ lower boundary can cause trouble: the measure at hand is truly required to include the lower bound. A person can perform a sequence with no errors, catch zero items or have no friends on social media. But, you cannot complete an interaction sequence in zero steps or have a conversation with less than two statements. Fortunately, once a count measure has a lower boundary right of zero, the offset is often available, such as the minimum necessary steps to complete a task. In such a case, the number of erroneous steps can be derived and used as a measure, instead: \\[ \\textrm{\\#errors} = \\textrm{\\#steps} - \\textrm{\\#neccessary steps} \\] 4.4.2.14 ZAP Another lower bound problem arises, when there first is a hurdle before . In traffic research, the frequency of use public transport certainly is an interesting variable. A straight-forward assessment would be to ask bus passengers “How many times have you taken the bus the last five days?”. This clearly is a count measure, but it cannot be zero, because the person is sitting in the bus right now. This could be solved by a more inclusive form of inquiry, such as approaching random households. But, the problem is deeper: actually, the whole population is of two classes, those who use public transport and those who don’t. 4.4.2.15 Exponential distribution [TBC] Exponential distributions apply for measures of duration. Exponential distributions have the same generating process as Poisson distributions, except, that the duration for an event to happen is the variable of interest, rather than number of events in a given time. The same idealized conditions of a completely unaffected subway smurfer player and constant catchability of items, the duration between any two catches is exponentially distributed. In more general, the chance for the event to happen is the same at any moment, completely independent of how long one has been waiting for it. For this property, the exponential distribution is called memoryless. Durations are common measures in design research, most importantly, time-on-task and reaction time. Unfortunately, the exponential distribution is a poor approximation of the random pattern found in duration measures. That is for two reasons: first, the exponential distribution shares with Poisson, that it does not allow variance between participants. Second, the distribution always starts at zero, whereas human reactions always include some basic processing, and be this just the velocity of signals passing nerve cells, which is far below speed of sound (in air). 4.4.2.16 FINISH 4.4.2.17 Gamma distribution [TBD] 4.4.2.18 WRITE 4.4.2.19 Gaussian distributions 4.4.2.20 EDIT The best known distributions are normal distributions or Gaussian distributions. These distributions arise mathematically under the assumption of a myriad of small unrelated forces (SMURF) pushing performance (or any other outcome) up or down. As SMURFs work in all directions independently, their effects often average out and the majority of observations stays clumped together in the center, more or less. Normal distributions have two parameters: \\(\\mu\\) marks the center and mean of the distribution. The linear models introduced later are aiming at predicting \\(\\mu\\). The second parameter \\(\\sigma\\) represents the dispersion of the random pattern. When randomness is pronounced, the center of the distribution gets less mass assigned, as the tails get wider. Different to Poisson and Binomial distributions, mean and variance of the distribution can be set independently and over-dispersion is never an issue. Normal distributions have the compelling interpretation of summarizing the effect of SMURFs. They serve to capture randomness in a broad class of regression models and other statistical approaches. The problem with normal distributions is that they only capture the pattern of randomness under two assumption. The first assumption is that the outcome is continuous. While that holds for duration as a measure of performance, it would not hold for counting the errors a user makes. The second assumption is that the SMURFs are truly additive, like the forces add up, when two pool balls collide. This appears subtle at first, but it has the far reaching consequence that the outcome variable must have an infinite range in both directions, which is impossible. The normal distribution is called “normal”, because people normally use it. Of course not. It got its name for a deeper reason, commonly known (and held with awe) as the central limit theorem. Basically, this theorem proves what we have passingly observed at binomial and Poisson distributions: the more they move to the right, the more symmetric they get. The central limit theorem proves that, in the long run, a wide range of distributions are indistinguishable from the normal distribution. In practice, infinity is relative. In some cases, it is reasonable to trade in some fidelity for convenience and good approximations make effective statisticians. As a general rule, the normal distribution approximates other distributions well, when the majority of measures stay far from the natural boundaries. That is the case in experiments with very many attempts and moderate chances(e.g. signal detection experiments), when counts are in the high numbers (number of clicks in a complex task) or with long durations. However, these rules are no guarantee and careful model criticism is essential. Measurement is prime and specialized (non-central-limit) distributions remain the first recommendation for capturing measurement errors. The true salvation of normal distributions is their application in multi-level models. While last century statistics was reigned by questions of location, and variance considered nuisance, new statistics care for variation. Most notably, amount of variation in a population is added as a central idea in multi-level modeling, which is commonly referred to as random effects. These models can become highly complex and convenience is needed more than ever. Normal distributions tie things together in multi-level models, as they keep location and dispersion apart, tidy. The dilemma is then solved with the introduction of generalized linear models, which is a framework for using linear models with appropriate error distributions. Fortunately, MLM and GLM work seamlessly together. With MLM we can conveniently build graceful likelihood models, using normal distributions for populations. The GLM part is a thin layer to get the measurement scale right and choose the right error distribution, just like a looking glass. 4.5 Bayesian estimation 4.5.0.1 COMPLETE ME Frequentist statistics falls short on recognizing that research is incremental. Bayesian statistics embraces the idea of gradual increase in certainty when new data arrives. Why has Bayesian statistics not been broadly adopted earlier? The reason is that Bayesian estimation was computationally unfeasible for many decades. In practice, the seemingly innocent multiplication of prior and likelihood results in a complex integral, which in most cases has no analytic solution. If you have enjoyed a classic statistics education, you may remember how the computation of sum of squares (explained and residual) can be done by paper and pencil in reasonable time (e.g. during an exam). And that is precisely how statistical computations have been performed before the advent of electronic computing machinery. In the frequentist statistical framework, ingenious mathematicians have developed procedures that were rather efficient to compute. That made statistical data analysis possible in those times. It came at costs, though: procedures make more or less strong assumptions, limiting their applicability. procedures are asymptotically accurate with inference being accurate at large sample sizes only common researchers do not understand crucial elements, for example how the F distribution is derived Expensive computation is in the past. Modern computers can simulate realistic worlds in real time and the complex integrals in Bayesian statistics they solve hands down. When analytic solutions do not exist, the integrals can still be solved using numerical procedures. Numerical procedures have been used in frequentist statistics, too, for example the iterative least squares algorithm applies for Generalized Linear Models, Newton-Rapson optimizer can be used to find the maximum likelihood estimate and boot-strapping produces accurate confidence limits. However, these procedures are too limited as they fail for highly multidimensional problems as they are common in advanced regression models. Most Bayesian estimation engines these days ground on a numerical procedure called Markov-Chain Monte-Carlo (MCMC) sampling. This method differs from the earlier mentioned in that it basically is a random walk. The closest frequentist counterpart to MCMC is the bootstrapping algorithm, which draws many samples from data and computes the estimates many times. In some way, MCMC turns this upside down, by randomly drawing possible parameter values and computing the posterior probability many times. Similar to boot strapping, the basic MCMC algorithm is so simple, it can be explained on half a page and implemented with 25 lines of code. Despite its simplicity, the MCMC algorithm is applicable to practically all statistical problems one can imagine. Being so simple and generic at the same time must come at some costs. The downside of MCMC sampling still is computing time. Models with little data and few variables, like the examples in [#rational], are estimated within a few minutes. Multi-level models, which we will encounter later in this book, can take hours and large psychometric models can take up to a few days of processing time. The particular merit of the MCMC algorithm is that it not only delivers accurate point estimates in almost any situation, it produces the full posterior probability distribution. This lets us characterize a parameters magnitude and degree of (un-)certainty. Let’s run an analysis on the 20 rainfall observations to see how this happens. What the estimation does, is to calculate the posterior distribution from the observations. The posterior distribution contains the probability (more precisely: the density) for all possible values of the parameter in question. The following density plot represents our belief about the parameter \\(P(rain|cloudy)\\) after we have observed twenty days: From the posterior distribution, we can deduct all kinds of summary statistics, such as: the most likely value for a parameter in question is called the posterior mode and is the same as the maximum likelihood estimate when prior knowledge is absent. the average of parameter values, weighted by their probability is called the posterior mean a defined range to express 95% (or any other level of) certainty is the 95% credibility interval We can also make non-standard evaluations on the posterior distribution, for example: How certain is it that \\(P(rain|cloudy) &lt; 0.7\\)? We’ll demonstrate the use of this in the next section. Coming back to MCMC: how is this distribution actually produced. In plain words, MCMC makes a random walk through parameter space. Regions where the true value is more likely to be are just visited more often. The posterior distribution plots above are really just marginal frequencies. Note how the gray connecting lines show the jumps in the MCMC random walk. "],
["linear-models.html", "5 Linear models 5.1 Quantification at work: grand mean models 5.2 Walk the line: linear regression 5.3 Factorial Models 5.4 Putting it all together: multi predictor models 5.5 Conditional effects models 5.6 Doing the rollercoaster: polynomial regression models", " 5 Linear models Linear models answer the question of how one quantitative outcome, say ToT, decreases or increases, when a condition changes. First, I will introduce the most basic LM. The grand mean model (GMM does not have a single predictors and produces just a single estimate: the grand mean in the population. That can be useful, when there exists an exernal standard to which the design must adhere to. In R, the GMM has a formaula, like this: ToT ~ 1. At the example of the GMM, I describe some basic concepts of Bayesian linear models. On the practical side of things, CLU tables are introduced, which is one major work horse to report our results. The most obvious application of LM comes next: in linear regression models (LRM), a metric predictor (e.g. age) is linked to the outcome by a linear function, such as: \\(f(x) = \\beta_0 + \\beta_1x\\). In R, this is: ToT ~ 1 + age. Underneath is a very practical section that explains how simple transformations can make the results of an estimation more clear. Correlations work in similar situations as LRMs, and it is good to know what the differences and how correlations are linked to linear slope parameters. I end this section with a warning: the assumption of linearity is limited to a straight line and this is violated not by some, but all possible data. A very common type of research question is how an outcome changes under different conditions. In design research this is always the case, when designs are compared. In R, a factorial model looks just like an LRM: ToT ~ 1 + Design. The rest of the section is dedicated to the techniques that make it possible to put a qualitative variable into a linear equation. Understanding these techniques opens the door to making your own variants that exactly fit your purpose, such as when a factor is ordered and you think of it of as a stairway, rather than treatments. Finally, we will see how factorial models can resolve problems of linearity, like they appear with learning curves. Once linear regression and factorial models are united, nothing can stop us to combine them into multi-preditor models (MPM). I will introduce all possible combinations: multi-linear, multi-factorial and mixed. However, this section is more intended as a smooth transition models with conditional effects, as these are often more realistic (and interesting). Conditional models go one step further in that it allows predictors to change there efefct, depending on the other predictors. First I will introduce the practical issues of interpreting conditional effects models. Then we come to some explanations: We will see that conditional effects can adjust for saturation effects, which can severely compromise the linearity assumption. Then we will move on to conditional models which are theoretically more interesting. And if it weren’t enough about non-linearity in a chapter on linear models. Turns out you can estimate wobbly lines, too, by using polynomials. The linear model gives you endless possibilities to recombine any number of predictors. But, how good are these models, actually? The last part of this chapter introduces basic techniques of model criticism and model comparison. In model criticism, a single estimated model is under scrunity. The structural part of the model we examine by looking at model fit, where fitted responses are compared to measures. With residual analysis, the pattern of randomness is checked. Model selection deals with comparing models. We will learn about a surprsingly simple technique to compare a set of models by their forecasting accuracy. And finally, I will introduce another p-value. It is not what you think, but it is something you want. 5.1 Quantification at work: grand mean models Reconsider Jane [#design_research]. She was faced with the problem that potential competitors could challenge the claim “rent a car in 99 seconds” and in consequence drag them to court. More precisely, the question was: “will users on average be able …”, which is nothing but the population mean. A statistical model estimating just that, we call a grand mean model (GMM). The GMM is the most simple of all models, so in a way, we can also think of it as the “grandmother of all models”. Although its is the simplest of all, it is of useful application in design research. For many high risk situations, there often exist minimum standards for performance to which one can compare the population mean, here are a few examples: with medical infusion pump the frequency of decimal input error (giving the tenfold or the tenth of the prescribed dose) must be below a bearable level the checkout process of an e-commerce website must have a a cancel rate not higher than … the timing of a traffic light must be designed to give drivers enough time to hit the brakes. A GMM predicts the average expected level of performance in the population (\\(\\beta_0\\)). Let’s atsrt with a toy example: When you want to predict the IQ score of a totally random and anonymous individual (from this population), the population average (which is standardized to be 100) is your best guess. However, this best guess is imperfect due to the individual differences. and chances are rather low that 100 is the perfect guess. # random IQ sample, rounded to whole numbers set.seed(42) N &lt;- 1000 D_IQ &lt;- tibble(score = rnorm(N, mean = 100, sd = 15), IQ = round(score, 0)) # proportion of correct guesses pi_100 &lt;- sum(D_IQ$IQ == 100)/N str_c(&quot;Proportion of correct guesses (IQ = 100): &quot;, pi_100) ## [1] &quot;Proportion of correct guesses (IQ = 100): 0.031&quot; This best guess is imperfect, for a variety reasons: People differ a lot in intelligence. The IQ measure itself is uncertain. A person could have had a bad day, when doing the test, whereas another person just had more experience with being tested. If test items are sampled from a larger set, tests may still differ a tiny bit. The person is like the Slum Dog Millionaire, who by pure coincidence encountered precisely those questions, he could answer. In a later chapters we will investigate on the sources of randomness [REF MLM]. But, like all other models in this chapter, the GMM is a single level linear model. This single level is the population-level and all unexplained effects that make variation are collected in \\(\\epsilon_i\\), the residuals or errors, which are assumed to follow a Gaussian distribution with center zero and standard error \\(\\sigma_\\epsilon\\). Formally, a GMM is written as follows, where \\(\\mu_i\\) is the predicted value of person \\(i\\) and \\(\\beta_0\\) is the population mean \\(\\beta_0\\), which is referred to as Intercept (see [#CGM]). \\[ \\mu_i = \\beta_0\\\\ y_i = \\mu_i, + \\epsilon_i\\\\ \\epsilon_i \\sim \\textrm{Gaus}(0, \\sigma_\\epsilon) \\] This way of writing a linear model only works for Gaussian linear models, as only here, the residuals are symmetric and are adding up to Zero. In chapter [#GLM], we will introduce linear models with different error distributions. For that reason, I will use a slightly different notation throughout: \\[ \\mu_i = \\beta_0\\\\ y_i \\sim \\textrm{Gaus}(\\mu_i, \\sigma_\\epsilon) \\] The notable difference between the two notations is that in the first we have just one error distribution. In the second model, every observation actually is taken from its own distribution, located at \\(\\mu_i\\), albeit with a constant variance. Enough about mathematic formulas for now. In R regression models are specified by a dedicated formula language, which I will develop step-by-step in chapter. This formula language is not very complex, at the same time provides a surprisingly high flexibility for specification of models. The only really odd feature of this formula language is that it represents the intercept \\(\\beta_0\\) with 1. To add to the confusion, the intercept means something different, depending on what type of model is estimated. In GMMs, it is the grand mean, whereas in group-mean comparisons, it is the mean of one reference group [REF CGM] and in linear regression, it is has the usual meaning as in linear equations. Below we estimate a GMM on (simulated) IQ scores using the stan_glm regression engine. The clu() command extracts all the parameters of a models and reports them with 95% certainty limits: M_IQ &lt;- stan_glm(IQ ~ 1, data = D_IQ) bayr::clu(M_IQ) parameter type fixef center lower upper Intercept fixef Intercept 99.6 98.7 100.5 sigma_resid disp NA 15.1 14.4 15.8 So, when estimating the grand mean model, we estimate the intercept \\(\\beta_0\\) and the standard error \\(\\sigma\\). In R, the analysis of the 99 seconds problem [REF Design Research] unfolds as follows: completion times (ToT) are stored in a data frame, with one observation per row. This data frame is send to the R command stan_glm for estimation, using data = D_1. The formula of the grand mean model is ToT ~ 1. Left of the ~ (tilde) operator is the outcome variable. In design research, this often is a performance measure, such as time-on-task, number-of-errors or self-reported cognitive workload. The right hand side specifies the deterministic part, containing all variables that are used to predict performance. attach(Sec99) M_1 &lt;- stan_glm(ToT ~ 1, data = D_1) summary(M_1) ## ## Model Info: ## function: stan_glm ## family: gaussian [identity] ## formula: ToT ~ 1 ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 100 ## predictors: 1 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 105.9 3.0 102.0 106.0 109.8 ## sigma 31.4 2.1 28.8 31.3 34.2 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 105.9 4.3 100.3 105.9 111.2 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.1 1.0 2575 ## sigma 0.0 1.0 3070 ## mean_PPD 0.1 1.0 3445 ## log-posterior 0.0 1.0 1840 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). Most of the time a researcher does not want to deal with the posterior directly, but desires a brief summary of where the effects lie and what the level of certainty is. Tables of estimates, like the one shown below, serve exactly this purpose. Estimates tables report the central tendency of every estimate, which is the best guess for the true magnitude of an effect. Next to that, the spread of the posterior distribution is summarized as 95% credibility intervals and represent the degree of uncertainty: the less certain an estimate is, the wider is the interval. A 95% credibility interval gives a range of possible values where you can be 95% certain that it contains the true value. A complete center-lower-upper table of estimates is produced by the clu command (bayr package): clu(M_1) parameter type fixef center lower upper Intercept fixef Intercept 106.0 100.1 112 sigma_resid disp NA 31.3 27.6 36 The clu command being used in this book is from the accompanying R package bayr and produces tables of estimates, showing all parameters in a model, that covers the effects, i.e. coefficients the dispersion or shape of the error distribution, here the standard error. Often, the distribution parameters are of lesser interest and clu comes with sibling commands to only show the (population-level) coefficients: coef(M_1) parameter type fixef center lower upper Intercept fixef Intercept 106 100 112 Note that regression engines, such as rstanarm, bring their own commands to extract estimates, especially fixef, but these often report the center estimates, only. rstanarm:::coef.stanreg(M_1) ## (Intercept) ## 106 In order to always use the convenient commands from package bayr, it is necessary to load Bayr after package Rstanarm. library(rstanarm) library(bayr) Then, Bayr overwrites the fixef (coef and ranef) commands to produce coefficient tables. coef(M_1) parameter type fixef center lower upper Intercept fixef Intercept 106 100 112 detach(Sec99) A GMM is the simplest linear model and as such makes absolute minimal use of knowledge when doing its predictions. The only thing one knows is that test persons come from one and the same population (humans, users, psychology students). Accordingly, predictions are very inaccurate. From the GMM we will depart in two directions. First, in the remainder of this chapter, we will add further predictors to the model, for example age of participants or a experimental conditions. These models will improve our predictive accuracy by using additional knowledge about participants and conditions of testing. In the following chapter on mixed-effects models [REF MLM], the error term is partitioned into its sources. Reporting a model estimate together with its level of certainty is what makes a statistic inferential (rather than merely descriptive). In Bayesian statistics, the posterior distribution is estimated (usually by means of MCMC sampling) and this distribution carries the full information on certainty. If the posterior is widely spread, an estimate is rather uncertain. You may still bet on values close to the center estimate, but you should keep your bid low. Some authors (or regression engines) express the level of certainty by means of the standard error. However, the standard deviation is a single value and has the disadvantage that a single value does not represent non-symmetric distributions well. A better way is to express certainty as limits, a lower and an upper. The most simple method resembles that of the median by using quantiles. It is common practice to explain and interpret coefficient tables for the audience. My suggestion of how to report regression results is to simply walk through the table row-by-row and for every parameter make three statements: What the parameter says a quantitative statement based on the central tendency an uncertainty statement based on the CIs In the present case Sec99 that would be: The intercept (or \\(\\beta_0\\)) is the population average and is in the region of 106 seconds, which is pretty far from the target of 99 seconds. The certainty is pretty good. At least we can say that the chance of the true mean being 99 seconds or smaller is pretty marginal, as it is not even contained in the 95% CI. And for \\(\\sigma\\): The population mean is rather not representative for the observations as the standard error is almost one third of it. There is much deviation from the population mean in the measures. From here on, we will build up a whole family of models that go beyond the population mean, but have effects. A linear regression model can tell us what effect metric predictors, like age or experience have on user performance. [#LRM] Factorial models we can use for experimental conditions, or when comparing designs. 5.1.1 Likelihood and random term In formal language, regression models are usually specified by likelihood functions and one or more random terms (exactly one in linear models). The likelihood represents the common, predictable pattern in the data. Formally, the likelihood establishes a link between predicted values \\(\\mu_i\\) and predictors. It is common to call predictors with the Greek letter \\(\\beta\\) (beta). If there is more than one predictor, these are marked with subscripts, starting at zero. The “best guess” is called the expected value and is denoted with \\(\\mu_i\\) (“mju i”). If you just know that the average ToT is 106 seconds and you are asked to guess the performance of the next user arriving in the lab, the reasonable guess is just that, 106 seconds. \\[\\mu_i = \\beta_0\\] Of course, we would never expect this person to use 106 second, exactly. All observed and imagined observations are more or less clumped around the expected value. The random term specifies our assumptions on the pattern of randomness. It is given as distributions (note the plural), denoted by the \\(\\sim\\) (tilde) operator, which reads as: “is distributed”. In the case of linear models, the assumed distribution is always the Normal or Gaussian distribution. Gaussian distributions have a characteristic bell curve and depend on two parameters: the mean \\(\\mu\\) as the central measure and the standard deviation \\(\\sigma\\) giving the spread. \\[y_i \\sim \\textrm{Gaus}(\\mu_i, \\sigma_{\\epsilon})\\] The random term specifies how all unknown sources of variation take effect on the measures, and these are manifold. Randomness can arise due to all kinds of individual differences, situational conditions, and, last but not least, measurement errors. The Gaussian distribution sometimes is a good approximation for randomness and linear models are routinely used in research. In several classic statistics books, the following formula is used to describe the GMM (and likewise more complex linear models): \\[ y_i = \\mu_i + \\epsilon_i\\\\ \\mu_i = \\beta_0\\\\ \\epsilon_i \\sim \\textrm{Norm}(0, \\sigma_\\epsilon) \\] First, it is to say, that these two formulas are mathematically equivalent. The primary difference to our formula is that the residuals \\(\\epsilon_i\\), are given separately. The pattern of residuals is then specified as a single Gaussian distribution. Residual distributions are a highly useful concept in modelling, as they can be used to check a given model. Then the the classic formula is more intuitive. The reason for separating the model into likelihood and random term is that it works in more cases. When turning to Generalized Linear Models (GLM) in chapter ??, we will use other patterns of randomness, that are no longer additive, like in \\(\\mu_i + \\epsilon_i\\). As I consider the use of GLMs an element of professional statistical practice, I use the general formula throughout. 5.1.2 Working with the posterior distribution Coefficient tables are the standard way to report regression models. They contain all effects (or a selection of interest) in rows. For every parameter, the central tendency (center, magnitude, location) is given, and a statement of uncertainty, by convention 95% credibility intervals (CI). attach(Sec99) The object M_1 is the model object created by stan_glm. When you call summary you get complex listings that represent different aspects of the estimated model. These aspects and more are saved inside the object in a hierarchy of lists. The central result of the estimation is the posterior distribution (HPD). With package Rstanarm, the posterior distribution is extracted as follows: P_1_wide &lt;- as_tibble(M_1) %&gt;% rename(Intercept = `(Intercept)`) str(P_1_wide) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 4000 obs. of 2 variables: ## $ Intercept: num 113 110 109 105 106 ... ## $ sigma : num 29.3 28.6 29 30 31.4 ... The resulting data frame is a matrix, where each of the 4000 rows is one coordinate the MCMC walk has visited in a two-dimensional parameter space [REF MCMC]. For the purpose of reporting parameter estimates, we could create a coefficient table like follows: P_1_wide %&gt;% summarize(center_Intercept = median(Intercept), center_sigma = median(sigma), lower_Intercept = quantile(Intercept, .025), lower_sigma = quantile(sigma, .025), upper_Intercept = quantile(Intercept, .975), upper_sigma = quantile(sigma, .975)) center_Intercept center_sigma lower_Intercept lower_sigma upper_Intercept upper_sigma 106 31.3 100 27.6 112 36 As can be seen, creating coefficient tables from wide posterior objects is awful and repetitive, even when there are just two parameters (some models contain hundreds of parameters). Additional effort would be needed to get a well structured table. The package Bayr can extract posterior distributions, too, but produces a long format. This works approximately like can be seen in the following code, which employs tidyr::gather to make the wide Rstanarm posterior long. P_1_long &lt;- P_1_wide %&gt;% tidyr::gather(key = parameter) P_1_long %&gt;% sample_n(10) %&gt;% arrange(parameter) parameter value Intercept 105.7 Intercept 100.1 Intercept 103.3 sigma 30.0 sigma 31.0 sigma 34.5 sigma 30.6 sigma 30.6 sigma 32.5 sigma 30.1 With long posterior objects, summarizing over the parameters is efficient and straight-forward, in other words: it is tidy. P_1_long %&gt;% group_by(parameter) %&gt;% summarize(center = median(value), lower = quantile(value, .025), upper = quantile(value, .975)) parameter center lower upper Intercept 106.0 100.1 112 sigma 31.3 27.6 36 With the Bayr package, the posterior command produces such a long posterior object: P_1 &lt;- bayr::posterior(M_1) P_1 ** tbl_post: 4000 samples in 1 chains model parameter type fixef entities M_1 Intercept fixef Intercept 1 When called, the posterior object identifies itself by telling the number of MCMC samples, and the estimates contained in the model, grouped by type of parameter. In the case here, there is just one coefficient, the intercept and one dispersion parameter, the standard deviation of residuals The following gives a glance on the real structure of the long posterior object. Most essential are the identification of the iteration (and chain), the parameter name and the value. However, there is a lot more information stored along side, much of which we will only use in later chapters. P_1 %&gt;% as_tibble() %&gt;% filter(!as.logical(iter %% 500)) ## &lt;-- modulo division selects every 500th iteration model chain iter order parameter type nonlin fixef re_factor re_entity value M_1 NA 500 2 sigma_resid disp NA NA NA NA 33.1 M_1 NA 500 1 Intercept fixef NA Intercept NA NA 106.7 M_1 NA 1000 2 sigma_resid disp NA NA NA NA 28.9 M_1 NA 1000 1 Intercept fixef NA Intercept NA NA 108.7 M_1 NA 1500 2 sigma_resid disp NA NA NA NA 29.8 M_1 NA 1500 1 Intercept fixef NA Intercept NA NA 109.1 M_1 NA 2000 2 sigma_resid disp NA NA NA NA 30.2 M_1 NA 2000 1 Intercept fixef NA Intercept NA NA 101.4 M_1 NA 2500 2 sigma_resid disp NA NA NA NA 32.0 M_1 NA 2500 1 Intercept fixef NA Intercept NA NA 109.8 M_1 NA 3000 2 sigma_resid disp NA NA NA NA 30.3 M_1 NA 3000 1 Intercept fixef NA Intercept NA NA 110.5 M_1 NA 3500 2 sigma_resid disp NA NA NA NA 32.4 M_1 NA 3500 1 Intercept fixef NA Intercept NA NA 103.5 M_1 NA 4000 2 sigma_resid disp NA NA NA NA 28.7 M_1 NA 4000 1 Intercept fixef NA Intercept NA NA 106.6 Note how the two parameters Intercept and sigma are assigned different parameter types: Effects and Dispersion. That is a generally useful classification made by the command bayr::posterior. It allows us to filter by type of parameter and produce CLUs: P_1 %&gt;% filter(type == &quot;fixef&quot;) %&gt;% clu() model parameter type fixef center lower upper M_1 Intercept fixef Intercept 106 100 112 That is almost precisely how the bayr::fixef command is implemented. Note that coef and fixef can be called on the rstanarm model object, directly, which produces the long posterior in the background. coef(M_1) parameter type fixef center lower upper Intercept fixef Intercept 106 100 112 5.1.3 Center and interval estimates The authors of Bayesian books and the various regression engines have different opinions on what to use as center statistic and credibility limits in a coefficient table. The best known option are: the mean, the median and the mode. T_1 &lt;- P_1 %&gt;% group_by(parameter) %&gt;% summarize(mean = mean(value), median = median(value), mode = mascutils::mode(value), lower = quantile(value, .025), upper = quantile(value, .975)) T_1 parameter mean median mode lower upper Intercept 105.9 106.0 105.7 100.1 112 sigma_resid 31.4 31.3 31.1 27.6 36 We observe that for the Intercept it barely matters which center statistic we use, but there are differences for the standard error. We investigate this further by producing a plot with the marginal posterior distributions of \\(\\mu\\) and \\(\\sigma\\) with mean, median and mode. T_1_long &lt;- T_1 %&gt;% gather(key = center, value = value, -parameter) T_1_long parameter center value Intercept mean 105.9 sigma_resid mean 31.4 Intercept median 106.0 sigma_resid median 31.3 Intercept mode 105.7 sigma_resid mode 31.1 Intercept lower 100.1 sigma_resid lower 27.6 Intercept upper 111.8 sigma_resid upper 36.0 G_1 &lt;- P_1 %&gt;% ggplot(aes(x = value)) + facet_wrap(~parameter, scales = &quot;free_x&quot;) + geom_density(fill = 1) + geom_vline(aes(xintercept = value, col = center), data = T_1_long) G_1 This example demonstrates how the long format posterior works together with the GGplot graphics engine. A density plot very accurately renders how certainty is distributed over the range of a parameter. In order to produce vertical lines for point estimate and limits, we first make the summary table long, with one value per row. This is not how we would usually like to read it, but it is very efficient for adding to the plot. When inspecting the two distributions, it appears that the distribution of Intercept is completely symmetric. For the standard error, in contrast, we note a slight left skewness. This is rather typical for dispersion parameters, as these have a lower boundary. The closer the distribution sits to the boundary, the steeper becomes the left tail. A disadvantage of the mean is that it may change under monotonic transformations. A monotonic transformations is a recoding of a variable \\(x_1\\) into a new variable \\(x_2\\) by a transformation function \\(\\phi\\) (\\(phi\\)) such that the order of values stays untouched. Examples of monotonic functions are the logarithm (\\(x_2 = \\log(x_1)\\)), the exponential function (\\(x_2 = \\exp(x_1)\\)), or simply \\(x_2 = x_1 + 1\\). A counter example is the quadratic function \\(x_2 = x_1^2\\). In data analysis monotonous transformations are used a lot. Especially Generalized Linear Models make use of monotonous link functions to establish linearity ??. Furthermore, the mean can also be highly influenced by outliers. The mode of a distribution is its point of highest density. It is invariant under monotonic transformations. It also has a rather intuitive meaning as the most likely value for the true parameter. Next to that, the mode is compatible with classic maximum likelihood estimation. When a Bayesian takes a pass on any prior information, the posterior mode should precisely match the results of a classic regression engine (e.g. glm). The main disadvantage of the mode is that it has to be estimated by one of several heuristic algorithms. These add some computing time and may fail when the posterior distribution is bi-modal. However, when that happens, you probably have a more deeply rooted problem, than just deciding on a suitable summary statistic. The median of a distribution marks the point where half the values are below and the other half are equal or above. Technically, the median is just the 50% quantile of the distribution. The median is extremely easy and reliable to compute, and it shares the invariance of monotonous transformations. This is easy to conceive: The median is computed by ordering all values in a row and then picking the value that is exactly in the middle. Obviously, this value only changes if the order changes, i.e. a non-monotonous function was applied. For these advantages, I prefer using the median as center estimates. Researchers who desire a different center estimate can easily write their own clu. In this book, 2.5% and 97.5% certainty quantiles are routinely used to form 95% credibility intervals (CI). There is nothing special about these intervals, they are just conventions, Again, another method exists to obtain CIs. Some authors prefer to report the highest posterior interval (HPD), which is the narrowest interval that contains 95% of the probability mass. While this is intriguing to some extent, HPDs are not invariant to monotonic transformations, either. So, the parameter extraction commands used here give the median and the 2.5% and 97.5% limits. The three parameters have in common that they are quantiles, which are handled by Rs quantile command. To demystify the clu, here is how you can make a basic coefficient table yourself: P_1 %&gt;% group_by(parameter) %&gt;% summarize(center = quantile(value, 0.5), lower = quantile(value, 0.025), upper = quantile(value, 0.975)) %&gt;% kable() parameter center lower upper Intercept 106.0 100.1 112 sigma_resid 31.3 27.6 36 Note that we get CIs for the dispersion parameter \\(\\sigma\\), too. Many classic analyses call \\(\\sigma\\) are nuisance parameter and ignore it, or they blame high variation between observations for not reaching “statistical significance” for the parameter of interest. Furthermore, classic regression engines don’t yield any measures of certainty on dispersion parameters. I believe that understanding the amount of variation is often crucial for design research and several of the examples that follow try to build this case. This is why we should be glad that Bayesian engines report uncertainty on all parameters involved. 5.1.4 Do the random walk: Markov Chain Monte Carlo sampling So far, we have seen how linear models are specified and how parameters are interpreted from standard coefficient tables. While it is convenient to have a standard procedure it may be useful to understand how these estimates came into being. In Bayesian estimation, an approximation of the posterior distribution (PD) is the result of running the engine and is the central point of departure for creating output, such as coefficient tables. PD assigns a degree of certainty for every possible combination of parameter values. In the current case, you can ask the PD, where and how certain the population mean and the residual standard error are, but you can also ask: How certain are we that the population mean is smaller than 99 seconds and \\(\\sigma\\) is smaller than 10? In a perfect world, we would know the analytic formula of the posterior and derive statements from it. In most non-trivial models, though, there is no such formula one can work with. Instead, what the regression engine does is to approximate the PD by a random-walk algorithm called Markov-Chain Monte Carlo sampling (MCMC). The stan_glm command returns a large object that stores, among others, the full random walk. This random walk represents the posterior distribution almost directly. The following code extracts the posterior distribution from the regression object and prints it. When calling the new object (class: tbl_post) directly, it provides a compact summary of all variables in the model, in this case the intercept and the residual standard error. P_1 &lt;- posterior(M_1) P_1 ** tbl_post: 4000 samples in 1 chains model parameter type fixef entities M_1 Intercept fixef Intercept 1 The 99 second GMM has two parameters and therefore the posterior distribution has three dimensions: the parameter dimensions \\(\\beta_0\\), \\(\\sigma\\) and the probability density. Three dimensional plots are difficult to put on a surface, but for somewhat regular patterns, a density plot with contour lines does a sufficient job: P_1 %&gt;% select(chain, iter, parameter, value) %&gt;% spread(parameter, value) %&gt;% ggplot(aes(x = Intercept, y = sigma_resid, fill = ..level..)) + stat_density_2d(geom = &quot;polygon&quot;) + xlim(95, 115) + ylim(25, 40) + scale_fill_continuous(name=&quot;relative frequency&quot;) Let’s see how this PD “landscape” actually emerged from the random walk. In the current case, the parameter space is two-dimensional, as we have \\(\\mu\\) and \\(\\sigma\\). The MCMC procedure starts at a deliberate point in parameter space. At every iteration, the MCMC algorithm attempts a probabilistic jump to another location in parameter space and stores the coordinates. This jump is called probabilistic for two reasons: first, the new coordinates are selected by a random number generator and second, it is either carried out, or not, and that is probabilistic, too. If the new target is in a highly likely region, it is carried out with a higher chance. This sounds circular, but it provenly works. More specifically, the MCMC sampling approach rests on a general proof, that the emerging frequency distribution converges towards the true posterior distribution. That is called ergodicity and it means we can take the relative frequencies of jumps into a certain area of parameter space as an approximation for our degree of belief that the true parameter value is within this region. The regression object stores the MCMC results as a long series of positions in parameter space. For any range of interest, it is the relative frequency of visits that represents its certainty. The first 50 jumps of the MCMC random walk are shown in @ref(99_seconds_random_walk)`. Apparently, the random walk is not fully random, as the point cloud is more dense in the center area. This is where the more probable parameter values lie. One can clearly see how the MCMC algorithm jumps to more likely areas more frequently. These areas become more dense and, finally, the cloud of visits will approach the contour density plot above. G_random_walk &lt;- P_1 %&gt;% filter(iter &lt;= 50) %&gt;% select(iter, parameter, value) %&gt;% spread(parameter, value) %&gt;% ggplot(aes(x = Intercept, y = sigma_resid, label = iter)) + geom_text() + geom_path(alpha = .3) + ylab(&quot;residual sd&quot;) + xlab(&quot;intercept mu&quot;) + xlim(95, 115) + ylim(25, 40) G_random_walk The more complex regression models grow, the more dimensions the PD gets. The linear regression model in the next chapter has three parameter dimensions, which is difficult to visualize. Multi-level models [#MLM] have hundreds of parameters, which is impossible to intellectually grasp at once. Therefore, it is common to use the marginal posterior distributions (MPD), which give the density of one coefficient at time. My preferred geometry for plotting many MPDs is the violin plot, which packs a bunch of densities and therefore can be used when models of many more dimensions. P_1 %&gt;% ggplot(aes(x = parameter, y = value)) + geom_violin() + ylim(0, NA) In our example, in @ref(99_seconds_post) we can spot that the most likely value for average time-on-task is \\(105.97\\). Both distributions have a certain spread. With a wider PD, far-off values have been visited by the MCMC chain more frequently. The probability mass is more evenly distributed and there is less certainty for the parameter to fall in the central region. In the current case, a risk averse decision maker would maybe take the credibility interval as “reasonably certain”. Andrew and Jane expect some scepticism from the marketing people, and some lack in statistical skills, too. What would be the most comprehensible single number to report? As critical decisions are involved, it seems plausible to report the risk to err: how certain are they that the true value is more than 99 seconds. We inspect the histograms. The MPD of the intercept indicates that the average time-on-task is rather unlikely in the range of 99 seconds or better. But what is the precise probability to err for the 99 seconds statement? The above summary with coef() does not accurately answer the question. The CI gives lower and upper limits for a range of 95% certainty in total. What is needed is the certainty of \\(\\mu \\geq 99\\). Specific questions deserve precise answers. And once we have understood the MCMC chain as a frequency distribution, the answer is easy: we simply count how many visited values are larger than 99. In R, the quantile function handles the job: T_certainty &lt;- P_1 %&gt;% filter(parameter == &quot;Intercept&quot;) %&gt;% summarize(certainty_99s = mean(value &gt;= 99), certainty_111s = mean(value &gt;= 111)) kable(T_certainty) certainty_99s certainty_111s 0.991 0.046 It turns out that the certainty for average time-on-task above the 99 is an overwhelming 0.991. The alternative claim, that average completion time is better than 111 seconds, has a rather moderate risk to err (0.046). detach(Sec99) 5.2 Walk the line: linear regression In the previous section we have introduced the most basic of all regression models: the grand mean model. It assigns rather coarse predictions, without any real predictors. Routinely, design researchers desire to predict performance based on metric variables, such as: previous experience age font size intelligence level and other innate abilities level of self efficiacy, neuroticism or other traits number of social media contacts To carry out such a research question, the variable of interest needs to be measured next to the outcome variable. And, the variable must vary. You cannot examine the effects of age or font size on reading performance, when all participants are of same age and you test only one size. Then, for specifying the model, the researcher has to come up with an expectation of how the two are related. Theoretically, that can be any mathematical function, but practically, a linear function is often presumed. The following plot shows a variety of linear relations between two variables \\(x\\) and \\(y\\). mascutils::expand_grid(intercept = c(0, 1, 2), slope = c(-.5, 0, 1.5), x = -3:3) %&gt;% arrange(x) %&gt;% mutate(y = intercept + x * slope, slope = as.factor(slope)) %&gt;% ggplot(aes(x = x, y = y, color = slope)) + geom_line() + facet_grid(~intercept) A linear function is a straight line, which is specified by two parameters: intercept \\(\\beta_0\\) and slope \\(\\beta_1\\): \\[f(x_1) = \\beta_0 + \\beta_1x_{1i}\\] The intercept is “the point where a function graph crosses the x-axis”, or more formally: \\[f(x_1 = 0) = \\beta_0\\] The second parameter, \\(\\beta_1\\) is called the slope. The slope determines the steepness of the line. When the slope is \\(.5\\), the line will rise up by .5 on Y, when moving one step to the right on X. \\[f(x_1 + 1) = \\beta_0 + \\beta_1x_{1i} + \\beta_1\\] There is also the possibility that the slope is zero. In such a case, the predictor has no effect and can be left out. Setting \\(\\beta_1 = 0\\) produces a horizontal line, with \\(y_i\\) being constant over the whole range. This shows that the GMM is a special case of LRMs, where the slope is fixed to zero, hence \\(\\mu_i = \\beta_0\\). Linear regression gives us the opportunity to discover how ToT can be predicted by age (\\(x_1\\)) in the BrowsingAB case. In this hypothetical experiment, two designs A and B are compared, but we ignore this for now. Instead we ask: are older people slower when using the internet? Or: is there a linear relationship between age and ToT? The structural and random terms of the LRM are: \\[\\mu_i = \\beta_0 + \\beta_1x_{1i}\\] \\[y_i \\sim \\textrm{Gaus}(\\mu_i, \\sigma)\\] This literally means: with every year of age, ToT increases by \\(\\beta_1\\) seconds. Before we run a linear regression with stan_glm, we visually explore the association between age and ToT using a scatter plot. The blue line in the graph is a so called a smoother, more specifically a LOESS. A smoother is an estimated line, just as linear function. But, it is way more flexible. Where the linear function is a straight stick fixed at a pivotal point, LOESS is more like a pipe cleaner. here, LOESS shows a more detailed picture of the relation between age and ToT. There is a rise between 20 and 40, followed by a stable plateau, and another rise starting at 60. Actually, that does not look like a straight line, but at least there is steady upwards trend. attach(BrowsingAB) BAB1 %&gt;% ggplot(aes(x = age, y = ToT)) + geom_point()+ geom_smooth(se = F, fullrange = F) In fact, the BrowsingAB simulation contains what one could call a psychological model. The effect of age is partly due to farsightedness of participants (making them slower at reading), which more or less suddenly kicks in at a certain range of age. Still, we make do with a rough linear approximation. To estimate the model, we use the stan_glm command in much the same way as before, but add the predictor age. The command will internally check the data type of your variable, which is metric in this case. Therefore, it is treated as a metric predictor (sometimes also called covariate) . M_age &lt;- BAB1 %&gt;% stan_glm(ToT ~ 1 + age, data = .) T_age &lt;- coef(M_age) T_age parameter fixef center lower upper Intercept Intercept 164.217 144.057 184.13 age age 0.637 0.258 1.03 Is age associated with ToT? The coefficient table tells us that with every year of age, users get \\(0.64\\) seconds slower, which is considerable. It also tells us that the predicted performance at age = 0 is \\(164.22\\). 5.2.1 Transforming measures In the above model, the intercept represents the predicted ToT at age == 0, of a newborn. We would never seriously put that forward in a stakeholder presentation, trying to prove that babies benefit from the redesign of a public website, would we? The prediction is bizarre because we intuitively understand that there is a discontinuity up the road, which is the moment where a teenager starts using public websites. We also realize that over the whole life span of a typical web user, say 12 years to 90 years, age actually is a proxy variable for two distinct processes: the rapid build-up of intellectual skills from childhood to young adulthood and the slow decline of cognitive performance, which starts approximately, when the first of us get age-related far-sightedness. Generally, with linear models, one should avoid making statements about a range that has not been observed. Linearity, as we will see in [REF: debunking], always is an approximation for a process that truly is non-linear. Placing the intercept where there is no data has another consequence: the estimate is rather uncertain, with a wide 95% CI, \\(164.22 [144.06, 184.13]_{CI95}\\). As a metaphor, think of the data as a hand that holds the a stick, the regression line and tries to push a light switch. The longer the stick, the more difficult is becomes to hit the target. 5.2.1.1 Shifting an centering Shifting the predictor is a pragmatic solution to the problem: “Shifting” means that the age predictor is moved to the right or the left, such that point zero is in a region populated with observations. In this case, two options seem to make sense: either, the intercept is in the region of youngest participants, or it is the sample average, which is then called centering. To shift a variable, just subtract the amount of units (years) where you want the intercept to be. The following code produces a shift of -20 and a centering on the original variable age: BAB1 &lt;- BAB1 %&gt;% mutate(age_shft = age - 20, age_cntr = age - mean(age)) BAB1 %&gt;% tidyr::gather(&quot;predictor&quot;, &quot;age&quot;, starts_with(&quot;age&quot;)) %&gt;% ggplot(aes(x = age, y = ToT)) + facet_grid(predictor~.) + geom_point() + geom_smooth(se = F, method = &quot;lm&quot;, fullrange = T) By shifting the age variable, the whole data cloud is moved to the left. To see what happens on the inferential level, we repeat the LRM estimation with the two shifted variables: M_age_shft &lt;- stan_glm(ToT ~ 1 + age_shft, data = BAB1) M_age_cntr &lt;- stan_glm(ToT ~ 1 + age_cntr, data = BAB1) We combine the posterior distributions into one multi-model posterior and read the multi-model coefficient table: P_age &lt;- bind_rows(posterior(M_age), posterior(M_age_shft), posterior(M_age_cntr)) T_age &lt;- coef(P_age) T_age model parameter fixef center lower upper M_age Intercept Intercept 164.217 144.057 184.13 M_age age age 0.637 0.258 1.03 M_age_cntr Intercept Intercept 195.825 189.651 201.91 M_age_cntr age_cntr age_cntr 0.649 0.246 1.05 M_age_shft Intercept Intercept 176.883 163.803 190.13 M_age_shft age_shft age_shft 0.641 0.250 1.02 detach(BrowsingAB) ## [1] &quot;BAB1&quot; When comparing the regression results the shifted intercepts have moved to higher values, as expected. Surprisingly, the simple shift is not exactly 20 years. This is due to the high uncertainty of the first model, as well as the relation not being exactly linear (see Figure XY). The shifted age predictor has a slightly better uncertainty, but not by much. This is, because the region around the lowest age is only scarcely populated with data. Centering, on the other hand, results in a highly certain estimate, due to the dence data. The slope parameter, however, practically does not change, neither in magnitude nor in certainty. Shift (and centering) move the scale of measurement and make sure that the intercept falls close (or within) the cluster of observations. Shifting does not change the unit size, which is still years. For most metric predictors that would also not be desireable, as the unit of measurement is natural and intuitive. 5.2.1.2 Rescaling Most rating scales are not natural units of measure. Most of the time it is not meaningful to say: “the user experience rating improved by one”. The problem has two roots, as I will illustrate by the following four rating scale items: This product is … difficult to use |1 ... X ... 3 ... 4 ... 5 ... 6 ... 7| easy to use from hell |-----X----------------------| (10cm) heavenly neutral |1 ... X ... 3 ... 4| uncanny If you would employ these three scales to assess one and the same product, the data could look like this: set.seed(42) Raw_ratings &lt;- tibble(Part = 1:100, difficult_easy = mascutils::rrating_scale(100, 0, .5, ends = c(1,7)), heavenly_hell = mascutils::rrating_scale(100, 0, .2, ends = c(0,10), bin = F), neutral_uncanny = mascutils::rrating_scale(100, -.5, .5, ends = c(1,5))) head(Raw_ratings) Part difficult_easy heavenly_hell neutral_uncanny 1 5 5.60 1 2 4 5.52 3 3 4 4.50 3 4 5 5.91 4 5 4 4.67 2 6 4 5.05 2 In the following, we are comparing the results of these three items. However, they came in the wide format, as you would use to create a correlation table. For a tidy analysis, we first make the data set long. Ratings are now classified by the item they came from. We can produce a grid histogram. D_ratings &lt;- Raw_ratings %&gt;% gather(key = Item, value = rating, -Part) %&gt;% mascutils:::as_tbl_obs() D_ratings D_ratings %&gt;% ggplot(aes(x = rating)) + facet_grid(Item ~ .) + geom_histogram() + xlim(0, 10) The first problem is that rating scales have been designed with different end points. The first step when using different rating scales is shifting the left-end point to zero and dividing by the range of the measure (upper - lower boundary). That brings all items down to the range between zero and one. Note how the following tidy code joins in a table that holds the properties of our items. D_Items &lt;- tribble(~Item, ~lower, ~upper, &quot;difficult_easy&quot;, 1, 7, &quot;heavenly_hell&quot;, 0, 10, &quot;neutral_uncanny&quot;, 1, 5) D_ratings &lt;- D_ratings %&gt;% left_join(D_Items, by = &quot;Item&quot;) %&gt;% mutate(scaled = (rating - lower)/(upper - lower)) D_ratings %&gt;% ggplot(aes(x = scaled)) + facet_grid(Item ~ .) + geom_histogram(bins = 100) + xlim(0,1) This partly corrects the horizontal shift between scales. However, the ratings on the third item still are shifted relative to the other two. The reason is that the first two items have the neutral zone right in the center, whereas the third item is neutraul at its left-end point. The second inconsistency is that the second item uses rather extreme anchors (end point labels), which produces a tight accumulation in the center of the range (with a lot of polite people in the sample, at least). The three scales have been rescaled by their nominal range, but they differ in their observed variance. z-transformation rescales a measure by its observed variance. A set of measures is z-transformed by centering it and scaling it by its own standard deviation. D_ratings %&gt;% group_by(Item) %&gt;% mutate(zrating = (rating - mean(rating))/sd(rating)) %&gt;% # mascutils::z_score(rating) %&gt;% ggplot(aes(x = rating)) + facet_grid(Item ~ .) + geom_histogram(bins = 100) By z-transformation, the three scales now exhibit the same mean location and the same dispersion. This could be used to combine them into one general score. Note however, that information is lost by this process, namely the differences in location or dispersion. If the research question is highly detailed, such as “Is the design consistently rated low on uncanniness?”, this can no longer be answered from the z-transformed variable. Finally, sometimes researchers use logarithmic transformation of outcome measures to reduce what they perceive as pathologies of tha data. In particular, many outcome variables do not follow a Normal distribution, as the random term of linear models assumes, but are left-skewed. Log-transformation often mitigates such problems. However, as we will see in chapter [REF GLM], linear models can be estimated gracefully with a random component that precisely matches the data as it comes. The following time-on-task data is from the IPump study, where nurses have tested two infusion pump interfaces: attach(IPump) D_pumps %&gt;% mutate(logToT = log(ToT)) %&gt;% select(Design, ToT, logToT) %&gt;% gather(key = Measure, value = value, -Design) %&gt;% ggplot(aes(x = value, color = Design)) + facet_wrap(Measure~., scale = &quot;free&quot;) + geom_density() detach(IPump) Frequently, it is count measures and temporal measures to exhibit non-symmetric error distributions. By log transformation one often arrives at a reasonably Gaussian distributed error. However, the natural unit of te measure (seconds) gets lost by the transformation, making it very difficult to report the results in a quantitative manner. 5.2.2 Correlations LRM render the quantitative relationship between two metric variables. Another commonly known statistic that seems to do something similar is Pearson’s correlation statistic \\(r\\) (@(#associations)). In the following, we will see that a tight connection between correlation and linear coefficients exists, albeit both having their own advantages. For a demonstration, we reproduce the steps on a simulated data set where X and Y are linearly linked: D_cor &lt;- tibble(x = runif(100, 0, 50), y = rnorm(100, x *.2, 3)) D_cor %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) Recall, that \\(r\\) is covariance standardized for dispersion, not unsimilar to z-transformation [REF transformations] and that a covariance is the mean squared deviance from the population mean. This is how the correlation is decontaminated from the idiosyncracies of the involved measures, their location and dispersion. Similarly, the slope parameter in a LRM is a measure of association, too. It is agnostic of the overall location of measures since this is captured by the intercept. However, dispersion remains intact. This ensures that the slope and the intercept together retain information about location, dispersion and association of data, and we can ultimately make predictions. Still, there is a tight relationship between Pearson’s \\(r\\) and a slope coefficient \\(\\beta_1\\), namely: \\[ r = \\beta_1 \\frac{sd_X}{sd_Y} \\] For the sole purpose of demonstration, we here resort to the built-in non-Bayesian command lm for doing the regression. M_cor &lt;- lm(y ~ x, D_cor) beta_1 &lt;- stats::coef(M_cor)[2] r &lt;- beta_1 * sd(D_cor$x) / sd(D_cor$y) r ## x ## 0.66 The clue with Pearson’s \\(r\\) is that it normalized the slope coefficient by the variation found in the sample. This resembles z-transformation as was introduced in ??. In fact, when both, predictor and outcome, are z-transformed before estimation, the coefficient equals Pearson’s \\(r\\) exactly: M_z &lt;- D_cor %&gt;% mutate(x_z = (x - mean(x))/sd(x), y_z = (y - mean(y))/sd(y)) %&gt;% lm(y_z ~ x_z, .) stats::coef(M_z)[2] ## x_z ## 0.66 In regression modelling the use of coefficients allows for predictions made in the original units of measurement. Correlations, in contrast, are unit-less. Still, correlation coefficients play an imortant role in exploratory data analysis (but also in multilevel models, see @(re_correlations) for the following reasons: Correlations between predictors and responses are a quick and dirty assessment of the expected associations. Correlations between multiple response modalities (e.g., ToT and number of errors) indicate to what extent these responses can be considered exchangeable. Correlations between predictors should be checked upfront to avoid problems arising from so-called collinearity [REF colinearity]. The following table shows the correlations between measures in the MMN study, where we tested the association between verbal and spatial working memory capacity (Ospan and Corsi tests and performance in a search task on a website (clicks and time). attach(MMN) MMN_2 %&gt;% select(Corsi, Ospan.A, Ospan.B, time, clicks) %&gt;% corrr::correlate() rowname Corsi Ospan.A Ospan.B time clicks Corsi NA 0.177 0.116 0.068 0.137 Ospan.A 0.177 NA 0.876 0.053 0.111 Ospan.B 0.116 0.876 NA 0.060 0.119 time 0.068 0.053 0.060 NA 0.838 clicks 0.137 0.111 0.119 0.838 NA detach(MMN) These correlations give an approximate picture of associations in the data: Working memory capacity is barely related to performance. There is a strong correlations between the performance measures. There is a strong correlation between the two predictors Ospan.A and Ospan.B. Linear coefficients and correlations both represent associations between measures. Coefficients preserve units of measuremnent, allowing us to make meaningful quantitative statements. Correlations are re-scaled by the observed dispersion of measures in the sample, making them unit-less. The advantage is that larger sets of associations can be screened at once and compared easily. 5.2.3 Endlessly linear On a deeper level the bizarre age = 0 prediction is an example of a principle , that will re-occur several times throughout this book. In this universe everything is finite. A well understood fact about LRM is that they allow us to fit a straight line to data. A lesser regarded consequence from the mathematical underpinnings of such models is that this line extends infinitely in both directions. To fulfill this assumption, the outcome variable needs to have an infinite range, too, \\(y_i \\in [-\\infty; \\infty]\\) (unless the slope is zero). Every scientifically trained person and many lay people know, that even elementary magnitudes in physics are finite: all speeds are limited to \\(\\approx 300.000 km/s\\), the speed of light, and temperature has a lower limit of \\(-276°C\\) (or \\(0°K\\)). If there can neither be endless acceleration nor cold, it would be daring to assume any psychological effect to be infinite in both directions. The endlessly linear assumption (ELA) is a central piece of all LRMs. From a formal perspective, the ELA is always violated in a universe like ours. So, should we never ever use a linear model and move on to non-linear models right away? Pragmatically, the LRM often is a reasonably effective approximation. From figure [#G_eda_1] we have seen that the increase of time-on-task by age is not strictly linear, but follows a more complex curved pattern. This pattern might be of interest to someone studying the psychological causes of the decline in performance. For the applied design researcher it probably suffices to summarize the monotonous relationship by one slope coefficient. In 5.4.1 we will estimate the age effects for designs A and B separately, which lets us compare fairness towards older people. As has been said, theorists may desire a more detailed picture and see disruptions of linearity as indicators for interesting psychological processes. An uncanny example of theoretical work will be given when I introduce polynomial regression [#PRM]. For the rest of us, linear regression is a pragmatic choice, as long as: the pattern is monotonically increasing any predictions stay in the observed range and avoid the boundary regions, or beyond. 5.3 Factorial Models In the previous section we have seen how linear models are fitting the association between a metric predictor X and an outcome variable Y to a straight line with a slope and a point of intercept. Such a model creates a prediction of Y, given you know the value of measure X. However, in many research situations, the predictor variable carries not a measure, but a group label. Factor variables assign observations to one of a set of predefined groups, such as the following variables do in the BrowsingAB case: attach(BrowsingAB) BAB5 %&gt;% select(Part, Task, Design, Gender, Education, Far_sighted) %&gt;% sample_n(8) detach(BrowsingAB) Two of the variables, Gender and Education clearly carry a group membership of the participants. That is a natural way to think of people groups, such as male of female, or which school type they went to. But, models are inert to anthropocentrism and can divide everything into groups. Most generally, it is always the observations, i.e. the rows in a (tidy) data table, which are divided into groups. Half of the observations habe been made with design A, the rest with B. The data also identifies the participant and the task for every observation. Although do we see numbers on participants, these are factors, not metric variables. If one had used initials of participants, that would not make the slightest difference of what this variable tells. It also does not matter, whether the researcher has actually created the levels, for example by assigning participants to one of two deisgn conditions, or has just observed it, such as demographic variables. Factorial models are frequently used in experiments, where the effect a certain condition on performance is measured. In design research, that is the case when comparing two (or more) designs and the basic model for that, the comparison of group means model, will be introduced, first [#CGM], with more details on the inner workings and variations in the two sections that follow [#dummy_variables #treatment contrasts]. A CGM requires that one can think of one of the groups as some kind of default to which all the other conditions are compared to. That is not always given. When groups are truly equal among sisters, the absolute means model (AMM) [#AMM] does just estimates the absolute group mans, being like multi-facetted GMMs. Factors are not metric, but sometimes they have a natural ordered, for example levels of education, or position in a sequence. In section [#OFM] we will apply an ordered factorial model to a learning sequence. 5.3.1 A versus B: Comparison of groups The most common linear models on factors is the comparison of groups (CGM), which replaces the commonly known analysis of variance (ANOVA). In design research group comparisons are all over the place, for example: comparing designs: as we have seen in the A/B testing scenario comparing groups of people, based on e.g. gender or whether they have a high school degree comparing situations, like whether an app was used on the go or standing still In order to perform a CGM, a variable is needed that establishes the groups. This is commonly called a factor. A factor is a variable that identifies members of groups, like “A” and “B” or “male” and “female”. The groups are called factor levels. In the BrowsingAB case, the most interesting factor is Design with its levels A and B. Asking for differences between two (or more) designs is routine in design research. For example, it could occur during an overhaul of a municipal website. With the emerge of e-government, many municipal websites have grown wildly over a decade. What once was a lean (but not pretty) 1990 website has grown into a jungle over time, to the disadvantage for users. The BrowsingAB case could represent the prototype of a novel web design, which is developed and tested via A/B testing at 200 users. Every user is given the same task, but sees only one of the two designs. The design team is interested in: Do the two web designs A and B differ in user performance? Again, we first take a look at the raw data: attach(BrowsingAB) BAB1 %&gt;% ggplot(aes(x = ToT)) + geom_histogram() + facet_grid(Design~.) This doesn’t look too striking. We might consider a slight advantage for design B, but the overlap is immense. We perform the CGM. Again, this is a two-step procedure: The stan_glm command lets you specify a simple formula to express the dependency between one or more predictors (education) and an outcome variable (ToT). It performs the parameter estimation using the method of Markov-Chain Monte-Carlo Sampling. The results are stored in a new object M_CGM. With the fixef command the estimates are extracted and can be interpreted. M_CGM &lt;- BAB1 %&gt;% stan_glm(ToT ~ 1 + Design, data = .) T_Design &lt;- coef(M_CGM) T_Design parameter fixef center lower upper Intercept Intercept 203 194.6 212.19 DesignB DesignB -15 -27.6 -2.61 The model contains two parameters, one Intercept and one slope. Wait a second? How can you have a slope and a “crossing point zero”, when there is no line, but just two groups? This will be explained further in [dummy_variables] and [treatment_contrasts]. Fact is, in the model at hand, the Intercept is the mean of a reference group. Per default, stan_glm chooses the alphabetically first group label as the reference group, in this case design A. We can therefore say that design A has an average performance of \\(203.36 [194.57, 212.19]_{CI95}\\). The second parameter is the effect of “moving to design B”. It is given as the difference to the reference group. With design B it took users \\(14.98 [27.58, 2.61]_{CI95}\\) seconds less to complete the task. However, this effect appears rather small and there is huge uncertainty about it. It barely justifies the effort to replace design A with B. If the BrowsingAB data set has some exciting stories to tell, the design difference is not it. A frequent user problem with CGMs is that the regression engine selects the alphabetically first level as the reference level, which often is not correct. Supposed, the two designs had been called Old (A) and New (B), then regression engine would pick New as the reference group. Or think of non-discriminating language in statistical reports. In BrowsingAB, gender is coded as f/m and female participants conquer the Intercept. But, sometimes my students code gender as v/m or w/m. Oh, my dear! The best solution is, indeed, to think upfront and try to find level names that make sense. If that is not possible, then the factor variable, which is often of type character must be made a factor, which is a data type in its own right in R. When the regression engine sees a factor variable, it takes the first factor level as reference group. That would be nice, but when a factor is created using the as.factor, it again takes an alphabethical order of levels. This is over-run by giving a vector of levels in the desired order. The tidy Foracts package provides further commands to set the order of a factor levels. Gender &lt;- sample(c(&quot;v&quot;,&quot;m&quot;), 4, replace = T) factor(Gender) ## [1] m m v v ## Levels: m v factor(Gender, c(&quot;v&quot;, &quot;m&quot;)) ## [1] m m v v ## Levels: v m detach(BrowsingAB) 5.3.2 Not stupid: dummy variables Are we missing anything so far? Indeed, I avoided to show any mathematics on factorial models. The CGM really is a linear model, although it may not appear so, at first. So, how can a variable enter a linear model equation, that is not a number? Linear model terms are a sum of products \\(\\beta_ix_i\\), but factors cannot just enter such a term. What would be the result of \\(\\mathrm{DesignB} \\times\\beta_1\\)? Factors basically answer the question: What group does the observation belong to?. This is a label, not a number, and cannot enter the regression formula. Dummy variables solve the dilemma by converting factor levels to numbers. This is done by giving every level \\(l\\) of factor \\(K\\) its own dummy variable K_l. Now every dummy represents the simple question: Does this observation belong to group DesignB?. The answer is coded as \\(0\\) for “Yes” and \\(1\\) for “No”. attach(BrowsingAB) BAB1 &lt;- BAB1 %&gt;% mutate(Design_A = if_else(Design == &quot;A&quot;, 1, 0), Design_B = if_else(Design == &quot;B&quot;, 1, 0)) BAB1 %&gt;% select(Obs, Design, Design_A, Design_B, ToT) %&gt;% sample_n(8) %&gt;% kable() Obs Design Design_A Design_B ToT 32 A 1 0 196 90 A 1 0 193 97 A 1 0 183 188 B 0 1 146 178 B 0 1 235 184 B 0 1 286 151 B 0 1 152 18 A 1 0 260 The new dummy variables are numerical and can very well enter a linear formula, every one getting its own coefficient. For a factor K with levels A, B and C the linear formula can include the dummy variables \\(K_{Ai}\\) and \\(K_{Bi}\\): \\[ \\mu_i = K_{Ai} \\beta_{A} + K_{Bi} \\beta_{B} \\] The zero/one coding acts like a switches. When \\(K_{Ai}=1\\), the parameter \\(\\beta_A\\) is switched on and enters the sum, and \\(\\beta_B\\) is switched off. An observation of group A gets the predicted value: \\(\\mu_i = \\beta_A\\), vice versa for members of group B. M_dummy_1 &lt;- stan_glm(ToT ~ 0 + Design_A + Design_B, data = BAB1) coef(M_dummy_1) parameter fixef center lower upper Design_A Design_A 203 194 212 Design_B Design_B 188 180 197 In its predictions, the model M_dummy should be equivalent to the CGM model M_CGM, but the coefficients mean something different: they are exactly the group means. This model we call an absolute means model (AMM) and will discuss it in section [AMM]. First, we have to come back to the question, how the regression engine produces its dummy variables, such that the coefficients are differences towards one Intercept. This is called treatment contrasts [#TC]. 5.3.3 Treatment contrast The default behaviour of regression engines, when encountering a factor, is to select the first level as reference group and estimate all other levels relative to that. Coefficients express differences. This fully makes sense if the effect of a treatment is what you are after, and is therefore called treatment contrasts. Treatment contrasts do not have anything special or natural to them, but is a very particular way of thinking about levels of a factor, namely that one level is special. In controlled experiments, this special level often is the control condition, whereas the coefficients are the effects of well-defined manipulations. This most prominently is the case for clinical trials, where the placebo group is untreated. This works well in all situations where a default situation exists and the other factor levels can be thought of manipulations of the default: A redesign as an improvement over the current design. A quiet, comfortable environment is the optimal situation for cognitive performance. There is a minimum level of education requird for most jobs We have seen how to create dummy variables ourselves by means of mututally exclusive on-off switches, which results in absolute means coefficients. Regression engines quietly assume that treatment effects is what the user wants and expand dummy variables in a different way: For a factor with levels A and B, the dummy for B is an on-off switch, whereas the reference level A is set always on. This is called treatment contrast coding: BAB1 &lt;- BAB1 %&gt;% mutate(Intercept = 1, Design_B = if_else(Design == &quot;B&quot;, 1, 0)) BAB1 A frequent user problem with treatment coding is that the regression engine selects the alphabetically first level as the reference level. Supposed, the two designs had been called Old (A) and New (B), then regression engine would pick New as the reference group. By the following you can define dummy variables to have Old be the reference. (But recall the more convenvient ways that were outlined earlier [CGM].) BAB1 %&gt;% mutate(Design = if_else(Design == &quot;A&quot;, &quot;Old&quot;, &quot;New&quot;)) %&gt;% mutate(Intercept = 1, Design_B = if_else(Design == &quot;New&quot;, 1, 0)) The following chapters deal with more variations of factorial models. Next, we will take a closer look at the absolute means model [#AMM], which is useful, when a reference group does not come natural. In section [#OFM], we deal with factorial models, where levels are ordered and introduce contrast codings. detach(BrowsingAB) 5.3.4 Absolute Means Model Not all factor variables are experimental and identifying a default can be difficult or unnatural. This often happens when the levels are just a set of conditions that you have found as given, such as the individuals in the human population, or all words in a language. Such is the case in the IPump study, where every session was composed of a set of tasks, such as starting the device or entering a dose. These tasks were taken from existing training material and including them as a factor could help identify areas for improvement. Although the tasks form a sequence, they are equally important for the operation. Not one can be singled out as default. Treatment coding would force us to name one default task for the Intercept. The absolute means model represents all levels by their absolute means. If you put in a factorial predictor with eight levels, you will get eight coefficients, which are the mean outcomes of every level. Of course, what you can no longer do is find differences between levels. We have seen in [#dummy_variables] how to create a AMM dummy variables. IN fact, the linear models formula language this can be done more directly by either of the two option below, (but just leaving out the 1 + does not suffice): 0 + Task Task - 1 In the following, we estimate an AMM using the formula method. In the IPump study we had a sample of nurses do a sequence of eight tasks on a medical infusion pump with a novel interface design. For the further development of such a design it may be interesting to see, which tasks would most benefit from design improvements. A possible way to look at it is by saying that a longer task has more potential to be optimized. Under such a perspective, tasks are an equal set and there is no natural reference task for a CGM. Instead we estimate the absolute group means and visualize the marginal postertior distributions. attach(IPump) M_AMM_1 &lt;- D_Novel %&gt;% stan_glm(ToT ~ 0 + Task, data = .) coef(M_AMM_1) %&gt;% rename(Task = fixef ) %&gt;% ggplot(aes(x = Task, y = center, ymin = lower, ymax = upper)) + geom_point(size = 2) + geom_errorbar() The plot shows the absolute means and we can easily discover that Task 2 is by far the longest and that tasks differ a lot, indeed. None of these relations can easily be seen in the CGM plot. Note that the AMM is not a different model than the treatment effects model. It is just a different parametrization, which makes interpretation easier. Both models produce the exact same predictions. detach(IPump) The choice between CGM and AMM depends on whether a factor represents designed manipulations or whether it is more something that has been collected. Psychological experiments often have fully designed stimuli, because only then can differences between stimuli get an unambiguous causal interpretation. In the Stroop experiment, stimuli show a color word (red) written in a color (Red). Both properties are well-defined and can be manipulated freely by the experimenter. That is what you can call a fully controlled stimulus design. One could argue, that colors and color words have been collected from our perceptual and cultural heredity and are therefore are not really manipulated. This is a valid issue, but it does not matter so much, because the real manipulation is congruency and that is crystal clear: tribble(~Word, ~Color, &quot;red&quot;, &quot;Red&quot;, &quot;blue&quot;, &quot;Blue&quot;, &quot;green&quot;, &quot;Green&quot;) %&gt;% tidyr::complete(Word, Color) %&gt;% mutate(Condition = if_else(Word == str_to_lower(Color), &quot;congruent&quot;, &quot;incongruent&quot;)) Word Color Condition blue Blue congruent blue Green incongruent blue Red incongruent green Blue incongruent green Green congruent green Red incongruent red Blue incongruent red Green incongruent red Red congruent In more common version of the Stroop task, a neutral condition is added, where the task stays the same, but the word is a non-color word. Almost every word in a language is a non-color word. And among all those words, color neutrality is not so clear cut. All objects around us show colors, and color symbolism pervades all areas of our lives. We can easily imagine that “tree” is a less neutral word (green leaves), but to what extent is “hood” (little red riding …) or “sad” (blue) color-neutral. As long as we don’t know this precisely for a word, it is not controlled by manipulation, and we can give no default. That being said, I do not recommend you examine the words by a factor, as we have done for tasks. Instead, multi-level models apply best when factors represent a collection of natural objects [#MLM]. 5.3.5 Ordered Factors Models Factors usually are not metric, which would require them to have units (like years or number of errors) and an order. Age, for example, has the unit of years, which makes statements possible such as: “per year of age, participants slow down by …”. The same cannot be said for levels of education. We could assign these levels the numbers 0, 1 and 2 to express the order, but we cannot assume that going from Low to Middle is the same amount of effective education as going from Middle to High. Factorial models are indifferent towards orders and therefore can simply be used for ordered factors. For level of education, we could just use a CGM or AMM, the only issue being that the graphics and regression engines order factors alphabetically: High, Low, Middle. attach(BrowsingAB) BAB1 %&gt;% ggplot(aes(x = Education, y = ToT)) + geom_boxplot() The following changes the order of levels of GGplot engine is respecting that, and so is the regression engine, putting the intercept on level Low. BAB1$Education &lt;- factor(as.character(BAB1$Education), levels = c(&quot;Low&quot;, &quot;Middle&quot;, &quot;High&quot;)) BAB1 %&gt;% group_by(Education) %&gt;% summarize(mean_ToT = mean(ToT)) %&gt;% ggplot(aes(x = as.integer(Education), y = mean_ToT)) + geom_step() + scale_x_continuous(breaks=1:3) Note that R also knows a separate variable type called ordered factors. This only seemingly is useful. In fact, if we run a linear model with an ordered factor as predictor, the estimated model will be so unintelligible that I will not attempt to explain it here. M_OFM_1 &lt;- BAB1 %&gt;% stan_glm(ToT ~ 1 + Education, data = .) coef(M_OFM_1) parameter fixef center lower upper Intercept Intercept 211.2 199.9 223.21 EducationMiddle EducationMiddle -13.1 -29.8 2.91 EducationHigh EducationHigh -29.0 -44.2 -14.33 detach(BrowsingAB) A basic ordered factor model is just a CGM where the coefficients are shown in the desired order. The second and third coefficient carry the respective difference towards level Low. EducationHigh is not the difference towards EducationMiddle. In this case it makes sense to understand Middle and High as a smaller step or a larger step up from Low. It is not always like that. Sometimes, the only way of moving from the reference group to some other level implies going through all the intermediates, just like walking up a stairway. Then it makes more sense to use a model where coefficients are individual steps. In the IPump study, we looked at the speed of learning of a novel interface design by letting the participants repeat a set of tasks in three successive sessions. here the three sessions make a stairway: going from the fist to the third session always involves the second session. Before we come to that, we first have to see, why Session must be an ordered factor and not a metric predictor. The first idea that could come to mind is to take session as a metric predictor and estimate a LRM – it has an order and it is the same amount of training, which you could call a unit. The thing with learning processes is that they are curved, more precisely, they gradually move towards an asymptote. The following curve shows the effect of a hypothetical training over 12 sessions. What we see is that the steps are getting smaller when training continues. While the amount of training is the same, the effect on performance declines, which is also called a curve of diminishing returns. The asymptote of this curve is the maximum performance the participant can reach, which theoretically is only reached in infinity. The following code defines an exponential learning curve function and renders an example. learning_curve &lt;- function(session, amplitude, rate, asymptote) amplitude * exp(-rate * session) + asymptote tibble(session = as.integer(1:12)) %&gt;% mutate(ToT = learning_curve(session, 10, .3, 2)) %&gt;% ggplot(aes(x = session, y = ToT)) + geom_step() + scale_x_continuous(breaks=1:12) LRMs can only do straight lines, which means constant effects, whereas learning curves have diminishing effects. For short learning sequences, we can use ordered factorial models, where every session becomes a level. As these levels get their own coefficients, the steps no longer have to be constant. When levels are ordered, the two endpoint levels (first session, last session) can serve as a natural reference group for the intercept. However, how useful would it be to express the performance in session 3 as differences to reference level (session 1). It is more natural to think of learning to take place incrementally, like walking up stairways, where the previous step always is your reference. attach(IPump) D_Novel %&gt;% group_by(Session, session) %&gt;% summarize(mean_ToT = mean(ToT)) %&gt;% ggplot(aes(x = as.integer(Session), y = mean_ToT)) + geom_step() + scale_x_continuous(breaks=1:3) This is what a factorial model with stairway dummy coding does. The first coefficient \\(\\beta_0\\) is the starting point, for example the first session, and all other coefficients ( \\(\\beta_1, \\beta_2\\)) are a sequence of step sizes. The expected value \\(\\mu_i\\) for session \\(K\\), using stairways dummies \\(K_0, K_1, K_2\\) is: \\[ \\mu_i = K_{1i} \\beta_{0} + K_{2i} (\\beta_{0} + \\beta_{1}) + K_{2i} (\\beta_{0} + \\beta_{1} + \\beta_{2}) \\] Thinking of these dummy variables as switches once again: Recall that treatment dummies have an always-on reference level and exclusive switches for the other levels [#dummy_variables]. Stairways dummies are like a incremental switches: when switch \\(K\\) is on, this implies all previous switches are on, too. Stairways-down dummies are made as follows: D_Novel &lt;- D_Novel %&gt;% mutate(Session_1 = 1, Step_1 = as.integer(session &gt;= 1), Step_2 = as.integer(session &gt;= 2)) D_Novel %&gt;% distinct(session, Session_1, Step_1, Step_2) %&gt;% arrange(session) %&gt;% as_tibble() session Session_1 Step_1 Step_2 0 1 0 0 1 1 1 0 2 1 1 1 Now we can run a factorial model using these stairway-down dummies, where the intercept is the upper floor and we are loosing height at every step: M_OFM_2 &lt;- stan_glm(ToT ~ Session_1 + Step_1 + Step_2, data = D_Novel) coef(M_OFM_2) parameter fixef center lower upper Intercept Intercept 23.73 21.62 25.89 Step_1 Step_1 -10.39 -13.37 -7.32 Step_2 Step_2 -2.38 -5.47 0.69 The Intercept is the performance in the first level, which is initial performance. The first step is huge, almost reducing ToT by one half. The second step is much smaller than the first and tiny compared to initial performance. We see that high performance can be reached after just a few training sessions. Clearly, the device is easy to learn. Another question that arises is what level of performance is reached in the end. Is maximum performance good enough, actually? Strictly, this would require a non-linear learning curve model, which would contain an estimate for maximum performance. With an OFM, the best guess we have is final performance. Because the second step was already small, we may believe that the asymptote is not so far any more. And we know that final performance is a conservative estimate for maximum performance. With a stairway-up model model, the Intercept is the basement an we walk up step-by-step. D_Novel &lt;- D_Novel %&gt;% mutate(Session_3 = 1, Step_1 = as.integer(session &lt;= 1), Step_2 = as.integer(session &lt;= 0)) D_Novel %&gt;% distinct(session, Session_3, Step_1, Step_2) %&gt;% arrange(desc(session)) %&gt;% as_tibble() session Session_3 Step_1 Step_2 2 1 0 0 1 1 1 0 0 1 1 1 M_OFM_3 &lt;- stan_glm(ToT ~ Session_3 + Step_1 + Step_2, data = D_Novel) coef(M_OFM_3) parameter fixef center lower upper Intercept Intercept 10.96 8.807 13.05 Step_1 Step_1 2.42 -0.563 5.43 Step_2 Step_2 10.30 7.387 13.37 The Intercept is an estimate of final performance and we can ask whether this level of efficiency is actually good enough. In [#reporting_RE] we will see that it is a significant improvement towards the legacy design, hence the name of the level. From a methodological perspective the results of this study indicate that it is worth-while to let participants do multiple session and observe the learning process. In particular, when users do their tasks routinely with a device, like the nurses, initial performance can be a very poor estimate for long-term performance. detach(IPump) To wrap it up: Factorial models use dummy variables to make factor levels numerical. These dummy variables can be understood as arrays of switches that can be arranged in different patterns: Exclusive on-off switches produce an AMM. An AMM is the least specified among factorial models, all levels are equal. Often this is the best choice when the levels were drawn from a population. One always-on Intercept and exclusive on-off switches produces a CGM with treatment effects. When a default level can be identified, such as the placebo condition in clinical trials, treatment contrasts are a good choice. Stairway dummies produce an OFM, taking one end-point as first coefficient and stepping up (or down). This is particularly useful for short learning curves. 5.4 Putting it all together: multi predictor models Design researchers are often collecting data under rather wild conditions. Users of municipal websites, consumer products, enterprise information systems and cars can be extremely diverse. At the same time, Designs vary in many attributes, affecting the user in many different ways. There are many variables in the game, and even more possible relations. With multi predictor models we can examine the simultaneous influence of everything we have recorded. First, we will see, how to use models with two or more continuous predictors. Subsequently, we address the case of multi-factorial designs. Finally, we will see examples of models, where metric predictors and factors make a bunch of regression lines. 5.4.1 On surface: multiple regression models Productivity software, like word processors, presentation and calculation software or graphics programs have evolved over decades. For every new release, dozens of developers have worked hard to make the handling more efficient and the user experience more pleasant. Consider a program for drawing illustrations: basic functionality, such as drawing lines, selecting objects, moving or colourizing them, have practically always been there. A user wanting to draw six rectangles, painting them red and arranging them in a grid pattern, can readily do that using basic functionality. At a certain point of system evolution, it may have been recognized that this is what users repeatedly do: creating a grid of alike objects. With the basic functions this is rather repetitive and a new function was created, called “copy-and-arrange”. Users may now create a single object, specify rows and columns of the grid and give it a run. The new function saves time and leads to better results. Users should be very excited about the new feature, should they not? Not quite, as [Carroll in Rosson] made a very troubling observation: adding functionality for the good of efficiency may turn out ineffective in practice, as users have a strong tendency to stick with their old routines, ignoring new functionality right away. This troubling observation has been called the active user paradox (AUP) [REF]. Do all users behave that way? Or can we find users of certain traits that are different? What type of person would be less likely to fall for the AUP? And how can we measure resistance towards the AUP? We did a study, where we explored the impact of two user traits need-for-cognition (ncs) and geekism (gex) on AUP resistance. To measure AUP resistance we observed users while they were doing drawing tasks. A behavioural coding system was used to derive an individual AUP resistance score. Basically, we counted behaviour associated with exploration and elaboration during the task and produced a single score. So, are users with high need-for-cognition and geekism more resistant to the AUP? We first look at the two predictors, separately: As we will see later, it is preferable to build one model with two simultaneous predictors. For instructive purposes we begin with two separate LRMs, one for each predictor. Throughout the regression models we use z-transformed scores. Neither the personality nor the resistance scores truly have a metric interpretation, so nothing is lost in translation. \\[ \\mu_i = \\beta_0 + \\beta_\\mathrm{ncs} x_\\mathrm{ncs}\\\\ \\mu_i = \\beta_0 + \\beta_\\mathrm{gex} x_\\mathrm{gex} \\] attach(AUP) M_1 &lt;- AUP_1 %&gt;% stan_glm(zresistance ~ zncs, data = .) M_2 &lt;- AUP_1 %&gt;% stan_glm(zresistance ~ zgex, data = .) detach(AUP) @ref(tab:AUP_coef) shows the two separate effects (M_1 and M_2). Due to the z-transformation of predictors, the intercepts are practically zero. Both personality scores seem to have a weakly positive impact on AUP resistance. Next, we estimate a model that regards both predictors simultaneously. For linear models, that requires nothing more than to make a sum of all involved predictor terms (and the intercept). The result is a multiple regression model (MRM): \\[ \\mu_i = \\beta_0 + \\beta_\\mathrm{ncs} x_\\mathrm{ncs} + \\beta_\\mathrm{gex} x_\\mathrm{gex} \\] In R’s regression formula language, this is similarly straight-forward. The + operator directly corresponds with the + in the likelihood formula. attach(AUP) M_3 &lt;- AUP_1 %&gt;% stan_glm(zresistance ~ zncs + zgex, data = .) #&lt;-- detach(AUP) For the comparison of the three models we make use of a feature of the package bayr: the posterior distributions of arbitrary models can be combined into one multi-model posterior object, by just stacking them upon each other. The coefficient table of such a multi-model posterior gains an additional column that identifies the model: attach(AUP) P &lt;- bind_rows(posterior(M_1), posterior(M_2), posterior(M_3)) T_coef_3 &lt;- P %&gt;% posterior() %&gt;% coef() T_coef_3 model parameter fixef center lower upper M_1 Intercept Intercept -0.003 -0.298 0.312 M_1 zncs zncs 0.374 0.059 0.682 M_2 Intercept Intercept 0.002 -0.312 0.315 M_2 zgex zgex 0.294 -0.037 0.620 M_3 Intercept Intercept 0.002 -0.307 0.302 M_3 zncs zncs 0.299 -0.074 0.673 M_3 zgex zgex 0.118 -0.267 0.500 The intercepts of all three models are practically zero, which is a consequence of the z-transformation. Recall, that the intercept in an LRM means that the predictor is zero. In MRM this is just the same: here, the intercept is the predicted AUP resistance score, for when NCS and GEX are both zero. When using the two predictors simultaneously, the overall positive tendency remains. However, we observe major and minor shifts: in the MRM, the strength of the geekism score is reduced to less than half: \\(0.12 [-0.27, 0.5]_{CI95}\\). NCS has shifted, too, but lost only little of its original strength: \\(0.3 [-0.07, 0.67]_{CI95}\\). For any researcher who has carefully conceived a research question this appears to be a disappointing outcome. The reason is that the two predictors are correlated. In this study, participants who are high on NCS also tend to have more pronounced geekism. @ref(AUP_corr_predictors) reveals the situation: G_eda_4 &lt;- AUP_1 %&gt;% ggplot(aes(x = zncs, y = zgex)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + labs(title = str_c(&quot;Correlation between ncs and gex: &quot;, round(cor(AUP_1$zncs, AUP_1$zgex), 2))) G_eda_4 detach(AUP) Participants with a higher NCS also tend to score higher on geekism. Is that surprising? Actually, it is not. People high on NCS love to think. Computers are a good choice for them, because these are complicated devices that make you think. (Many users may even agree that computers help you think, for example when analyzing your data with R.) In turn, geekism is a positive attitude towards working with computers in sophisticated ways, which means such people are more resistant towards the AUP. [NCS: love to think] –&gt; [GEX: love computers] –&gt; [resist AUP] When such a causal chain can be established without doubt, some researchers speak of a mediating variable GEX. Although a bit outdated [REF], mediator analysis is correct when the causal direction of the three variables is known. Then, a so-called step-wise regression is performed to find the pure effects. A better alternative to that is structural equation modelling. Unfortunately, in the situation here, the causal direction is partly ambiguous. We can exclude that the resistance test has influenced the personality scores, because of the order of appearance in the study. But, causally speaking, geekism may well preceed NCS. For example, computers reward you for thinking hard and, hence, you get used to it and make it your lifestyle. If you like thinking hard, then you probably also like the challenge that was given in the experiment. [GEX: love computers] –&gt; [NCS: love to think] –&gt; [resist AUP] In the current case, we can not distinguish between these two competing theories with this data alone. This is a central problem in empirical research. An example, routinely re-iterated in social science methods courses is the observation that people who are more intelligent tend to consume more fresh vegetables. Do carrots make us smart? Perhaps, but it is equally plausible that eating carrots is what smart people do. The basic issue is that a particular direction of causality can only be established, when all reverse directions can be excluded by logic. Behavioural science reseachers know of only two ways to do so: By the arrow of time, it is excluded that a later event caused a preceding one. In the AUP study, there is no doubt that filling out the two personality questionnaires cause the behaviour in the computer task, because of the temporal order. In strictly controlled experiments, participants are assigned to the conditions, randomly. To come back to the AUP study: There is no way to establish a causal order of predictors NCS and Gex. If nothing is known but covariation, they just enter the model simultaneously, as in model M_3. This results in a redistribution of the overall covariance and the predictors are mutually controlled. In M_2 the effect of GEX was promising at first, but now seems spurious in the simultaneous model. Most of the strength was just borrowed from NCS by covariation. The model suggests that loving-to-think has a considerably stronger association with AUP resistance than loving-computers. That may suggest, but not prove, that geekism precedes NCS, as in a chain of causal effects, elements that are closer to the final outcome (AUP resistance) tend to exert more salient influence. But, without further theorizing and experimenting this is weak evidence of causal order. If I would want to write a paper on geekism, NCS and the AUP, I might be tempted to report the two separate LRMs, that showed at least moderate effects. The reason why one should not do that is that separate analyses suggest that the predictors are independent. To illustrate this at an extreme example, think of a study where users were asked to rate their agreement with an interface by the following two questions, before ToT is recorded: Is the interface beautiful? Does the interface have an aesthetic appearance? Initial separate analyses show strong effects for both predictors. Still, it would not make sense to give the report the title: “Beauty and aesthetics predict usability”. Beauty and aesthetics are practically synonyms. For Gex and NCS this may be not so clear, but we cannot exclude the possibility that they are linked to a common factor, perhaps a third trait that makes people more explorative, no matter whether it be thoughts or computers. So, what to do if two predictors correlate strongly? First, we always report just a single model. Per default, this is the model with both predictors simultaneously. The second possibility is to use a disciplined method of model selection and remove the predictor (or predictors) that does not actually contribute to prediction. The third possibility is, that the results with both predictors become more interesting when including conditional effects @ref(interaction_effects) 5.4.2 Crossover: multifactorial models The very common situation in research is that multiple factors are of interest. In [#OFM], we have seen how we can use an OGM to model a short learning sequence. What if I tell you now, that in this study, we have compared two designs against each other, and both were tested in three sessions. That makes 2 x 3 conditions. Here, I introduce a multi-factorial model, that has main effects only. Such a model actually is of very limited use for the IPump case, where we need conditional effects to get to a valid model. We take as an example the BrowsingAB study: the primary research question regarded the design difference, but the careful researcher also recorded gender of participants. One can always just explore variables that one has. The following model estimates the gender effect alongside the design effect attach(BrowsingAB) M_mfm_1 &lt;- BAB1 %&gt;% stan_glm(ToT ~ 1 + Design + Gender, data = .) coef(M_mfm_1) parameter fixef center lower upper Intercept Intercept 203.397 191.8 215.0 DesignB DesignB -15.085 -28.3 -2.6 GenderM GenderM 0.058 -12.5 12.4 By adding gender to the model, both effects are estimated simultaneously. In the following multi-factorial model (MFM) the intercept is a reference group, once again. Consider that both factors have two levels, forming a \\(2 x 2\\) matrix. tribble(~Condition, ~F, ~M, &quot;A&quot;, &quot;reference&quot;,&quot;difference&quot;, &quot;B&quot;, &quot;difference&quot;, &quot;&quot;) Condition F M A reference difference B difference The first one, A-F, has been set as reference group. The intercept coefficient tells that women in condition A have an average ToT of \\(203.4 [191.84, 214.97]_{CI95}\\) seconds. The second coefficient says that design B is slightly faster and that there seemingly is no gender effect. How comes that the model only has three parameters, when there are four groups? In a CGM, the number of parameters always equals the number of levels, why not here? We can think of the 2 x 2 conditions as flat four groups, A-F, A-M, B-F and B-M and we would expect four coefficients, say absolute group means. But with this model I thought of the two effects and how they change the default condition. In this model they do so, but without ever influencing each other. Design is assumed to have the same effect for men and women. In many multi-factorial situations, one is better advised to use a model with conditional effects. Broadly, with conditional effect we can assess, how much effects influence each other. Basically, a model with conditional effects is a re-parametrization of a multifactorial AMM (MAMM), with the following dummy coding: DesignA DesignB GenderF GenderM 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 For your convenience, there also exists a an R formula to estimate an MAMM. This formula supresses the intercept and uses an interaction term without main effects (as will be explained in [IFX]). M_amfm_1 &lt;- stan_glm(ToT ~ 0 + Design:Gender, data = BAB1, iter = 500) coef(M_amfm_1) parameter fixef center lower upper DesignA:GenderF DesignA:GenderF 202 189 215 DesignB:GenderF DesignB:GenderF 188 174 202 DesignA:GenderM DesignA:GenderM 204 192 216 DesignB:GenderM DesignB:GenderM 187 176 199 The coefficient table carries the four group means and be can further processed as a conditional plot. coef(M_amfm_1) %&gt;% separate(parameter, into = c(&quot;Design&quot;, &quot;Gender&quot;)) %&gt;% ggplot(aes(x = Design, col = Gender, y = center)) + geom_point(size = 2) + geom_line(aes(group = Gender)) If the two effects were truly independent, these two lines had to be parallel, because the effect of Gender had to be constant. What this graph now suggests is that there is an interaction between the two effects. There is a tiny advantage for female users with design A, whereas men are faster with B with about the same difference. Because these two effects cancel each other out, the combined effect of Gender in model M_mpm_1 was so close to zero. detach(BrowsingAB) 5.4.3 Line-by-line: grouped regression models Recall, that dummy variables make factors compatible with linear regression. We have seen how two metric preditors make a surface and how factors can be visualized by straight lines in a conditional plot. And that is precisely what happens when a factor is combined with a metric predictor: we get a group of lines, one per factor level. For example, we can estimate the effects age and design simultaneously: attach(BrowsingAB) M_grm_1 &lt;- BAB1 %&gt;% stan_glm(ToT ~ 1 + Design + age_shft, data = .) coef(M_grm_1) parameter fixef center lower upper Intercept Intercept 184.462 169.38 199.74 DesignB DesignB -15.088 -27.67 -2.20 age_shft age_shft 0.634 0.24 1.03 Once again, we get an intercept first. Recall, that in LRM the intercept is the the performance of a 20-year old (age was shifted!). In the GCM it was the mean of the reference group. When marrying factors with continuous predictors, the intercept is point zero in the reference group. The predicted average performance of 20-year old with design A is \\(184.46 [169.38, 199.74]_{CI95}\\). The age effect has the usual meaning: by year of life, participants get \\(0.63 [0.24, 1.03]_{CI95}\\) seconds slower. The factorial effect B is a vertical shift of the intercept. A 20-year old in condition B is \\(15.09 [27.67, 2.2]_{CI95}\\) seconds faster. It is important to that this is a model of parallel lines, implying that the age effect is the same everywhere. The following model estimates intercepts and slopes separately for every level, making it an absolute mixed-predictor model (AMPM). The following formula produces such a model: M_ampm_1 &lt;- BAB1 %&gt;% stan_glm(ToT ~ (0 + Design + Design:age_shft) , data = .) coef(M_ampm_1) parameter fixef center lower upper DesignA DesignA 194.363 176.157 213.059 DesignB DesignB 156.895 137.843 175.350 DesignA:age_shft DesignA:age_shft 0.299 -0.279 0.827 DesignB:age_shft DesignB:age_shft 1.051 0.510 1.596 It turns out, the intercepts and slopes are very different as it can be. The first two coefficients represent the two Design intercepts: for a 20 year old, design B works much better, but at the same time design B puts a much stronger penalty on every year of age. With these coefficients we can also produce a conditional plot, with one line per Design condition. coef(M_ampm_1) %&gt;% select(fixef, center) %&gt;% mutate(Design = str_extract(fixef, &quot;[AB]&quot;), Coef = if_else(str_detect(fixef, &quot;age&quot;), &quot;Slope&quot;, &quot;Intercept&quot;)) %&gt;% select(Design, Coef, center) %&gt;% spread(key = Coef, value = center) %&gt;% print() %&gt;% ggplot() + geom_abline(aes(color = Design, intercept = Intercept, slope = Slope)) + geom_point(data = BAB1, aes(x = age, col = Design, y = ToT)) ## # A tibble: 2 x 3 ## Design Intercept Slope ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 194. 0.299 ## 2 B 157. 1.05 Note how the coefficient table is first made flatter with spread, where Intercept and Slope become variables. that the Abline geometry is specialized on plotting linear graphs, but it requires its own aesthetic mapping (the global will not work). So, if we can already fit a model with separate group means (an AMFM) or a bunch of straight lines (AMPM), why do we need a more elaborate account of conditional effects, as in chapter [IFX]? The answer is, that conditional effects often carry important information, but are notoriously difficult to interpret. As it will turn out, conditional effects sometimes are due to rather trivial effects, such as saturation. But, like in this case, they can give the final clue. It is hard to deny that design features can work differently to different people. The hypothetical situation is BrowsingAB is that design B uses a smaller font-size, which makes it harder to read with elderly users, whereas younger users have a benefit from more compactly written text. And, sometimes, experimental hypotheses are even formulated as conditional effects, like the following: some control tasks involve long episodes of vigilance, where mind wandering can interrupt attention on the task. If this is so, we could expect people who meditate to perform better at a long duration task, but showing no difference at short tasks. In a very simple experiment participants reaction time could be measured in a long and short task condition. detach(BrowsingAB) 5.4.4 Empirical versus statistical control Fundamental researchers have a knack for the experimental method. An experiment, strictly, is a study where you measure the effects of variables you manipulate. Manipulation is, almost literally, that it is in your hands, who receives the treatment. The fantastic thing about manipulation is that it allows for causal conclusions. A strictly controlled experiment is when all influencing variables are either manipulated or kept constant. That is an ideal and would not even be the case if you test the same person over-and-over again (like researchers in psychophysics often do). You never jump into the same river twice. Sometimes an influencing variable lends itself to be kept constant. For example, in cognitive psychological experiments environment and equipment is usually kept constant. For applied research, keeping things constant comes at a major disadvantage: it limits the possible conclusions drawn from the study. Imagine, you tested a smartphone app with participants, all students, comfortably sitting in a quiet environment. Would you dare to make conclusions on how any users perform in real life situations, say while driving a car? When keeping things constant, ecological validity and generalizability suffer. In most applied design studies we need ecological validity and generalizability. If performance differs under certain conditions, you certainly want to know that. The solution is to let conditions vary and record them as variables, as good as possible. For example, if you were to compare two voice-controlled intelligent agent apps, you could manipulate the ambient noise level, if you are in the lab. In practically all applied studies, variables may exist which you cannot manipulate. Especially, user traits are impossible to manipulate; If someone has an extrovert character or did a lot of gaming in the past, you cannot change that. Diversity of users is a fact and people come as they are. Field studies usually aim for high ecological validity. Participants are supposed to use the system in the situations they encounter. If a smartphone app is being used sitting, walking, driving or at a secret place, it is crucial to observe all situations. Consider a car navigation system that is tested in a long, lonely highway situation only. How much would the results tell your for performance in dense city traffic? Design researchers frequently need results that are highly representative for various users and situations of use. Fundamental lab researchers are afraid of individual differences, too. The reasons are different, though: all non-manipulated influencing factors add noise to the study, which makes it harder to find the effects of interest. While lab researchers do there best to keep the environment constant, they cannot keep all participant traits constant. Lab researchers have two solutions to the problem: matching and randomized control. With pair matching, potentially relevant participant traits are recorded upfront; then participants are assigned to conditions such that groups have about the same composition. For example, one makes sure that the age distribution is about the same and both genders are equally represented. When all other influencing variables are constant between groups, the lab researcher can be sure that the effect is unambiguously caused by the manipulation. So they say and routinely record participants age, gender and nationality. However, there are better alternatives: the best pair match is the person herself. Experimental studies that expose the same person to several conditions are called within-subject. In the special case that all participants encounter all conditions, the variable is complete within-subject. [A special case of within-subject design is repeated measures.] In the following chapter, we use mixed-effects models [LMM] to deal with within-subject designs, gracefully. In design research, pair matching applies for situations where designs are compared. In the simple situation that a design is evaluated against a set standard (e.g. 111 seconds to rent a car), it is more important to do population matching. The sample of participants is drawn to be representative for the target population. Representativeness comes in two levels: coverage representation is reached when all influencing properties have occurred a few times during observation. So, if your target population contains several subgroups, such as age groups, experience or people with different goals, they should all be covered to some extent. Proportional representation means all user and situational properties are covered and they have about the same proportion in the sample as in the population. You can only match what you can measure and you only measure what you expect. Human behaviour in everyday life is influenced by many factors in complex ways. Although a plethora of personality inventories exists, doing them all prior to the real study is impossible. It would probably not even be effective. Never have I seen a design research study, where even the most established personality tests explain more than a few percent of variation. As another example, take the primacy effect: what you experienced first, has the strongest influence. In real life, impressions are constantly pouring on people and you will never be able to record and match that to a reasonable extent. When influencing variables cannot be measured for matching or statistical control, the last resort is randomized control. This is a misleading term, insofar as what the researcher actually does is to let go to chance. Indeed, if the process of drawing participants and assigning them to manipulations is completely left to chance, then in the long-term, the sample will be proportional representative and all groups will have the same composition of traits. Randomization works well with larger samples. With small samples, it can still easily happen that one ends up with more or less biased samples or heterogeneous groups. Just by chance, more higher-educated people could have ended up in condition A of BrowsingAB. Using manipulation, matching or randomization in in-the-wild research may work in some cases. In other cases, it will be ineffective or impractical. The ultimate problem is the attempt to keep things constant. In applied design research the questions rarely come down to a “Is A better than B?”. If there is an age effect, you may certainly want to know it and see how the design effect compares to it. But, you can only examine what is varied and recorded. The approach of statistical control is to record (instead of manipulate) all variables that may influence the results and add them to the statistical model. As we have seen in this section now, the linear model puts no limits on the number of predictors. That allows us to use control variables and evaluate multiple research questions in a single model. And this is also how it should be done. In the next section we will take multi-predictor models to a new level. As we have seen, multiple effects can be conditional upon each other and I gave you a straight-forward way to check this with AMFMs and AMPMs. What you could not do with these models is interpret how strong the conditional effect is. In the following section, I will elaborate on what conditional effects can mean and how these can be quantified as differences, using treatment contrasts. 5.5 Conditional effects models With the framework of MPM, we can use an arbitrary number of predictors. These can represent properties on different levels, for example, two design proposals for a website can differ in font size, or participants differ in age. So, with MPM we gain much greater flexibility in handling data from applied design research, which allows us to examine user-design interactions more closely. The catch is that if you would ask an arbitrary design researcher: Do you think that all users are equal? Or, could it be that one design is better for some users, but inferior for others? you would in most cases get the answer: Of course users differ in many ways and it is crucial to know your target group. Some will also refer to the concept of usability by the ISO 9241-11, which contains the famous four words: “… for a specified user …” The definition explicitly requires you to state for for whom you intended to design. It thereby implicitly acknowledges that usability of a design could be very different for another user group. In other words, statements on usability are by the ISO 9241-11 definition conditional on the target user group. In statistical terms, conditional statements have this form: the effect of design depends on who the user is. In regression models, conditional statements like these are represented by conditional effects. Interactions between user properties and designs are central in design research, and deserve a neologism: differential design effects models (DDM). Sometimes, conditional effects are needed for a less interesting reason: saturation occurs when physical (or other) boundaries are reached and the steps are getting smaller, for example, the more you train, the less effect it seems to have. Saturations counter part is amplification, a rare one, because is like compound glue: it will harden only if the two components are present. And finally, we will look at an experimental example, where a conditional effect can be linked to a more generic truth. 5.5.1 Conditional multiple regression In section [#LRM] we have seen how the relationship between predictor and outcome variable can be modelled as a linear term. We analysed the relationship between age and ToT in the (fictional) BrowsingAB case and over both designs combined and observed just a faint decline in performance, which also seemed to take a wavey form. It is commonly held that older people tend to have lower performance than younger users. A number of factors are called responsible, such as: slower processing speed, lower working memory capacity, lower motor speed and visual problems. All these capabilities interact with properties of designs, such as legibility, visual simplicity and how well the interaction design is mapped to a user’s task. It is not such a stretch to assume that designs can differ in how much performance degrades with age. In turn, a design can also comtain compromises that limit the performance of younger users. For example, assume that main difference between design A and B in the BrowsingAB example is that A uses larger letters than B. Would that create the same benefit for everybody? It is not unlikely, that larger letters really only matter for users that have issues with farsightedness, which is associated with age. Maybe, there is even an adverse effect for younger users, as larger font size takes up more space on screen and more scrolling is required. We take a first look at the situation: attach(BrowsingAB) BAB1 %&gt;% ggplot(aes(x = age, col = Design, y = ToT)) + geom_point() + geom_smooth(se = F) + geom_smooth(se = F, aes(col = &quot;combined&quot;)) The graph suggests that designs A and B differ in the effect of age. Design B appears to perform much better with younger users. At the same time, it seems as if A could be nmore favorable for users at a high age. By adding the conditional effect Design:age_shft the following model estimates the linear relationship for the designs separately. This is essentially the same model as the absolute mixed-predictor model M_ampm_1 [#MPM], which also had four coefficients, the intercepts and slopes of two straight lines. We have already seen how the GRM and the AMPM produce different fitted responses. Predictions are independent of contrast coding, but coefficients are not. The following conditional model uses treatment contrasts, like the GRM, and we can compare the coefficients side-by-side. M_cmrm &lt;- BAB1 %&gt;% stan_glm(ToT ~ Design + age_shft + Design:age_shft, data = .) # T_resid &lt;- mutate(T_resid, M_cmrm = residuals(M_cmrm)) P_comb &lt;- bind_rows( posterior(M_grm_1), posterior(M_cmrm) ) coef(P_comb) model parameter fixef center lower upper M_cmrm Intercept Intercept 195.458 176.933 214.332 M_cmrm DesignB DesignB -37.781 -63.045 -12.494 M_cmrm age_shft age_shft 0.250 -0.300 0.831 M_cmrm DesignB:age_shft DesignB:age_shft 0.771 0.008 1.547 M_grm_1 Intercept Intercept 184.462 169.383 199.742 M_grm_1 DesignB DesignB -15.088 -27.671 -2.197 M_grm_1 age_shft age_shft 0.634 0.240 1.034 The conditional model shares the first three coefficients with the unconditional model, but only the first two, Intercept and DesignB have the same meaning. and we regard them first. The intercept is the performance of an average twenty-year-old using design A, but the two models diverge in where to place this and the conditional model is less in favor of design A (\\(195.46 [176.93, 214.33]_{CI95}\\) seconds). Conversely, the effect of design B at age of 20 improved dramatically: accordingly, a twenty-year-old is \\(37.78 [63.04, 12.49]_{CI95}\\) faster with B. The third coefficient Age_shift appears in both models, but really means something different. The GRM assumes that both designs have the same slope of \\(0.63\\) seconds per year. The conditional model produces one slope per design and here the coefficient refers to design A only, as this is the reference group. Due to the treatment effects, DesignB:age_shft is the difference in slopes: users loose \\(0.63\\) seconds per year with A, and on top of that \\(0.25 [-0.3, 0.83]_{CI95}\\) with design B. detach(BrowsingAB) 5.5.2 Conditional multifactorial models In a conditional multifactorial model (CMFM), the treatment effect depends on another factor. When the second factor changes in level, this influences the coefficients. Because of that a CMFM is more flexible. Actually, a full CMFM has as many coefficients as there are multi-level groups and is flexible enough that all group means can be completely independent, just like an AMM does it. Let us see this on an almost trivial example, first. In the fictional BrowsingAB case, a variable rating has been gathered. Let us imagine this is a vague emotional rating in the spirit of user experience. Some claim that emotional experience is what makes the sexes different, so one could ask whether this makes a difference for the comparison two designs A and B. attach(BrowsingAB) BAB1 %&gt;% ggplot(aes(y = rating, x = Gender, color = Design)) + geom_boxplot() In a first exploratory plot it looks like the ratings are pretty consistent across gender, but with a sensitive topic like that, we better run a model, or rather two, a plain MFM and a conditional MFM: M_mfm_2 &lt;- BAB1 %&gt;% stan_glm(rating ~ Design + Gender, data = .) M_cmfm_1 &lt;- BAB1 %&gt;% stan_glm(rating ~ Design + Gender + Design:Gender, data = .) # T_resid &lt;- mutate(T_resid, M_ia2 = residuals(M_ia2)) T_ratings &lt;- bind_rows( posterior(M_mfm_2), posterior(M_cmfm_1)) %&gt;% coef() T_ratings %&gt;% ggplot(aes(y = parameter, col = model, xmin = lower, xmax = upper, x = center)) + geom_errorbarh(height = .2) + geom_point(size = 2) The CLU plots above show both models in comparison. Both models use treatment effects and put the intercept on female users with design A. We observe that there is barely a difference in the estimated intercepts. The coefficient DesignB means something different in the models: in the MFM it represents the difference between Designs. In the CMFM, it is the difference by design with female users. The same is true for GenderM, which is a general effect in MFM and a local effect in the CMFM, the gender difference with design A. For that reason, it is not useful to speak of these coefficients as main effects. They are main effects in a plain MFM, but once the effects become conditional, there is nothing such as a main effect any more. At least, this is the case for treatment effect coding and stairways coding (as we will see next). All three coefficients of the MFM barely change by introducing the conditional effect DesignB:GenderM. Recall that in the MFM, the group mean of design B among men is calculated by adding the two main effects to the intercept. This group mean is fixed. The conditional effect DesignB:GenderM is the difference to the fixed group in the MFM. It can be imagined as an adjustment parameter, that gives the fourth group its own degree of freedom. In the current CMFM the conditional coefficient is very close to zero, with a difference of just \\(0.1 [-0.4, 0.6]_{CI95}\\). It seems we are getting into a lot of null results here. If you have a background in classic statistics, you may get nervous at such a point, because you remember that in case of null results someone said: “one cannot say anything”. This is true when you are testing null hypothesis significance testing. But, when you interpret coefficients, you are speaking quantities and zero is a quantity. What the MFM tells us is that male users really don’t give any higher or lower ratings, in total, although there remains some uncertainty. Actually, the purpose of estimating a CMFM can just be to show that some effect is unconditional. As we have seen earlier [MFM], conditional effects can cancel each other out. Take a look at the following hypothetical results of the study. Here, male and female users do not agree. If we would run an MFM in such a situation, we would get very similar coefficients, but would overlook that the relationship between design and rating is just poorly rendered. tribble(~Design, ~Gender, ~mean_rating, &quot;A&quot;, &quot;F&quot;, 5.6, &quot;A&quot;, &quot;M&quot;, 5.6 + .4, &quot;A&quot;, &quot;Total&quot;, mean(c(5.6, 6.0)), &quot;B&quot;, &quot;F&quot;, 5.6 - .3, &quot;B&quot;, &quot;M&quot;, 5.6 + .4 -.3 -.6, &quot;B&quot;, &quot;Total&quot;, mean(c(5.3, 5.1))) %&gt;% ggplot(aes(x = Design, col = Gender, y = mean_rating)) + geom_point(size = 2) + geom_line(aes(group = Gender)) If something like this happens in a real design study, it may be a good idea to find out, why this difference appears and whether there is a way to make everyone equally happy. These are questions a model cannot answer. But a CMFM can show, when effects are conditional and when they are not. Much of the time, gender effects is what you rather don’t want to have, as it can become a political problem. If conditional adjustment effects are close to zero, that is proof (under uncertainty) that an effect is unconditional. If that is the case, modelling it as a true main effect in a plain MFM is justified, and one is out of the trouble. Let’s see a more complex example of conditional MFMs, where conditional effects are really needed. In the IPump study, two infusion pump designs were compared in three successive sessions. In [OFM] we saw how a factorial model can render a learning curve using stairway dummies. With two designs, we can estimate separate learning curves and make comparisons. Let’s take a look at the raw data: attach(IPump) D_agg %&gt;% group_by(Design, Session) %&gt;% summarize(mean_ToT = mean(ToT)) %&gt;% ggplot(aes(x = Session, y = mean_ToT, color = Design)) + geom_point() + geom_line(aes(group = Design)) + ylim(0,350) We note that the learning curves do not cross, but are not parallel either, which means the stairway coefficients will be different. We need a conditional model. The first choice to make is between treatment dummies and stairway dummies and both have their applications. With treatment effects, we would get an estimate for the total learning between session 1 and 3. That does not make much sense here, but could be interesting to compare trainings by the total effect of a training sequence. We’ll keep the stairway effects on the sessions, but have to now make a choice on where to fix the intercept, and that depends on what aspect of learning is more important. If this were any walk-up-and-use device or a website for making your annual tax report, higher initial performance would indicate that the system is intuitive to use. Medical infusion pumps are used routinely by trained staff. What matters here is long-term performance, and the final session is the best estimate we have for that. We create stairway dummies for session and make this conditional on Design: T_dummy &lt;- tribble(~Session, ~Session3, ~Step3_2, ~Step2_1, &quot;1&quot;, 1, 1, 1, &quot;2&quot;, 1, 1, 0, &quot;3&quot;, 1, 0, 0) D_agg &lt;- left_join(D_agg, T_dummy, by = &quot;Session&quot;) M_cmfm_2 &lt;- stan_glm(ToT ~ 1 + Design + Step3_2 + Step2_1 + Design:(Step3_2 + Step2_1), data = D_agg) coef(M_cmfm_2) parameter fixef center lower upper Intercept Intercept 150.9 121.902 181.8 DesignNovel DesignNovel -63.1 -106.357 -20.8 Step3_2 Step3_2 42.0 0.095 85.6 Step2_1 Step2_1 130.2 86.750 172.8 DesignNovel:Step3_2 DesignNovel:Step3_2 -23.2 -82.801 36.7 DesignNovel:Step2_1 DesignNovel:Step2_1 -48.2 -107.636 12.4 detach(IPump) Note that … here I demonstrate a different technique to attach dummies to the data. First a coding table T_dummy is created, which is then combined with the data, using a (tidy) join operation. we have expanded the factor Session into three dummy variables and we have to make every single one conditional. Design:(Step3_2 + Step2_1) is short for Design:Step3_2 + Design:Step2_1. But, you should never use the fully factorial expansion (Factor1 * Factor2), a this would make dummy variables conditional. In conditional learning curve model, the intercept coefficient tells us that the average ToT with the Legacy in the final session is \\(150.9 [121.9, 181.75]_{CI95}\\) seconds. Using the Novel design the nurses were \\(63.09 [106.36, 20.84]_{CI95}\\) seconds faster and that is our best estimate for the long-term improvement in efficiency. Again, the learning step coefficients are not “main” effects, but is local to Legacy. The first step Step2_1 is much larger than the second, as is typical for learning curves. The adjustment coefficients for Novel have the opposite direction, meaning that the learning steps in Novel are smaller. That is not as bad as it sounds, for two reasons: first, in this study, the final performance counts, not the training progress. Second, an more generally, we have misused a linear model to smooth a non-linear model. Learning processes are exponential. \\(\\beta_0 - \\beta_1x_ {1i}\\) is a linear term. But, when we put it into an exponent, like \\(\\exp(\\beta_0 - \\beta_1 x_ {1i})\\) this is the same as the quotient \\(\\exp(\\beta_0)/ \\beta_1 x_ {1i}\\). Linear models and “linear-in-exponent” models differ in one more property: linear models can be negative or positive, depending on which number is larger. But the linear-in-exponent model will always stay in the positive range. Learning curves are saturation processes, which can look linear when viewed in segments, but unlike linear models, they never cross the lower boundary. This is simply, because there is a maximum performance limit, which can only be reached asymptotically. In teh following section, I will argue that basically all measures we take have natural boundaries. Under common circumstances, this can lead to conditional effects which are due to saturation. In chapter [GLM], we will pick up again the idea of putting the linear term into the exponent. This is what some Generalized Linear Models do to avoid crossing natural boundaries of measures. 5.5.3 Hitting the boundaries of saturation Most statistically trained researchers are aware of some common assumptions of linear regression, such as the normally distributed residuals and variance homogeneity. Less commonly regarded is the assumption of linearity, which arises from the basic regression formula: \\[y_i = \\beta_0 + \\beta_1 x_{1i}\\] The formula basically says, that if we increase \\(x_1\\) (or any other influencing variable) by one unit, \\(y\\) will increase by \\(\\beta_1\\). It also says that \\(y\\) is composed as a mere sum. In this section, we will discover that these innocent assumptions do not hold. In this and the next section, we will use conditional effects to account for non-linearity. We can distinguish between saturation effects, which are more common and amplification effects. mascutils::expand_grid(effect = c(&quot;saturation&quot;, &quot;amplification&quot;), A = c(0,1), B = c(0,1)) %&gt;% left_join(tibble(effect = c(&quot;saturation&quot;,&quot;amplification&quot;), beta_1 = c(.1,.1), beta_2 = c(.2,.2), beta_3 = c(-0.3, 0.3))) %&gt;% mutate(Outcome = A * beta_1 + B * beta_2 + A * B * beta_3) %&gt;% mutate(B = factor(B, labels = c(&quot;low&quot;, &quot;high&quot;)), A = factor(A, labels = c(&quot;low&quot;, &quot;high&quot;))) %&gt;% ggplot(aes(x = A, col = B, y = Outcome)) + geom_point(size = 3) + geom_smooth(aes(group = B, col= B), method = &quot;lm&quot;) + facet_grid(.~effect) Interactions &lt;- expand.grid(effect = c(&quot;saturation&quot;, &quot;amplification&quot;), A = seq(0,1, length.out = 11), B = seq(0,1, length.out = 11)) %&gt;% left_join(tibble(effect = c(&quot;saturation&quot;,&quot;amplification&quot;), beta_1 = c(.1,.1), beta_2 = c(.2,.2), beta_3 = c(-0.3, 0.3))) %&gt;% mutate(Outcome = A * beta_1 + B * beta_2 + A * B * beta_3) library(lattice) grid.arrange( wireframe(Outcome ~ A + B, data = filter(Interactions, effect == &quot;saturation&quot;), main = &quot;saturation&quot;), wireframe(Outcome ~ A + B, data = filter(Interactions, effect == &quot;amplification&quot;), main = &quot;amplification&quot;), ncol = 2 ) A major flaw with the linear model is that it presumes the regression line to rise or fall infinitely. However, in an endless universe everything has boundaries. Just think about your performance in reading this text. Several things could be donme to improve reading performance, such as larger font size, simpler sentence structure or translation into your native language. Still, there is a hard lower limit for time to read, just by the fact, that reading involves saccades (eye movements) and these cannot be accelerated any further. The time someone needs to read a text is limited by fundamental cognitive processing speed. We may be able to reduce the inconvenience of deciphering small text, but once an optimum is reached, there is no further improvement. Such boundaries of performance inevitably lead to non-linear relationships between predictors and outcome. Modern statistics knows several means to deal with non-linearity, some of them are introduced in [GLM]). Still, most researchers use linear models, and it often can be regarded a reasonable approximation under particular circumstances. Mostly, this is that measures keep a distance to the hard boundaries. Because if performance is pushed to the limits, saturation occurs. When there is just one treatment repeatedly pushing towards a boundary, we get the diminishing returns effect seen in learning curves [OFM]. If two or more variables are pushing simultaneously, saturation appear as conditional effects. Before we turn to a genuine design research case, let me explain saturation effects by an example that I hope is intuitive. The hypothetical question is: do two headache pills have twice the effect of one? Consider a pharmaceutical study on the effectiveness of two pain killer pills A and B, taking place in the aftermath of a huge party on a university campus. Random strolling students are asked to participate. First, they rate their experienced headache on a Likert scale ranging from “fresh like the kiss of morning dew” to “dead highway opossum”. Participants are randomly assigned to four groups, each group getting a different combination of pills: no pill, only A, only B, A and B. After 30 minutes, headache is measured again and the difference between both measures is taken as the outcome measure: headache reduction. We inspect the position of four group means graphically: attach(Headache) T_means &lt;- Pills %&gt;% group_by(PillA, PillB) %&gt;% summarise(mean_reduction = round(mean(reduction),1)) T_means %&gt;% ggplot(aes(x = PillA, col = PillB, mean_reduction)) + geom_point() + geom_line(aes(group = PillB)) + ylim(0,2.5) When neither pill is given a slight spontaneous reduction seems to occur, which is the placebo-effect. Both pills alone are much stronger than te placebo and giving them both has the most beneficial effect. However, the combined effect is just a tad stronger than the effect of A and stays far from being the sum. One could also say, that the net effect of B is weaker when A has been given first. When the effect of one predictor depends on the level of another, this is just a conditional effect. Does that make sense? Do the effects of headache pills simply add up like this? This question is easily answered by contemplating what may happen with not two but five headache pills. If we assume linear addition of effects, participants in the group with all pills would even experience the breathtaking sensation of negative headache. So, certainly, the effect cannot be truly linear. Al headache pills are pushing into the direction of the boundary called no-headache. At the example of headache pills, I will now demonstrate that saturation can cause a severe bias when not accounted for by a conditional effect. We estimate both models: a factorial unconditional MFM and a conditional MFM. M_mfm &lt;- stan_glm(reduction ~ 1 + PillA + PillB, data = Pills) M_cmfm &lt;- stan_glm(reduction ~ 1 + PillA + PillB + PillA:PillB, data = Pills) # M_3 &lt;- stan_glm(reduction ~ 0 + PillA:PillB, data = Pills, iter = 100) P_1 &lt;- bind_rows( posterior(M_mfm), posterior(M_cmfm) #, #posterior(M_3) ) The following table puts the center estimates of both models side-by-side. coef(P_1) %&gt;% select(model, fixef, center) %&gt;% spread(key = model, value = center) fixef M_1 Intercept 106 # %&gt;% arrange(c(1,2,4,3)) Both intercepts indicate that headache diminishes due to the placebo alone, but M_mfm over-estimates the placebo effect. At the same time, the treatment effects PillA and PillB are under-estimated. That happens, because the unconditional model averages over two conditions, under which pill A or B are given: with the other pill or without. As M_cmfm tells, when taken with the another pill, effectiveness is reduced by \\(-0.37\\). The effectiveness of two pills is not their sum, but less than that. One can have headache to a certain degree or no headache at all. If it’s gone, any more pills have no additional effects. detach(Headache) In general, if two predictors work into the same direction (here the positive direction) and the intercation effect has the opposite direction, this is likely a saturation effect: the more of similar is given, the closer it gets to the natural boundaries and the less it adds. Remember that this is really not about side effects in conjunction with other medicines. Quite the opposite: if two type of pills effectively reduce headache, but in conjunction produce a rash, this would be an amplification effect. Amplification effects are theoretically interesting, not only for pharmacists. Saturation effects are boring. When they happen, they only tell us that we have been applying more of the similar and that we are running against a set limit of how much we can improve things. Back to design research with a another hypothetical study that works similar to the Pills case. Imagine a study aiming at ergonomics of reading for informational websites. In a first experiment, the researcher found that 12pt font effectively reduces reading time as compared to 10pt by about 5 seconds. D_reading_time &lt;- tibble(font_size = c(4, 10, 12, 14, 16, 18), observed_time = c(NA, 40, 30, NA, NA, NA), predicted_time = 60 - font_size/4 * 10) D_reading_time font_size observed_time predicted_time 4 NA 50 10 40 35 12 30 30 14 NA 25 16 NA 20 18 NA 15 It should be clear by now, that these expectations have no ground: for normally sighted persons, a font size of 12 is easy enough to decipher and another increase will not have the same effect. Taking this further, one would even arrive at absurdly short or impossible negative reading times. At the opposite, a font size of four point may just render unreadable on a computer screen. Instead of a moderate increase by 10 seconds, participants may have to decipher and guess the individual words, which will take much longer. Researching the effect of font sizes between 8pt and 12pt font size probably keeps the right distance, with approximate linearity within that range. But what happens if you bring a second manipulation into the game with a functionally similar effect? A likely outcome is that the fundamental assumption of predictors sum up no longer holds, but saturation occurs. Still, another option to improve readability of text is to improve the contrast, using black-on-white fonts, instead of the more fanbcy looking grey-on-white. Let’s turn to a fictional, yet realistic problem in design research. Design of systems is a matter of compromises. A common conflict of interests is between the aesthetic appearance and the ergonomic properties. From an ergonomic point of view, one would probably favor a typesetting design with crisp fonts and maximum contrast. However, if a design researcher were now to suggest using 12pt black Arial on white background as body font, this is asking for trouble. Someone in charge will probably insist on a fancy serif font in an understating blueish-grey tone. For creating a relaxed reading experience, the only option left is to increase the font size. The general question arises: can one sufficiently compensate lack of contrast by setting the text in the maximum reasonable font size 12pt, as compared to the more typical 10pt? In the fictional study Reading, this is examined in a 2x2 experimental design: the same page of text is presented in four versions, with either 10pt or 12pt, and grey versus black font colour. Performance is here measured as time-on-task of reading the full page. attach(Reading) D_1 D_1 %&gt;% ggplot(aes(col = font_color, x = font_size, y = ToT)) + geom_boxplot() We see immediately, that both design choices have an impact: black letters, as well as larger letters are faster to read. But, do they add up? Or do both factors behave like headache pills, where more is more, but less than the sum. Clearly, the 12pt-black group could read fastest on average. Neither with large font, nor with optimal contrast alone has the design reached a boundary, i.e. saturation. We run two regression models, a plain MFM and a conditional MFM, that adds an interaction term. We extract the coefficients from both models and view them side-by-side: M_mfm &lt;- D_1 %&gt;% stan_glm(ToT ~ 1 + font_size + font_color, data = .) M_cmfm &lt;- D_1 %&gt;% stan_glm(ToT ~ 1 + font_size + font_color + font_size : font_color, data = .) T_read_fixef &lt;- bind_rows(posterior(M_mfm), posterior(M_cmfm)) %&gt;% coef() %&gt;% print() ## ## ## Table: (\\#tab:tab:comp_coef)Estimates with 95% credibility limits ## ## model parameter fixef center lower upper ## ------- ------------------------------ ------------------------------ ------- ------- ------ ## M_cmfm Intercept Intercept 61.20 57.40 65.00 ## M_cmfm font_size12pt font_size12pt -12.59 -17.80 -6.98 ## M_cmfm font_colorblack font_colorblack -10.89 -16.35 -5.56 ## M_cmfm font_size12pt:font_colorblack font_size12pt:font_colorblack 5.32 -2.33 12.89 ## M_mfm Intercept Intercept 59.84 56.29 63.40 ## M_mfm font_size12pt font_size12pt -9.89 -13.92 -5.93 ## M_mfm font_colorblack font_colorblack -8.22 -12.15 -4.23 The estimates confirm, that both manipulations have a considerable effect in reducing reading time. But, as the conditional effect works in the opposite direction, this reeks of saturation: in this hypothetical case, font size act as contrast are more-of-the-similar and the combined effects is less than the sum. This is just like taking two headache pills. If this was real data, we could assign the saturation effect a deeper meaning. It is not always obvious that two factors work in a similar way. From a psychological perspective this would indicate that both manipulations work on similar cognitive processes, for example, visual letter recognition. Knowing more than one way to improve on a certain mode of processing can be very helpful in design, where conflicting demands arise often and In the current case, a reasonable compromise between ergonomics and aesthetics would be to either use large fonts or black letters. Both have the strongest ergonomic net effect when they come alone. Conditional effects are notoriously neglected in research and they are often hard to grasp for audience, even when people have a classic statistics education. Clear communication is often crucial and conditional models are best understood by using conditional plots. A conditional plot for the 2x2 design contains the four estimated group means. These can be computed from the linear model coefficients, but often it easier to just estimate an AGM alongsite the CGM: M_amm &lt;- D_1 %&gt;% stan_glm(ToT ~ 0 + font_size : font_color, data = .) coef(M_amm) parameter fixef center lower upper font_size10pt:font_colorgray font_size10pt:font_colorgray 60.8 56.8 64.8 font_size12pt:font_colorgray font_size12pt:font_colorgray 48.2 44.2 51.9 font_size10pt:font_colorblack font_size10pt:font_colorblack 49.8 45.8 53.5 font_size12pt:font_colorblack font_size12pt:font_colorblack 42.7 38.8 46.5 T_amm &lt;- coef(M_amm) %&gt;% separate(fixef, c(&quot;font_size&quot;, &quot;font_color&quot;), sep = &quot;:&quot;) %&gt;% mutate(font_size = str_replace(font_size, &quot;font_size&quot;, &quot;&quot;), font_color = str_replace(font_color, &quot;font_color&quot;, &quot;&quot;)) G_amm &lt;- T_amm %&gt;% ggplot(aes(x = font_color, color = font_size, shape = font_size, y = center)) + geom_point() + geom_line(aes(group = font_size)) Note that in a CLU table the column fixef stores two identifiers, the level of font-size and the level of font_color. For putting them on different GGplot aesthetics we first have to rip them apart using separate before using mutate and str_replace to strip the group labels off the factor names. Since the coefficient table also contains the 95% certainty limits, we can produce a conditional plot with overlayed credibility intervals (geom_errorbar). These limits belong to the group means, and generally cannot be used to tell about the treatment effects. G_amm + geom_errorbar(aes(ymin = lower, ymax = upper), width = .2) Still, it gives the observer some sense of the overall level of certainty. And, when two 95% CIs do not overlap, that means that the difference is different from zero with 95% credibility, at least. Another useful Ggplot geometry is violin plots, as these make the overlap between CIs visible and reduce visual clutter caused by all these vertical error bars. However, a violin plot requires more that just three CLU estimates. Recall from [random_walk] that the posterior object, obtained with posterior stores the full certainty information gained by the MCMC estimation walk. The CLU estimates we so commonly use, are just condensing this information into three numbers (CLU). By pulling the estimated posterior distribution into the plot, we can produce a conditional plot that conveys more information and is easier on the eye. P_amm &lt;- posterior(M_amm) %&gt;% filter(type == &quot;fixef&quot;) %&gt;% select(fixef, value) %&gt;% separate(fixef, c(&quot;font_size&quot;, &quot;font_color&quot;), sep = &quot;:&quot;) %&gt;% mutate(font_size = str_replace(font_size, &quot;font_size&quot;, &quot;&quot;), font_color = str_replace(font_color, &quot;font_color&quot;, &quot;&quot;)) G_amm + geom_violin(data = P_amm, aes(y = value, fill = font_size), alpha = 0.5, position = position_identity(), width = .2) detach(Reading) As the figure shows, ergonomics is maximized by using large fonts and high contrast. Still, there is saturation and therefore it does little harm to go with the gray font, as long as it is 12pt. We have seen in the Headache example that conditional effects occur as non-linearity. The more a participant approaches the natural boundary of zero headache, the less benefit is created by additional effort. This we call saturation. Saturation is likely to occur when multiple factors influence the same cognitive or physical system or functioning. In quantitative comparative design studies, we gain a more detailed picture on the co-impact of design interventions and can come to more sophisticated decisions. If we don’t account for saturation by introducing interaction terms, we are prone to underestimate the net effect of any of these measures and may falsely conclude that a certain treatment is rather ineffective. Consider a large scale study, that assesses the simultaneous impact of many demographic variables on how willing customers are to take certain energy saving actions in their homes. It is very likely that subsets of variables are associated with similar cognitive processes. For example, certain action requires little effort (such as switching off lights in unoccupied rooms), whereas others are time-consuming (drying the laundry outside). At the same time, customers may vary in the overall eagerness (motivation). For high effort actions the impact of motivation level probably makes more of a difference than when effort is low. Not including the conditional effect would result in the false conclusion that suggesting high effort actions is rather ineffective. 5.5.4 More than the sum: amplification Saturation effects occur, when multiple impact factors act on the same system and work in the same direction. When reaching the boundaries, the change per unit diminishes. We can also think of such factors as exchangeable. Amplification conditional effects are the opposite: It only really works, if all conditions are fulfilled. Conceiving good examples for amplification effects is far more challenging as compared to saturation effects. Probably this is because saturation is a rather trivial phenomenon, whereas amplification involves some non-trivial orchestration of cognitive or physiological subprocesses. Here, a fictional case on technology acceptance will serve to illustrate amplification effects. Imagine a start-up company that seeks funding for a novel augmented reality game, where groups of gamers compete for territory. For a fund raising, they need to know their market potential, i.e. which fraction of the population is potentially interested. The entrepreneurs have two hypotheses they want to verify: Only technophile persons will dare to play the game, because it requires some top-notch equipment. The game is strongly cooperative and therefore more attractive for people with a strong social motif. Imagine a study, where they asked a larger set of participants to rate their technophily and sociophily. They were then given a description of the planned game and were asked how much they intended to participate in the game. While the example primarily serves to introduce amplification effects, it is also an opportunity to get familiar with conditional effects between metric predictors. Although this is not very different to conditional effects on groups, there are a few peculiarities, one being that we cannot straight-forwardly make an exploratory plot. For factors, we have used box plots, but these do not apply for metric predictors. In fact, it is very difficult to come up with a good graphical representation. One might think of 3D wire-frame plots, but these transfer poorly to the 2D medium of these pages. Another option is to create a scatter-plot with the predictors on the axes and encode the outcome variable by shades or size of dots . These options may suffice to see any present main effects, but are too coarse to discover subtle non-linearity. The closest we can get to a good illustration is to create groups and continue as usual. Note, that turning metric predictors into factors is just a hack to create exploratory graphs. By no means do I intend to corroborate the use of group-mean models on metric data. attach(AR_game) D_1 %&gt;% mutate(Sociophile = forcats::fct_rev(ifelse(sociophile &gt; median(sociophile), &quot;high&quot;, &quot;low&quot;)), Technophile = forcats::fct_rev(ifelse(technophile &gt; median(technophile), &quot;high&quot;, &quot;low&quot;))) %&gt;% ggplot(aes(y = intention, x = Technophile, col = Sociophile)) + geom_boxplot() + ylim(0, 0.5) From the boxplot it seems that both predictors have a positive effect on intention to play. However, it remains unclear whether there is a conditional effect. In absence of a better visualization, we have to rely fully on the numerical estimates of a conditional linear regression model (CMRM). M_cmrm &lt;- D_1 %&gt;% stan_glm(intention ~ 1 + sociophile + technophile + sociophile : technophile, data = .) coef(M_cmrm) parameter fixef center lower upper Intercept Intercept 0.273 0.258 0.288 sociophile sociophile 0.114 0.073 0.153 technophile technophile 0.182 0.153 0.212 sociophile:technophile sociophile:technophile 0.163 0.080 0.243 The regression model confirms that sociophilia and technophilia both have a moderate effect on intention. Both main effects are clearly in the positive range. Yet, when both increase, the outcome increases over-linearly. The sociophile-technophile personality is the primary target group. While plotting the relationship between three metric variables is difficult, there are alternative ways to illustrate the effect. For example, we could ask: how does intention change by .1 unit sociophilia for two imaginary participants with extreme positions on technophilia (e.g., .2 and .8). We could calculate these values arithmetically using the model formula and the center estimates. The better and genuinely Bayesian way of doing it is sampling from the posterior predictive distribution (PPD). This distribution is generated during parameter estimation. While the posterior distribution (PD) represents the predictors, the PPD is linked to the outcome variable. More specifically, the PPD is the models best guess of the expected value \\(\\mu\\). Once the posterior has been estimated, we can draw from it with any values of interest. As these can be combinations of values that have never been observed, we can truly speak of prediction. Regression engines usually provide easy means to simulate from a PD and generate predictions. In the following example, we simulate some equidistant steps of sociophilia for three participants with technophilia scores from .1 to .9. D_2 &lt;- mascutils::expand_grid(technophile = seq(.1, .9, by = .1), sociophile = seq(.3, .6, by = .1)) %&gt;% arrange(technophile) T_comb_pred &lt;- post_pred(M_cmrm, newdata = D_2, thin =2) %&gt;% predict() D_2 &lt;- D_2 %&gt;% mutate(intention = T_comb_pred$center) D_2 %&gt;% mutate(technophile = as.factor(technophile)) %&gt;% ggplot(aes(x = sociophile, col = technophile, y = intention)) + geom_point() + geom_line(aes(group = technophile)) The effect is not stunning, but visible. The lines diverge, which means they have different slopes. With high technophilia, every (tenth) unit of sociophilia has a stronger effect on intention to play. detach(AR_game) Saturation effects are about declining net effects, there more similar treatments pile up, that can be the same amount of training (which gives a curve of diminshing returns) or two similar treatments. Amplification effects are more like two-component glue. When using only one of the components, all you get you get is a smear. The only way of getting a strong hold is to put them together. This has a parallel in Boolean logic [#Boolean?]. The Boolean OR operator returns TRUE when one of the operands is TRUE. Adding more True does not change anything anymore, which is an extreme case of saturation. The operator AND requires both operands to be TRUE for something to happen. T_bool_interaction &lt;- tibble(A = c(F, F, T, T), B = c(F, T, F, T)) %&gt;% as_tibble() %&gt;% mutate(&quot;A OR B&quot; = A | B) %&gt;% mutate(&quot;A AND B&quot; = A &amp; B) kable(T_bool_interaction) A B A OR B A AND B FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE TRUE FALSE TRUE TRUE TRUE TRUE For decision making in design research, the notion of saturation and amplification are equally important. Saturation effects can happen with seemingly different design choices that act on the same cognitive (or other) processes. That is good to know, because it allows the designer to compensate one design feature with the other, should there be a conflict between different requirements, such as easthetics and readability of text. Amplification effects are interesting, because they break barriers. Only if the right ingredients are present, a system is adopted by users. Many technology break-throughs can perhaps be attributed to adding the final necessary ingredient. Sometimes you can also ask: what ingredient has been missing for some earlier technology flops. For example, the first commercial smart phone (with touchscreen, data connectivity and apps) has been the IBM Simon Personal Communicator, introduced in 1993, which was discontinued after only six months on the market. It lasted more than ten years before smartphones actually took off. What were the magic ingredients added? A feature that must be present for the users to be satisfied (in the mere sense of absence-of-annoyance) is commonly called a necessary user requirements. That paints a more moderate picture of amplification in everyday design work: The peak, where all features work together usually is not the magic break-through; it is the strenuous path of user experience design, where user requirements whirl around you and not a single one must be left behind. 5.5.5 Conditional effects and theory [ZAP ME] Explaining or predicting complex behaviour with psychological theory is a typical approach in design research. Unfortunately, it is not an easy one. While design is definitely multifactorial, with a variety of cognitive processes, individual differences and behavioural strategies, few psychological theories cover more than three associations between external or individual conditions and behaviour. The design researcher is often forced to enter a rather narrow perspective or knit a patchwork model from multiple theories. Such a model can either be loose, making few assumptions on how the impact factors interact which others. A more tightened model frames multiple impact factors into a conditional network, where the impact of one factor can depend on the overall configuration. A classic study will now serve to show how conditional effects can clarify theoretical reasoning. Vigilance is the ability to remain attentive for rarely occurring events. Think of truck drivers on lonely night rides, where most of the time they spend keeping the truck on a straight 80km/h course. Only every now and then is the driver required to react to an event, like when braking lights flare up ahead. Vigilance tasks are among the hardest thing to ask from a human operator. Yet, they are safety relevant in a number of domains. Keeping up vigilance most people perceive as tiring, and vigilance deteriorates with tiredness. Several studies have shown that reaction time at simple tasks increases when people are tired. The disturbing effect of noise has been documented as well. A study by [Corcoran (1961)] examined the simultaneous influence of sleep deprivation and noise on a rather simple reaction task. They asked: will the effects of noise summate with those of loss of sleep to induce an even greater performance decrement or will noise subtract from the performance decrement caused by loss of sleep? The central argument is that sleep deprivation deteriorates the central nervous arousal system. In consequence, sleep deprived persons cannot maintain the necessary level of energy that goes with the task. Noise is a source of irritation and therefore usually reduces performance. At the same time, noise may have an arousing effect, which may compensate for the loss of arousal due to sleep deprivation. To re-iterate on the headache pills analogy [#Saturation], noise could be the antidote for sleepiness. The Sleep case study is a simplified simulation of Corcoran’s results. Participants were divided into 2x2 groups (quiet/noisy, rested/deprived) and had to react to five signal lamps in a succession of trials. In the original study, performance measure gaps were counted, which is the number of delayed reactions (\\(&gt;1500ms\\)). Here we just go with (simulated) reaction times, assuming that declining vigilance manifests itself in slower reactions. attach(Sleep) D_1 %&gt;% ggplot(aes(x = Environment, color = Sleep, y = RT)) + geom_boxplot() Using a 2x2 model including a conditional effect, we examine the conditional association between noise and sleepiness. M_1 &lt;- D_1 %&gt;% stan_glm(RT ~ Environment * Sleep, data = .) Note that the * operator in the model formula is an abbreviation for a fully factorial model 1 + Environment + Sleep + Environment:Sleep. parameter fixef center lower upper Intercept Intercept 98.545 63.5 134.8 EnvironmentNoisy EnvironmentNoisy 0.532 -49.9 50.8 SleepSleepy SleepSleepy 160.667 110.9 209.2 EnvironmentNoisy:SleepSleepy EnvironmentNoisy:SleepSleepy -98.096 -171.0 -30.2 Recall, that treatment contrasts were used, where all effects are given relative to the reference group quiet-rested (intercept). The results confirm the deteriorating effect of sleepiness, although its exact impact is blurred by pronounced uncertainty \\(160.67 [110.94, 209.24]_{CI95}\\). Somewhat surprisingly, noise did not affect well-rested persons by much \\(0.53 [-49.91, 50.77]_{CI95}\\). Note however, that we cannot conclude a null effect, as the credibility limits are wide. Maybe the lack of a clear effect is because steady white noise was used, not a disturbing tumult. The effect of sleepiness on RT is partly reduced in a noisy environment \\(-98.1 [-170.97, -30.2]_{CI95}\\). This suggests that the arousal system is involved in the deteriorating effect of sleep deprivation, which has interesting consequences for the design of vigilance tasks in the real world. These findings reverb with a well known in Psychology of Human Factors, the Yerkes-Dodson law. The law states that human performance at cognitive tasks is influenced by arousal. The influence is not linear, but better approximated with a curve as shown in the figure below. Performance is highest at a moderate level of arousal. If we assume that sleepy participants in Corcona’s study showed low performance due to under-arousal, the noise perhaps has increased the arousal level, resulting in better performance. If we accept that noise has an arousing effect, the null effect of noise on rested participants stands in opposition to the Yerkes-Dodson law: if rested participants were on an optimal arousal level, additional arousal would usually have a negative effect on performance. There is the slight possibility, that Corcona has hit a sweet spot: if we assume that calm/rested participants were still below an optimal arousal level, noise could have pushed them right to the opposite point. detach(Sleep) To sum it up, saturation and amplification effects have in common that performance is related to design features in a monotonous increasing manner (, albeit not linearly increasing). Such effects can be interpreted in a straight-forward manner: when saturation occurs with multiple factors, it can be inferred that they all impact the same underlying cognitive mechanism and are therefore interchangeable to some extent, like compensating letter size with stronger contrast. In turn, amplification effects indicate that multiple cognitive mechanisms (or attitudes) are necessarily involved and must be regarded during design. The Sleep study demonstrates that conditional effects can also occur in situations with non monotonously increasing relationships between design features and performance. When such a relationship takes the form of a parabole, like the Yerkes-Dodson law, the designer (or researcher) is faced with the more complex problem of finding the sweet spot. Central to the Yerkes-Dodson law is that arousal is a gradually increasing condition, and so are noise level and degree of sleep deprivation. A consequential follow-up study would be one, where these levels are manipulated (or measured) on more levels than the original could serve to identify the position of optimal performance more accurately. In [#PRM] we have encountered a similar case: the trough of the Uncanny Valley effect can be estimated and this estimate can help researchers to avoid the critical region. 5.6 Doing the rollercoaster: polynomial regression models In the preceding four sections, we used linear models to render processes that are not linear, but curved. These non-linear processes fell into two classes: dull diminishing return curves for learning curve OFMs or saturation effects and the more sudden peaks of amplification. But, what can we do when a process follows more complex curves, with more ups-and downs? In the following I will introduce polynomial regression models, which still can be formulated as linear models, but can take a wide variety of shapes. Robots build our cars and sometimes drive them. They mow the lawn and may soon also deliver parcels to far-off regions. Prophecy is that robots will also enter social domains, such as care for children and the elderly. One can assume that in social settings emotional acceptance plays a significant role for technology adoption. Next to our voices, our faces and mimic expressions are the main source of interpersonal messaging. Since the dawn of the very idea of robots, anthropomorphic designs have been dominant. Researchers and designers all around the globe are currently pushing the limits of human-likeness of robots. One could assume that emotional response improves with every small step towards perfection. Unfortunately, this is not the case. [Mori] discovered a bizarre non-linearity in human response: people’s emotional response is proportional t human-likeness, but only at the lower end. A robot design with cartoon style facial features will always beat a robot vacuum cleaner. But, an almost anatomically correct robot face may provoke a very negative emotional response, which is called the uncanny valley. tibble(hl = seq(-1, 1, length.out = 100), emotional_valence = -.5 * hl + .6 * hl^3 + .2 * hl^4) %&gt;% mutate(human_likeness = (hl + 1)/2) %&gt;% ggplot(aes(x = human_likeness, y = emotional_valence)) + geom_line() [Mathur et al.] study aimed at rendering the association between human-likeness and liking at full range. They collected 60 pictures of robots and attached a score for human likeness to them. Then they asked participants how much they liked the faces. Fort the data analysis they calculated an average score of likability per robot picture. Owing to the curved shape of the uncanny valley, linear regression is not applicable to the problem. Instead, Mathur et al. applied a third degree polynomial term. A polynomial function of degree \\(k\\) has the form: \\[y_i = \\beta_0 x_i^0 + \\beta_1 x_i^1 + ... + \\beta_{k} x_i^{k}\\] In fact, you are already familiar with two polynomial models. The zero degree polynomial is the grand mean model. This follows from \\(x_i^0 = 1\\), which makes \\(\\beta_0\\) a constant, the intercept. Also, a first degree polynomial is simply the linear model. By adding higher degrees we can introduce more complex curvature to the association. D_poly &lt;- tibble(x = seq(-2, 3, by = .1), degree_0 = 2, degree_1 = 1 * degree_0 + 3 * x, degree_2 = 0.5 * (degree_1 + 2 * x^2), degree_3 = 0.5 * (degree_2 + -1 * x^3), degree_4 = 0.4 * (degree_3 + 0.5 * x^4), degree_5 = 0.3 * (degree_4 + -0.3 * x^5)) %&gt;% gather(polynomial, y, degree_0:degree_5) %&gt;% arrange(polynomial, y, x) D_poly %&gt;% ggplot(aes(x, y)) + geom_line() + facet_wrap(~polynomial) Mathur et al. argue that the Uncanny Valley curve possesses two stationary points, with a slope of zero: the valley is a local minimum and represents the deepest point in the valley, the other is a local maximum and marks the shoulder left of the valley. Such a curvature can be approximated with a polynomial of (at least) third degree, which has a constant term \\(\\beta_0\\), a linear slope \\(x\\beta_1\\), quadratic component \\(x^2\\beta_2\\) and a cubic component \\(x^3\\beta_3\\). While R provides high-level methods to deal with polynomial regression, it is instructive to build the regression manually. The first step is to add variables to the data frame, which are the predictors taken to powers (\\(x_k = x^k\\)). These variables are then straight-forwardly added to the model term, as if they were independent predictors. For better clarity, we rename the intercept to be \\(x_0\\), before summarizing the fixed effects. attach(Uncanny) M_poly_3 &lt;- UV_1 %&gt;% mutate(huMech_0 = 1, huMech_1 = huMech, huMech_2 = huMech^2, huMech_3 = huMech^3) %&gt;% stan_glm(avg_like ~ 1 + huMech_1 + huMech_2 + huMech_3, data = ., iter = 2500) # stan_glm(avg_like ~ poly(huMech, 4), # data = ., iter = 100) P_poly_3 &lt;- posterior(M_poly_3) coef(P_poly_3) parameter fixef center lower upper Intercept Intercept -0.457 -0.532 -0.379 huMech_1 huMech_1 0.219 -0.309 0.743 huMech_2 huMech_2 -1.232 -2.347 -0.104 huMech_3 huMech_3 0.992 0.253 1.721 parameter fixef center lower upper Intercept Intercept -0.457 -0.532 -0.379 huMech_1 huMech_1 0.219 -0.309 0.743 huMech_2 huMech_2 -1.232 -2.347 -0.104 huMech_3 huMech_3 0.992 0.253 1.721 We can extract the fixef effects table as usual. The four coefficients specify the polynomial to approximate the average likeability responses. The polynomial parameters have little explanatory value, neither one alone relates to a relevant property of the uncanny valley. One relevant property would be the location of the deepest point of the uncanny valley, its trough. The trough is a local minimum of the curve and we can find this point with polynomial techniques. Finding a local minimum is a two step procedure: first, we must find all stationary points, which includes local minima and maxima. Then, we determine which of the resulting points is the local minimum. Stationary points occur, where the curve bends from a rising to falling or vice versa. At these points, the slope is zero, neither rising nor falling. Therefrore, stationary points are identified by the derivative of the polynomial, which is a second degree (cubic) polynomial: \\[f&#39;(x) = \\beta_1 + 2\\beta_2x + 3\\beta_2x^2\\] The derivative \\(f&#39;(x)\\) of a function \\(f(x)\\) gives the slope of \\(f(x)\\) at any given point \\(x\\). When \\(f&#39;(x) &gt; 0\\), \\(f(x)\\) is rising at \\(x\\), with \\(f&#39;(x) &lt; 0\\) it is falling. Stationary points are precisely those points, where \\(f&#39;(x) = 0\\) and can be found by solving the equation. The derivative of a third degree polynomial is of the second degree, which has a quadratic part. This can produce a parabolic form, which hits point zero twice, once rising and once falling. A rising encounter of point zero indicates that \\(f(x)\\) has a local minimum at \\(x\\), a falling one indicates a local maximum. In consequence, solving \\(f&#39;(x) = 0\\) can result in two solutions, one minimum and one maximum, which need to be distinguished further. If the stationary point is a local minimum, as the trough, slope switches from negative to positive; \\(f&#39;(x)\\) crosses \\(x = 0\\) in a rising manner, which is a positive slope of \\(f&#39;(x)\\). Therefore, a stationary point is a local minimum, if \\(f&#39;&#39;(x) &gt; 0\\). Mathur et al. followed these analytic steps to arrive at an estimate for the position of the trough. They did so on the point estimates they got with their frequentist estimation method. In the following, we will do the same operations on the center estimates of the CLU table. After that, we apply it to the posterior distribution which enables us to make uncertainty statements. library(polynom) poly &lt;- polynomial(T_fixef_1$center) # UC function on center dpoly &lt;- deriv(poly) # 1st derivative ddpoly &lt;- deriv(dpoly) # 2nd derivative stat_pts &lt;- solve(dpoly) # finding stat points slopes &lt;- as.function(ddpoly)(stat_pts)# slope at stat points trough &lt;- stat_pts[slopes &gt; 0] # selecting the local minimum cat(&quot;The trough is most likely at a huMech score of &quot;, round(trough, 2)) ## The trough is most likely at a huMech score of 0.73 Note how the code uses high-level functions from package polynom to estimate the location of the trough, in particular the first and second derivative d[d]poly This procedure repeats the original analysis, but there is a limitation: using the center estimates, we get a point estimate for the position of the trough, but without any information on uncertainty. The solution is that we let the trough calculation work on every single draw of the posterior distribution. That will produce a posterior distribution of the derived trough position. Every step of the MCMC walk produces a simultaneous draw of the four parameters huMech_[0:3], and therefore fully specifies a third degree polynomial. The posterior distribution of the position of the trough can be computed, by computing the position for every iteration. For the convenience, the R package Uncanny contains a function trough(coef) that includes all the above steps. The following code creates a data frame with one row per MCMC draw and the four huMech variables, the function trough acts on this data frame as a matrix of coefficients and returns one trough point per row. We have obtained the PD of the trough. devtools::install_github(&quot;schmettow/Uncanny&quot;) P_trough &lt;- P_poly_3 %&gt;% filter(type == &quot;fixef&quot;) %&gt;% select(chain, iter, fixef, value) %&gt;% spread(fixef, value) %&gt;% select(Intercept, starts_with(&quot;huMech&quot;)) %&gt;% mutate(trough = uncanny::trough(.)) %&gt;% gather(key = parameter) This posterior distribution we can plot, or put it into a CLU table: P_trough %&gt;% group_by(parameter) %&gt;% summarize(center = median(value, na.rm = T), lower = quantile(value, .025, na.rm = T), upper = quantile(value, .975, na.rm = T)) parameter center lower upper huMech_1 0.219 -0.309 0.743 huMech_2 -1.232 -2.347 -0.104 huMech_3 0.992 0.253 1.721 Intercept -0.457 -0.532 -0.379 trough 0.727 0.654 0.840 The 95% CI is a conventional measure of uncertainty and may be more or less irrelevant. The most generous display on uncertainty is a density plot on the full posterior. The density function just smooths over the frequency distribution of trough draws, but makes no arbitrary choices on where to cut it. UV_1$M_poly_3 &lt;- predict(M_poly_3)$center gridExtra::grid.arrange( UV_1 %&gt;% ggplot(aes(x = huMech, y = avg_like)) + geom_point(size = .3) + geom_smooth(aes(y = M_poly_3), se = F), P_trough %&gt;% filter(parameter == &quot;trough&quot;) %&gt;% ggplot(aes(x = value)) + geom_density() + xlim(0, 1), heights = c(.8, .2) ) With reasonable certainty, we can say that the trough is at approximately two-thirds of the huMech score range. In contrast, the illustration of the uncanny valley as they used to be perpetuated from the original source, place the trough at about four quarters of the scale. The Uncanny Valley effect seems to set in “earlier” than we thought. A closer look at the scatterplot above reveals a problem with the data set: It seems that data is sparsest right where the valley is deepest. Since there also is a lot of noise, the concern is that there actually is no trough. This can be tested on the same posterior. The uncanny::trough function returns a missing value, when no minimum stationary point could be found. Hence, the proportion of non-NAs is the certainty we have that a trough exists: print(&quot;Probability that there is a trough:&quot;, 1 - mean(is.na(P_trough)) ) ## [1] &quot;Probability that there is a trough:&quot; detach(Uncanny) 5.6.1 Make yourself a test statistic Generally, in design research we are interested in real world impact and this book takes a strictly quantitative stance. Rather than testing the hypothesis whether any effect exists or not, we interpret coefficients by making statements on their magnitude and uncertainty. Most often, when a higher polynomial models is required, the coefficients do not have a clear meaning. We evaluated the position of the local minimum, the trough. The theory goes that the Uncanny Valley effect is a disruption of a slow upwards trend, the disruption creates the shoulder and culminates in the trough. But, there is no single coefficient telling us directly that there actually are a shoulder and a trough. Polynomial theory tells us that a cubic function can have two stationary points, but it can also just have one or zero. After all, straight line is a cubic, too, if we set the quadratic and cubic coefficients to zero. The fact that a cubic poylnomial fits the data best is a good indicator that exactly two stationary points are needed. But, compared to the straight-in-the-face Uncanny Valley this is unsatisfyingly indirect. Wouldn’t it be bold if we could say: the Uncanny Valley exists with a certainty of …? Recall, that when a cubic model is estimated, the MCMC walk makes random visits in a four-dimensional coefficient space [#MCMC] (five-dimensional, if we count the error variance). These coordinates are stored per iteration in a posterior distribution object. Every iteration represents one possible polynomial. attach(Uncanny) post_pred(M_poly_3, thin = 10) %&gt;% left_join(UV_1, by = &quot;Obs&quot;) %&gt;% ggplot(aes(x = huMech, y = value, group = iter)) + stat_smooth(geom=&#39;line&#39;, alpha=0.2, se=FALSE) All we have to do is count the number of MCMC visits, that have a trough and a shoulder. The function trough in the Uncanny package (on Github) is designed to return the position, when it exists and simply returns NA otherwise. The same goes for the function shoulder, which finds the local maximum, if any. With these two functions, we can create two simple test statistics. All we have to do is count how many of the MCMC draws represent a cubic polynomial with shoulder and trough. # devtools::install_github(&quot;schmettow/uncanny&quot;) library(uncanny) P_wide &lt;- P_poly_3 %&gt;% filter(type == &quot;fixef&quot;) %&gt;% as_tibble() %&gt;% select(iter, parameter, value) %&gt;% spread(key = parameter, value = value) %&gt;% select(-iter) %&gt;% mutate(trough = uncanny::trough(.), shoulder = uncanny::shoulder(.), is_Uncanny = !is.na(trough) &amp; !is.na(shoulder) ) print(&quot;The probability that the Uncanny Valley does NOT exist is:&quot;) ## [1] &quot;The probability that the Uncanny Valley does NOT exist is:&quot; print(mean(P_wide$is_Uncanny)) ## [1] 0.0368 So, with the data of Mathur &amp; Reichling we can be pretty sure that the Uncanny Valley effect is present. This conclusion sounds strong, and is even more interesting from a philosophy-of-science point-of-view. It was in 1970, when Masahiro Mori published his theory on the relation between human likeness and emotional response. This article is astonishingly recent and clearly anticipates the emerge of human-like robots and virtual characters [https://spectrum.ieee.org/automaton/robotics/humanoids/the-uncanny-valley]. But for a modern reader in Social Sciences the article abruptly stops, right where we would expect the experimental part confirming the theory. It seems that Mori’s theory sprang just from his own feelings. Introspection as a scientific method is likely to give seasoned researcher another uncanny feeling. But, we must not mistaken here, Mori used his inner world for finding a theory, not for testing it. Once the world was ready, Mori’s theory turned out to be immensely useful and can be proven. Still, I argue that we have not yet fully confirmed Mori’s theory. Strictly spoken, the data of Mathur &amp; Reichling only prove that on average the effect exists. That is something, but it is much stronger to state: everyone experiences the Uncanny Valley. In essence, we could estimate the same cubic models, but one per participant. That requires more data, because the analysis of very participant requires the data from every participant. The next chapter will introduce multi-level models, which can simultaneously estimate a model on population level and participant level. At the end of that chapter, we will return to Uncanny Valley with more data to feed our chains. Spoiler alert: the Uncanny Valley effect could be universal. "],
["MLM.html", "6 Multilevel models 6.1 The Human Factor: Intercept random effects 6.2 Slope random effects: variance in change 6.3 Thinking multi-level 6.4 Testing universality of theories 6.5 Non-human populations and cross-overs 6.6 Nested random effects 6.7 What are random effects? On pooling and shrinkage 6.8 Psychometrics and designometric models", " 6 Multilevel models In the previous chapters we have seen several examples of conditional effects: groups of users responding differently to design conditions, such as font size, noise and emerging technology. Dealing with differential design effects seems straight forward: identify the relevant property, record it and add an conditional effect to the model. Identifying the relevant property is, in fact, a catch-22: how would you know what is relevant before you actually conducted the research. Researchers routinely record basic demographic properties such as age and gender, but these frequently show little effects, or the effects are obvious, i.e. not interesting. In addition, such predictors are rarely more than approximations of the properties that make the real difference. Older people have weaker vision by tendency, but the individual differences in any age group are immense. Boys tend to be more technophile, but there are some real geeky girls, too. Identifying properties that matter upfront requires careful review of past research or deep theorizing, and even then it remains guesswork. Presumably, hundreds of studies attempted to explain differences in usage patterns or performance by all sorts of psychological predictors, with often limited results. That is a big problem in design research, as variation in performance can be huge and good predictors are urgently needed. Identifying the mental origins of being fast versus slow, or motivated versus bored, is extremely useful to improve the design of systems to be more inclusive or engaging. As we will see in this chapter, individual differences can be accounted for and measured accurately without any theory of individual differences. For researchers trained in experimental social sciences it may require a bit of getting used to theory-free reasoning about effects, as it is always tempting to ask for the why. But in applied evaluation studies, what we often really need to know is by how much users vary. The key to measuring variation in a population is to create models that operate on the level of participants, in addition to the population level, for example on population level, users prefer design B over A on average (\\(\\beta_1 = 20\\)) on the participant-level, participant \\(i\\) preferred B over A (\\(\\beta_{1i} = 20\\)), \\(j\\) preferred A over B (\\(\\beta_{1j} = -5\\)), + When adding a participant-level effects, we still operate with coefficients, but in contrast to single-level linear models, every participant gets their own estimate (\\(\\beta_{1\\cdot}\\)). The key to estimating individual parameters is simply to regard participant (Part) a grouping variable on its own, and introduce it as a factor. Multi-level analysis is not limited to estimating models, and in [#thinking_ml] we will use the participant factor for producing multi-level exploratory plots. That should get you started. The subsequent two sections introduce the basics of estimating multi-level linear models, first introducing intercept-only participant-level effects [#intercept_re] and then slope (or group difference) effects [#slope_re]. Typically, fixed and random effects appear together in a linear mixed-effects model. It depends on the research question whether the researcher capitalizes on the average outcome, the variation in the population or participant-level effects. In section [#reporting_re] we will see how to report multi-level results depending on the type of research question. The participant-level is really just the factor and once it is regarded alongside the population level, a model is multi-level. However, in multi-level linear modelling we usually use a different type of factor, for the particpant level. The additional idea is that the levels of the factor, hence the individuals, are part of a population. The consequences of this perspective, will be discussed in [#re_shrinkage]: a population is a set of entities that do vary to some extent but also clump around a typical value. And that is precisely what random effects do: levels are drawn from an overarching distribution, usually the Gaussian. This distribution is estimated simultaneously to the individual parameters (\\(\\beta_{1\\cdot}\\)), with some interesting consequences. Once the concept of random effects is clear, we will see that it transfers with grace to non-human populations, such as designs, teams or questionnaire items. Three sections introduce multi-population multi-level models: In [#crossed_re] we will use a random effects model with four populations and compare their relative contribution to overall variance in performance. Section [#re_nested] will show how multiple levels can form a hierarchy and in [#re_psychometrics] we will see that multi-level models apply with grace for the development of tests, which is called psychometrics, when people are tested and which I call designometrics when a sample of designs is evaluated. Finally, we return to a more fundamental research case, the Uncanny Valley, and examine the universality of this strange effect [#universality]. 6.1 The Human Factor: Intercept random effects 6.1.0.1 REWORK Design science fundamentally deals with interaction between systems and humans. Every measure we take in a design study is an encounter of an individual with a system. As people differ in many aspects, it is likely that people differ in how they use a system. In the previous chapter we have already dealt with differences between users: in the BrowsingAB case, we compared two designs in how inclusive they are with respect to elderly users. Such a research question seeks for a definitive answer on what truly causes variation in performance. Years of age is a standard demographic variable and in experimental studies can be collected without hassle. If we start from deeper theoretical considerations than that, for example, we suspect a certain personality trait to play a significant role, it becomes more tricky. Perhaps, you need a 24-item scale to measure the construct, perhaps you first have to translate this particular questionnaire into three different languages, and perhaps you have to first invent and evaluate a scale. While seeking good explanatory variables is essential for testing theories, for applied research it is sometimes fully sufficient to simply quantify the amount of variation. At first, one might think that the grand mean model would do, take \\(\\beta_0\\) as the population mean and \\(\\sigma_\\epsilon\\) as a measure for individual variation. Unfortunately, it is not valid to take the residual variance as variance between individuals, because the error is composed of multiple sources, in particular: inter-individual variation intra-individual variation, e.g. by different levels of energy over the day variations in situations, e.g. responsiveness of the website inaccuracy of measures, e.g. misunderstanding a questionnaire item In the IPump study we have collected performance data of 25 nurses, operating a novel interface for a syringe infusion pump. Altogether, every nurse completed a set of eight tasks three times. Medical devices are high-risk systems where a single fault can cost a life. It is required that user performance is on an uniformly high level. We start the investigation with the global question: What is the average ToT in the population? attach(IPump) D_Novel %&gt;% summarize(mean_Pop = mean(ToT)) mean_Pop 16 The answer is just one number and does not refer to any individuals in the population. This is called the population-level estimate or fixed effect estimate. The following question is similar, but is grouped by the participant level: What is the average ToT of individual participants? D_Novel %&gt;% group_by(Part) %&gt;% summarize(mean_Part = mean(ToT)) %&gt;% sample_n(5) Part mean_Part 4 22.4 14 15.5 5 16.0 15 17.2 25 15.7 Such a grouped summary can be useful for situations where we want to directly compare individuals, like in performance tests. In experimental research, individual participants are of lesser interest, as they are exchangeable entities (a sample). The total effect on the population is usually of greater The amount of differences can be summarized by the standard deviation of participant-level estimates: D_Novel %&gt;% group_by(Part) %&gt;% summarize(mean_Part = mean(ToT)) %&gt;% ungroup() %&gt;% summarize(sd_Part = var(mean_Part)) sd_Part 13.8 Generally, these are the three types of parameters in multi-level models: the population-level estimate (commonly called fixed effects), the participant-level estimates (random effects) and the participant-level standard deviation. detach(IPump) Obviously, the variable Part is key to build such a model. This variable groups observations by participant identity and, formally, is a plain factor. A naive approach to multi-level modelling would be to estimate an AGM, like ToT ~ 0 + Part, grab the center estimates and compute the standard deviation. Different to the descriptive analysis above and the naive approaczh, a multi-level model estimates fixed effects, random effects and random standard deviation simultaneously. For the IPump study we can formulate a GMM model with participant-level random effect \\(\\beta_{p0}\\) as follows: \\[ \\mu_i = \\beta_0 + x_p\\beta_{p0}\\\\ \\beta_{p0} \\sim \\textrm{Gaus}(0, \\sigma_{p0})\\\\ y_i \\sim \\textrm{Gaus}(\\mu_i, \\sigma_\\epsilon) \\] There will be as many parameters \\(\\beta_{p0}\\), as there were users in the sample, and they have all become part of the likelihood. The second term describes the distribution of the levels. And finally, there is the usual random term. Before we examine further features of the model, let’s run it. In the package rstanarm, the command stan_glmer() is dedicated to estimate mixed-effects models with the extended formula syntax. attach(IPump) M_hf &lt;- stan_glmer(ToT ~ 1 + (1|Part), data = D_Novel) P_hf &lt;- posterior(M_hf) The posterior of the mixed-effects model contains four types of variables: the fixed effect captures the population average (Intercept) random effects capture how individual participants deviate from the population mean random factor variation (or group effects) captures the overall variation in the population. the residual standard deviation captures the amount of noise With the bayr package these parameters can be extracted using the respective commands: fixef(P_hf) model type fixef center lower upper M_hf fixef Intercept 16 14.6 17.5 ranef(P_hf) %&gt;% sample_n(5) re_entity center lower upper 21 -0.277 -4.23 2.21 19 0.030 -2.85 3.33 14 -0.032 -3.30 3.05 18 -0.282 -4.26 2.32 23 -0.458 -4.88 1.90 grpef(P_hf) model type fixef re_factor center lower upper M_hf grpef Intercept Part 1.5 0.077 3.76 Random effects are factors and enter the model formula just as linear terms. What sets them apart, is that they are estimated together with the overarching Gaussian distribution. To indicate that to the regression engine, a dedicated syntax is used in the model formula (recall that 1 represents the intercept parameter): (1|Part) In probability theory expressions, such as the famous Bayes theorem, the | symbol means that something to the left is conditional on something to the right. Random effects can easily be conceived as such conditional effects. Left of the | is the fixed effect that is conditional on (i.e. varies by) the factor to the right. In the simplest form the varying effect is the intercept and in the case here could be spoken of as: ToT depends on the participant you are looking at Speaking of factors: so far, we have used treatment contrasts most of the time, which represent the difference towards a reference level. Analogue to a factor model, we could estimate the first participants performance level and make it the reference group, intercept \\(\\beta_0\\). All other average scores, we would express as differences to the reference participant. This seems odd and, indeed, has two disadvantages: first, whom are we to select as the reference participant? The choice would be arbitrary, unless we wanted to compare brain sizes against the grey matter of Albert Einstein, perhaps. Second, most of the time the researcher is after the factor variation rather than differences between any two individuals, which is inconvenient to compute from treatment contrasts. The solution is to use a different contrast coding for random factors: deviation contrasts represent the individual effects as difference (\\(\\delta\\)) towards the population mean. As the population mean is represented by the respective fixed effect, we can compute the absolute individual predictions by adding the fixef effect to the random effect: data_frame(mu_i = ranef(P_hf)$center + fixef(P_hf)$center) %&gt;% ggplot(aes(x = mu_i)) + geom_histogram() Finally, we can assess the initial question: are individual differences a significant component of all variation in the experiment? Assessing the impact of variation is not as straight-forward as with fixed effects. One heuristic is to compare it against the residual variance, which is: T_sov_vc &lt;- coef(P_hf, type = c(&quot;grpef&quot;, &quot;disp&quot;)) T_sov_vc parameter type fixef re_factor center lower upper sigma_resid disp NA NA 16.4 15.489 17.38 Sigma[Part:Intercept,(Intercept)] grpef Intercept Part 1.5 0.077 3.76 detach(IPump) The variation due to individual differences is half of the noise, which is considerable. It seems in order to further investigate why and how users vary in performance, as this is the key to improving the design for all users. 6.2 Slope random effects: variance in change So far, we have dealt with Intercept random effects that capture the gross differences between participants of a sample. We introduced these random effects as conditional effects like: “the overall performance depends on what person you are looking at”. However, most research questions rather regard differences between conditions. Slope random effects, we can examine, how much individuals differ in their response to a new design. Consider case BrowsingAB, where the population averages of the designs were not that far apart. That can mean they are truly not that different. But it can also mean that some users do a lot better with A, and others with B. Which design is preferred could largely depend on the person. For an illustration od slope ramdom effects, we take a look at a data set that ships with package Lme4 (which mainly provides is a non-Bayesian engine for multi-level models). 18 participants underwent sleep deprivation on ten successive days and the average reaction time on a set of testshas been recorded per day and participant. The research question is: what is the effect of sleep deprivation on reaction time and, again, this question can be asked on population level and participant level. The participant-level plot below shows the individual relationships between days of deprivation and reaction time. For most participants a increasing straight line seems to be a good approximation, so we can go with a parsimonous linear regression model, rather than an ordered factor model. One noticeable exception is participant 352, which is fairly linear, but reaction times get shorter with sleep deprivation. (What would be the most likely explanation? Perhaps 352 is a cheater, who slept well every night and only gained experience in doing the tests). D_slpstd &lt;- lme4::sleepstudy %&gt;% select(Part = Subject, days = Days, RT = Reaction) %&gt;% mutate(days = as.integer(days)) D_slpstd %&gt;% ggplot(aes(x = days, y = RT)) + facet_wrap(~Part) + geom_point() + geom_smooth(se = F, aes(color = &quot;LOESS&quot;)) + geom_smooth(se = F, method = &quot;lm&quot;, aes(color = &quot;lm&quot;)) + labs(color = &quot;Smoothing function&quot;) A more compact way of plotting multi-level slopes is the spaghetti plot below. By superimposing the population level effect, we can clearly see that participants vary in how sleep deprivation delays the reactions. D_slpstd %&gt;% ggplot(aes(x = days, y = RT, group = Part)) + geom_smooth(aes(color = &quot;participant effects&quot;), size = .5, se = F, method = &quot;lm&quot;)+ geom_smooth(aes(group = 1, color = &quot;population effect&quot;), size = 2, se = F, method = &quot;lm&quot;) + labs(color = NULL) For a single level model, the formula would be RT ~ 1 + days, with the intercept being RT at day Zero and the coefficient days representing the change per day of sleep deprivation. The multi-level formula retains the population level and adds the participant-level term as a conditional statement: the effect depends on whom you are looking at. RT ~ 1 + days + (1 + days|Part) Remember to always put complex random effects into brackets, because the + operator has higher precedence than |. We estimate the multi-level model using the rstanarm engine. M_slpsty_1 &lt;- stan_glmer(RT ~ 1 + days + (1 + days|Part), data = D_slpstd, iter = 2000) The Bayr package provides a specialized command for multi-level tables. fixef_ml extracts the population-level estimates in CLU form and adds the participant-level standard deviation. The overall penalty for sleep deprivation is around ten milliseconds per day, with a 95% CI ranging from 7ms to 14ms. At the same time, the participant-level standard deviation is around 6.5ms, which is considerable. Based on the assumption that the central two standard deviations of a Gaussian distribution contain two-thirds of the total mass, we can expect that roughly one third of the population has a penalty of smaller than 4ms or larger than 17ms. fixef_ml(M_slpsty_1) fixef center lower upper SD_Part Intercept 251.4 237.72 265 23.11 days 10.4 7.08 14 6.58 The following plot shows the slope random effects, ordered by the center estimate. ranef(M_slpsty_1) %&gt;% filter(fixef == &quot;days&quot;) %&gt;% mutate(Part_ord = rank(center)) %&gt;% ggplot(aes(x = Part_ord, ymin = lower, y = center, ymax = upper)) + geom_crossbar() The multi-level regression model is mathematically specified as follows. Note how random coefficients \\(\\beta_{.(Part)}\\) are drawn from a Gaussian distribution with their own standard deviation, very similar to the errors \\(\\epsilon_i\\). \\[ y_i = \\mu_i + \\epsilon_i\\\\ \\mu_i = \\beta_0 + \\beta_{0(Part)} + x_1 \\beta_1 + x_{1}\\beta_{1(Part)}\\\\ \\beta_{0(Part))} \\sim \\textrm{Gaus}(0,\\sigma_{0(Part)})\\\\ \\beta_{1(Part))} \\sim \\textrm{Gaus}(0,\\sigma_{1(Part)})\\\\ \\epsilon_i = \\textrm{Gaus}(0, \\sigma_\\epsilon) \\] The second line can also be written as: \\[ \\mu_i = \\beta_0 + \\beta_{0(Part)} + x_1 (\\beta_1 + x_{1}\\beta_{1(Part)})\\\\ \\] which underlines that random coefficients are additive correction terms to the population-level effect. Whereas the ranef command reports only these corrections, it is sometimes useful to look at the total scores per participant. In package Bayr, the command re_scores computes total scores on the level of the posterior distribution. The following plot uses this command and plots the distribution. posterior(M_slpsty_1) %&gt;% re_scores() %&gt;% clu() %&gt;% ggplot(aes(x = center)) + facet_grid(~fixef, scales = &quot;free&quot;) + geom_density() 6.3 Thinking multi-level There is a lot of confusion about the type of models that we deal with in this chapter. They have also been called hierarchical models or mixed effects models. The “mixed” stands for a mixture of so called fixed effects and random effects. The problem is: if you start by understanding what fixed effects and random effects are, confusion is programmed, not only because there exist several very different definitions. In fact, it does not matter so much whether an estimate is a fixed effect or random effect. As we will see, you can construct a multi-level model by using just plain descriptive summaries. What matters is that a model contains estimates on population level and on participant level. The benefit is, that a multi-level model can answer the same question for the population as a whole and for every single participant. For enetering the world of multi-level modelling, we do not need fancy tools. More important is to start thinking multi-level on a familiar example: the IPump case. A novel syringe infusion pump design has been tested against a legacy design by letting trained nurses complete a series of eight tasks. Every nurse repeated the series three times on both designs. Time-on-task was measured and the primary research question is: Does the novel design lead to faster execution of tasks? attach(IPump) To answer this question, we can compare the two group means using a basic CGM: M_cgm &lt;- D_pumps %&gt;% stan_glm(ToT ~ 1 + Design, data = .) fixef(M_cgm) fixef center lower upper Intercept 27.9 25.8 29.8 DesignNovel -11.9 -14.6 -9.1 This model is a single-level model. It takes all observations as “from the population” and estimates the means in both groups. It further predicts that with this population of users, the novel design is faster on average, that means taking the whole population into account, (and forgetting about individuals). An average benefit sounds promising, but we should be clear what it precisely means, or better what it does not mean: That there is a benefit for the population does not imply, that every individual user has precisely that benefit. It does not even imply that every user has a benefit at all. In extreme case, a small subgroup could be negatively affected by the novel design, but this could still result in a positive result on average. In the evaluation of high-risk devices like infusion pumps concerns about individual performance are real and this is why we designed the study with within-subject conditions, which allows to estimate the same model on population level and participant level. The following code produces a multi-level descriptive model. First, a summary on participant level is calculated, then it is summarized to obtain the population level. By putting both summaries into one figure, we are doing a multi-level analysis. T_Part &lt;- D_pumps %&gt;% group_by(Part, Design) %&gt;% summarize(mean_Part = mean(ToT)) T_Pop &lt;- T_Part %&gt;% group_by(Design) %&gt;% summarize(mean_Pop = mean(mean_Part)) gridExtra::grid.arrange(nrow = 1, T_Pop %&gt;% ggplot(aes(x = Design, group = NA, y = mean_Pop)) + geom_point() + geom_line() + ggtitle(&quot;Population-level model (average benefits)&quot;) + ylim(0, 60), T_Part %&gt;% ggplot(aes(x = Design, y = mean_Part, group = Part, label = Part)) + geom_line() + ggrepel::geom_label_repel(size = 3, alpha = .5) + ggtitle(&quot;Participant-level model (individual benefits)&quot;) + ylim(0, 60)) Note how with gridExtra::grid.arrange() we can multiple plots into a grid, which is more flexible than using facetting that ggrepel::geom_label_repel produces non-overlapping labels in plots This is a full multi-level analysis, as it shows the same effect on two different levels alongside. In this case, the participant-level part removes all worries about the novel design. With the one small exception of participant 3, all users had net benefit from using the novel design and we can call the novel design universally better. In addition, some users (4, 8 and 9) seem to have experienced catastrophes with the legacy design, but their difficulties disappear when they switch to the novel design. If you look again at the participant-level spaghetti plot and find it similar to what you have seen before, you are right: This is an design-by-participant conditional plot. Recall, that conditional effects represent the change of outcome, depending on another factor. In this multi-level model, this second factor simply Part(icipant). That suggests that it is well within reach of plain linear models to estimate design-by-participant conditional effects. Just for the purpose of demonstration, we can estimate a population level model, conditioning the design effect on participants. Ideally, we would use a parametrization giving us separate Intercept and DesignNovel effects per participant, but the formula interface is not flexible enough and we would have to work with dummy variable expansion. Since this is just a demonstration before we move on to the multi-level formula extensions, I use an AMM instead. A plain linear model can only hold one level at a time, which is why we have to estimate the two separate models for population and participant levels. Then we merge the posterior objects, produce a combined CLU table for plotting. M_amm_pop &lt;- D_pumps %&gt;% stan_glm(ToT ~ 0 + Design, data = ., iter = 50, chains = 2) M_amm_part &lt;- D_pumps %&gt;% stan_glm(ToT ~ (0 + Design):Part, data = ., iter = 50, chains = 2) T_amm &lt;- bind_rows(posterior(M_amm_pop), posterior(M_amm_part)) %&gt;% fixef() %&gt;% separate(fixef, into = c(&quot;Design&quot;, &quot;Part&quot;)) T_amm %&gt;% ggplot(aes(x = Design, y = center, group = Part, color = model)) + geom_line() In the first place, the convenience of (true) multi-level models is that both (or more) levels are specified and estimated as one model. For the multi-level models that follow, we will use a specialized engine, stan_glmer() (generalized mixed-effects regression) that estimates both levels simultaneously and produce multi-level coefficients. The multi-level CGM we desire is written like this: M_mlcgm &lt;- D_pumps %&gt;% stan_glmer(ToT ~ 1 + Design + (1 + Design|Part), data = ., iter = 100) In the formula of this multi-level CGM the predictor term (1 + Design) is just copied. The first instance is the usual population-level averages, but the second is on participant-level. The | operator in probability theory means “conditional upon” and here this can be read as effect of Design conditional on participant. For linear models we have been using the coef() command to extract all coefficients. Here it would extract all coefficients on both levels. With multi-level models, two specialized command exist to separate the levels: we can extract population-level effects using the fixef() command (for “fixef effects”). All lower level effects can be accessed with the ranef command, which stands for random effects. Here, the population level coefficients are absolute means, whereas random effects are not. Usually, random effects are differences towards the population-level. This is why random effects are always centered at zero. In the following histogram, the distribution of the DesignNovel random effects are shown. This is how much users deviate from the average effect in the population. fixef(M_mlcgm) fixef center lower upper Intercept 28.1 24.4 31.6 DesignNovel -12.1 -15.0 -8.3 ranef(M_mlcgm) %&gt;% rename(Part = re_entity, `deviation` = center) %&gt;% ggplot(aes(x = deviation)) + facet_grid(~fixef) + geom_histogram() The distribution of random effects should resemble a Gaussian distribution. It is usually hard to tell with such small sample sizes, but it seems that the Intercept effects have a left skew. As we will see in chapter [#GLM], this problem is not surprisung and can be resolved. The distributions are also centered at zero, which is not a coincidence, but the way random effects are designed: deviations from the population mean. That opens up two interesting perspectives: first, random effects look a lot like residuals [#residuals], and like those we can summarize a random effects vector by its standard deviation, using the grpef command from package Bayr. bayr::fixef_ml(M_mlcgm) fixef center lower upper SD_Part Intercept 28.1 24.4 31.6 8.45 DesignNovel -12.1 -15.0 -8.3 6.07 Most design research is located on the population level. We want to know how a design works, broadly. Sometimes, stratified samples are used to look for conditional effects in (still broad) subgroups. Reporting individual differences makes little sense in such situations. The standard deviation summarizes individual differences and can be interpreted the degree of diversity. The command bayr::fixef_ml is implementing this by simply attaching the standard deviation center estimates to the respective population-level effect. As coefficients and standard deviations are on the same scale, they can be compared. Roughly speaking, a two-thirds of the population is contained in an interval twice as large as the SD. fixef_ml(M_mlcgm) fixef center lower upper SD_Part Intercept 28.1 24.4 31.6 8.45 DesignNovel -12.1 -15.0 -8.3 6.07 That having said, I believe that more researchers should watch their participant levels more closely. Later, e will look at two specific situations: psychometric models have the purpose of measuring individuals [#psychometrics] and those who propose universal theories (i.e., about people per se) must also show that their predictions hold for each and everyone [#universality]. detach(IPump) 6.3.0.1 COMPILES TO THIS POINT 6.4 Testing universality of theories Often, the applied researcher is primarily interested in a population-level effect, as this shows the average expected benefit. If you run a webshop, your returns are exchangeable. One customer lost can be compensated by gaining a new one. In such cases, it suffices to report the random effects standard deviation. If user performance varies strongly, this can readily be seen in this one number. In at least two research situations, going for the average is just not enough: when testing hazardous equipment and when testing theories. In safety critical research, such as a medical infusion pump, the rules are different than for a webshop. The rules are non-compensatory, as the benefit of extreme high performance on one patient cannot compensate the costs associated with a single fatal error on another patient. For this asymmetry, the design of such a system must enforce a robust performance, with no catastrophes. The multi-level analysis of the infusion pumps in [#thinking-multi-level] is an example. It demonstrated that practically all nurses will have a benefit from the novel design. The other area where on-average is not enough, is theory-driven experimental research. Fundamental behavioural researchers are routinely putting together theories on The Human Mind and try to challenge these theories. For example the Uncanny Valley effect [#rollercoaster]: one social psychologist’s theory could be that the Uncanny Valley effect is caused by religious belief, whereas a cognitive psychologist could suggest that the effect is caused by a category confusion on a fundamental processing level (seeing faces). Both theories make universal statements, about all human beings. Universal statements can never be proven, but can be is tested by finding counter-evidence. If there is one participant who is provenly non-religious, but falls into the Uncanny Valley, our social psychologist would be proven wrong. If there is a single participant at all, who does not fall for the Uncanny Valley, the cognitive psychologist was wrong. Obviously, this counter-evidence can only be found on participant level. In some way, the situation is analog to robustness. The logic of universal statements is that they are false if there is one participant who breaks the pattern, and there is no compensation possible. Unfortunately, the majority of fundamental behavioural researchers, have ignored this simple logic and still report population-level estimates when testing universal theories. In my opinion, all these studies should not be trusted, before a multi-level analysis shows that that the pattern exists on participant level. In [#rollercoaster], the Uncanny Valley effect has been demonstrated on population level. This is good enough, if we just want to confirm the Uncanny Valley effect as an observation, something that frequently happens, but not necessarily for everyone. The sample in our consisted of mainly students and their closer social network. It is almost certain, that many of the tested persons were religious and others were atheists. If the religious-attitude theory is correct, we would expect to see the Uncanny Valley in several participants, but not in all. If the category confusion theory is correct, we would expect all participants to fall into the valley. The following model performs the polynomial analysis as before [#rollercoaster], but multi-level: attach(Uncanny) M_poly_3_ml &lt;- RK_1 %&gt;% stan_glmer(response ~ 1 + huMech1 + huMech2 + huMech3 + (1 + huMech1 + huMech2 + huMech3|Part), data = ., iter = 2500) P_poly_3_ml &lt;- posterior(M_poly_3_ml) PP_poly_3_ml &lt;- post_pred(M_poly_3_ml, thin = 5) One method for testing universality is to extract the fitted responses (predict) and perform a visual examination: can we see a valley for every participant? T_pred &lt;- RK_1 %&gt;% mutate(M_poly_3_ml = predict(PP_poly_3_ml)$center) T_pred %&gt;% ggplot(aes(x = huMech, y = M_poly_3_ml, group = Part)) + geom_smooth(se = F) This spaghetti plot broadly confirms, that all participants experience the Uncanny Valley. For a more detailedanalysis, a facetted plot would be better suited, allowing to inspect the curves case-by-case. We proceed directly to a more formal method of testing universality: In [#test_statistic] we have seen how the posterior distributions of shoulder and trough can be first derived and then used to give a more definitive answer on the shape of the polynomial. It was argued that the unique pattern of the Uncanny Valley is to have a shoulder left of a trough. These two properties can be checked by identifying the stationary points. The proportion of MCMC iterations that fulfill these properties can is evidence that the effect exists. For testing universality of the effect, we just have to run the same analysis on participant-level. Since the participant-level effects are deviations from the population-level effect, we first have to add the population level effect to the random effects (using the Bayr command re_scores), which creates absolute polynomial coefficients. The two command trough and shoulder from package Uncanny ((github.com/schmettow/uncanny)[http://github.com/schmettow/uncanny]) require a matrix of coefficients, which is done by spreading out the posterior distribution table. T_univ_uncanny &lt;- P_poly_3_ml %&gt;% re_scores() %&gt;% select(iter, Part = re_entity, fixef, value) %&gt;% tidyr::spread(key = &quot;fixef&quot;, value = &quot;value&quot;) %&gt;% select(iter, Part, huMech0 = Intercept, huMech1:huMech3) %&gt;% mutate(trough = uncanny::trough(select(.,huMech0:huMech3)), shoulder = uncanny::shoulder(select(.,huMech0:huMech3)), has_trough = !is.na(trough), has_shoulder = !is.na(shoulder), shoulder_left = trough &gt; shoulder, is_uncanny = has_trough &amp; has_shoulder &amp; shoulder_left) T_univ_uncanny %&gt;% group_by(Part) %&gt;% summarize(prob_uncanny = mean(is_uncanny), prob_trough = mean(has_trough), prob_shoulder = mean(has_shoulder)) %&gt;% ggplot(aes(x = Part, y = prob_uncanny)) + geom_col() + geom_label(aes(label = prob_uncanny)) + theme(axis.text.x = element_text(angle = 45)) The above plot shows the probability that a participant experiences the Uncanny Valley, as defined by polynomial stationary points. Everyone in our sample experienced the Uncanny Valley effect. Every single MCMC step (remember, every step is a complete polynomial) is positive. This complete absence of counter-evidence may raise suspicions. If this is all based on a random walk, we should at least see a few deviations. The reason for that is simply that the number of MCMC runs puts a limit on the resolution. If we increase the number of iterations enough, we would eventually see few deviant sample drop and measure the tiny chance that a participant does not fall for the Uncanny Valley. detach(Uncanny) While this is great news for all scientists who think that the Uncanny Valley effect is innate (rather than cultural), it does not demonstrate the actual identification of deviant participants. Therefore, we briefly re-visit case Sleepstudy, for which we have estimated a multi-level linear regression model to render individual detoriation of reaction time as result of sleep deprivation. By visual inspection, we identified a single deviant participant who showed an improvement over time. However, the fitted lines a based on point estimates, only (the median of the posterior). Using the same technique as above, it is possible to calculate the participant-level probabilities for the slope being positive, as expected. attach(Sleepstudy) P_scores &lt;- M_slpsty_1 %&gt;% posterior() %&gt;% re_scores() %&gt;% mutate(Part = re_entity) P_scores %&gt;% filter(fixef == &quot;days&quot;) %&gt;% group_by(Part) %&gt;% summarize(prob_positive = mean(value &gt;= 0)) %&gt;% mutate(label = str_c(100 * round(prob_positive, 4), &quot;%&quot;)) %&gt;% ggplot(aes(x = Part, y = prob_positive)) + geom_col() + geom_label(aes(label = label), vjust = 1) + theme(axis.text.x = element_text(angle = 45)) All, but participants 309 and 335 almost certainly have positive slopes, experiencing a detoriation of reaction time. Participant 335 we had identified earlier by visual inspection. Now, that we account for the full posterior distribution, it seems a little less suspicious. Basically, the model is almost completely undecisive whether this is a positive and negative effect. The following plot shows all the possible slopes the MCMC random walk has explored. While participant 335 clearly is an outlier, there is no reason to get too excited and call him or her a true counter-example from the rule that sleep deprivation reduced performance. P_scores %&gt;% as_tibble() %&gt;% filter(Part == 335) %&gt;% ggplot() + xlim(0.5, 9) + ylim(-50, 50) + geom_abline(aes(intercept = 0, slope = value), alpha = .01) detach(Sleepstudy) That being said, the method of posterior-based test statistics can also be used for analysis of existence. In the Sleepstudy case a hypothetical question of existence would be that there exist persons who are completelyunsensitive to sleep deprivation. Why not? Recently, I saw a documentary about a guy who could touch charged electric wires, because due to a rare genetic deviation, his skin had no sweat glants. Universal statements can only be falsified by a counter-example. Statements of existence can be proven by just a single case. For example, in the 1980 dyslexia became more widely recognized as a defined condition. Many parents finally got an explanation for the problems their kids experienced in school. Many teachers complained that many parents would just seek cheap excuses for their lesser gifted offsprings. And some people argued that dyslexia does not exist and that the disability to read is just a manifestation of lower intelligence. According to the logic of existence, a single person with otherwise good functioning, but slow in learning how to read suffices to proof dyslexia. These have been found in the meantime. 6.5 Non-human populations and cross-overs With multi-level models design researchers can examine how a design affects the population of users as a whole, as well as on individual level. If there is little variation between users, it can be concluded that the effect is uniform in the population of users. In this section we will generalize the term population and extend the application of multi-level modelling to other types of research entities, such as designs, questionnaire items and tasks. Many studies in, what one could call fundamental design research seek to uncover general laws of design that may guide future system development. Fundamental design research is not concerned with choosing between individual designs, whether they are good enough or not, but with seperating the population of possible designs into good ones and bad ones by universal statements, such as “For informational websites, broad navigation structures are better than deep ones”. Note how this statement speaks of designs (not users) in an unspecified plural. It is framed as a universal law for designs. Comparative evaluation studies, such as the IPump case, are not adequate to answer such questions, simply because you cannot generalize from a sample of two to all possible designs. This is only possible under strict constraints, namely that the two designs under investigation only differ in one design property. For example two versions of a website present the same web content in a deep versus a wide hierarchy, but layout, functionality are held constant. And even then, we should be very careful with conclusions, because there can be interaction effects. For example, the rules could be different for a website used by expert users. If the research question is universal, i.e. aiming at general conclusions on all designs (of a class), it is inevitable to see designs as a population from which we collect a sample. The term population suggests a larger set of entities, and in fact many application domains have an abundance of existing designs and a universe of possible designs. Just to name a few: there exist dozens of note taking apps for mobile devices and hundreds of different jump’n’run games. Several classes of websites count in the ten thousands, such as webshops and municipal websites. Whereas we can define classes in any way and for everything we want, the term population, in a statistical not biological sense, has a stronger implication. A population contains individuals and these individuals vary only to some extent. At the same time, it is implied that we can identify some sort of typical value for a population, such that most individuals are clumped around this typical value. Essentially, if it looks similar to one of the basic statistical distributions [#stat_dist], we can call it a population. To illustrate the difference between a class and a population. Vehicles are a class of objects that transport people or goods. This broad definition covers many types of vehicles, including bicycles, rikshas, cars, buses, trucks and container vessels. If the attribut under question is the weight, we will see a distribution spreading from a 10 kilograms up to 100 tons. That is a a scale of 1:10.000 and the distribution would spread out like butter on a warm toast. Formally, we can calculate the average weight of a vehicle, but that would in no way be a typical value. It does not stop here. While design research is clearly about the user-design encounter, other research entities exist that we could call a population. The most routinely used non-human populations in design research besides designs are tasks, situations and questionnaire items. We briefly characterize the latter three and proceed to a case study that employs three populations at once. Tasks: Modern informational websites contain thousands of information pieces and finding every one of those can be considered a task. At the same time, it is impossible to cover them all in one study, such that sampling a few is inevitable. We will never be able to tell the performance of every task, but using random effects it is possible to estimate the variance of tasks. Is that valuable information? Probably it is in many cases, as we would not easily accept a design that prioritizes on a few items at the expense of many others, which are extremely hard to find. Situations: With the emerge of the web, practically everyone started using computers to find information. With the more recent breakthrough in mobile computing, everyone is doing everything using a computer in almost every situation. People chat, listen to music and play games during meals, classes and while watching television. They let themselves being lulled into sleep, which is tracked and interrupted by smart alarms. Smartphones are being used on trains, while driving cars and bikes, however dangerous that might be, and not even the most private situations are spared. Does the usability of messaging, navigation and e-commerce apps generalize across situations? Again, a study to examine performance across situations would best sample from a population of situations. Questionnaire items: Most design researchers cannot refrain from using questionnaires to evaluate certain elucive aspects of their designs. A well constructed rating scale consists of a set of items that trigger similar responses. At the same time, it is desireable that items are unsimilar to a degree, as that establishes good discrimination across a wide range. In ability tests, for example to assess people’s intelligence or math skills, test items are constructed to vary in difficulty. The more ability a person has, the more likely will a very difficult item be solved correctly. In design research, rating scales cover concepts such as perceived mental workload, perceived usability, beauty or trustworthiness. Items of such scales differ in how extreme the proposition is, like the following three items that could belong to a scale for aesthetic perception: The design is not particularly ugly. The design is pretty. The design is a piece of art. For any design it is rather difficult to get a positive response on item 3, whereas item 1 is little of a hurdle. So, if one thinks of all possible propositions about beauty, any scale is composed of a sample from the population of beauty propositions. If we look beyond design research, an abundance of non-human populations can be found in other research disciplines, such as: products in consumer research phonemes in psycholinguistics test items in psychometrics pictures of faces in face recognition research patches of land in agricultural studies In all these cases it is useful (if not inevitable) to ask multi-level questions not just on the human population, but on the encounter of multiple populations. In research on a single or few designs, such as in A/B testing, designs are usually thought of (and modelled) as common fixed-effects factors. However, when the research question is more fundamental, such that it regards a whole class of designs, it is more useful to think of designs as a population and draw a sample. In the next secion we will see an example, where a sample of users encounters a sample of designs and tasks. In experimental design research, the reserach question often regards a whole class of designs and it inevitable (although often done wrong) to view designs as a population. As we usually want to generalize across users, that is another sample. A basic experimental setup would be to have every user rate (or do a task) on every design, which is called a complete (experimental) design, but I prefer to think of it as a complete encounter. Every measure is an encounter of one participant and one design. If a multi-item rating scale is used, measures are an encounter between three populations. Every measure combines the impact from three members from these populations. With a single measure, the impact factors are inseparable. But if we have many measures, we can apply a cross-classified multi-level model (CRMM). An intercept-only CRMM just adds intercept random effects for every population. As we will see in [#designometrix], the individual random coefficients of a CRMM can be used for psychometric evaluation of rating scales. In the following example, the question is a comparison of diversity across populations. Three decades ago, Dennis Egan published one of the first papers on individual differences in computer systems design and made the following claim: ‘differences among people usually account for much more variability in performance than differences in system designs’ 1 What is interesting about this research question is that it does not speak about effects, but about variability of effects and seeks to compare variability of two totally different populations. In the following we will see how this claim can be tested by measuring multiple encounters between designs and users, apply an intercept-only CRMM and compare the random factor standard deviations. Egan’s claim has been cited in many papers that regarded individual differences and we were wondering how it would turn out in the third millenium, with the probably most abundant of all application domains: informational websites. For the convenience, we chose the user population of student users, with ten unversity websites as our design sample. Furthermore, ten representative tasks on such websites were selected, which is another population. During the experiment, all 41 participants completed 10 information search items such as: On website [utwente.nl] find the [program schedule Biology]. attach(Egan) D_egan &lt;- D_egan %&gt;% mutate(logToT = log(ToT)) D_egan %&gt;% as_tbl_obs() Note that ToT has been log-transformed for compliance with the assumptions of Linear Models. Generally, the advice is to use a Generalized Linear Model instead [#GLM, #exgaussian]. Egan’s claim is a two-way encounter to which we added the tasks as a third population. However, our data seemed to require a fourth random effect, which essentially is an interaction effect between tasks and websites: how easy a task is, largely depends on the webite where it is carried out. For example, one university website could present the library on the homepage, whereas another websites hides it deep in its navigation structure. The following grid of histogram shows the marginal distributions of human and non-human populations. The individual plots were created using the following code template: D_egan %&gt;% group_by(Part) %&gt;% summarize(avg_logToT = mean(logToT)) %&gt;% ggplot(aes(x = avg_logToT)) + geom_histogram() + labs(title = &quot;distribution of participant average log-times&quot;) + xlim(1.5,6.5) There seems to be substantial variation between participants, tasks and items, but very little variation in designs. We build a GMM for the encounter of the four populations. M_1 &lt;- D_egan %&gt;% stan_glmer(logToT ~ 1 + (1|Part) + (1|Design) + (1|Task) + (1|Design:Task),data = ., iter = 100) P_1 &lt;- posterior(M_1) A Bayesian multi-level model estimates the standard deviation alongside with coefficients, such that we can compare magnitude and certainty of variability. In addition, we can always compare a random factor standard deviation to the standard error. P_1 %&gt;% filter(type == &quot;grpef&quot; | type == &quot;disp&quot;) %&gt;% mutate(re_factor = if_else(type == &quot;disp&quot;, &quot;Obs&quot;, re_factor), # re_factor = factor(re_factor, # levels = c(&quot;Obs&quot;, &quot;Part&quot;, &quot;Design&quot;, &quot;Task&quot;, &quot;Design:Task&quot;)), re_factor = mascutils::reorder_levels(re_factor, c(4, 2, 5, 3, 1))) %&gt;% ggplot(aes(x = value)) + geom_density() + labs(x = &quot;random effect standard deviation&quot;) + facet_grid(re_factor~.) The outcomes of our study are indecisive regarding Egan’s claim. Variance of participants is stronger thnan variance of designs, but not by much. Both factors also produce much less variability in measures than does the noise, which we here regard a observation-level random effect. Tasks seem to have the overall strongest effect, but this comes with huge uncertainty. The strongest variability is found in the sample of Design-Task pairs, which is interesting A secondary observation on the posterior plot is that some effects are rather certain, such as Obs and Design:Task, whereas others are extremely uncertain, especially Task. There is a partial explaination for this: the variation is estimated from the “heads” in thehuman or non-human population. It therefore strongly depends on respective sample size. Design and Task have a meager \\(N = 10\\), which is why the estimate are so uncertain. With \\(N = 41\\) the participant level estimate has more data and reaches better certainty, same for the pairs (\\(N = 100\\)). The observations level can employ to all 410 observations, resulting in a highly certain estimate. We proceed with a formal check of Egans claim, using the same technique as in [#test_stat, #universlity]. What is the probability, that Egan is right? We create a Boolean variable and summarize the proportion of MCMC draws, where \\(\\sigma_ \\textrm{Part} &gt; \\sigma_ \\textrm{Design}\\) holds. C_Egan_is_right &lt;- P_1 %&gt;% filter(type == &quot;grpef&quot;, re_factor %in% c(&quot;Part&quot;, &quot;Design&quot;)) %&gt;% select(chain, iter, re_factor, value) %&gt;% spread(re_factor, value) %&gt;% summarize(Prop_Egan_is_right = mean(Part &gt; Design)) %&gt;% c() C_Egan_is_right ## $Prop_Egan_is_right ## [1] 0.944 A chance of \\(0.944\\) can count as good evidence in favour of Egan’s claim, although it certainly does not match the “much more” in the original quote. However, if we take the strong and certain Design:Task effect into account, the claim could even be reversed. Apparently, the difficulty of a task depends on the design, that means, it depends on where this particular designer has placed an item in the navigation structure. That clearly is a design feature and therefore counts as strong counter-evidence. detach(Egan) 6.6 Nested random effects 6.6.0.1 INTRO Nested random effects (NREs) represent nested sampling schemes. A classic example is from educational research: a sample of schools is drawn and inside every school a sample of students is selected. Like cross-classified models, nested models consist of multiple levels. The difference is that if one knows the lowest (or: a lower) level of an observation, the next higher level is unambiguous, like: every student is in exactly one class every participant is from exactly one professional group As we have seen above, cross-classified models play a primary role in design research, due to the user/task/design encounter. NREs are more common in research disciplines where organisation structures or geography plays a role, such as education science (think of the international school comparison studies PISA and TIMMS), organisational psychology or political science. One examples of a nested sampling structure in design research is the CUE8 study, which is the eighth instance of Comparative Usability Evaluation (CUE) studies, pioneered by Rolf Molich [CUE8]. Different to what the name might suggest, not designs are under investigation in CUE, but usability professionals. The over-arching question in the CUE series is the performance and reliability of usability professionals when evaluating designs. Earlier studies sometimes came to devastating results regarding consistency across professional groups when it comes to identifying and reporting usability problems. We always knew, qualitative analysis is much harder to do in an objective way, did we not? The CUE8 study lowered the bar, by asking whether several professional groups will arrive at consistent measures for time-on-task. The CUE8 study measured time-on-task in usability tests, which had been conducted by 14 different teams. The original research question was: How reliable are time-on-task measures across teams? All teams used the same website (a car rental company) and the same set of tasks. All teams did moderated or remote testing (or both) and recruited their own sample of participants. So, the analysis can performed on three levels: the population level would tell us the overall performance on this website. That could be interesting for the company running it. Below that are the teams and asking how they vary is the primary research question. At the same time, participants make another level of analysis, but every participant encounters only a single team. This is why it is called nested. If the original research question is on the consistency across teams, we can readily take the random effect variance as a measure for the opposite: when variance is high, consietsncy is low. But, how low is low? It is difficult to come up with an absolute standard for inter-team reliability. Because we also have the participant-level, we can resort to a relative standard: how does the variation between teams compare to variation between individual participants? Under this perspective, we examine the data. This time, we have real time-on-task data and as so often, it is highly skewed. Again, we use the trick of logarithmic transformation to obtain a more symmetric distribution of residuals. The downside is that the outcome variable may not be zero. For time-on-task data this is not an issue. In fact, the original CUE8 data set contains several observations with unrealistically low times. Before proceeding to the model, we explore the original variable ToT on the two levels (Participant and Team): In the following code the mean ToT is computed for the two levels of analysis, participants and teams and shown in ascending order. attach(CUE8) D_cue8 D_part_mean &lt;- D_cue8 %&gt;% group_by(Part, Condition) %&gt;% summarize(mean_ToT = mean(ToT), na.rm = T, n_Obs = n()) %&gt;% ungroup() %&gt;% rename(Unit = Part) %&gt;% mutate(percentile = percent_rank(mean_ToT), Level = &quot;Part&quot;) D_team_mean &lt;- D_cue8 %&gt;% group_by(Team, Condition) %&gt;% summarize(mean_ToT = mean(ToT, na.rm = T), n_Obs = n()) %&gt;% ungroup() %&gt;% rename(Unit = Team) %&gt;% mutate(percentile = percent_rank(mean_ToT), Level = &quot;Team&quot;) D_task_mean &lt;- D_cue8 %&gt;% group_by(Task, Condition) %&gt;% summarize(mean_ToT = mean(ToT, na.rm = T), n_Obs = n()) %&gt;% ungroup() %&gt;% rename(Unit = Task) %&gt;% mutate(percentile = percent_rank(mean_ToT), Level = &quot;Task&quot;) bind_rows(D_team_mean, D_part_mean) %&gt;% ggplot(aes(x = percentile, y = mean_ToT, col = Level, shape = Level, size = Level)) + geom_point() detach(CUE8) 6.6.0.2 EDIT 6.6.0.3 ALIGN It seems there is ample variation in ToT for participants, with mean ToT ranging from below 100 to almost 500 seconds. There is considerable variation in on team level, too, only the range is less. This observation is congruent with the law of small numbers, which will be discussed in the following section. The right model is easily specified with the familiar syntax. In fact, it is not even necessary to specify that participants are nested within teams. The only requirement is that participant identifiers are unique across the whole study (not just within a team unit). The model below contains another feature of ths CUE8 study. Participants were tested in one of two conditions: moderated and remote testing sessions. Mixing fixed and random effects, we examine all influences simultaneously. Note that this treatment is between-subject for participants. There is also just one team that did both conditions. For specifying hierarchical random effects we can use the same syntax as before, if all participants have a unique identifier. If the participant identifier is only unique within a team, it is either to be recoded, e.g. mutate(Part = str_c(Team, Part)), or one uses the nesting operator Part/Team. attach(CUE8) M_1 &lt;- D_cue8 %&gt;% stan_lmer(logToT ~ Condition + (1|Part) + (1|Team), data = .) # logToT ~ Condition + 1|Part/Team P_1 &lt;- posterior(M_1) After running the model, we compare the sources of variation. Recall, that LRMs assume that the ramdom effects are drawn from a Gaussian distribution and the amount of systematic variation is represented by \\(\\sigma\\). As this is precisely the way, Gaussian distributed errors are represented, we can actually distinguish three levels of variation: participants, team and residuals, which is nothing else, but an observation-level random effect (OLRE) . Here, the remaining noise can serve as a baseline for quantifying the diversity. In conjunction with Generalized Linear Models, we will re-encounter this idea and use it to model over-dispersion. The table below tells that the strongest variation is still on observation-level, i.e. noise. Almost the same amount of variation is due to teams. And, in contrast to what the exploratory analysis suggested, the variation due to teams is considerably smaller than participant-level variation. clu(P_1, type = c(&quot;grpef&quot;, &quot;disp&quot;)) parameter type fixef re_factor center lower upper sigma_resid disp NA NA 0.611 0.592 0.632 Sigma[Part:Intercept,(Intercept)] grpef Intercept Part 0.427 0.388 0.468 Sigma[Team:Intercept,(Intercept)] grpef Intercept Team 0.589 0.405 0.940 It is hard to deny that, at least for consumer systems, people vary greatly in performance. That is the whole point about universal design. However, the discordance between professional teams is also concerning. And that arises after controlling for an experimental factor, remote or moderated. By the way, the difference between moderated and remote testing is \\(0.33 [-0.31, 1.05]_{CI95}\\). The fixed effect is rather weak and highly uncertain. T_fixef &lt;- fixef(P_1) T_fixef fixef center lower upper Intercept 4.614 4.093 5.10 Conditionmoderated 0.331 -0.311 1.05 detach(CUE8) 6.7 What are random effects? On pooling and shrinkage At least half a dozen of defintions exist for the term random effect. This is so confusing that some authors refrain to use the term altogether. Here, the definition is conceptually based on the idea of a population. Technically, it is compatible with the implementation found in rstanarm and other engines. Unfortunately, the very terms random effect and random factor are highly misleading, as there is nothing more or less random in a random factors as compared to fixed factors. The opposite is the case: as we have seen above, a random effects model pulls a variance component from the random term and explains it by assigning coefficients to entities (teams or users). The best advice is to not contemplate over what makes a factor random. It is just a name and because random factors are so amazingly useful, they should be called fonzy factors, instead. When a data set contains a factor that we may wish to add to the model, the question is: fixed effect or random effect? Above, I have introduced the heuristic of populations. If one can conceive tasks, designs, or whatever set of items as a population, there is clumping to some degree, but also variation. The more clumping there is, the better is the guess for unobserved members by observing some members. In such a case, the predictor is introduced as a fonzy factor. Now it is time to more formally conceive when a set of things is a population. Obviously, we would never speak of a population, when the objects of interest are from different classes. Entities gathering on super market parking lots, like persons, cars, baskets and and dropped brochures, we would never see as a population. People, we would generally see as a population, as long as what we want to observe is somewhat homogenous. When the question is, how fast persons can do a 2000 meter run at the olympic games, we would certainly want one population per discipline (swimming, running, etc). Why is that so? It is because we expect members of a population to have some similarity, with the consequence that, if you already have observed some members of the population, this tells you something about any unobserved members. Reconsider the Bayesian principle of prior knowledge by an experiment of thought: Consider, a UX expert with experience in e-commerce is asked to estimate how long it takes users to do the checkout, without being shown the system. The expert will probably hesitate briefly, and then come up with an estimate of, let’s say, 45 seconds. Without any data, the expert made some reasonable assumptions, e.g. that a disciplined design process has been followed, and then relies on experience. The experts personal experience has formed prior to the study by observing many other cases. Now, we confront the expert with an even more bizzare situation: guess the time-on-task for an unknown task with an unseen system of unknown type! The expert will probably refuse to give an answer, arguing that some systems have tasks in the millisecond range (e.g. starting a stopwatch), whereas other processes easily run for hours or days (e.g. doing data exploration). This is agreeable, and we provide the expert with average ToT of four other tasks within the same system: \\(ToT_{1-4} = {23, 45, 66, 54}\\) Now, the expert is confident that ToT be around 50 seconds and that is probably a good guess. What has happened is that prior belief about the unkown task parameter has been formed not externally, but by data as it arrived. The likely value of one unit has been learned from the other units and this appears pretty reasonable. The same principle applies when removing outliers. When staring at a boxplot or scatterplot the mind of the observer forms a gestalt that covers the salient features of data, for example: almost all points are located in the range 100 - 500. Once this pattern has formed, deviant points stand out. However, the salience of the gestalt may vary. Consider a situation where ToT has been measured by the same procedure, but using five different stop watches. Stop watches are so incredibly accurate that if you know one measure, you basically know them all. What many researchers do with repeated measures data, is take the average. This is the one extreme called total pooling. In the stopwatch case the average of the five measures would be so highly representative, that total pooling is a reasonable thing to do. In other cases, the levels of a factor are more or less independent, for example tasks in a complex system, where procedure duration ranges from seconds to hours. Guessing the duration of one task from a set of others is highly susceptible and the average duration across tasks is not representative at all. The best choice then is to see tasks as factor levels, that are independent. This extreme of no pooling is exactly represented by fixed effect factors as they have been introduced in @ref(linear_models). Random effects sit right between these two extremes of no and total pooling and implement partial pooling: the more the group mean is representative for the units of the group, the more it is taken into account. The best thing about partial pooling is that, unlike real priors, there is not even the need to determine the amount of pooling in advance. The variation of entities has been observed. The stronger the enities vary, the less can be learned from the group level. The variation is precisely the group-level standard deviation of the random effect. So, we can think of random factors as factors where there is a certain amount of cross-talk between levels. The random effect estimate then draws on two sources of evidence: all data from the overarching population and data that belongs just to this one entity. As that is the case, the exploratory analysis of individual performance in SOV does not resemble a true random effect, as group means were calculated independently. How are random effects implemented to draw on both sources? Obviously, the procedure must be more refined than just putting dummy variables in a likelihood formula. In the Bayesian framework a remarkably simple trick suffices, and it is even a familiar one. By the concept of prior distributions, we already know a way to restrict the range of an effect based on prior knowledge. For example, intelligence test results have the prior distribution \\(IQ ~ \\textrm{Gaus}(100, 15)\\), just because they have been empirically calibrated this way. In most other cases, we do have rough ideas about the expected magnitude and range in the population, say: healthy human adults will finish a 2000m run in the range of 5-12 minutes. As prior knowledge is external to the data, it often lacks systematic evidence, with the exception of a meta analyses. This is why we tend to use weak informative priors. Like priors, random effects take into account knowledge external to the entity under question. But, they draw this knowledge from the data, which is more convincing after all. The basic trick to establish the cross-talk between random factor levels, is to simultaneously estimate factor levels and random factor variation. This has several consequences: All random effects get a more or less subtle trend towards the population mean. As a side effect, the random factor variance is usually smaller than variance between fixed factors, or naive group means. This effect is called shrinkage. When the random factor variation is small, extreme factor levels are pulled stronger towards the population mean, resulting in stronger shrinkage. Or vice versa: When random variation is large, the factor levels stand more on their own. The random factor variation is an estimate and as such it is certain only to a degree. As we have seen in ??, the more levels a random factor comprises, the more precise is the estimate of random factor variation. The strongest shrinkage occurs with few observations per factor levels and highly certain random factor variation. Previously, I have stressed how important repeated measures design is, as the number of observations per entity plays a role, too. The more observations there are, the less is the group mean overruled by the population mean. Less shrinkage occurs. This is why mixed-effects models gracefully deal with imbalanced designs. Groups with more observations are just gradually more self-determined. Taking this to the opposite extreme: when a factor level contains no data at all, it will just be replaced by the population mean. This principle offers a very elegant solution to the problem of missing data. If you know nothing about a person, the best guess is the population mean. Under the perspective of populations as a more or less similar set of entities, these principles seem to make sense. Within this framework, we can even define what fixed effects are: a fixed effect is a factor where levels are regarded so unsimilar, that the factor-level distribution can be practically considered infinite. We routinely approximate such a situation when using non-informative prior distributions, like \\(\\beta_0 ~ \\textrm{Gaus}(0, 10000)\\). In the extreme case, a uniform distribution with an infinite upper boundary truly has an infinite variance: \\(\\beta_0 ~ U(0, \\infty)\\). A finite population mean doesn’t even exist with such a distribution. So, when a design researcher has observed that with design A, ToT is approximately distributed as \\(ToT_A ~ \\textrm{Gaus}(120, 40)\\), is it realistic to assume that design B has ToT in the range of several hours? Would a cognitive psychologist see it equally likely that the difference in reaction time on two primitive tasks is 200ms or 2h? Probably not. Still, using fixed effects for factors with very few levels is a justifyable approximation. First of all, priors can be used at any time to factor in reasonable assumptions about the range. Second, with very few estimates, the random factor variation cannot be estimated with any useful certainty. Very small shrinkage would occur and the results would practically not differ. The CUE8 study makes a case for seeing shrinkage in action: Teams of researchers were asked to conduct a performance evaluation on a website. Tasks and website were the same, but the teams followed their own routines. Some teams tested a few handful of participants, whereas others tested dozens remotely. Teams, as another non-human population (sic!) differ vastly in the number of observations they collected. We can expect differences in shrinkage. To see the effect, we compare the team-level group means as fixed factor versus random factor. All teams have enough participants tested to estimate their mean with some certainty. At the same time, the group sizes vary so dramatically that we should see clear differences in tendency towards the mean. We estimate two models, a random effects (RE) model and a fixed effects (FE) model. For the RE model, the absolute group means are calculated on the posterior. Figure XY shows the comparison of FE and RE estimates. attach(CUE8) M_2 &lt;- D_cue8 %&gt;% stan_glm(logToT ~ Team - 1, data = .) M_3 &lt;- D_cue8 %&gt;% stan_glmer(logToT ~ 1 + (1|Team), data = ., iter = 100) P_2 &lt;- posterior(M_2, type = &quot;fixef&quot;) P_3_fixef &lt;- posterior(M_3, type = &quot;fixef&quot;) P_3_ranef &lt;- posterior(M_3, type = &quot;ranef&quot;) ## Creating a derived posterior with absolute team-level random effects P_3_abs &lt;- left_join(P_3_ranef, P_3_fixef, by = c(&quot;chain&quot;, &quot;iter&quot;, &quot;fixef&quot;), suffix = c(&quot;&quot;,&quot;_fixef&quot;)) P_3_abs$value &lt;- P_3_abs$value + P_3_abs$value_fixef T_shrinkage &lt;- D_cue8 %&gt;% group_by(Team) %&gt;% summarize(N = n()) %&gt;% mutate(fixef = fixef(P_2)$center, ranef = ranef(P_3_abs)$center, diff = fixef - ranef) sd_fixef &lt;- sd(T_shrinkage$fixef) sd_ranef &lt;- sd(T_shrinkage$ranef) T_shrinkage %&gt;% ggplot(aes(x = Team, y = fixef, size = N, col = &quot;fixef&quot;)) + geom_point() + geom_point(aes(y = ranef, col = &quot;ranef&quot;), alpha = .5) detach(CUE8) Team H is far off the population average, but almost no shrinkage occurs due to the large number of observations. Again, no shrinkage occurs for Team L, as it is close to the population mean, and has more than enough data to speak for itself. Team B with the fewest observation (a genereous \\(N = 45\\), still), gets noticable shrinkage, although it is quite close to the population mean. Overall, the pattern resembles the above properties of random effects: groups that are far off the population mean and have comparably small sample size get a shrinkage correction. In the case of CUE8, these correction are overall negligible, which is due to the fact that all teams gathered ample data. Recall the SOV simulation above, where the set of tasks every user did was beyond control of the researcher. In situations with quite heterogeneous amount of missing data per participant, shrinkage is more pronounced and more information is drawn from the population mean. At the same time, shrinkage adjusts the estimates for variation, with \\(sd_{RE} = 0.583 &lt; sd_{FE} = 0.588\\). The random effects estimate is an unbiased estimate for the population variance, whereas fixed effects variation would be overestimating. [REF] So, random effects are factors with the additional assumption of Gaussian distribution. When a multi-level model is estimated, the population level effect, the random effects levels and the variance of the distribution are estimated simultaneously. This creates two particular advantages of multi-level models with random effects: In unbalanced research designs (with unequal number of observations per subject) small groups are corrected towards the population mean. Strong outliers are corrected towards the population mean. In conclusion, whereas classical techniques for repeated measures often require additional tweaks to work well with unbalanced designs and outliers, multi-level models with random effects handle those situations gracefully. 6.8 Psychometrics and designometric models Up to to this point, we have characterized random factors mainly by their variance. However, a random effect is just like a factor, where the levels are typically entities in a sample. In a multi-level model, these levels are represented as coefficients. Every entity gets their own estimate which represents the level of functioning of the entity. These values can either be compared against an external benchmark, or they are compared to each other. When human individuals are being compared this is called psychometrics. Traditionally, psychometrics deals with the valid and reliable measurement of personal characteristics, such as individual levels of performance, motivation, socio-cognitive attitude and the like. Advanced statistical models have been devised, such as confirmatory factor analysis or item response models. Formally, these applications establish an order among population entities, from low to high. The least one is aiming for is that for any pair of entities \\(A\\) and \\(B\\), we can tell which entity has the more pronounced property. This is called an ordering structure. Fewer applications go beyond mere ranking and establish metric interpretations. For example, in an interval-scaled test, the researcher may compare differences between participants. In design research, multi-item validated scales are routinely used for one of two purposes: A design-related research questions involve traits or abilities of users. For example: Do social network users with high Openness have more connections? A six-item test for Openness is used on every individual in the sample and the scores are compared the number of connections. This is the basic psychometric situation, which is an encounter of persons and items. In design research one frequently assesses properties of a design by using multi-item questionnaires. One example would be the comparison of user experience among a set of e-commerce homepages using scales such as the AttrakDiff (the hedonic dimension). Another example would be to map the uncanny valley effect by letting participants judge a set of artificial faces with a multi-item scale to measure eeriness. Apparently, when the aim is to measure a design, the situation is an encounter of users, items and designs. We call this the psychometric situation. We begin with the first case, standard psychometrics to assess user characteristics. For example, one could ask how a persons visual-spatial abilities are related to performance in navigating a complex hypertext environment, exploring body cavities during surgical procedures or monitoring the scattered displays in air traffic control centers. In the case of visual-spatial ability, the researcher could administer a test for visual-spatial abilities: for example, participants solve a set of mental rotation tasks and reaction times are collected as a score for spatial processing speed; this would later be compared to performance in a real (or simulated) task, let’s say the number of times an obstacle is hit in a driving simulator. The straight-forward approach would be to take the average measure per person as a score for mental rotation speed. n_Part &lt;- 20 n_Trial &lt;- 10 n_Obs &lt;- n_Part * n_Trial D_Part &lt;- tibble(Part = 1:n_Part, true_score = rnorm(n_Part, 900, 80)) D_Trial &lt;- tibble(Trial = 1:n_Trial) D_CTT &lt;- mascutils::expand_grid(Part = D_Part$Part, Trial = D_Trial$Trial) %&gt;% left_join(D_Part) %&gt;% mutate(RT = rnorm(n_Obs, mean = true_score, sd = 50)) %&gt;% mascutils::as_tbl_obs() D_CTT %&gt;% group_by(Part) %&gt;% summarize(score = mean(RT)) Part score 1 861 2 958 3 890 4 920 5 870 6 858 7 905 8 917 9 918 10 944 11 828 12 1063 13 810 14 970 15 979 16 910 17 774 18 1050 19 1096 20 1050 Note how the table only contains the identifiers, but no additional information. The trials in the mental rotation task are assumed to be exchangeable. This is how the so-called classical test theory approach works. In classical test theory, the observed test score \\(y_i\\) for participant \\(i\\) is composed of the the true score of participant \\(i\\), \\(\\mu_i\\), and a Gaussian measurement error \\(\\epsilon_{ij}\\). \\[y_{ij} = \\mu_i + \\epsilon_{ij}\\] The following model implements CTT as an absolute group means model [AGM, REF], with the only difference that the person factor is a random effect, i.e. it assumes a Gaussian distribution of person scores. CTT assumes that all items function precisely the same way and can therefore be ignored, which is a bold claim. For a set of experimental trials it may (or may not) be true that they are all equally hard. In most other situations item equality is highly questionnable. Two items from the same scale can differ in several aspects, one of which is how hard (or strong) an item is. Consider the following two items from a fictional user experience scale; most likely, the second item would get lower ratings on average, because it is stronger: The interface is nice. The interface is really cool. One problem with CTT is that by averaging scores, the CTT swallows any information on item functioning. In contrast, the families of Item response models (IRM), as well as factor analysis models (FAM), do not take for granted that all items act the same. As diverse and elaborated these models are today, they all have in common, that items are modelled explicitly and get their own estimates. Discussing these models in more depth would require a separate book. Still, a simple item response model is nothing but an encounter of persons and test items. With only two populations involved, the standard psychometric model simply is a crossover of person and item random effects. Some years ago, I proposed a novel personality construct geekism, which states that users differ in how enthusiastic they are about tinkering with technology. The hope was that we could explain differences in user behaviour, such as how they react when having to learn a new software application. A qualitative study with self-proclaimed geeks and several psychometric studies resulted in rating scale with 32 items. The Hugme case is one of the quantitative follow-up studies, where the Geekism scale was used together with the Need for Cognition scale (NCS), which assesses the tendency to enjoy intellectual puzzles in general. We were interested in (1) how the items function, (2) how reliable the scale is and (3) how Geekism correlates with Need-for-cognition. attach(Hugme) D_quest &lt;- D_quest %&gt;% mutate(Session = as.factor(session)) One important thing to note at this point is that psychometricians like to put things in matrices. An item response matrix is squared, whereas we need the long format for the regression engine. As is shown below, the long form can be transformed into a matrix and vice versa. D_long &lt;- expand_grid(Part = 1:8, Item = 1:5) %&gt;% mutate(rating = rnorm(40)) %&gt;% mascutils::as_tbl_obs() D_long D_long %&gt;% select(Part, Item, rating) %&gt;% spread(key = Item, value = rating) Part 1 2 3 4 5 1 0.558 -1.624 0.221 -0.464 -0.088 2 0.246 0.175 -1.372 0.154 -0.701 3 -0.253 1.164 0.006 0.977 -0.234 4 -0.310 -1.243 -0.205 -0.187 -0.506 5 -0.333 0.280 -0.707 -0.255 -0.227 6 -0.503 0.262 0.038 -1.857 0.037 7 0.035 -1.936 0.058 0.639 -0.758 8 -0.217 -1.435 -0.971 1.548 -0.656 Psychometric programs often require matrix data, but for a multi-level models we need the long format. IRM models regard items as populations, too, and the basic IRT model is a cross-classified intercept-only model [#crossover]. D_psymx_1 &lt;- D_quest %&gt;% filter(Scale == &quot;Geek&quot;, Session == 1) M_psymx_1 &lt;- D_psymx_1 %&gt;% stan_glmer(rating ~ 1 + (1|Part) + (1|Item), data = .) With such an IRT model, we can extract the person scores, if we want to compare persons. Psychometric evaluation of a rating scale also draws upon items scores. In the following I will demonstrate two psychometric evaluations, using multi-level models: Test coverage of a scale can be assessed by comparing the distribution of item scores with the distribution of person scores Test reliability can be estimated by comparing scores across two sessions of testing. Test validity can be estimated as person score correlations between scales. 6.8.1 Coverage Geekism was assumed to vary widely in the population of users and we wanted to be able to cover the whole range with good precision. In IRT psychometrics, items and persons are actually scored on the same scale. The person-level coefficients represent the persons’ level of geekism. The item-level effects can best be called item sensitivity. A rule in IRT psychometrics is that for accuracy in a certain range, this range has to be covered by items with a matching sensitivity. An item with consistently high ratings gets a high score, and is able to distinguish low levels of geekism. But, that makes it barely useful for discriminating between geeks on high levels. Just think of how poorly a very simple arithmetic question, like “Which of the following numbers is divisible by 3? [2, 3, 5, 7, 9]” would be able to diagnose the math skills of you, the readers of this book. The inverse is also true: an item with a very strong proposition, like I always build my own computers may be great to distinguish between amateur and pro level geekism, but the majority of persons will just say No. So, item sensitivity and a person tendency are inverse. In order to derive a picture on test coverage, it is useful to take the inverse effects and call this strength. We have a linear model, where the rating is weighted sums of person tendency and item sensitivity. A high rating can mean two things (or both): coming from a very geek person, indeed, or it was a very sensitive item. For a good test coverage we need sensitive items for levels of low geekism and strong, i.e. less sensitive, items for the pros. Because random effects are centered at zero, we can simply reverse the scale with item strength being the negative sensitivity. Now we can compare the distributions of person and item scores side-by-side and check how the person tendencies are covered by item strength. Note that for obtaining the absolute scores, we can use the Bayr function re_scores, but for psychometric analysis, the deviation from the population average is sufficient, hence ranef. P_psymx_1 &lt;- posterior(M_psymx_1) T_ranef &lt;- ranef(P_psymx_1) %&gt;% rename(geekism = center) %&gt;% mutate(geekism = if_else(re_factor == &quot;Item&quot;, -geekism, geekism)) # reversing T_ranef %&gt;% ggplot(aes(x = re_factor, y = geekism, label = re_entity)) + geom_violin() + geom_jitter(width = .2) + ylim(-2, 2) It turns out that the 32 items of the test cover the range of very low to moderately high geekism quite well. The upper 20 percent are not represented so well, as it seems. If we were to use the scale to discriminate between geeks and totally geeks, more strong item had to be added. 6.8.2 Reliability Next, we examine the reliability of the Geekism scale. Reliability is originally a CTT concept and means that the measurement error is small. For example, a reliable personality scale produces almost exactly the same score when applied to a person on different occasions. Is the Geekism score reliable? In our study we asked participants to fill out the questionnaire twice, with an experimental session in-between. If reliability of Geekism is good, the correlation of scores between sessions should be very strong. In order to obtain the scores per session, we add an effect to the model. For reliability we are interested in correlation between person scores, so it suffices to add the Session random effect to the participant level, only. However, the same model can be used to do assess stability of item scores, too. This is rarely practiced, but as we will see, there is an interesting pattern. D_psymx_2 &lt;- D_quest %&gt;% filter(Scale == &quot;Geek&quot;) M_psymx_2 &lt;- D_psymx_2 %&gt;% brm(rating ~ 0 + Session + (0 + Session|Part) + (0 + Session|Item), data = .) We extract the random effects and plot test-retest scores for participants and items. The red line in the plots indicates perfect stability for comparison. T_ranef &lt;- ranef(M_psymx_2) %&gt;% select(re_factor, re_entity, Session = fixef, score = center) %&gt;% spread(key = Session, value = score) sample_n(T_ranef, 5) re_factor re_entity Session1 Session2 Item Geek18 0.156 0.157 Part 55 -1.112 -1.040 Part 10 -0.100 -0.095 Item Geek28 0.174 0.155 Part 20 -0.497 -0.428 plot_stability &lt;- function(T_ranef) T_ranef %&gt;% ggplot(aes(x = Session1, y = Session2)) + facet_grid(.~re_factor) + geom_point() + geom_smooth(aes(color = &quot;observed stability&quot;), se = F) + geom_abline(aes(intercept = 0, slope = 1, color = &quot;perfect stability&quot;)) T_ranef %&gt;% plot_stability() The participant scores are highly reliable. If you measure the score of a person, you almost precisely know the result of another measure a few hours later. At least in short terms, the Geekism construct - whatever it may truly be - can be measured with almost no error. Only ever so slightly is there a trend that lower scores get higher the second time and higher get lower, which could be called a trend towards the average. Something during the experiment has led participants to report a more mediocre image of themselves. The psychometric model further contains intercept and slope random effects for items, and we can examine test-retest patterns in the same way. We see the same trend towards the average, but much stronger. In psychometric analysis it is common to assess participant-level test-retest reliability, but rarely is that done on items. In the present case, we see that this can be a mistake. Here it seems that the trend towards mediocracy does not produce a bias on the population mean, because it is bi-directional and the item and participant scores are nicely symmetric around the center of the scale. Not every test may have these properties and any asymmetric wear-off effect of items would produce more serious biases. Another situation where item stability matters is when a person doing the test is actually learning from it. Usually, it is not desired that a test can be learned, because that means people can train for it. would be unfortunateis unlikely to occur in a regular math or intelligence test, but when the items are real-world tasks, like programming a medical infusion pump or driving a car, the participant get a lot of feedback and will learn. The example of test-retest stability shows one more time, how useful plots are for discovering patterns in data. More formally, test-retest stability is reported as a correlation. We can produce a correlation estimate by using the standard cor command on the participant-level random effects: T_ranef %&gt;% group_by(re_factor) %&gt;% summarize(cor = cor(Session1, Session2)) re_factor cor Item 0.998 Part 1.000 Unfortunately, this lacks information about the degree of certainty. The better way is to let the regression engine estimate all correlations between random factors that are on the same level (Part, Item). The rgression engine brm fromn package Brms package does that by default, and this is why it has been used, here. The following code extracts the posterior distributions of all correlations in the model, creates an estimates table and a plot of 95% certainties. clu_cor &lt;- function(model){ model %&gt;% posterior() %&gt;% filter(type == &quot;cor&quot;) %&gt;% mutate(parameter = str_remove_all(parameter, &quot;cor_&quot;)) %&gt;% group_by(parameter) %&gt;% summarize(center = median(value), lower = quantile(value, .025), upper = quantile(value, .975)) %&gt;% separate(parameter, into = c(&quot;re_factor&quot;, &quot;between&quot;, &quot;and&quot;), sep = &quot;__&quot;) } M_psymx_2 %&gt;% clu_cor() re_factor between and center lower upper Item Session1 Session2 0.984 0.915 0.999 Part Session1 Session2 0.995 0.968 1.000 With random effects correlations assessing test-retest-stability is straight-forward. If test and retest ramdom effects correlate strongly, we can be sure that the error of measurement is low and we can call it a reliable scale. Good reliability is necessary, but not sufficient to also call a scale valid. 6.8.3 Validity Reliability doesn’t say anything about what the scale actually measures. In psychometric studies, validity of a scale is routinely evaluated by comparing the scores to external measures. In a perfect world, it would be assessed how scores are related to relevant real-world behaviour, such as: Are high-Geek persons more enthusiastic to learn a programming language? Do high-Geek persons perform better in computer jobs? Are high-Geek persons more likely to buy robot toys for their offsprings? In the real world, researchers in the field of personality are usually content with relating their scales to other, validated personality scales. In the Hugme study, participants were also asked to rate themselves on the Need-for-Cognition scale (NCS). In very brief NCS measures how much a person enjoys intellectual puzzles. Since computers are intellectual puzzles, sometimes in a good way, often not, we thought that high-Geek persons must also score high on NCS. At the same time, a very strong correlation between Geek and NCS would indicate that the two scales render the same property, which would make one of them redundant, probably the newcomer. The following model estimates the person scores per scale and we can extract the correlation. M_psymx_3 &lt;- D_psymx_3 %&gt;% brm(rating ~ 0 + Scale + (0 + Scale|Part), data = .) M_psymx_3 %&gt;% clu_cor() re_factor between and center lower upper Part Intercept ScaleNCS -0.524 -0.698 -0.3 We observe a weakly positive association between Geek and NCS, just as was hoped for. detach(Hugme) 6.8.4 Design-o-metrix Leaving the field of psychometrics, we revisit the Uncanny Valley data set [#polynomials]. The experiment used eight items from the Eeriness scale [#MacDorman] to ask the judgment of participants on 82 stimuli showing robot faces. In one of our experiments (RK_1), participants simply rated all robots face in three separate session. attach(Uncanny) RK_1 %&gt;% select(Part, Item, Session, response) %&gt;% sample_n(5) Part Item Session response p2_02 nE5 1 -0.008 p1_09 nE3 1 -0.193 p2_12 nE6 2 -0.158 p1_04 nE2 3 -0.890 p1_13 nE2 2 -0.503 With this data we seem to be standing on familiar psychometric grounds: Items are used on persons and we have three measures over time. We can calculate test-retest stability of items and persons using a multi-level model. Voila! Here are your correlations, person and item stability - with credibility limits. Wait a second! What is being measured here? Persons? No, robot faces. The original question was, how human-likeness of robot faces is related to perceived eeriness of robot faces and the Eeriness scale intended purpose is the comparison of designs, not persons. For example, it could be used by robot designers to check that a design does not trigger undesireable emotional responses. Without knowing the humen-likeness scores, robot faces become just a naked sample of designs [#crossover]: UV_dsgmx &lt;- RK_1 %&gt;% rename(Design = Stimulus) %&gt;% select(Part, Item, Design, Session, response) %&gt;% as_tbl_obs() UV_dsgmx Measures in the Uncanny experiment are an encounter of three samples: Part, Item and Design, and Design is the one of interest. That means we need a model that produces Design-level scores. For the user of multi-level models that just means to add a Design random effect to the psychometric model (Part, Item). Models, where a design random factor sits on top of a psychometric model, I call from here on a designometric models. The most basic designometric model is a three-way cross-classified intercept-only model, from which design scores can be extracted. By extending the test-retest psychometric model M_psymx_2, we can estimate test-retest stability. M_dsgmx_1 &lt;- UV_dsgmx %&gt;% brm(response ~ 0 + Session + (0 + Session|Design) + (1 + Item) + (0 + Session|Part), data = .) Like in the psychometric situation, we extract the correlations. Since we have three sessions, we get two stability scores per level. M_dsgmx_1 %&gt;% posterior() %&gt;% clu_cor() %&gt;% print() %&gt;% ggplot(aes(x = re_factor, y = center, ymin = lower, ymax = upper)) + facet_grid(and ~ between) + geom_crossbar() + ylim(0,1) ## # A tibble: 6 x 6 ## re_factor between and center lower upper ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Design Session1 Session2 0.987 0.946 0.999 ## 2 Design Session1 Session3 0.987 0.945 0.999 ## 3 Design Session2 Session3 0.991 0.962 0.999 ## 4 Part Session1 Session2 0.815 0.552 0.944 ## 5 Part Session1 Session3 0.688 0.343 0.875 ## 6 Part Session2 Session3 0.815 0.567 0.933 detach(Uncanny) The test-retest stability for designs is very reassuring. Ratings on the Eeriness scale are highly reproducable and the error will be very small. To a lesser, but still sufficient degree are person scores stable. But, what does the person score (and its stability) actually mean? It describes the tendency of a person to give high ratings on Eeriness. Should a researcher want to assess how vulnerable a person is to the Uncanny Valley effect, the Eeriness scale is also reliable for measuring persons. Many scales in design research lend themselves to be looked at from a designometric and psychometric perspective. For example, a hypothetical scale to measure comfort of sitting can be used to evaluate seats, but can also be used to measure how comfortable a person is with sitting. No seat fits every person, or put differently: the comfort of a seat depends on the person sitting in it. This points us at one of many possible extensions to carry out deeper designometric analysis. If the difficulty of an item in a psychometric test depends on who is being tested, this is called differential item functioning. For example, the large international student evaluations PISA and TIMMS routinely check their test items for cultural differences. The aim is to formulate test questions in such a way that they are equally comprehensible with all cultural backgrounds. Most likely, this is a desireable property for designometric scales, too. In a multi-level designometric model, this could be incorporated as an interaction effect between cultural background and item-level coefficients. That all being said about designometric models, my observation is that practically all published rating scales in design research have been validated under a psychometric perspective, rather than a designometric. This is a mistake! If the purpose of the scale is to compare designs, the scale validation must be carried out on the design scores. In the worst case, a designometric scale is evaluated by a study, where a sample of participants and a sample of items do not encounter a sample of designs, but just one. I understand how this mistake happens in the first place. But, it worries me that practically all purportedly designometric scales out there have been validated under the wrong perspective. I call this the psychometric fallacy in design research. "],
["GLM.html", "7 Generalized Linear Models 7.1 Elements of Generalized Linear Models 7.2 Count data 7.3 Measures of time 7.4 Rating scales", " 7 Generalized Linear Models In the preceding chapters we got acquainted with the linear model as an extremely flexible tool to represent dependencies between predictors and outcome variables. We saw how factors and covariates gracefully work together and how complex research designs can be captured by multiple random effects. It was all about specifying an appropriate (and often sophisticated) right-hand side of the regression formula, the predictor term. Little space has been dedicated to the outcome variables. That is now going to change, and we will start by examining the assumptions that are associated with the outcome variable. Have you wondered about the the many simulated data sets up to this point? The reason for using simulated data is: the linear model, as introduced so far, makes assumptions that are never truly met by real data. The simulated data sets so far were meant to demonstrate some features found in real data sets, but generously wiped over some other frequent peculiarities. Another question is probably lurking in the minds of readers with some classic statistics training: what has happened to the assumptions of ANOVA and the like, and where are all the neat tests that check for Normality, constant variance and such? The Gaussian linear model, which we used throughout [#LM] and [#MLM], makes many assumptions, but in my view, the three crucial assumptions are: Linearity of the association between predictors and outcome variable. Gaussian distribution of responses constant variance of response distribution In the next section we will review these assumptions and lead them ad absurdum. Simply put, in the real world is no such thing as Gaussian distribution and linearity. Checking assumptions on a model that you know is inappropriate, seems a futile exercise, unless better alternatives are available, and that is the case: with Generalized Linear Models (GLMs) we extend our regression modelling framework once again. As we will see, GLMs solves some common problems with linearity and gives us more choices on the shape of randomness. What GLMs do not do is just relax teh assumptions of linear models. And because I have met at least one seasoned researcher who divided the world of data into two categories: parametric data, that meets ANOVA assumptions, and non-parametric that does not, let me get this perfectly straight: First of all, data is neither parametric nor non-parametric. Instead, data is the result of a process that distributes measures in some form and a good model aligns to this form. Second, a model is parametric, when the statistics it produces have a useful interpretations, like the intercept is the group mean of the reference group and the intercept random effect represents the variation between individuals. All models presented in this chapter fulfil this requirement and all are parametric. The GLM framework rests on two extensions that bring us a huge step closer to the data. The first one is a minor mathematical trick to establish linearity, the link function. The second is the expected pattern of randomness. After we established the GLM framework [#EGLM], I will introduce a good dozen of model classes, that leaves little reason for non-parametric procedures, data transformations and using Gaussian linear models as crude approximations. There almost always is a clear choice that largely depends on the properties of the response variable, for example: Poisson LM is the first choice for outcome variables that are counted (with no upper limit), like number of errors. Binomial (aka logistic) LM covers the case of successful task completion, where counts have an upper boundary. These two GLM families have been around for more than half a century in statistics. The quest for a good model for reaction time and time-on-task was more difficult, as there does not seem to be a generally accepted default. Luckily, with recent developments in Bayesian regression engines the choice of random distributions has become much broader. For RT and ToT, I will suggest primarily the exponentially-modified Gaussian (ExGauss) LM and, to some extent, Gamma LM. Along the way, this very chapter introduces a basic way of choosing between response distributions based on model comparison. For binned rating scales, where responses fall into a few ordered categories, ordinal logistic regression is a generally accepted approach. For (quasi)continuous rating scales will I make a novel suggestion, the Beta LM. Too many choices can be a burden, but as we will see, most of the time the appropriate model family is obvious. For the impatient readers, here is the recipe: Answer the following three questions about the outcome variable. It is always safe to to answer Yes to the third question. Is the outcome variable discrete or continuous? What are the lower and upper boundaries of outcome measures? Can we expect over-dispersion? Then use the graph below to identify the correct distribution family and jump to the respective section. 7.1 Elements of Generalized Linear Models GLM is a framework for modelling that produces a family of models. Every member of this family uses specific link functions to establish linearity and chooses a particular random distribution, that has an adequate shape and mean-variance relationship. Sometimes GLMs are mistaken as a way to relax assumptions of linear models, (or even called non-parametric). They are definitely not! Every member makes precise assumptions on the level of measurement and the shape of randomness (see Table A). One can even argue that Poisson, Binomial and exponential regression are stricter than Gaussian, as they use only one parameter, with the consequence of a tight association between variance and mean. A few members of GLM are classic: Poisson, Binomial (aka logistic) and exponential regression have routinely been used before they were united under the hood of GLM. These and a few others are called canonical GLMs, as they possess some convenient mathematical properties, that made efficient estimation possible, back in the days of expensive computer time. For a first understanding of Generalized Linear Models, you should know that linear models are one family of Generalized Linear Models, which we call a Gaussian linear model. The three crucial assumptions of Gaussian linear models are encoded in the model formula: \\[ \\mu_i=\\beta_0+ \\beta_1 x_{1i}+ …+\\beta_k x_{ki}\\\\ y_i \\sim \\textrm{Norm}(\\mu_i,\\sigma) \\] The first term, we call the likelihood and it represents the systematic quantitative relations we expect to find in the data. When it is a sum of products, like above, we call it linear. Linearity is a frequently under-regarded assumption of linear models and it is doomed to fail in [#linearity]. The second term defines the pattern of randomness and it hosts two further assumptions: Gaussian distribution and constant error variance of the random component. In classic statistics education, the tendency is still to present these assumptions as preconditions for a successful ANOVA or linear regression. The very term precondition suggest, that they need to be checked upfront and the classic statisticians are used to deploy a zoo of null hypothesis tests for this purpose, although it is widely held among statisticians that this practice is illogical. If an assumptions seems to be violated, let’s say Normality, researchers then often turn to non-parametric tests. Many also just continue with ANOVA and add some shameful statements to the discussion of results or humbly cite one research paper that claims ANOVAs robustness to violations. The parameters of a polynomial model usually don’t have a direct interpretation. However, we saw that useful parameters, such as the minimum of the curve, can be derived. Therefore, polynomial models can be called semiparametric, at least. As an example for a non-parametric test, the Mann-Whitney U statistic is composed of the number of times observations in group A are larger than in group B. The resulting sum U usually bears little relation to any real world process or question. Strictly speaking, the label non-parametric has nothing to do with ANOVA assumptions. It refers to the usefulness of parameters. A research problem, where U as the sum of wins has a useful interpretation could be that in some dueling disciplines, such as Fencing, team competitions are constructed by letting every athlete from a team duel every member of the opponent team. We could call the U-test parametric, and perhaps, the group means turn out to be meaningless. 7.1.1 Re-linking linearity (#relinking_linearity) [TBC] In the chapter on Linear Models, we encountered several situations where linearity itself was violated: in [#OFM] we used ordered factors to estimate a learning curve in [#staturation], we used conditional effects when two or more interventions improve the same process, e.g. visual recognition of letters and in [#PLM] we used polynomials to estimate wildly curved relationships The third case is special, in that the curvature itself is of theoretical interest (e.g. finding the “trough” of the UNcanny Valley efect). In contrast, learning curves and saturation effects have in common that in both situations performance has a limit and as much that limit is approached, training (or other treatments) takes less effect and the curve bends towards an asymptote. attach(IPump) D_Novel %&gt;% ggplot(aes(x= session, y = deviations)) + # geom_jitter() + geom_smooth(se = F) We used an OFM with stairways coding to account for this non-linearity, but that has one disadvantage. From a practical perspective it would interesting to know, how performance improves when practice continues. What would be performance in (hypothetical) sessions 4, 5 and 10. Because the OFM just makes up one estimate for every level, there is no way to get predictions beyond the observed range. With an LRM, the slope parameter applies to all steps, which gives us the possibility of deriving predictions beyond the observed range. To demonstrate this on the deviations from optimal path, the following code estimates a plain LRM and then injects some new (virtual) data to get predictions beyond the observed range: M_LRM_1 &lt;- stan_glm(deviations ~ 1 + session, data = D_Novel) D_new &lt;- tibble(session = as.integer(c(0:9)), range = if_else(session &lt; 3, &quot;observed&quot;, &quot;beyond obs.&quot;)) %&gt;% as_tbl_obs() predict(M_LRM_1, newdata = D_new) %&gt;% print() %&gt;% left_join(D_new) %&gt;% ggplot(aes(x = session, y = center, ymin = lower, ymax = upper)) + geom_ribbon(aes(fill = range), alpha = .2) + geom_step() + geom_hline(aes(yintercept = 0), color = &quot;red&quot;, linetype = 2) + scale_x_continuous(breaks = 0:10) ## ** 10 predictions (scale: resp) with 95% credibility limits (five shown below) ## Obs center lower upper ## 1 1 0.8141 -1.35 2.91 ## 2 5 0.1607 -2.01 2.33 ## 3 6 -0.0244 -2.21 2.19 ## 4 7 -0.1970 -2.29 2.10 ## 5 8 -0.3071 -2.61 1.93 detach(IPump) When we use a linear model when there truly is an asymptotic curve, negative values are produced, which are impossible. As the graph shows, this is most pronounced the further we move away from teh observed range. But, it also effects the observed levels, as the lower credibility limits are negative, too. Such a non-linearity happens to all outcome variables that have natural lower or upper boundaries, and that includes all outcome variables in the universe, except its very own spatial extension, perhaps. Speed of light can only be approached asymptotically (if you have some mass to carry around) and even the darkest intergalactic spaces are lit by cosmic background radiation and therefor have a temperature just slightly above absolute zero. It is an unescapable truth, that all our ephemeral measures in design research have boundaries and therefore suffer from wrong predictions: Errors and other countable incidences are bounded at zero Rating scales are bounded at the lower and upper extreme item Task completion has a lower bound of zero and upper bound is the number of tasks. Temporal measures formally have lower bound of zero, but psychologically, the lower bound always is a positive number. The bare linear term is always inadequate, which is unfortunate, because its strength is the endless versatility in specifying relations between predictor variables and outcome. How can we escape the problem of boundaries, without sacrificing such a useful tool? Generalized linear models use a simple mathematical trick that keeps linear terms, but confines the fitted responses to the natural boundaries of the measures. In linear models, the linear term \\(\\mu\\) is mapped directly to fitted responses: \\(\\mu_i = \\beta_0 + x_1\\beta_1\\). In GLMs, a layer is drawn between the fitted response \\(\\mu\\) and the linear term, linear predictor \\(\\theta\\). The link function \\(\\phi\\) transforms between \\(\\mu\\) and \\(\\theta\\). In order to transform back to the scale of measurement, the inverse of the link function, the mean function is used. \\[ \\theta_i = \\beta_0 + x_1\\beta_1\\\\ \\mu_i = \\phi(\\theta_i) \\] In arithmetics an abundance of functions exists for every possible purpose. However, a link function \\(\\phi\\) must fulfill two criteria: mapping from the (linear) range \\([-\\infty; \\infty]\\) to the range of the response, e.g. \\([0; \\infty]\\). be monotonically increasing Intuitively speaking, a monotonically increasing function always preserves the order, such that the following holds for a link function. \\[ \\theta_a &gt; \\theta_b \\rightarrow \\mu_a &gt; \\mu_b \\] One reason for monotonicity is that for a link function \\(\\phi\\) there must exist the inverse, which is called the mean function (\\(\\phi^{-1}\\)). For example, \\(x^2\\) is not a proper link function, because its inverse, \\(\\sqrt{x}\\) can take two values (e.g. \\(\\sqrt{x} = [2, -2]\\)) and therefore is not a function on the desired range, strictly speaking. The most typical cases are that there is a lower boundary of zero, or there are two boundaries. An adequate link function for count variables would map the range of natural numbers (only lower bound) to the linear range of \\(\\eta\\) that is \\([-\\infty; \\infty]\\). The logarithm is such a function and its inverse is the exponential function, which bends the linear range back into the boundary. Other variables, like success rates or rating scales, have lower and upper boundaries. A suitable pair of functions is the logit link function and the logistic mean function. plot_glmfun &lt;- function(f = log, title = &quot;log link function&quot;, lower = .01, upper = 3, dir = &quot;link&quot;){ out &lt;- tibble(x = seq(lower, upper, (upper - lower)/100)) %&gt;% ggplot(aes(x)) + stat_function(fun = f) + labs(title = title) + labs(x = expression(mu), y = expression(eta)) if(dir == &quot;mean&quot;) out &lt;- out + labs(x = expression(eta), y = expression(mu)) out } gridExtra::grid.arrange( plot_glmfun(), plot_glmfun(f = exp, &quot;exponential mean function&quot;, -3.5, 1, dir = &quot;mean&quot;), plot_glmfun(f = logit, &quot;logit link function&quot;, 0.01, .99), plot_glmfun(f = inv_logit, &quot;logistic mean function&quot;, -5, 5, dir = &quot;mean&quot;)) Using the link function comes at a cost: the linear coefficients \\(\\beta_i\\) is losing its interpretation as increment-per-unit and no longer has a natural interpretation. Later we will see that logarithmic and logit scales gain an intuitive interpretation when parameters are exponentiated, \\(\\textrm{exp}(\\beta_i)\\) (7.2.1 and ?? 7.1.2 Choosing patterns of randomness (#choosing_randomness) The second term of a linear model, \\(y_i \\sim Norm(\\mu_i, \\sigma)\\) states that the observed values are drawn from Gaussian distributions(see @ref(resid_normality)). But Gaussian distributions have the same problem as the linearity assumption: the range is \\([-\\infty; \\infty]\\). in fact, a Gaussian distribution can only be a reasonable approximation when the measures are far off the boundaries of measures and the error is much smaller than the predicted values (@ref(normal_distributions)). The problem can be demonstrated by simulating observations, using a Gaussian pattern of randomness, and see how this fails to produce realistic data. Imagine a study comparing a novel and a legacy interface design for medical infusion pumps. The researchers let trained nurses perform a single task on both devices and count the errors. Assuming, the average number of errors per tasks is \\(\\mu_L = 3\\) for the legacy device and \\(\\mu_N = 1.2\\) for the novel device, with standard deviation of \\(\\sigma = .8\\). We can simulate a basic data set as: N = 80 D_pumps_sim &lt;- tibble(Design = rep(c(&quot;L&quot;, &quot;N&quot;), N/2), mu = if_else(Design == &quot;L&quot;, 3, 1.2), errors = rnorm(N, mu, sd = 1)) We illustrate the data set using histograms: D_pumps_sim %&gt;% ggplot(aes(x = errors)) + facet_grid(~Design) + geom_histogram(bins = 20) + geom_vline(col = &quot;red&quot;, xintercept = 0) + coord_flip() We immediatly see, that simulation with Gaussian distributions is rather inappropriate: a substantial number of simulated observations is negative, which strictly makes no sense for error counts. The pragmatic and impatient reader may suggest to adjust the standard deviation (or move the averages up) to make negative values less unlikely. That would be a poor solution as Gaussian distributions support the full range of real numbers, no matter how small the variance is (but not zero). There is always a chance of negative simulations, as tiny as it may be. Repeatedly running the simulation until pumps contains exclusively positive numbers (and zero), would compromise the idea of random numbers itself. The second reason is that the simulations very purpose was to express and explore expectations from the linear model (CG). We can simply conclude that any model that assumes normally distributed errors must be wrong when the outcome is bounded below or above, which means: always. Recall how linearity is gradually bent when a magnitude approaches its natural limit. A similar effect occurs for distributions. Distributions that respect a lower or upper limit get squeezed like chewing gum into a corner when approaching the boundaries. Review Binomial and Poisson distribution in [#EBS] for illustrations. As a matter of fact, a lot of real data in design research is skewed that way, whereas the Gaussian distribution eternally claims symmetry. A common misconception is that random distributions approach the Gaussian distribution with larger sample sizes. But, what really happens only, is that increasing the number of observations renders the true distribution more clearly. In chapter 4.4.2 a number of random distributions were introduced, together with conditions of when they arise. The major criteria were related to properties of the outcome measure: how it is bounded and whether it is discrete (countable) or continuous. Generalized Linear Models give the researcher a larger choice for modelling the random component and the following table lists some common candidates. boundaries discrete continuous unbounded NA Normal lower Poisson Exponential lower and upper Binomial Beta That is not to say that these five are the only possible choices. Many dozens of statistical distributions are known and these five are just making the least assumptions on the shape of randomness in their class (mathematicians call this maximum entropy distributions). In fact, we will soon discover that real data frequently violates principles of these distributions. For example, count measures in behavioural research typically show a variance that exceeds the mean, which speaks against the Poisson distributions. As we will see in ?? and ??, Poisson distribution can still be used in such cases with some additional tweaks borrowed from multi-level modelling (observation-level random effects). As we will see, response times in design research are particularly misbehaved, as they do not have their lower boundary at zero, but at the lowest human possible time to solve the task. In contrast, most continuous distributions assume that measures near zero are at least possible. In case of response times, we will take advantage of the fact, that modern Bayesian estimation engines support a larger range of distributions than ever seen before (i.e., in classic statistics). The stan_glm regression engine has been designed with downwards compatibility in mind, which is why it includes fewer distributions. Luckily, there is a sibling engine in the package brms, which is more progressive and gives many more choices, such as the Exponential-Gaussian distribution. 7.1.3 Mean-variance relationship The third assumption of linear models is rooted in the random component term as well. Recall, that there is just one parameter \\(\\sigma\\) for the dispersion of randomness and that any Gaussian distribution’s dispersion is exclusively determined by \\(\\sigma\\). That is more harmful as it may sound. In most real data, the dispersion of randomness depends on the location, as can be illustrated by the following simulation. Imagine a survey on commuter behaviour that asks the following questions: How long is the route? How long does it typically take? What are the maximum and minimum travel times you remember? If we simulate such data from a linear model, the relationship between length of route and travel time would look like a evenly wide band, which is due to the constant variance: N = 100 tibble(Obs = as.factor(1:N), km = runif(N, 2, 40), min = rnorm(N, km * 2, 10)) %&gt;% ggplot(aes(x = km, y = min)) + geom_point() + geom_quantile(quantiles = c(.25, .5, .75)) Again, we get some impossible negative data points, but what is also unrealistic is that persons who live right around the corner experience the same range of possible travel times than people who drive dozens of kilometers. Most of the time, we intuit the dispersion of randomness to increase with the magnitude. Most other distributions do not have constant variance. For example, a Gamma distribution takes two parameters, shape \\(\\alpha\\) and scale \\(\\tau\\) and both of them influence mean and variance of the distribution, such that the variance increases by the mean by square: \\[ X \\sim \\textrm{Gamma}(\\alpha, \\theta)\\\\ E(X) = \\alpha \\theta\\\\ \\textrm{Var}(X) = \\alpha \\theta^2\\\\ \\textrm{Var}(X) = E(X) \\theta \\] tibble(Obs = as.factor(1:100), km = runif(100, 2, 40), min = rgamma(100, shape = km * .5, scale = 4)) %&gt;% ggplot(aes(x = km, y = min)) + geom_point() + geom_quantile(quantiles = c(.25, .5, .75)) A similar situation arises for count data. When counting user errors, we would expect a larger variance for complex tasks and interfaces, e.g. writing an article in a word processor, as compared to the rather simple situation like operating a medical infusion pump. For count data, the Poisson distribution is often a starting point and for Poisson distributed variables, mean and variance are both exactly determined by the Poisson rate parameter \\(\\lambda\\), and therefore linearly connected. \\[ X \\sim \\textrm{Poisson}(\\lambda)\\\\ \\textrm{Var}(X) = E(X) = \\lambda \\] tibble(Obs = as.factor(1:100), Task = rep(c(&quot;article&quot;, &quot;infusion&quot;), 50), errors = rpois(100, lambda = if_else(Task == &quot;article&quot;, 200, 8))) %&gt;% ggplot(aes(x = Task, y = errors)) + geom_boxplot() + geom_jitter() Not by coincidence, practically all distributions with a lower boundary have variance increase with the mean. Distributions that have two boundaries, like binomial or beta distributions also have a mean-variance relationship, but a different one. For binomially distributed variables, mean and variance are determined as follows: \\[ X \\sim \\textrm{Binom}(p, k)\\\\ E(X) = p k\\\\ \\textrm{Var}(X) = p (1-p) k\\\\ \\textrm{Var}(X) = E(X)(1-p) \\] To see this, imagine a study that examines the relationship between user expertise (for the convenience on a scale of zero to one) and success rate on ten tasks. The result is a cigar-like shape. For binomial distributions, variance gets largest, when the chance of success is centered at \\(p = .5\\). This is very similar for other distributions with two boundaries, such as beta and logit-Gaussian distributions. tibble(expertise = runif(1000, 0, 1), successes = rbinom(1000, 25, expertise)) %&gt;% ggplot(aes(x = expertise, y = successes)) + geom_point() In conclusion, the Gaussian distribution assumption of constant variance is flawed in two aspects: real distributions are typically asymmetric and have mean and variance linked. Both phenomena are tightly linked to the presence of boundaries. Broadly, the deviation from symmetry gets worse when observations are close to the boundaries (e.g. low error rates), whereas differences in variance is more pronounced when the means are far apart from each other. Still, using distributions that are not Gaussian sometimes carries minor complications. Gaussian distributions have the convenient property that the amount of randomness is directly expressed as the parameter \\(\\sigma\\). That allowed us to compare the fit of two models A and B by comparing \\(\\sigma_A\\) and \\(\\sigma_B\\). In random distributions with just one parameter, the variance of randomness is fixed by the location (e.g. Poisson \\(\\lambda\\) or Binomial \\(p\\)). For distributions with more than one parameter, dispersion of randomness typically is a function of two or more parameters, as can be seen in teh formulas above. For example, Gamma distributions have two parameters, but these do not pull location and dispersion as neatly apart as Gaussian distributions do. Instead, mean and variance depend on both parameters for Gamma distributions. Using distributions with entanglement of location and dispersion seems to be a step back, but frequently it is necessary to render a realistic association between location of fitted responses and amount of absolute randomness. Most distributions with a lower bound (e.g. Poisson, exponential and Gamma) increase variance with mean, whereas double bounded distributions (beta and binomial) typically have maximum variance when the distribution is centered and symmetric. For the researcher this all means that selecting a distribution class for a Generalized Linear Model, the choice determines the shape of randomness and the relation between location and variance. The following sections are organized by type of typical outcome variable (counts, time intervals and rating scales). Each section (except rating scales) first introduces a one-parametric model (e.g. Poisson). A frequent problem with these models is that the location-variance relation is too strict. When errors are more widely dispersed than is allowed, this is called over-dispersion and one can either use a trick borrowed from multi-level models (observation-level random effects) [#OLRE] or select a two-parametric distribution class (e.g., Negative-Binomial). 7.2 Count data Gaussian distributions assume that the random variable under investigation is continuous. For measures, such as time, it is natural and it can be a reasonable approximation for measures with fine-grained steps, such as average scores of self-report scales with a large number of items. Other frequently used measures are clearly, i.e. naturally, discrete, in particular everything that is counted. Examples are: number of errors, number of succesfully completed tasks or the number of users. Naturally, count measures have a lower bound and frequently this is zero. A distinction has to be made, though, for the upper bound. In some cases, there is no well defined upper bound, or it is very large (e.g. number of users) and Poisson regression applies. In other cases, the upper bound is given by the research design, for example the number of tasks given to a user. When there is an upper bound, Binomial distributions apply, which is called logistic regression. 7.2.1 Poisson regression If data can be considered successes in a fixed number of trials, logistic regression is the model type of choice. When the outcome variable is of type count, but there is no apparent upper limit, Poisson regression applies. In brief, Poisson regression has the following attributes: The outcome variable is bounded at zero (and that must be a possible outcome, indeed). The linear predictor is on a logarithmic scale, with the exponential function being the inverse. The random component follows a Poisson distribution. Variance of randomness increases linearly with the mean. The link function is the logarithm, as it transforms from the non-negative range of numbers to real numbers. For a start, we have a look at a Poisson GMM. Recall the smart smurfer game from section [Poisson]. Imagine that in an advanced level of the game , items are well hidden from the player and therefore extremely difficult to catch. To compensate for the decreased visibility of items, the level carries an abundance them. In fact, the goal of the designers is that visibility and abundance are so carefully balanced that, on average, a player finds three items. We simulate a data set for one player repeating the level 30 times and run our first Poisson model, which is a plain GMM. set.seed(6) D_Pois &lt;- tibble(Obs = 1:30, items_found = rpois(30, lambda = 3.4)) D_Pois %&gt;% ggplot(aes(x = items_found)) + geom_histogram() M_Pois &lt;- stan_glm(items_found ~ 1, family = poisson, data = D_Pois) fixef(M_Pois) model type fixef center lower upper object fixef Intercept 1.31 1.16 1.48 Poisson distributions have only one parameter \\(\\lambda\\) (lambda), has a direct interpretation as it represents the expected mean (and variance) of the distribution. On the contrary, the regression coefficient is on a logarithmic scale to ensure it has no boundaries. To reverse to the scale of measurement, we use the exponential function is the mean function [#relinking]: fixef(M_Pois, mean.func = exp) model type fixef center lower upper object fixef Intercept 3.72 3.18 4.41 The exponentiated intercept coefficient can be interpreted as the expected number of items found per session. Together with the credibility limits it would allow the conclusion that the items are slightly easier to find than three per session. Finally, let’s take a look of the formalism of the Poisson GMM: \\[ \\theta_i = \\beta_0\\\\ \\mu_i = \\exp(\\theta_i)\\\\ y_i \\sim \\textrm{Pois}(\\mu_i) \\] In linear models, the first equation used to directly relate fitted responses \\(\\mu_i\\) to the linear term. As any linear term is allowed to have negative results, this could lead to problems in the last line, because Poisson \\(\\lambda\\) is strictly non-negative. Linear predictor \\(\\theta_i\\) is taking the punches from the linear term and hands it over to the fitted responses \\(\\mu_i\\) via the exponential function. This function takes any number and returns a positive number, and that makes it safe for the last term that defines the pattern of randomness. 7.2.1.1 Speaking multiplicative To demonstrate the interpretation of coefficients other than the intercept (or absolute group means), we turn to the more complex case of the infusion pump study. In this study, the deviations from a normative path were counted to serve as a measure for safety. In the following regression analysis, we examine the reduction of deviations by training sessions as well as the differences between the two devices. As we are interested in the improvement from first to second session and second to third, successive difference contrasts apply [#contrasts]. 7.2.1.2 ALIGN attach(IPump) M_dev &lt;- stan_glmer(deviations ~ Design + session + session:Design + (1 + Design + session|Part) + (1 + Design|Task) + (1|Obs), ## observation-level ramdom effect family = poisson, data = D_pumps) M_dev fixef(M_dev) fixef center lower upper Intercept 0.829 0.233 1.405 DesignNovel -1.522 -2.307 -0.740 session -0.236 -0.337 -0.133 DesignNovel:session -0.075 -0.244 0.090 Again, the coefficients are on a logarithmic scale and cannot be interpreted right away. By using the exponential mean function, we obtain the following table: fixef(M_dev, mean.func = exp) fixef center lower upper Intercept 2.291 1.262 4.076 DesignNovel 0.218 0.100 0.477 session 0.790 0.714 0.875 DesignNovel:session 0.927 0.784 1.094 The intercept now has the interpretation as the expected number of deviations with the legacy design in the first session. However, it is incorrect to speak of the effects in terms of differences now, they are multipliers. This is thanks to the following arithmetic law: \\[ \\mu_i =\\\\ \\exp(\\beta_0 + x_1\\beta_1 + x_2\\beta_2) =\\\\ \\exp(\\beta_0) \\exp(x_1\\beta_1) \\exp(x_2\\beta_2) \\] On the left-hand side, fitted responses are obtained by the usual linear combination, then the whole sum is taken to the exponent. This is precisely how fitted responses are generated. In contrast, when coefficients are taken to the exponent, first, their effects become multiplicative and that can be reported in a very natural way: In the first session, the novel design produces 2.291 times the deviations than with the legacy design. For the legacy design, every new training session reduces the number of deviations by factor 0.79 The reduction rate per training session of the novel design is *92.73% as compared to the legacy design. detach(IPump) 7.2.1.3 Dissolving saturation effects With link functions (other than the identity link), interpreting coefficients is all about getting back to the measured scale. However, in the IPump case, the measured scale had one nasty property: performance improvement is not linear. This is why earlier in this book we modelled the relationship between amount of training and performance with an ordered factorial model [#OFM]. A constant difference by unit, as the LRM requires, is of little value for learning curves, because it would inevitably predict impossible fitted responses at some point. In contrast, on the linear predictor scale, negative values are perfectly fine, they just become multipliers smaller than one on the response scale. The idea of replacing the OFM with a linear regression, which one would better call a *linear__ized__ regression model (LzRM), is attractive. For one, with such a model we can obtain valid forecast* of the learning process. And second, the LRM is more parsimonous [#parsimony]. Even with longer sequences, an LzRM just needs two parameters: intercept and slope, whereas the OFM requires one coefficient per session. What if I also told you, that exponential functions make pretty good learning curves? (Review the simulation in [#OFM]) Even the idea of a multiplicative effect bears some good intuition for learning processes, for example, if we say that by every session, errors are reduced to, say 80%, compared to the previous. This can be demonstrated by simulation of a learning experiment. This simulation takes a constant step size of \\(\\log(.8) = -0.223\\) on the linearized scale, resulting in a reduction of 20% per session. initial_deviations &lt;- 20 learning_rate &lt;- .8 D_learn &lt;- tibble(session = 0:10, theta = log(initial_deviations) + session * log(learning_rate), mu = exp(theta), errors = rpois(11, mu)) D_learn %&gt;% ggplot(aes(x = session, y = errors)) + geom_point(aes(col = &quot;simul. responses&quot;)) + geom_line(aes(y = mu, col = &quot;response scale&quot;)) + geom_line(aes(y = theta, col = &quot;linearized scale&quot;)) + ylim(0,20) While the linear predictior scale is a straight line, the response scale clearly is a curve-of-diminishing returns. That opens up the possibility that learning the novel pump design has a constant multiplier (or rate) on the response scale, which would result in a constant difference on the linearized scale. In the following, we estimate two Poisson models, one linearized OFM (OzFM) (with stairway dummies [#OFM]) and one LzRM. Then we will assess the model fit (using fitted responses). If the learning process is linear on the linearized scale, we can expect to see the following: The two step coefficients of the OzFM become similar (they were wide apart for ToT). The slope effect of the LzRM is the same as teh step sizes. Both models fit similar initial performance (intercepts) We estimate both models as usual, with conditional effects for Design: attach(IPump) D_agg &lt;- D_agg %&gt;% mutate(Step_1 = as.integer(session &gt;= 1), Step_2 = as.integer(session &gt;= 2)) M_pois_cozfm &lt;- D_agg %&gt;% brm(deviations ~ 0 + Design + Step_1:Design + Step_2:Design, family = &quot;poisson&quot;, data = .) M_pois_clzrm &lt;- D_agg %&gt;% brm(deviations ~ 0 + Design + session:Design, family = &quot;poisson&quot;, data = .) T_fixef &lt;- bind_rows( posterior(M_pois_cozfm), posterior(M_pois_clzrm) ) %&gt;% fixef(mean.func = exp) %&gt;% separate(fixef, into = c(&quot;Design&quot;, &quot;Learning_unit&quot;), sep = &quot;:&quot;) %&gt;% mutate(model = str_to_upper(str_remove(model, &quot;M_pois_&quot;)), Learning_unit = replace_na(Learning_unit, &quot;Initial_performance&quot;), Parameter_type = if_else(Learning_unit == &quot;Initial_performance&quot;, &quot;Initial_performance&quot;, &quot;Performance_increment&quot;)) %&gt;% arrange(Design, Learning_unit, model) %&gt;% discard_redundant() %&gt;% print() ## # A tibble: 10 x 7 ## model Design Learning_unit center lower upper Parameter_type ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 CLZRM DesignLega~ Initial_performan~ 26.3 24.4 28.2 Initial_performance ## 2 COZFM DesignLega~ Initial_performan~ 26.6 24.7 28.7 Initial_performance ## 3 CLZRM DesignLega~ session 0.772 0.726 0.824 Performance_increm~ ## 4 COZFM DesignLega~ Step_1 0.730 0.653 0.818 Performance_increm~ ## 5 COZFM DesignLega~ Step_2 0.827 0.726 0.943 Performance_increm~ ## 6 CLZRM DesignNovel Initial_performan~ 6.46 5.61 7.43 Initial_performance ## 7 COZFM DesignNovel Initial_performan~ 6.61 5.65 7.69 Initial_performance ## 8 CLZRM DesignNovel session 0.773 0.681 0.876 Performance_increm~ ## 9 COZFM DesignNovel Step_1 0.704 0.558 0.891 Performance_increm~ ## 10 COZFM DesignNovel Step_2 0.865 0.656 1.13 Performance_increm~ T_fixef %&gt;% ggplot(aes(x = Learning_unit, y = center, col = Design, alpha = model, ymin = lower, ymax = upper)) + #geom_point(size = 2) + geom_crossbar(position = &quot;dodge&quot;, width = .2, fill = &quot;grey&quot;) + facet_wrap(Parameter_type ~ ., scales = &quot;free&quot;) Note that two additional classifiers (Learning_unit, Parameter_type) are added to the table to produce a nicely grouped graph. Indeed, the coefficients Step_1 and Step_2 appear to be in a similar region for both designs, although Step_1 remains slightly stronger. As must be expected, the slope of the LzRM sits right in the middle. Both models estimate almost the same Intercepts, i.e. initial performance. Another observation exceeds our expectations on constancy of the learning rate: on the linearized scale, the performance increments are even very similar for the two designs. This opens the possibility for a LzRM that is unconditional on the learning rate (but with conditional intercepts): M_pois_lzrm &lt;- D_agg %&gt;% brm(deviations ~ 0 + Design + session, family = &quot;poisson&quot;, data = .) From the coefficients it seems that the OzFM can best be replaced by the more parsimonous LzRM. Still, the there remains a slight differences in the two steps. This is a good example where formal model selection is useful. As these are rather small models, we can afford leave-one-out cross validation, rather than approximating forecasting accuracy with information criteria [#model_comparison] L_pois_cozfm &lt;- loo(M_pois_cozfm) L_pois_clzrm &lt;- loo(M_pois_clzrm) L_pois_lzrm &lt;- loo(M_pois_lzrm) loo_compare(x = list(L_pois_lzrm, L_pois_clzrm, L_pois_cozfm)) elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic M_pois_lzrm 0.00 0.000 -482 21.6 7.33 1.14 964 43.1 M_pois_clzrm -1.49 0.208 -484 21.6 9.21 1.33 967 43.3 M_pois_cozfm -3.55 2.301 -486 21.3 13.29 1.60 971 42.7 Model comparison with LOO is preferring the most parsimonous model, the unconditional LzRM, followed by the conditional LzRM, where the conditional OFM goes in last. Model comparison confirms, that we may assume a learning rate that is approximately constant across the learning process and the designs. It is even imagineable that there is something special about a learning rate of around 75%. But that would require more data. As the LzRM is our best model candidate, We may now use it to forecast what would happen in future sessions. We inject fabricated data (predictors only) into the model and plot the results: D_forecast &lt;- expand_grid(session = c(0:10), Design = c(&quot;Novel&quot;, &quot;Legacy&quot;), Part = 1:50) %&gt;% as_tbl_obs() %&gt;% mutate(Session = as.factor(session + 1)) T_pred &lt;- post_pred(M_pois_lzrm, newdata = D_forecast) %&gt;% left_join(D_forecast, by = &quot;Obs&quot;) %&gt;% group_by(Design, Session) %&gt;% summarize(mean_deviations = mean(value)) T_pred %&gt;% ggplot(aes(x = Session, col = Design, y = mean_deviations)) + geom_line(aes(group = Design)) detach(IPump) Based on these forecasts, we can make an interesting comparison of the two devices. Notice that initial performance with Novel is around five deviations. With Legacy this level is reached at about the seventh session. As the learning rate is the same for both devices, we can say that the Novel design is always six sessions of training ahead of Legacy. The conclusion is that log-linearized scales can reduce or even entirely remove saturation effects, if the learning rate is approximately constant. Another requirement is that the lower bound is (practically) really zero. This may sound subtle at first, and for deviations and errors this assumption is legit, but other performance variables have no lower bound of zero. For example, the minimum number of steps to find something on the internet is One. In this case the lower bound is precisely known and the varable can be shifted such that it starts at zero, e.g. mutate(addional_steps = steps + 1). In contrast, Time-on-Task always has a strictly positive lower bound, which is not fixed an probably varies between individuals. Learning curves that approach strictly positive asymptotes \\(\\omega\\) (omega) have the following mathematical form, and this one cannot be reduced to a linearized term. Such a model is non-linearizeable, and requires a truly non-linear model (which can be constructed with the Brm engine, but beyond the scope of this book. \\[y_i = \\phi\\exp(\\rho + x) + \\omega\\] where \\(\\omega\\) is the positive asymptote # devtools::install_github(&quot;schmettow/asymptote&quot;) asymptote::ARY ## perf ~ ampl * exp(-rate * trial) + asym ## &lt;environment: namespace:asymptote&gt; The bottom line is that variables that count errors, deviations or additional steps are great measures for experiments on learning. By model comparison against an OFM you first must assess whether the learning rate is approximately constant. If it is, then a log-linearized model renders a valid learning curve and can be used to predict future performance. When two designs are learned new, as in the IPump case, the difference can be interpreted as saved amount of training. Another interesting application would be how much training is needed for a novel design to actually beat a default design that users are highly trained with. 7.2.1.4 HERE 7.2.2 Logistic (aka Binomial) regression (#logistic_regression) 7.2.2.1 REDACT In the last section, we have seen how Poisson regression applies, when outcome variables are count numbers. More strictly, Poisson regression applies to count data, when there is no upper limit to counts (or if this limit is extremely large, as in the Smart Smurfer example). When the outcome variable is counts, but an upper limit exists and is known, logistic regression is the better model. Such a situation often arises, when the counts are successes in a fixed number of trials. More precisely, logistic regression has the following properties: The outcome variable has a zero lower bound and a fixed upper bound, e.g. number of trials \\(k\\). The linear predictors are on a logit scale, which is reversed by a logistic function. The random component follows a binomial distribution. Due to the former, the variance of randomness is largest at \\(\\mu = 0.5\\) or \\(\\eta = 1\\) and declines towards both boundaries, taking a characteristic cigar shape. The most simple form of successes-in-trials measure is when there is only one trial. This is called a dichtotomous variable, and that is very common: a user is successful at a task, or fails a visitor returns to a website or does not a usability problem is discovered or remains unseen a driver brakes just in time or crashes a customer recommends a product to a friend or does not a web user starts a search for information by keyword query or by following links Most dichotomous outcome variables have a more or less clear notion of success and failure (although not necessarily as in the last example). When the outcome casts a positive light on the design, by convention it is coded as 1, otherwise 0. In computer science jargon, every dichotomous observation accounts to a bit, which is the smallest amount of information ever possible. Since in inferential statistics the amount of information is tantamount to the reduction in uncertainty, with dichotomous data one usually needs an abundance of observations to reach reasonably certain conclusions. Because the information of a single observation is so sparse, large samples and repeated measures are important when dealing with dichtomous outcomes. Let us consider an example: early research on foraging strategies of web users revealed that they are extremely impatient companions. They scan a page for visual features, rather than reading [REF: high school students information mall]. Visitors of websites build their first judgement in a time as short as 17ms [REF: Tuch presentation time]. For e-commerce that is a highly important fact to know about their customers and nowadays practically all commercial websites shine with a pleasing visual appearance. But, how would one measure the gratitude of a visitor who actually used the website and may have something to tell beyond visual appeal? A simple measure for gratitude simply is when a visitor returns to buy more. This is usually a highly available measure, too, as any skilled web administrators can distill such data from the server logfiles with little effort. First, all unique visitors are extracted and if the same visitor returns within a given period of time, this is coded as a success (one) or otherwise failure (zero). We simulate such a data set: set.seed(42) D_ret &lt;- tibble(visitor = as.factor(1:100), returned = rbinom(100, 1, .4)) D_ret %&gt;% sample_n(6) %&gt;% kable() visitor returned 63 1 22 0 99 1 38 0 91 1 92 0 D_ret %&gt;% ggplot(aes(x = returned)) + geom_bar() In total, 46% of the visitors return. In order to estimate the return rate together with a statement on uncertainty, we run a logistic regression grand mean model and inspect the coefficient table. Note how the linear formula is completely common ground, but we explicitly pass the binomial family to the regression engine. M_ret &lt;- D_ret %&gt;% stan_glm(returned ~ 1, data = ., family = binomial, iter = iter) # &lt;-- fixef(M_ret) model type fixef center lower upper object fixef Intercept -0.148 -0.542 0.211 As expected from a GMM we retrieve one parameter that reflects the average tendency to return to the site. Recall, that the linear model assumes the predictions to be unbounded. However, in case of return rates, we rather speak of proportions of users to return. As a side note, I try to avoid speaking of probabilities in the context of logistic regression. While being mathematically correct, it sometimes causes confusion with certainty or, beware of this, the p-value. Proportions are on a range from zero to one. Because the quantity of interest is bounded, a link function is needed that stretches the bounded into an unbounded range. For logistic regression, the logit functions maps the fitted responses \\(\\mu_i \\in [0;1]\\) onto the linear predictor scale \\(\\eta_i \\in [-\\infty; \\infty]\\): \\[ \\eta_i = \\textrm{logit}(\\mu_i) \\] The inverse function, commonly called the mean function, of the logit is the logistic function. @ref(logit_logist) shows link and mean functions side-by-side. grid.arrange( ggplot(data.frame(mu=c(0, 1)), aes(x = mu)) + stat_function(fun = mascutils::logit) + xlab(expression(mu)) + ylab(expression(eta)) + ggtitle(&quot;logit link function&quot;), ggplot(data.frame(eta=c(-5, 5)), aes(x = eta)) + stat_function(fun = mascutils::inv_logit) + xlab(expression(mu)) + ylab(expression(eta)) + ggtitle(&quot;logistic mean function&quot;), nrow = 1) In order to obtain a statement on proportion \\(\\mu\\) (note that in a GMM, there is only one), we therefore have to perform the mean transformation: \\[ \\eta = \\beta_0\\\\ \\mu = \\textrm{logist}(\\eta) \\] The fixef command lets you pass on a mean function. However, this logistic mean function is only useful for intercepts and other absolute group means, as we will see in 7.2.2.3 fixef(M_ret, mean.func = inv_logit) model type fixef center lower upper object fixef Intercept 0.463 0.368 0.553 The apt reader may have noticed that the returners data set has been simulated with an exact return rate of 40%. Despite the sample size of 100, the center estimate seems rather off and hampered by considerable uncertainty. That is precisely because of the low level of information contained in dichotomous variables (one bit). For a reasonably certain estimate one would need many more observations. These can either be obtained through a larger sample or through repeated measures. Recall the fictional jump-and-run game smart smurfer in @ref(poisson_dist): the goal of the game is that players collect items and for the user experience it is crucial that this is neither too difficult nor too easy. Imagine, that for adjusting the difficulty level, the developers conduct a quick evaluation study, where they place a number of items (trials) in the game and the success rate of a single player is observed in a series of 15 game sessions. We simulate such a data set: D_smrf &lt;- tibble( Session = 1:15, trials = round(runif(15, 0, 25), 0), successes = rbinom(15, trials, .4), failures = trials - successes) %&gt;% mascutils::as_tbl_obs() D_smrf Per session the player has a number of opportunities for collecting an item, which makes it a repeated measures situation. One might expect that we need to include random effects into the model. Later we will see that this is necessary when the sessions were observed on a sample of players with different abilities. However, as long as one can reasonably assume the chance of catching an item to be constant across all sessions, plain logistic regression can deal with successes in multiple trials. In order to estimate a model with more than one trial per observation, it is necessary to add a variable for the number of failures and use a cbind(successes, failures) statement for the left-hand-side of the model formula. This may seem inconvenient, but it allows to have a different number of trials per observation. M_smrf &lt;- stan_glm(cbind(successes, failures) ~ 1,# &lt;-- family = binomial, data = D_smrf, iter = iter) fixef(M_smrf, mean.func = inv_logit) model type fixef center lower upper object fixef Intercept 0.485 0.418 0.559 ## [1] &quot;D_smrf&quot; &quot;D_ret&quot; We turn now to a real case study, the comparison of two medical infusion pumps (@ref(slope_RE)). On both devices (legacy and novel), 25 nurses completed a set of eight tasks repeatedly over three sessions. In @ref(slope_RE) a multi-level model was estimated on the workload outcome. It is tempting to apply the same structural model to success in task completion, using binomial random patterns and logit links. completion ~ Design*Session + (Design*Session|Part) + (Design*Session|Task) Such a model is practically impossible to estimate, because dichtomous variables are so scarce in information. Two populations encounter each other in the model: participants and tasks, with 6 observations per combination (6 bit). We should not expect to get reasonably certain estimates on that level and, in fact, the chains will not even mix well. The situation is a little better on the population level: every one of the six coefficients is estimated on 400 bit of raw information. We compromise here by estimating the full model on population level and do only intercept random effects to account for gross differences between participants and tasks. attach(IPump) M_cmpl &lt;- D_pumps %&gt;% stan_glmer(completion ~ Design * Session + (1|Part) + (1|Task), family = binomial, data = .) T_cmpl &lt;- fixef(M_cmpl) T_cmpl fixef center lower upper Intercept 1.388 0.069 2.755 DesignNovel 0.404 0.092 0.716 Session2-1 0.676 0.147 1.212 Session3-2 -0.069 -0.616 0.463 DesignNovel:Session2-1 -0.300 -1.055 0.478 DesignNovel:Session3-2 0.287 -0.485 1.068 Keep in mind that the estimates are on the scale of the linear predictor \\(\\eta_i\\). This is a boundless space, where we can freely create linear combinations of effects to obtain group means. To get a group mean prediction on the more meaningful measurement scale \\(\\mu \\in [0;1]\\), one must first do the linear combination, followed by the mean function. the completion rate in the first legacy session is 0.8 in novel/session 1: logist(Intercept + DesignNovel) = 0.857 in novel/session 2: logist(Intercept + DesignNovel + Session2-1 + DesignNovel:Session2-1) = 0.897 in legacy/session 3: logist(Intercept + DesignNovel + Session2-1) = 0.88 7.2.2.2 FINISH 7.2.2.3 Talking odds When presenting results of a statistical analysis, the linear predictor is likely to cause trouble, at least when the audience is interested in real quantities. The linear predictor scale has only very general intuition: zero marks a 50% chance positive values increase the chance, negative decrease bigger effects have larger absolute values That is sufficient for purely ranking predictors by relative impact (if on a comparable scale of measurement), or plain hypothesis testing, but it does not connect well with quantities a decision maker is concerned with, for example: What is the expected frequency of failure on first use? The novel design reduces failures, but is it sufficient? Is frequency of failures reduced to an acceptable level by two training sessions? Above we have used the mean logistic mean function to elevate the absolute group means to proportions. This is an intuitive scale, but unfortunately, the mean function does not apply to individual effects. It is for example, incorrect to apply it like: “the novel pumps proportion of failures in the first session increases by logist(DesignNovel) = 0.6”. However, there is another transformation, that does the trick. For a better understanding, we have to first inspect, what the logit actually is. The logit is also called a log-odds: \\(\\textrm{logit}(p) := \\log(p(1-p))\\). The inner part of the function, the odds, are the chance of success divided by the chance of failure. Odds are a rather common way to express ones chances in a game, say: odds are 1 against 1 that the coin flip produces Head. If you place €1 on Head, I put €1 on tail. odds are 1 against 12 that Santa wins the dog race. If you place 1€ on Santa, I place €12 against. If the coefficients are log-odds, than we can extract the odds by the inverse of the logarithm, the exponential function, like in the following call of fixef: T_fixef_cmpl_odds &lt;- fixef(M_cmpl, mean.func = exp) T_fixef_cmpl_odds fixef center lower upper Intercept 4.008 1.071 15.72 DesignNovel 1.497 1.096 2.05 Session2-1 1.966 1.159 3.36 Session3-2 0.933 0.540 1.59 DesignNovel:Session2-1 0.741 0.348 1.61 DesignNovel:Session3-2 1.333 0.615 2.91 But is it legitimate to apply the transformation on individual coefficients in order to speak of changes of odds? The following arithmetic law tells that what is a sum on the log-odds scale, is multiplication on the scale of odds: \\[ \\exp(x + y) = \\exp(x)\\exp(y) \\] Consequently, we may speak of changes of odds using multiplicative language: If you place €100 on failure in the next task with the legacy design in session 1, I place €400.761 on success. The odds of success with the novel design increase by factor 1.497. Now, I would place \\(400.761 \\times 1.497\\) = €599.962 on success. On success with the novel design in session 2, I would place \\(400.761 \\times 1.497 \\times 1.966 \\times 0.741\\) = €874.002 on success. Once, we have transformed the coefficients to the odds scale, we can read coefficients as multipliers and speak of them in hard currency. detach(IPump) 7.2.3 Modelling overdispersion 7.2.3.1 REDACT With count data, we usually expect the variance of randomness to be tied to the location (or magnitude). The Poisson distribution is very strict in the sense that the variance equals the mean. Real count data frequently has variance that raises proportionally with the mean, but is inflated in comparison to Poisson distribution. This is called overdispersion and must be accounted for. That can either be done by introducing an observation-level random effect [#OLRE], or taking a Negative-Binomial model [#Negbin]. Poisson and binomial distributions are one-parameter distributions. As there is only one parameter, it is impossible to choose location and dispersion independently. In effect, both properties are tightly entangled. For Poisson distributions they are even the same. \\(\\mu = \\sigma^2 = \\lambda\\) For binomial variables, mean and variance both depend on probability \\(p\\) and are entangled in cigar shaped form, as the dispersion shrinks when approaching either two boundaries. Binomial variance is also affected by the number of trials \\(k\\), but that hardly matters as the value of \\(k\\) is exactly known, in by far most cases. The strict variance assumptions of Poisson and binomial models are frequently violated by real data. The violation happens when real impact factors have not been included in the likelhood equation. Whenever there is one or more impact factors on the outcome at question that the researcher has included or measured, overdispersion inevitably happens. Let me give you an example. It is common saying that some people attract mosquito bites more than others. But is that really true? A simple lab experiment would do to test the “Sweet Blood” theory. A sample of participants are exposed to a pack of mosiquitos under carefully controlled conditions (time of day, environmental condition, hungriness of mosquitos). We don’t know the mechanisms that makes the blood sweeter, and hence cannot measure it. In the simulation below, it is just assumed that there is a such a property, but in a real study we would not know. N &lt;- 20 avg_sweet &lt;- 6 Sweet_blood &lt;- tibble(sweetness = rnorm(N, log(avg_sweet), .5), bites = rpois(N, exp(sweetness))) We also don’t have to know it, because if there is an invisible impact factor, we would see that in the extra variance, as compared to the mean: Sweet_blood %&gt;% summarize(mean(bites), var(bites)) mean(bites) var(bites) 6.85 17.8 Overdispersion practically always happens in studies involving objects with complex dynamics, such as the human mind. Two solutions exist for overdispersed count data: we can either switch to a two-parameter response distribution, that gives variance more flexibility (see table below). readxl::read_excel(&quot;Illustrations/GLM_distributions.xlsx&quot;, sheet = &quot;plugin&quot;) canonical generalization parameters limiting case binomial betabinomial \\(\\alpha &gt; 0, \\beta &gt; 0\\) \\(\\alpha, \\beta \\rightarrow \\infty\\) Poisson negative binomial NA NA exponential gamma rate, shape NA The alternative is to introduce an observation-level random effect, which is just like the Gaussian distributed variable Sweetness in the simulation above. 7.2.3.2 HERE 7.2.3.3 Negative-binomial regression for overdispersed counts When Poisson regression is used for overdispersed count data, the model will produce reliable center estimates, but the credibility limits will be too narrow. (The model suggests better certainty than there is.) To explain that in simple terms: The model “sees” the location of a measure, which makes it seek errors in a region with precisely that variance. There will be many measures outside the likely region, but the model will hold on tight, regard these as (gradual) outliers and give them less weight. A solution to the problem is using a response distribution with two parameters. A second parameter usually gives variance of the distribution more flexibility, although not setting it entirely loose. For the Poisson case (i.e. counts without an upper limit) negative binomial distributions do the job, for binomial the beta-binomial applies. Both distributions are so-called mixture distributions. In mixture distributions, the parameter of the “outer” distribution is not constant, but allowed to vary by a distribution itself. Under this perspective, negative binomial distribution is equivalent to a Poisson distribution, if we let parameter \\(\\lambda\\) follow a gamma distribution: rnegbinom &lt;- function(n, mu, size){ shape &lt;- size scale &lt;- mu/size lambdas &lt;- rgamma(n, shape = shape, scale = scale) rpois(n, lambda = lambdas) } rnegbinom(1000, mu = 3, size = 2) %&gt;% qplot() The figure below shows a negative binomial distribution and Poisson distribution with the same mean. tibble(x = 0:15, nbinom = dnbinom(x, mu = 3, size = 2), poisson = dpois(x, 3)) %&gt;% gather(distribution, prob, -x) %&gt;% ggplot(aes(x = x, y = prob)) + facet_grid(distribution ~ .) + geom_col(position = &quot;dodge&quot;) In [#dissolving_saturation] we have seen how log-linearization can accomodate learning curves, using a Poisson model. It is very likely that this data is over-dispersed. To demonstrate overdispersion, we estimate the unconditional learning curve model one more time, with a negative-binomial pattern of randomness: attach(IPump) M_negbin_lzrm &lt;- D_agg %&gt;% brm(deviations ~ 0 + Design + session, family = &quot;negbinomial&quot;, data = .) bind_rows( posterior(M_pois_lzrm), posterior(M_negbin_lzrm) ) %&gt;% clu() %&gt;% print() %&gt;% filter(type == &quot;fixef&quot;) %&gt;% ggplot(aes(x = fixef, y = center, ymin = lower, ymax = upper, col = model)) + geom_crossbar(position = &quot;dodge&quot;) ## ## ## Table: (\\#tab:unnamed-chunk-68)Estimates with 95% credibility limits ## ## model parameter type fixef center lower upper ## -------------- --------------- ------ ------------- ------- ------- ------- ## M_negbin_lzrm b_DesignLegacy fixef DesignLegacy 3.266 3.136 3.390 ## M_negbin_lzrm b_DesignNovel fixef DesignNovel 1.865 1.717 2.019 ## M_negbin_lzrm b_session fixef session -0.256 -0.349 -0.165 ## M_negbin_lzrm shape shape NA 9.457 6.126 15.095 ## M_pois_lzrm b_DesignLegacy fixef DesignLegacy 3.267 3.196 3.334 ## M_pois_lzrm b_DesignNovel fixef DesignNovel 1.869 1.759 1.972 ## M_pois_lzrm b_session fixef session -0.258 -0.314 -0.200 We observe that the center estimates are precisely the same. The credibility limits are much wider with an underlying negative-binomial distribution. In the CLU table we also see that the neg-binomial model got the new parameter shape, which is not a coefficient. Shape (often called \\(\\phi\\) (phi)) controls over-dispersion relative to a Poisson distribution, but in a somewhat convoluted way: \\[ \\textrm{Variance} := \\mu + \\mu^2/\\phi \\] The variance is composed of one \\(\\mu\\) which is just the Poisson baseline variance. The second summand adds the overdispersion, which is the reciprocal of \\(\\phi\\) times the squared mean. The smaller phi gets, the more overdispersion had to be accounted for. From this formula alone it may seem that neg-binomial distributions could also account for under-dispersion, but truth is that \\(\\phi\\) must be strictly positive. Phi is too convoluted to be useful as a practical measure for how much over-dispersion there actually is. Once again, we can turn to formal model selection with LOO (leave-one-out). The negative-binomial model is more complex, in that it has one additional parameter. Still, it is the preferred model.  L_pois_lzrm &lt;- loo(M_pois_lzrm) L_negbin_lzrm &lt;- loo(M_negbin_lzrm) loo_compare(L_pois_lzrm, L_negbin_lzrm) elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic M_negbin_lzrm 0.0 0.0 -446 10.8 4.17 0.522 891 21.5 M_pois_lzrm -36.5 12.6 -482 21.6 7.33 1.138 964 43.1 The bottom line is that whenever there is over-dispersion in count variables, a Poisson model will overstate certainty and a negative-binomial model is appropriate. This is practically always the case and interpreting a neg-binomial model is no different to a Poisson model. detach(IPump) 7.2.3.4 Beta-binomial regression for successes in trials Beta-binomial regression follows a similar pattern as neg-binomial. A two parameter distribution allows to scale up the variance relative to a binomial model [#logistic_regression]. A beta-binomial distribution is created by replacing binomial parameter \\(p\\) by a \\(beta distribution\\), with parameters \\(a\\) and \\(b\\): rbetabinom &lt;- function(n, size, a, b) rbinom(n, size, rbeta(n, a, b)) rbetabinom(1000, 10, 1, 2) %&gt;% qplot() The brms regression engine currently only implements the negative binomial, but not the beta-binomial family. That is a minor problem, because the brms regression engine can be extended. The following code is directly taken from the brms documentation and adds beta-binomial models. The built-in betabinomial distribution takes the parameters a and b from the inner beta distribution. However, for a GLM a parametrization is needed, such that one parameter is the mean (or fitted responses \\(\\mu_i\\)). Note, how the author of this code created a beta_binomial2 distribution family, by transforming the parameters to \\(\\mu\\) and a dispersion parameter \\(\\phi\\). # define a custom beta-binomial family beta_binomial2 &lt;- custom_family( &quot;beta_binomial2&quot;, dpars = c(&quot;mu&quot;, &quot;phi&quot;), links = c(&quot;logit&quot;, &quot;log&quot;), lb = c(NA, 0), type = &quot;int&quot;, vars = &quot;trials[n]&quot; ) # define custom stan functions bb_stan_funs &lt;- &quot; real beta_binomial2_lpmf(int y, real mu, real phi, int N) { return beta_binomial_lpmf(y | N, mu * phi, (1 - mu) * phi); } int beta_binomial2_rng(real mu, real phi, int N) { return beta_binomial_rng(N, mu * phi, (1 - mu) * phi); } &quot; D_betabin &lt;- tibble(y = VGAM::rbetabinom(1000, 9, .1, .3), y_bin = rbinom(1000, 9, .1), n = 9) %&gt;% as_tbl_obs() qplot(D_betabin$y) qplot(D_betabin$y_bin) M_betabin &lt;- D_betabin %&gt;% brm(y | trials(n) ~ 1, family = beta_binomial2, stan_funs = bb_stan_funs, data = .) M_bin &lt;- brm(y |trials(n) ~ 1, family = &quot;binomial&quot;, data = D_betabin) bind_rows( posterior(M_bin), posterior(M_betabin)) %&gt;% mutate(value = mascutils::inv_logit(value)) %&gt;% clu() model parameter type fixef center lower upper M_betabin b_Intercept fixef Intercept 0.706 0.663 0.747 M_betabin phi shape NA 0.998 0.983 1.000 M_bin b_Intercept fixef Intercept 0.706 0.679 0.734 7.2.3.5 FINISH_ME 7.2.3.6 Using observation-level random effects 7.2.3.7 EDIT 7.2.3.8 MAKE IPump With the broad implementation of random effects models in Bayesian regression engines, there is a generic procedure to capture the extra variance even with one parameter distributions. The trick is to introduce an observation-level random effect (OLRE). Recall how we regard variation between members of a population as normally distributed deviations from the population mean, by the example of a Poisson grand mean model with a participant-level (\\(p\\)) random effect: \\[ \\theta_{pi} = \\beta_0 + x_p\\beta_{0p} \\\\ \\mu_{pi} = \\exp(\\theta_{pi})\\\\ \\beta_{0p} \\sim N(\\mu_{p}, \\sigma_p)\\\\ y_{p} \\sim \\textrm{Pois}(\\mu_{ij}) \\] The OLRE is normally distributed but does not cause any bounded range, as it is added on the level of the linear predictor before applying the exponential transformation. Observation-level random effects are completely analogous, except that every observation becomes its own group, in a Poisson grand mean model with added variation: \\[ \\theta_{i} = \\beta_0 + \\beta_{i} \\\\ \\mu_i = \\exp(\\theta_i)\\\\ y_{ij} \\sim \\textrm{Pois}(\\mu_{ij}) \\] See, how \\(\\beta_i\\) is a unique deviation per observation \\(i\\), and how a variance parameter \\(\\sigma\\) appears in an otherwise purely Poisson model. Observation-level random effects are on the linear predictor level, and therefore additive. Compare this to the negative binomial distribution where variance is scaled up, which is multiplication. We find a resemblance with how sums on the linear predictor become multiplications on the fitted responses scale. For demonstration of the concept, we simulate from an overdispersed Poisson grand mean model with participant-level random effects, and recover it via regression. sim_ovdsp &lt;- function( beta_0 = 2, # mu = 8 sd_Obs = .3, sd_Part = .4, N_Part = 30, N_Rep = 20, N_Obs = N_Part * N_Rep, seed = 42){ set.seed(seed) Part &lt;- tibble(Part = 1:N_Part, beta_0p = rnorm(N_Part, 0, sd_Part)) ## participant-level RE D &lt;- tibble(Obs = 1:N_Obs, Part = rep(1:N_Part, N_Rep), beta_0i = rnorm(N_Obs, 0, sd_Obs), ## observeration-level RE beta_0 = beta_0) %&gt;% left_join(Part) %&gt;% mutate(theta_i = beta_0 + beta_0p + beta_0i, mu_i = exp(theta_i), ## inverse link function y_i = rpois(N_Obs, mu_i)) D %&gt;% as_tbl_obs() } D_ovdsp &lt;- sim_ovdsp() D_ovdsp %&gt;% ggplot(aes(x = y_i)) + geom_histogram() ## [1] &quot;sim_ovdsp&quot; &quot;D_ovdsp&quot; M_ovdsp &lt;- D_ovdsp %&gt;% stan_glmer(y_i ~ 1 + (1|Part) + (1|Obs), data = ., family = poisson, iter = iter) Random effect variation is accurately recovered from the simulated data. The following two plots show, that the participant latent scores and even the observation levels themselves can be recovered. Every observation gets an accurate measure of how much it had been pushed by unrecorded sources of variation. Practically, we obtain residuals which can be used for model criticism. For example, extreme outliers can be identified and relations between random variation and predicted values (or groups) are open to scrutiny. grpef(M_ovdsp) re_factor center lower upper Obs 0.305 0.264 0.350 Part 0.536 0.403 0.719 Frequently, I reminded the reader to interpret parameters quantitatively by translating their magnitude to statements of practical relevance. For random effects variance this is not always straight forward. One possible way is to make comparative statements on the sources of variance “the variance due to individual differences exceeds all other sources of variation taken together”. OLREs are on the same scale as all other random effects in the model, which makes it a good default reference. A non-default comparison of sources of variance is the one of Dennis Egan, that people cause more variance than designs do. With GLMMs, the marriage of LMM and GLM this claim is testable. From LMM we borrow a surprisingly simple and general solution, observation-level random effects. So, most of the time we will not need one of those twisted two parameter random distributions to account for overdispersion. With OLRE models we get an estimate that is very similar to residuals, which has proven very useful in model criticism. ## [1] &quot;sim_ovdsp&quot; &quot;D_ovdsp&quot; 7.3 Measures of time Time is a highly accessible measure, as clocks are all around us: on your wrist, in transport stations, in your computers and a very big (and accurate) one is hosted at the Physikalisch-Technischen Bundesanstalt in Braunschweig (Physical-technological federal institute in Braunschweig, Germany). Temporal variables often carry useful information. Reaction time (RT) measures are prime in experimental cognitive studies and have revealed fascinating phenomena of the human mind, such as the Stroop effect or memory priming. In design research, reaction times are also frequently used, but even more common is time-on-task (ToT) as a measure of task efficiency. Formally, both outcome types measure a period of time. I am deliberately making a distinction between the two, because the data generating process of reacting to a simple task (like naming a color) may be different to a complex task, like fidning information on a website. Temporal variables are practically continuous (as long as one measures with sufficient precision), but always have lower bounds. First, I will introduce two classic types of models that use exponentially or Gamma distributed error terms. Both distribution families assume a lower bound at zero, which is problematic, as we will see. Modern Bayesian estimation engines offer an increasing variety of more exotic response distributions. Among those are Exgaussian response distributions, which works well when the lower bound is positive. 7.3.1 Exponential and Gamma regression Exponential distributions arise from basic random processes under some very idealized conditions. First, the lower boundary must be zero and second, the rate at which events happen is assumed to be constant rate, just like Poisson distributions assumes a constant \\(\\lambda\\). Reconsider the subway smurfer example [#distributions], where players collect items in a jump and run game. We have already seen how collection counts can be modelled using Poisson or binomial regression. Another way to look at it is the time between two events of item collection. For demonstration only, we assume such idealized conditions in the subway smurfer example and generate a data set. Exponential distributions are determined by one parameter, the rate parameter \\(\\lambda\\), which is strictly positive. The mean of an exponential distribution is the reciprocal \\(1/\\lambda\\) and the variance is \\(\\textrm{Var} = 1/\\lambda^2\\). Like with Poisson regression, variance is strictly tied to the mean. set.seed(20) D_exp &lt;- tibble(Obs = 1:100, time = rexp(100, rate = 1/3)) D_exp %&gt;% ggplot(aes(x = time)) + geom_histogram(bins = 10) mean(D_exp$time) ## [1] 3.29 var(D_exp$time) ## [1] 12.4 As the stan_glm engine does not support exponential response distributions, we use brm, instead, and recover the parameter. M_exp &lt;- brm(time ~ 1, family = &quot;exponential&quot;, data = D_exp) fixef(M_exp, mean.func = exp) type fixef center lower upper fixef Intercept 3.29 2.73 4 Exponential disribution are rarely used in practice for two shortcomings: first, the strict mean-variance relation makes it prone to over-dispersion. This can be resolved by using observation-level random effects [#OLRE] or using Gamma distributions, which accounts for extra variance by a second parameter. The second problem is the lower boundary of Zero, which will be resolved by using Exgaussian error distributions, instead. Exponential regression has a single parameter and therefore has the same problem as seen with Poisson and binomial regression before. Only if all events have the same rate to occur, will an exponential distribution arise, which means for behavioural research: never. A general solution to the problem is introducing an observation-level random effect [#OLRE]. Here we will tackle the problem by using continuous, zero-bounded distributions with two parameters, the Gamma family of distributions. While the two parameters rate and shape do not directly translate into location and dispersion as with Gaussian, it provides the extra degree of freedom to set them almost independently. The only limitation is that variance rises with the mean, but as we have argued in ??, this is rather a desired feature than a problem. In the following, we simulate Gamma distributed observations. set.seed(20) D_gam &lt;- tibble(Obs = 1:100, time = rgamma(100, rate = 1/3, shape = 2)) D_gam %&gt;% ggplot(aes(x = time)) + geom_histogram(bins = 10) mean(D_gam$time) ## [1] 7.61 In comparison to the exponential distribution above, a significant difference is that the mode of the gamma distribution (its peak) is not fixed at zero, but can move along the x-axis. That makes it appear a much more realistic choice for temporal data in behavioural research. We estimate a simple gamma GMM on the simulated data. For historical reasons, brm uses the inverse link function (\\(\\theta = 1/\\mu\\)) for Gamma regression per default, but that does not actually serve the purpose of link functions to stretch \\(\\mu\\) into the range of real numbers. Instead, we explicitly demand a log link, which makes this a multiplicative model. M_gam &lt;- brm(time ~ 1, family = Gamma(link = log), data = D_gam) M_gam fixef(M_gam, mean.func = exp) type fixef center lower upper fixef Intercept 7.64 6.7 8.71 Both, Exponential and Gamma distributions support the range of real numbers including zero. The weak point of both models is that they have zero as their natural starting point. As we will see in the following section, this assumption is usually violated with RT and ToT data. So, what are they good for, after all? These two models are routinely used for the time intervals (TI) between events that are triggered independently. In nuclear physics the individual triggers are atoms, each one deciding on their own when to decay. If you measure the interval between two decays the time interval is exponentially distributed. (And if you count the neutrons per time interval, the result is a Poisson distribution). Analog situations can be found in service design and logistics. Take the exanmple of for customer support systems. Customers are like atoms in that their decision to file a request is usually independent from each other. Just by chance it can truly happen that two customers call the service center practically the same moment, so that the lower bound of Zero can actually be reached by some observations. Overwhelmed hotline queues do not make people happy, if they have technical problems. When planning a support system, the risk of angry customers has to be weighed against the costs of over-staffing. A good design would hit a certain sweet spot and in the ideal case there would be a predictive model of inflow rate of customers. 7.3.2 ExGaussian regression The problem with RT and ToT data is that Zero is not a possible outcome, as any task uses up a minimum time to complete. For example, the table below shows the minimum ToT for finding the academic calendar on ten university websites (case Egan). This varies a lot between designs, but is never even close to zero. The last column puts the minimum observed ToT in relation to the observed range. On two of the websites, the offset was even larger than the obvserved range itself, hence the problem of positive lower boundaries is real in user studies. attach(Egan) D_egan %&gt;% filter(success, Task == &quot;academic calendar&quot;) %&gt;% group_by(Task, Design) %&gt;% summarize(min_time = min(ToT), range = max(ToT) - min_time, min_time/range) %&gt;% kable() Task Design min_time range min_time/range academic calendar VU Brussel 21 130 0.162 academic calendar KU Leuven 52 40 1.300 academic calendar UGent 8 70 0.114 academic calendar University of Antwerp 3 7 0.429 academic calendar UHasselt 21 48 0.438 academic calendar Leiden University 130 181 0.718 academic calendar VU Amsterdam 207 188 1.101 academic calendar RUG 119 132 0.902 academic calendar University Tilburg 24 39 0.615 detach(Egan) On the first glance, that does not seem to pose a major problem for Gamma distributions, as the left tail vanishes the more a Gamma distribution is shifted to the right, the impossible regions get smaller. However, Gamma distributions inevitably become more symmetric at larger values. A Gamma distribution far to the right has almost equally long tails and we may eventually use a Gaussian distribution, instead. As there is no separate parameter controlling the skewness of the curve it may happen that the random component captures the amount of variance, but overdoes the left tail, which introduces a bias on the coefficients. The following graphic illustrates the mean-variance-skew relationship on three Gamma distributions that move from left to right (M), keeping the variance constant (V): M = c(100,200, 400) V = 8000 ## gamma rate = M/V shape = rate^2 * V ggplot(data.frame(x = c(0, 3000)), aes(x = x)) + stat_function(fun = dgamma, args = list(rate = rate[1], shape = shape[1])) + stat_function(fun = dgamma, args = list(rate = rate[1], shape = shape[2])) + stat_function(fun = dgamma, args = list(rate = rate[1], shape = shape[3])) + labs(x = &quot;ToT&quot;, y = &quot;density&quot;) We have seen so far, that distributions with one parameter (Poisson, binomial, exponential) have a fixed relationship between location and dispersion. In order to vary location and dispersion independently, a second parameter is needed (neg-binomial, beta-binomial, Gamma, Gaussian). Only three-parameter distributions can do the trick of setting skewness separately. So called exponentially modified Gaussian (Exgaussian) distributions are convolutions of a Gaussian distribution and exponential distribution and have three parameters, \\(\\mu\\), \\(\\sigma\\) and rate \\(\\beta\\). Very roughly, the Gaussian component controls location and dispersion whereas the exponential part adjusts the skew. When \\(\\beta\\) is large in comparison to \\(\\mu\\), the distribution is more left skewed. With this additional degree of freedom we can simulate (and estimate) distributions that are far to the right, have strong dispersion and strong skewness. The following plot repeats the right-furthest Gamma distribution from above and adds a Gaussian and Exgaussian distributions with the exact same mean and variance. M = 400 V = 8000 ## Exgaussian mu = M beta = 80 sigma = sqrt(V - beta^2) ## Gamma rate = M/V shape = rate^2 * V ggplot(data.frame(x = c(0, 800)), aes(x = x)) + stat_function(fun = dgamma, args = list(rate = rate, shape = shape), mapping = aes(colour = &quot;Gamma&quot;)) + stat_function(fun = dnorm, args = list(mean = M, sd = sqrt(V)), mapping = aes(colour = &quot;Gaussian&quot;)) + stat_function(fun = brms::dexgaussian, args = list(mu = M, sigma = sigma, beta = beta), mapping = aes(colour = &quot;Exgaussian&quot;)) + labs(colour=&quot;Distribution&quot;, x = &quot;ToT&quot;, y = &quot;density&quot;) The Gamma distribution in this example starts approaching a the perfect bell curve of the Gaussian distribution. In contrast, the exgaussian distribution takes a steep left climb followed by a long right tail, which is caused by its pronounced exponential component. We do the usual exercise to simulate a grand mean model and recover the parameters with the help of the brm engine: attach(Chapter_GLM) D_exg &lt;- tibble(Y = rexgaussian(100, mu = 100, sigma = 20, beta = 30)) qplot(D_exg$Y) M_exg &lt;- brm(Y ~ 1, family = exgaussian, data = D_exg) fixef(M_exg) type fixef center lower upper fixef Intercept 99.4 92.9 107 detach(Chapter_GLM) Noteworthy, for Exgaussian models the brm engine uses the identity link function by default. While this is rather convenient for interpretation, it could theoretically lead to impossible predictions. As we will see later, the exgaussian is not immune, but robust to impossible predictions because of its tiny left tail. Any linear impact factor, like an experimental treatment can push it 150 ms to the left with insignificant risk of impossible predictions. All GLM family members introduced so far are established, have been presented in many textbooks and are routinely used in research of all kinds. The Exgaussian is a newcomer. It does not come with a solid background in physical systems and may very well be considered just a hack. The following two sections examine Exgaussian models more closely and sets them in competition against Gamma and Gaussian error terms. We will be using primarily graphical methods here, but will come back these cases in chapter [#WM] with a more formal approach. 7.3.2.1 Reaction times In experimental studies, the inertia of the nervous system sets a limit larger than zero for reaction times. This is partly due to to some hard electrochemical and biomechanical limits of the peripheral systems. (Octopuses have decentralized nervous system for a reason!) Nerve cells and muscle fibers are slow working horses. The same goes for our minds. Reportedly, they are blazingly fast at complex tasks, such as recognizing colors and written words. Still, there always is a minimum time necessary to collect an idea from the memories and activate the surrounding nodes. Experimental reaction times have a positive minimum and it is not much of a stretch to imagine that this is a sharp limit. In the Hugme case, we tried to pin down the hypothetical Geek personality. We used Need-for-cognition as a predictor for reaction times from the semantic Stroop task. Like in the original Participants must name the color of words, but these are non-color words from two categories (geek/non-geek). These words are preceded by Geek/non-geek pictures. The theory goes that a real geek, when seeing an open computer case followed by “root” will briefly reminisce and be distracted from the primary task. It did not work this way at all and we only saw a few miniscule effects. That is good news for the analysis here. The only effect was that Geek primes caused a minimal delay. Because the data is not contaminated by any factors, we can use a three multi-level CGMs to compare how Gaussian, Gamma and Exgaussian can fit experimental reaction times. Let’s take a first look some parts of the data: attach(Hugme) Most of the participant-level frequency distributions of RT have a clear cut-off at around .25 seconds. The steepness of the left climb varies between participants, but some at least are rather sharp, with a right tail that is leveling off slowly. When compared to the illustrations above, it seems that an Exgaussian model could accomodate this data well. D_hugme %&gt;% group_by(Part) %&gt;% filter(Part &lt;= 10) %&gt;% ggplot(aes(x = RT, color = PrimeGeek)) + facet_wrap(~Part, nrow = 2) + geom_density(adjust = 2) + xlim(0, 1.5) In the following, we run three CGM models with Exgaussian, Gamma or Gaussian response distributions. For the subsequent analysis, multi-model posterior distributions and posterior predictive distributions are extracted. memory.limit(16000) F_1 &lt;- formula(RT ~ 1 + PrimeGeek + (1 + PrimeGeek|Part)) M_1_gau &lt;- D_hugme %&gt;% brm(F_1, family = gaussian, data = .) M_1_gam &lt;- D_hugme %&gt;% brm(F_1, family = Gamma(link = identity), data = .) M_1_exg &lt;- D_hugme %&gt;% brm(F_1, family = exgaussian, data = .) P_1 &lt;- bind_rows( posterior(M_1_gau), posterior(M_1_gam), posterior(M_1_exg) ) T_1_predict &lt;- bind_rows( post_pred(M_1_gau, thin = 5), post_pred(M_1_gam, thin = 5), post_pred(M_1_exg, thin = 5) ) %&gt;% predict() Note that because the comparison below requires the predictive posterior distribution. With several hundred observations, this results in a very large object. To prevent running into a memory limit, we crank it up (memory.limit) and thin out the number of posterior predictive samples by factor 5. The below plot shows the population-level effects for the three models. The center estimates are very close, which means that neither of the models has a significant bias. However, the Exgaussian model produces much tighter credibility intervals. We have seen such an effect before, when a Poisson model produced tighter intervals than the Negbinomial model. Here it is the other way round: the more flexible model produces better levels of certainty. fixef(P_1) %&gt;% ggplot(aes(y = center, ymin = lower, ymax = upper, x = fixef, color = model)) + facet_wrap(~fixef, scales = &quot;free_y&quot;) + geom_crossbar(width = .2, position = &quot;dodge&quot;) The following residual plot give us a hint why this might be so: Apparently, the Exgaussian model takes a much steeper left climb, whereas In any case, if there is a reason to prefer the Exgaussian model, we should primarily see that in how the residuals are shaped. The Exgaussian distribution has one more degree of freedom, which can be used to set an arbitrary skew. The following reveals that the extra flexibility of the Exgaussian has been employed. Both, Gaussian and Gamma are almost symmetric, whereas the Exgausian takes a steeper left climb. The three distributions have almost the same right tail, but the left tail of the Exgaussian is smaller and the extra probability mass has moved to the center. D_hugme &lt;- D_hugme %&gt;% left_join(T_1_predict) %&gt;% mutate(resid = RT - center) D_hugme %&gt;% ggplot(aes(x = resid, color = model)) + facet_wrap(~PrimeGeek) + geom_density() We can carefully conclude that the Exgaussian may be very useful for analyzing pychological experiments as it seems to better accomodate reaction times, resulting in a better level of certainty. Given the novelty of Exgaussian models, it is recommended that researchers carry out a careful multi-model analysis. In [#model_selection] we will come back to this case with a more formal approach and confirm that the Exgaussian has the better predictive accuracy. detach(Hugme) 7.3.2.2 Time-on-task Experimental psychologists call the Stroop task a complex one. But, essentially it is a decision between three options and minimal processing time is rather short. Compared to tasks in usability studies, such as finding information on websites or renting cars online, this is almost nothing. Also, the complex dynamics of the task are rather different. For example, a single user error at the begin of a task sequence can have dramatic consequences, such as getting lost on a website. While ToT data also has a strictly positive lower boundary (the fastest way of achieving the goal), it often has a much wider spread and more pronounced We compare the three patterns of randomness on the CUE8 data set, which contains ToT measures on five tasks on a car rental website. In this study 14 professional teams took part with two conditions: remote and moderated sessions. As data from the remote condition is contaminated with cheaters, we only use the moderated sessions. In order to compare the impact of the chosen distribution on the coefficient estimates, we include the factor Task as a fixed effect (with treatment contrasts), despite this not being the most meaningful. attach(CUE8) D_cue8_mod &lt;- D_cue8 %&gt;% filter(Condition == &quot;moderated&quot;, !is.na(ToT)) %&gt;% as_tbl_obs() F_4 &lt;- formula(ToT ~ 1 + Task + (1|Part)) M_4_gau &lt;- D_cue8_mod %&gt;% brm(F_4, family = gaussian(link = log), data = ., iter = 2000) M_4_exg &lt;- D_cue8_mod %&gt;% brm(F_4, family = exgaussian(link = log), data = ., iter = 2000) M_4_gam &lt;- D_cue8_mod %&gt;% brm(F_4, family = Gamma(link = log), data = ., iter = 2000) P_4 &lt;- bind_rows( posterior(M_4_gau), posterior(M_4_gam) %&gt;% mutate(value = if_else(value %in% c(&quot;fixef&quot;, &quot;ranef&quot;), exp(value), value)), posterior(M_4_exg) ) T_4_predict &lt;- bind_rows( post_pred(M_4_gau, thin = 5), post_pred(M_4_gam, thin = 5), post_pred(M_4_exg, thin = 5)) %&gt;% predict() Note that the coefficients are on a log-scale for the practical reason that Gamma models are sometimes difficult to estimate on an identity link. Also, when speaking of ToT, it makes sense to speak multiplicative, such as “ToT with Task A is 80% of task B”. fixef(P_4, mean.func = exp) %&gt;% ggplot(aes(y = center, ymin = lower, ymax = upper, x = &quot; &quot;, color = model)) + facet_wrap(~fixef, scales = &quot;free_y&quot;, nrow = 1) + geom_crossbar(width = .2, position = &quot;dodge&quot;) Different to the reaction times in the previous section, the three models produce rather different coefficients. These effects are multiplicative and for Task 3, the three models even disgree on whether this task takes longer than Task 1 (Intercept), which in turn is the only effect where the three models seem to agree. Inspecting the residual distributions yields a different pattern as with reaction times: generally, the left skewness is much less pronounced and Gaussian and gamma even tend to be right skewed. Strikingly, the residuals of the exgaussian models sit much tighter around the center, which corresponds with the narrower credibility intervals for the fixed effects. left_join(T_4_predict, D_cue8_mod, by = &quot;Obs&quot;) %&gt;% mutate(resid = ToT - center) %&gt;% ggplot(aes(x = resid, color = model)) + facet_wrap(~Task, nrow = 1) + geom_density(adjust = 2) detach(CUE8) 7.3.2.3 EDIT: Move RT model selection up In general, it seems that Exgaussian models for RT and ToT accomodates left skewness better and produces estimates that are more conservative and certain at the same time. Could it be true, that Gaussian and gamma models overestimate group mean differences for left skewed RT and ToT responses? We examine this possibility by simulating a well-known experiment using an exgaussian distribution. One last issue remains to get clarified: using the identity link for exgaussian models is very convenient and is probably much safer as compared to Gaussian models with their longer left tails. But, what risk is there to get impossible, i.e. negative, predictions? We check this on the posterior predictive distributions of both studies, CUE8 and Hugme. The following table shows the proportion observations, that get a negative 2.5% credibility limit assigned: bind_rows(Hugme$T_1_predict, CUE8$T_4_predict) %&gt;% group_by(model) %&gt;% summarize(mean(lower &lt; 0)) %&gt;% kable() model mean(lower &lt; 0) M_1_exg 0.000 M_1_gam 0.000 M_1_gau 0.000 M_4_exg 0.015 M_4_gam 0.000 M_4_gau 0.712 7.3.2.4 REDACT For our RT data, impossible predictions is not a big issue with any of the models, as all 2.5% quantiles are positive. That is different for ToT: while the gamma model is inherently immune to negative predictions, the exgaussian model produced a few impossible lower 2.5% limits (around 3%). The Gaussian model is extremely off: more than 70% of all predictions have impossible lower 2.5% limits. In the scientific literature, the coverage on what random pattern to use for RT and ToT data is meager at this moment. Probably, that is due to the lack of user-friendly engines supporting the more exotic GLM family members, gamma or exgaussian regression. The brms engine covers a much broader set of distributions than any other implementation before and researchers have the choice. This chapter attempted to provide theoretical arguments as well as empirical indications that the exgaussian regression is a better choice than Gaussian and gamma. First of all, it accomodates the strong left skew of RT and ToT much better than the gamma, which takes a too symmetric form when far from the left boundary. Second, it is reasonably robust to impossible predictions, even when using the convenient identity link function. Third, and that is almost too good to be true, it massively improves certainty in predictors. Possibly, exgaussian models are more efficient for carving out delicate cognitive effects in comparison to Gaussian models (not to mention non-parametric tests). However, as the discussion has not even fully started, to declare it settled would be premature. In contrast, the aim of this chapter was to illustrate a semi-formal approach that reseachers can follow to choose among the candidate models for their specific RT and ToT data. Data from other RT paradigms might take different shapes. For example, when measuring RT by events in EEG signals (rather than actual key presses), motor time plays a much smaller role, pushing RTs closer to the left boundary. Then, the exgaussian model might produce higher rates of impossible predictions and the gamma model could sufficiently accomodate the left skewness. Note that even using a log link on the exgaussian model can produce visits to the negative range. When both accomodate the left skew equally well, the gamma model is to be preferred as it never produces impossible predictions and is more parsimomous. That being said, the brms engine offers even more opportunities. First, it supports two more distributions with an offset component: the shifted log-normal and the Wiener distribution. Interestingly, the latter grounds on one of the few formally specified cognitive process models, the diffusion model for simple choice tasks. All four parameters of the Wiener distribution are directly linked to individual elements of the cognitive process. This brings us to the second relevant extension of brms, which I will not fully cover, but is worth mentioning: distributional models. The vast majority of statistical analysis capitalizes on the location parameters. We ask whether an increase in a continuous predictor causes an increase in the outcome or if one group has a higher average than the other. Only in the analysis of random effects have we drawn conclusions from dispersion parameters, such as to test Egans claim. In design research, the variance in performance is a crucial issue. To give another example: reportedly, several areas of cognitive functioning deteriorate with age, on average, but variance typically increases. It was the very idea of Dennis Egan that designs should not just improve performance on average, but also keep variance at a minimum. Hence, linking variance to predictors, such as design and age, can be a fruitful endeavour under the paradigm of robust designs. For RT the beforementioned Wiener distribution matches RTs in simple choice tasks and every parameter corresponds with an element in the so called diffusion model. In design research, such ideas are almost unexplored. Perhaps, one day a researcher finds that the gaussian and the exponential component are influenced by different design features. 7.4 Rating scales In classic design research of the last millenium, die-hard Human Factors researchers have mainly been asking objectively sounding questions, like: Can a user achieve accurate results with the system? Can they do so in less time? Is the number of errors reasonably confined? For professional, and especially critical, systems, these are highly valid questions, indeed. The purpose of a medical infusion pump is to improve the health status of a patient by accurately and reliably delivering medication into the bloodstream and the extent to which this can be measured dircetly. However, starting with the 1990s, wave after wave of novel electronic entertainment systems and digital gadgets rolled over the comsumer mass market. The purpose of a video recorder or a smartphone is to deliver joy … and to be sold in large batches. The sole purpose of a commercial website is to sell and nothing else. With the new millenium, design researchers began to recognize what consumer psychologists had discovered two decades earlier: users are not rational decision makers in a utilitarian sense. When people decide to adopt (or buy) a new system, this is only partly driven by their expectation of productivity. These additional expectations are commonly called hedonistic values and cover a broad class of human needs, such as: positive emotions expression of self social connectedness aesthetic perception personal growth Whether or not these concepts are well-defined from a psychological point-of-view is beyond the scope of this book. What matters is that these concepts are so elusive that the most sincere researchers have not yet found objective criteria to measure them. Instead, almost everyone resorts to use of self-report rating scales, like this one: How beautiful do you perceive the user interface to be? unattractive 1 – 2 – 3 – 4 – 5 a piece of art If you use a 5 point rating scale like this one to measure perceived beauty, participants have to convert their gut feeling into a number, which involves the following three processes somewhere in their minds: anchoring introspection binning By anchoring participants establish an idea of how ugly or beautiful something has be to get an extreme rating of 1 or 5. These imaginary endpoints define the absolute range of the rating scale. The researcher might early on give explicit or implicit cues to let the participant guess the range the researcher has in mind. If an experiment is overtly about web design, then probably “very ugly” means the least attractive commercial website the participant can think of. However, participants old enough to remember web design in its infancy (say the early attempts of disney.com), may end up with a lower anchor than today’s kids. If too few cues are given upfront, participants will probably adjust their anchors to the stimuli they see throughout the experiment. Probably, it will make a difference for what 1 or 5 mean, when the set of stimuli contain just websites, or websites and impressionist paintings and screenshots from 1980 splatter movies. By introspection participants intuitively assess the intensity of their real feelings as compared to the anchors. Reportedly, feelings are influenced by: visual simplicity prototypicality second exposure Gestalt principles fluency of processing attribute substitution heuristics color aesthetics fashion previous stimuli current mood a person’s history and cultural background By binning the participant mentally divides the absolute range into five categories that are either fuzzy or defined by stereotypes, like “It must look at least as elegant as a certain other website to get a 4.” As the outcome of anchoring, introspection and binning are not under the control of the researcher, the response patterns can vary between participants. Let’s consider a few possible patterns of participants (and their dramatic stories): A is undecisive and stays in the center region B has a crush on the experimenter and responds slightly more positive C politely avoids the negative extremes D is a human rights activist and habitually treats the seven bins equally. E is annoyed by the experiment (rightly so) and falls into a dichotomous response pattern: 1 or 5 F is a surrealist and has a completely unique way to “look at things”. Many unkowns are in the game. Only one special case we can alleviate with the multilevel modelling tools in our hands. Anchoring can (but not necessarily does) result in a constant shift between participants. Compare participants A and B: A collects almost all stimuli in categories 2, 3 and 4, whereas B uses 3, 4 and 5. This is not much else than a participant-level intercept random effect. Unfortunately, the situation can be more difficult than that. When participants differ in how extreme they set their endpoints, like C and D, than their responses will differ in variance. The maximum variance, however, will be found in participant E. To speak of a real case: In the IPump study, a single-item rating scale was used to measure mental workload. The results suggest that all participants used the lower range of the scale, but differed vastly in where they set their upper point. Figure XY orders participants by the maximum value they used. This is obviously related to variance, but seemingly not so much with location. It does not suffice to use a response distribution with mean-variance relationship, as we used to. All these issues make rating scales peculiar and we should not pretend they have the same neat arithmetic properties as objective measures. attach(IPump) D_pumps %&gt;% group_by(Part) %&gt;% summarize(min = min(workload), max = max(workload), median = median(workload)) %&gt;% mutate(Part_ord = rank(max, ties.method = &quot;first&quot;)) %&gt;% ggplot(aes(x = Part_ord, ymax = max, ymin = min, y = median)) + geom_errorbar() + geom_point(size = 3) detach(IPump) Setting the idiosyncratic rating scale responses aside, how does a common rating scale appear in our framework of link functions and patterns of randomness? Rating scales are bounded on two sides and we already know what that means: a suitable model for rating scales will likely contain a logit link function and a distribution of randomness that is bounded on two sides. A real problem with rating scales is that they often are discrete. Most scales force participants to give their answer as a choice between five or seven ordered levels. When the response variable has just a few levels, ordinal regression is a good choice. Ordinal regression itself is a generalization of logistic regression. However, with most properly designed self-report instruments, a scale comprises several items, because only that can sufficiently reduce measurement error and allow for in-depth psychometric assessments. Take the well-known Cronbach \\(\\alpha\\), which assesses reliability of a scale by correlating every items score with the sum score. Obviously, that only makes sense when there are multiple items. While from a psychometric perspective, single-item scales are susceptible, there can be situations where a researcher may use a validated single item scale for pragmatic reasons. Especially, when measures happen in situ, such as during a usability test or even a real operation, being brief and unobtrusive might be more important than good quality of measures. With multi-item rating scales, one also has the possibility to build a psychometric multi-level model, where items are considered a sample of a population of possible items. That is actually a very good idea, as the item-level random effects control for differences in item location. For example, the following item is likely to produce generally lower beauty ratings than the one shown earlier, because the anchors have been moved downwards: How beautiful do you perceive the user interface? like a screenshot from a splatter movie 1 – 2 – 3 – 4 – 5 quite attractive Unless one builds such a psychometric multi-level model, ordinal regression is not very suitable for multi-item scales and here is why: The sum (or mean) score is still binned, but more finely grained. A sum score over three seven-binned items already has 21 bins, which would result in an inflation of number of parameters in ordinal regression. As a rescue, one might well regard a measure with 21 bins as continuous . Furthermore, there actually is no strong reason to use binned rating scales at all. So called visual analog scales let participants make continuous choices by either drawing a cross on a line or move a slider control. For sum scores and visual analogue scales, the problem of choice reduces to a logit link function (they still have two boundaries) and a continuous distribution bounded on both sides. That is precisely what is behind beta regression and, as we shall see, this distribution is flexible enough to smooth over several of the rating scale pathologies that were just discussed. 7.4.1 Ordered logistic regression When the ordinal response has a low number of response categories (between 4 and 7), ordinal regression applies. Recall logistic regression: the response falls into one of two categories, which are coded as 0 and 1. Although not in a strict sense, the two categories can often be thought of as in an order: success is better than failure, presence more than absence and a return better than staying away. Instead of two categories, we can also conceive the situation as a threshold between the categories, that needs force to jump over it. Any positive impact factor \\(x_i\\) can then be thought of as such a force that pushes a response probability to the higher category, by the amount \\(\\beta_i\\) (on logit scale). At the same time, the intercept \\(\\beta_0\\) represents the basic log-odds of falling into category 1 in a default state, that is \\(x_i = 0\\). In ordinal regression, this idea extends to cases with more than two ordered response categories. The only arising complication is that with two categories, we have one threshold to overcome, whereas with three categories there are two thresholds and generally, with \\(c\\) categories, there are \\(c - 1\\) thresholds. Ordinal regression deals with the problem by estimating \\(c - 1\\) intercept estimates \\(\\beta_{0[k]}\\). Each threshold intercept \\(\\beta_{0[k]}\\) represents the probability (on logit scale) that the response falls into category \\(k\\) or lower, or formally: \\[ \\text{logit}(P(y_i \\leq k)) = \\beta_{0[k]} \\] Let’s see this at the example of the BrowsingAB case, first. User ratings have been simulated with seven levels: attach(BrowsingAB) BAB1 %&gt;% ggplot(aes(x = rating)) + facet_grid(Design~.) + geom_histogram() + xlim(1,7) The brms regression engine implements ordinal regression by the family cratio (cumulative odds ratio ) with a default logit link function. M_ord_1 &lt;- BAB1 %&gt;% brm(rating ~ Design, family = &quot;cratio&quot;, data = .) fixef(M_ord_1) fixef center lower upper NA -13.411 -53.335 -5.928 NA -13.326 -53.333 -5.896 NA -4.053 -5.020 -3.251 NA -2.281 -2.778 -1.796 NA -1.121 -1.525 -0.734 NA 1.321 0.846 1.851 DesignB -0.856 -1.317 -0.410 The six intercepts correspond with the thresholds between the seven levels. It is no coincidence that the intercept estimates increase by order, as they are cumulative (the “c” in cratio). The first intercept estimate represents (the logit of) the proportion of responses \\(y_i \\leq 1\\), the second \\(y_i \\leq 2\\) etc. The Design effect has the usual interpretation as compared to logistic regression, an increase in logit. The only difference is that it refers to all six reference points. The expected proportion of responses equal to or smaller than 2 for design A is: \\[ \\pi(y_i \\leq 2|A) = \\\\ \\text{logit}^{-1}(\\beta_{0[2]}) = \\\\ \\text{logit}^{-1}(-13.3) = \\\\ 1.674\\times 10^{-6} \\] The expected proportion of responses equal to or smaller than 2 for design B we get by the usual linear combination: \\[ \\pi(y_i \\leq 2|B) = \\\\ \\text{logit}^{-1}(\\beta_{0[2]} + \\beta_1) = \\\\ \\text{logit}^{-1}(-14.2) = \\\\ 6.808\\times 10^{-7} \\] All coefficients are shifting all thresholds by the same amount (on the linear predictor scale). You can picture this as a single puppetier controlling multiple puppets by just one stick, making them dance synchronously. As long as the ordinal scale has only a low number of bins, that keeps the number of parameters at a reasonable level. Just imagine, you were estimating an ordinal multilevel model and all participant-level effects were five or sevenfolded, too. However, the equidistancy of effects on bin thresholds is an assumption by itself, and in the presence of response styles on rating scales, it cannot be taken for granted . Besides that, the ordinal model appears very snug to the structure of the data. It does not wipe over the fact that the response is discrete and the thresholds represent the order. Conveniently, effects are represented by a single estimate, which one can use to communicate direction and certainty of effects. On the downside, communicating absolute performance (that is, including the intercept) is more complicated. When presenting predictions from an ordinal model one actually has to present all thresholds, rather than a single mean. In practice that probably is less relevant than one might think at first, because predictions on self-report scales is less useful than metric performance data. Ordinal data also does not lend itself so much to further calculations. For example, you can use ToT measures on infusion pumps in calculating the required staffing of an intensive care unit, because seconds are metric and can be summed and divided. In contrast, it does not make sense to calculate the cognitive workload of a team of nurses by summing their self-report scores. The only possibility is to compare the strengths of predictors, but that does not require predictions. detach(BrowsingAB) 7.4.2 Beta regression One of the most futile discussions in methodology research to my mind is whether one should use a four, five or seven binned Likert scale. From a pure measurement point of view, more bins give better resolution, the ultimate consequence being not to bin at all, that is using continuous rating scales. At the same time, many rating responses come from multiple item scales, which multiplies the number of bins. Speaking of ordinal regression, it seems reasonable to have seven intercepts for a single item scale, but who would want 14 or 21 for a two or three-item scale? And most scales have more items than that, which is good from a psychometric perspective. In fact, psychometric research in the process of developing rating scales routinely uses a method called confirmatory factor analysis, which derives from the Gaussian linear model and inherits its assumptions. Not surprisingly, most research applying the very same instruments also use plain linear models. It seems fair enough to take a multi-item scale as a continuous measure, but given the framework of GLM, it is unnecessary (to put it mildly) to go along with the assumptions of Normality and linearity. While the link function for a double bounded response variable is simply the logit, the only missing ingredient is a double bounded error distribution. Enter beta distribution! We demonstrate beta regression on rating scales at the example of the CUE8 study. This study aimed at assessing whether remote usability testing arrives at the same ToT measures as in moderated sessions. As we have seen in [CROSSREF], the difference is marginal. But, rating scales are susceptible for all kinds of cognitive and social biases. For that reason, a golden rule for user test moderators is to constantly remind participants to not blame themselves for errors. Reportedly, test moderators also do help participants (after counting to 10) in order to minimize frustration (and maximize information flow). What could the presence or absence of a moderator do to satisfaction ratings? Perhaps, remote participants feel the lack of assurance and support as higher levels of frustration. Furthermore, it is not unlikely that satisfaction ratings are sensitive to idiosyncratics in the process and setting of the user test, such that we could even expect differences between teams. Before we build the model, there are two issues to regard: First, the boundaries are 0 and 1, which requires a rescaling of responses into this interval. The SUS scores are on a scale from 0 to 100 and a divisor of 100 would produce the desired interval. Second, the responses must actually lie strictly between 0 and 1, excluding the boundaries. On (quasi)continuous scales, it seems not very likely to have 0 or 1 as response, but it can happen. Indeed, participants in the CUE8 study have responded with a satisfaction rating of 100 quite often. A practical solution is to scale the responses in such a way as to avoid the two boundaries, which is what the following hack does. add a tiny value to all responses create a divisor by adding twice that value to the maximum value the responses can take divide all responses by that divisor You may find it inappropriate to mangle a response variable in such an arbitrary way. However, keep in mind that the levels of ordinal responses are highly arbitrary. In terms of measurement theory, all transformations that maintain the order are permitted for ordinal scales. For the following analysis, the data set was further reduced by averaging the scores across tasks and excluding probable cheaters with a ToT &lt; 30s. attach(CUE8) D_cue8_SUS &lt;- D_cue8 %&gt;% filter(!is.na(SUS)) %&gt;% group_by(Part, Team, Condition) %&gt;% dplyr::summarize(ToT = sum(ToT), SUS = mean(SUS)) %&gt;% ungroup() %&gt;% filter(ToT &gt; 30) %&gt;% mutate(SUS = (SUS + 1)/(100 + 2)) %&gt;% ## rescaling to ]0;1[ as_tbl_obs() D_cue8_SUS %&gt;% ggplot(aes(x = Team, y = SUS, fill = Condition)) + geom_violin() + geom_jitter() M_5_bet &lt;- D_cue8_SUS %&gt;% brm(SUS ~ Condition + (1 | Team), family = Beta(link = &quot;logit&quot;), iter = 0, chains = 1, data = .) M_5_bet &lt;- D_cue8_SUS %&gt;% brm(fit = M_5_bet, data = .) sync_CE(M_5_bet, Env = CUE8) M_5_bet ## Family: beta ## Links: mu = logit; phi = identity ## Formula: SUS ~ Condition + (1 | Team) ## Data: . (Number of observations: 363) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~Team (Number of levels: 7) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.81 0.39 0.34 1.95 1.01 554 231 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.79 0.52 -0.51 1.78 1.00 567 234 ## Conditionmoderated -0.34 0.70 -1.70 1.11 1.00 1124 1088 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## phi 4.13 0.30 3.56 4.74 1.00 3390 2547 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). fixef(M_5_bet) fixef center lower upper Intercept 0.798 -0.506 1.78 Conditionmoderated -0.343 -1.699 1.11 grpef(M_5_bet) type fixef re_factor center lower upper grpef Intercept Team 0.71 0.344 1.95 Do participants in remote sessions feel less satisfied? There seems to be a slight disadvantage, but we cannot confirm this with sufficient certainty. In contrast, the variation between teams is substantial, which indicates that SUS ratings are not independent of the particular setting. That is rather concerning for a widely used and allegedly validated rating scale. 7.4.3 Distributional models The framework of GLM, as flexible as it has proven to be up to this point, has one major limitation: it renders the relationship between predictors and the location of the response. We only think in terms of impact factors that improve (or damage) average response times, error rates, satisfaction ratings etc. one would think that multi-level models deal with variance to a large extent, and they do. But, they only give us the variance of a certain effect on a set of objects. That is absolutely not the same as asking: “Do teams in CUE8 differ in the variation of responses?”. That brings me to the final feature of modern regression modelling in the scope of this book: GLMs greatly enhanced the scope of modelling by giving us the choice of response distributions and linearizing functions. Still, all models introduced so far establish an association between predictors and the fitted response \\(\\mu\\), only. However, all but the one-parameter distributions come with additional parameters that that form the shape of the error distribution, such as its variance or skew in the case of Exgaussian models. Experimentalists usually consider error variance as a nuisance parameter, but in design research it can well be informative. Strong variance can be a real problem, because this implies that extremely poor performance is possible, increasing the risk of hazards. In hazardous environments, variance is a crucial parameter should be track carefully. Especially, a design improvement on average is not necessarily accompanied by less variance (other than the mean-variance relationship prescribed by the error distribution). As a first illustration, imagine two versions of a continuous rating scale for visual beauty that differ in how their extreme levels are labelled: like the ugliest website I have ever seen: 0 ——– 1 like the most beautiful website distgusting as a screenshot from a splatter movie: 0 ——– 1 like the most beautiful sunset For the sake of simplicity (not for a strong research design), let us assume that one participant has rated a sample of 400 websites in two conditions: narrow anchoring and wide anchoring. The following simulates data as if there were just a minor positive shift of the wider condition, accompanied by an immense up-scaling. set.seed(42) N = 400 Conditions &lt;- tribble(~Anchoring, ~mu, ~phi, &quot;wide&quot;, .3, 18, &quot;narrow&quot;, .3, 6) %&gt;% mutate(a = mu * phi, b = phi - mu * phi) D_Anchor &lt;- tibble(Obs = 1:N, Anchoring = rep(c(&quot;wide&quot;, &quot;narrow&quot;), N/2)) %&gt;% left_join(Conditions) %&gt;% mutate(rating = rbeta(N, a, b)) D_Anchor %&gt;% ggplot(aes(x = Anchoring, y = rating)) + geom_violin() + ylim(0,1) The two response distributions have the same location, but the more narrow anchoring produces a wider dispersion of responses. How would we confirm this statistically? The Brms engine can link predictors to any other parameter of the response distribution, which teh author of the package calls distributional models. They have an immense potential as they relax another assumption of GLM, namely that all variance parameters must strictly follow the mean-variance relationship demanded by a distribution family. As we seen, one can easily create a case where this assumption is violated. The brms engine uses a parametrization of the beta distribution (other than the more common a, b parametrization), where \\(\\mu\\) is the location and \\(phi\\) is a scale parameter. Different to what its name may suggest, a large scale is not an increase variance, but rather the opposite: It is comparable to the number of trials in a binomial process. The more trials there are, the tighter the distribution becomes. For Beta distributions, a large \\(\\phi\\) results in reduced dispersion and vive versa . Accordingly, when used in a distributional model, a positive effect decreases variance. When estimating dispersion or scale parameters, we have to regard that these are positive, strictly. The Brms engine simply extends the principle of link functions to parameters other than the \\(\\mu\\) and sets a default log link for \\(\\phi\\). In order to estimate the changes in \\(\\mu\\) and \\(\\phi\\) simultaneously, the brms engine receives two regression formulas. Having multiple formulas for a regresson model is a notable extension of the R model specification language, which is why Brms brings its own command bf() to collect these. We run a distributional Beta regression on the simulated rating scale responses: M_beta &lt;- brm(bf(rating ~ 1 + Anchoring, phi ~ 1 + Anchoring), family = Beta(), data = D_Anchor) The coefficient table below contains the two regular coefficients and as expected, there is no difference in location. The intercept on scale parameter \\(\\phi\\) is the scale in the narrow condition (with wider variance). The treatment effect on \\(\\phi\\) is positive on the log scale, which means it is deflates variance, just as expected. clu(M_beta) parameter fixef center lower upper b_Intercept Intercept -0.318 -0.544 -0.095 b_phi_Intercept NA 1.630 1.247 1.986 b_Anchoringwide Anchoringwide -0.529 -0.788 -0.274 b_phi_Anchoringwide NA 1.504 0.968 2.023 In [#EXgaus], the Exgaussian model seemed to sit well with reaction times, accomodating their left skew better. But if we review the participant-level plots carefully, we see different shapes. There is visible differences in variance, as well as in skew. The following distributional model estimates the same location effects (PrimeGeek), but give all participants their own shape, by a random factor. Two distributional parameters, \\(\\sigma\\) for variance and \\(\\beta\\) for skew are added to the multi-line formula interface. The maximum distributional model would estimate fully separate distributions per every participant and condition. But, since the two response distributions (Geek/nogeek) are similar for most participants, a reduced predictor term is used, assuming that variance and shape are not affected by PrimeGeek: attach(Hugme) M_1_exg_dist &lt;- brm(formula = bf(RT ~ 1 + PrimeGeek + (1 + PrimeGeek|Part), sigma ~ 1 + (1|Part), beta ~ 1 + (1|Part)), family = exgaussian(), data = D_hugme, inits = 0) Note that on such exotic parameters, Brm sometimes has difficulties in finding valid good starting values for the MCMC walk. Like here, it often helps to fix all starting values to Zero. If participants show different patterns of randomness, we should see that in participant-level variation of \\(\\sigma\\) and \\(\\beta\\), which is confirmed by certainly positive variance estimates. P_1_exg_dist &lt;- posterior(M_1_exg_dist) P_1_exg_dist %&gt;% filter(type %in% c(&quot;grpef&quot;, &quot;cor&quot;)) %&gt;% group_by(parameter) %&gt;% summarize(center = median(value), lower = quantile(value, .025), upper = quantile(value, .975)) %&gt;% ungroup() parameter center lower upper cor_Part__Intercept__PrimeGeekTRUE -0.104 -0.389 0.216 sd_Part__beta_Intercept 0.474 0.394 0.589 sd_Part__Intercept 0.113 0.095 0.137 sd_Part__PrimeGeekTRUE 0.027 0.020 0.035 sd_Part__sigma_Intercept 0.445 0.352 0.564 Note that the Bayr package does not fully support distributional models, yet, and we have to make without some of the high-level commands, such as grpef. Can we think of a reason for why participants differ in the skew of their response distribution? Perhaps. Highly, motivated participants are constantly pushing against the physical boundaries of the system, creating a sharp left rise. Generalized Linear Models gave us control over the average shape of randomness, average variance and average skew. When we use these models, we gain control over the data generating process on an individual level. Distributional models have enormous potential for research and applications. Rating scales are vulnerable to different response patterns when the anchoring is ambivalent. But that is not the only possible response pattern. For example, in our own studies on the Uncanny Valley effect, we used analog rating scales and found some participants to reduce the task to a binary answer (left or right end). A Beta distribution can handle such a pattern well, but only if it is consistent. Variance and skew may even carry additional information, for example on mental workload. Imagine you are evaluating a simulator-based driver training, designed with the idea that movement tasks are better learned if you begin fast, not accurate. This training uses speed episodes, where the trainee is asked to do 20% faster, at the expense of accuracy. In such an experiment, workload can play a moderating factor, because if you work very hard, you can be fast and accurate at the same time. And a few things you only learn, when you try really hard. Try you must! detach(Hugme) "],
["ME.html", "8 Working with models 8.1 Model criticism 8.2 Model selection", " 8 Working with models In the chapter [#LM], we have seen a marvelous variety of models spawning from just two basic principles, linear combinations and a Gaussian error term. In this chapter I will introduce methods to critically assess models and how to compare models. Every model we use on data is an ideal, that approximates reality, more or less. But how close is a model to reality? Under a Bayesian perspective, reality falls into two parts: past and future. We begin with model criticsm where we examine how well one model fits data, we have gathered in the past. In model comparison methods are introduces to decide which model performs better at predicting future data. With linear models we can combine and recombine any set of predictors in many ways. We saw examples, where the more simple of two model seemed to work better, but often it was the more complex conditional effects model [CEM] that was closer to the data. Visual methods to assess goodness-of-fit, that help scrutinize the structural part of linear models. The random part is rather inflexible for Linear Models, where the error follows Gaussian distribution, with a constant variance. Checking these assumptions is based on residual analysis. The second part deals with the problem of comparing multiple models. Predictive accuracy is introduced a a criterion to compare models. Methods to measure predictive accuracy are demonstrated and we will see how pruning unnecessary parameters can improve a model. Finally, model comparison can also be used the other way round: in hypotheses testing, researchers usually aim at showing that a parameter is neccessary. 8.1 Model criticism For a start, I would like to contrast the overall approach of model criticism to the workflow frequently encountered in classic statistics. Boldly speaking, classicically trained researchers often assume that the “assumptions of ANOVA” need to be checked beforehand. Often a process called assumptions checking is carried out before the researcher actually dares to hit the button labelled as RUN ANOVA. As we will see in this section, the order of actions is just the other way round. The straight-forward way to check the integrity of a model is to examine the fitted model and find its flaws, which is called model criticism. Another symptom of classic assumption checking is the batteries of arcane non-parametric tests that is often carried out. This practice has long been criticized in the statistical literature for its logical flaws and practical limitations. Here, I will fully abandon hypothesis tests for model criticism and demonstrate graphical methods, which are based on two additional types of estimates we can extract from linear models (next to coefficients): with residuals we test the assumptions on the shape of randomness with fitted responses we check the structural part of the model 8.1.1 Residual analysis So far, it seems, linear models equip us well for a vast variety of research situations. We can produce continuous and factorial models, account for saturation effects, discover conditional effects and even do the weirdest curved forms. just by the magic of combining linear coefficients. However, the random term stoically stayed the same every time, that it often wasn’t worth mentioning: \\[y_i \\sim N(\\mu_i, \\sigma_\\epsilon)\\] In words, the randomness term says: observed values \\(y_i\\) are drawn from a Gaussian distribution located at fitted response \\(\\mu_i\\), having a fixed standard deviation \\(\\sigma_\\epsilon\\). In the notation displayed above, there are indeed as many distributions as there are observed values (due the subscript in \\(\\mu_i\\)). It appears impossible to evaluate not just one distribution, but this many. However, an equivalent notation is routinely used for linear models, that specifies just one residual distribution. For the LRM that is: \\[ \\mu_i = \\beta_0 + \\beta_1x_1\\\\ y_i = \\mu_i + \\epsilon_i\\\\ \\epsilon_i \\sim \\textrm{Gaus}(0, \\sigma_\\epsilon) \\] In this notation, observed values \\(y_i\\) are decomposed into predicted values and individual residuals \\(\\epsilon_i\\). These are frequently called errors, hence the greek symbol \\(\\epsilon\\). The standard deviation of residuals \\(\\sigma_\\epsilon\\) is commonly called the standard error. The random pattern of the model can now be expressed as a single Gaussian distribution. The reason why I do not use this notation routinely, is that it only works for linear models, but not for models with other random patterns. More specifically, Generalized Linear Models @ref(generalized_linear_models) cannot be specified that way. But, for the purpose of residual analysis, it appears more intuitive. In the previous section we have seen how to extract fitted responses \\(\\mu_i\\) from estimated models. With fitted responses, we evaluate the structural part of the model, the linear connection of coefficients, which practically always carry the research question. to the main objectives of the research. Residual analysis looks at the pattern of randomness, which the classic linear notation defines as Gaussian distributed with constant variance. \\[ \\epsilon_i \\sim \\textrm{Gaus}(0, \\sigma_\\epsilon) \\] As we have seen in 4.4.2, Gaussian distributions are one pattern of randomness among many and this choice may therefore be appropriate, or not. As a heuristic, if observed values are located rather in the middle of the possible range of measurement (and avoid the edges), Gaussian distributions work well. But, like linearity is compromised by saturation, a distribution is affected when squeezed against a hard boundary, as will be discussed in more depth in [GLM]. Like with fitted responses, we will use graphical methods, to assess these two assumptions. When either one is violated, the validity of the model is compromised, although maybe not as much as a linear regression model is compromised curved associations like the Unbcanny Valley effect. It is noteworthy at this point, that these two assumptions severely limit what you may actually do with linear models. In some cases, the these assumptions are met well enough, but in many data sets they are visibly violated. Luckily, the linear model with a Gaussian term has been superceded by a class of models that give us more choice from patterns of random ness and let’s accomodate pattern of randomness as it is. I may go as far as saying: some readers can skip this part and jump right to chapter [#GLM]. 8.1.1.1 Gaussian distribution The first assumption of randomness underlying the linear model simply is that the distribution follows a Gaussian distribution. Visually, Gaussian distributions is characterized by: one curved peak (unimodality) from which density smoothly declines towards both ends (smoothness) at same rates (symmetry) For a rough evaluation of this assumption, it suffices to extract the residuals from the model at hand and plot it as a distribution. The residuals command returns a vector of residual values, exactly one per observation. With the vector of residuals at hand, we can evaluate this assumption by comparing the residual distribution to its theoretically expected form, a perfect bell curve. The following command chain extracts the residuals from the model and pipes them into the ggplot engine to create a histogram. With stat_function an overlay is created with the theoretical Gaussian distribution, which is centered at zero. The standard error \\(\\sigma_\\epsilon\\) has been estimated alongside the coefficients and is extracted using the function clu. attach(BrowsingAB) tibble(resid = residuals(M_age_shft)) %&gt;% ggplot(aes(x = resid)) + geom_histogram(aes(y = ..density..), bins = 15) + stat_function(fun = dnorm, args = c(mean = 0, sd = clu(M_age_shft, type = &quot;disp&quot;)$center), colour = &quot;red&quot;) The match of residual distribution with the theoretical distribution is not perfect, but overall this model seems to sufficiently satisfy the normality assumption. To give a counter example, we estimate the same model using the outcome variable returns, which captures the number of times a participant had (desparately) returned to the homepage. M_age_rtrn &lt;- stan_glm(returns ~ 1 + age_shft, data = BAB1) P_age_rtrn &lt;- posterior(M_age_rtrn) T_age_rtrn &lt;- clu(P_age_rtrn) T_age_rtrn parameter type fixef center lower upper Intercept fixef Intercept 2.564 2.02 3.113 age_shft fixef age_shft -0.004 -0.02 0.012 sigma_resid disp NA 1.872 1.70 2.064 We now graphically compare the distribution of estimated residuals against a perfect Gaussian distribution with the same standard error. The first chain in teh code extracts the center estimate from the table of estimates. C_age_rtrn_sd &lt;- T_age_rtrn %&gt;% filter(type == &quot;disp&quot;) %&gt;% select(center) %&gt;% as.numeric() tibble(resid = residuals(M_age_rtrn)) %&gt;% ggplot(aes(x = resid)) + geom_histogram(aes(y = ..density..), bins = 10) + stat_function(fun = dnorm, args = c(mean = 0, sd = C_age_rtrn_sd), colour = &quot;red&quot;) + xlim(-5, 5) detach(BrowsingAB) The estimation produces the usual coefficients, as well as a standard error. However, the residuals do not even remotely resemble the theoretical curve. While it is unimodal, it appears rather asymmetric, with a steep rise to the left and a long tail to the right. That is a typical outcome when count measures get too close to the left boundary. How about unimodality? We have not discussed any multimodal theoretical distributions in 4.4.2, but one has been displayed in @ref(first_program). In brief, a bimodal residual distribution can arise, when two groups exist in the data, which lay far apart. The following code illustrates the situation by simulating a simple data set with two groups, that is fed into a GMM. attach(Chapter_LM) set.seed(42) D_bimod &lt;- bind_rows( tibble(Group = &quot;A&quot;, y = rnorm(50, 4, 1)), tibble(Group = &quot;B&quot;, y = rnorm(50, 8, 1)) ) M_bimod &lt;- stan_glm(y ~ 1, data = D_bimod, iter = 500) D_bimod %&gt;% mutate(resid = residuals(M_bimod)) %&gt;% ggplot(aes(x = resid)) + geom_density() These two deviations from Gaussian distribution have very different causes: asymmetry is caused by scales with boundaries. This is an often arising situation and it is gracefully solved by Generalized Linear Models 7. This is a family of models, where each member covers a certain type of measurement scale. In the above example, Poisson regression, applies, taking care of count measures. Multimodality is caused by heterogeneous groups in the data, like experimental conditions, design or type of user. For a grouping structure to cause distinguished multimodality, differences between groups have to be pronounced in relation to the standard error. It is often the case, that these variables are controlled conditions, such as in an AB test. It is also quite likely that strong grouping structures can be thought of beforehand and be recorded. For example, in usability tests with diverse user samples, it almost comes natural to distinguish between users who have used the design before and those who did not. If the grouping variable is recorded, the solution is group comparison models 5.3.1, already introduced. Visual assessment of symmetry and unimodality is simple and effective in many cases. But, Gaussian distributions are not the only ones to have these properties. At least logistic distributions and t distributions have these, too, with subtle differences in curvature. Gaussian and t distributions differ in how quickly probability drops in the tails. Gaussian distributions drop much faster, meaning that extreme events are practically impossible. With t-distributions, extreme values drop in probability, too, but the possibility of catastrophies (or wonders) stays substantial for a long time. Provided one has a small abundance of data, quantile-quantile (qq) plots can be used to evaluate subtle deviations in curvature (and symmetry and unimodality). In qq plots, the observed and theoretical distributions are both flattened and put against each other. This is a powerful and concise method, but it is a bit hard to grasp. The following code illustrates the construction of a qq-plot that compares GMM residuals of a t-distributed measure against the Gaussian distribution. We simulate t-distributed data, run a GMM and extract residuals, as well as the standard error \\(\\sigma_\\epsilon\\). set.seed(2) D_t &lt;- tibble(y = rt(200, 2)) M_t &lt;- stan_glm(y ~1, data = D_t, iter = 500) We obtain the following residual and theoretical distributions. It is approximately symmetric and unimodal, but the curvature seems to be a bit off. D_t &lt;- mutate(D_t, resid = residuals(M_t)) C_sigma &lt;- rstanarm::sigma(M_t) D_t %&gt;% ggplot(aes(x = resid)) + geom_histogram(aes(y = ..density..)) + stat_function(fun = dnorm, args = c(mean = 0, sd = C_sigma), colour = &quot;red&quot;) The next step is where the two curves get flattened. First, we compute a sequence of quantiles with fixed steps, say 1%, 2%, … 99%%. Finally, theoretical and observed quantiles are fed into a scatterplot. D_QQ &lt;- tibble(step = 0:100/100, quant_smpl = quantile(D_t$resid, step), quant_theo = qnorm(step, 0, C_sigma)) D_QQ %&gt;% ggplot(aes(x = quant_theo, y = quant_smpl)) + geom_point() + geom_abline(slope = 1, intercept = 0, col = &quot;red&quot;) In the ideal case, they match perfectly and the quantiles are on a straight line. Instead, we see a rotated sigmoid shape and this is typical for fat-tailed distributions such as t. The shape is symmetric with turning points at around -4 and 4 on the theoretical scale. In the middle part the relation is almost linear, however, not matching a 1-by-1. The t distribution loses probability mass rather quickly when moving from the center to the turning points. From these points on the theoretical quantiles start to lag behind. The lower and upper 1% sampled quantiles go to much more extreme values, ranging from -10 to almost 20, whereas the Gaussian distribution renders such events practically impossible. Generally, a rotated sigmoid shape is typical for fat tailed distributions. The problem of misusing a Gaussian distribution is that it dramatically underestimates extreme events. Have you ever asked yourself, why in the 1990s, the risk for a nuclear meltdown were estimated to be one in 10.000 years, in face of two such tragic events in the past 40 years? Perhaps, researchers used the Gaussian distribution for the risk models, under-estimating the risk of extreme events. The ggplot engine provides an easy to use geometry for qqlots, which lets us further explore deviant patterns. Variables with t distribution take an inverse-sigmoid shape due to their fat tails. D_t %&gt;% ggplot(aes(sample = resid)) + geom_qq(distribution = qnorm) + geom_abline(intercept = 0, slope = 1, col = &quot;red&quot;) detach(Chapter_LM) Once mastered, the qq-plot is the swiss knife of Normality check. Next to the subtleties, we can also easily discover deviations from symmetry. This is how the residuals of the returns-to-homepage model look like: tibble(resid = residuals(BrowsingAB$M_age_rtrn)) %&gt;% ggplot(aes(sample = resid)) + geom_qq(distribution = qnorm) + geom_abline(intercept = 0, slope = 1, col = &quot;red&quot;) To the left, extreme values have a lower probability than predicted by the Gaussian distribution, but the right tail is much fatter, once again. We also see how residuals are clumped, which is characteristic for discrete (as compared to continuous) outcome measures. This is poor behaviour of the model and, generally, when a model is severely mis-specified, neither predictions nor estimates, nor certainty statements can be fully trusted. A model that frequently fits in case of count numbers is Poisson regression, which will enter the stage in chapter @ref(poisson_regression). 8.1.1.2 Constant variance In @ref(resid_normality), we assessed one assumption that underlies all linear models, namely Gaussian distribution of residuals. The second assumption underlying the linear model random (or residual) is that residual variance is constant throughout the whole range. In both classic and modern notation, this stems from the fact that there is just a single \\(\\sigma_\\epsilon\\) defined. However large \\(\\mu_i\\) is, the dispersion of residuals is not supposed to change. Before we dive into the matter of checking the assumption, let’s do a brief reality check using common sense: Consider people’s daily commute to work. Suppose you ask a few persons you know: “What is your typical way to work and what is the longest and the shortest duration you remember?”. In statistical terms, you are asking for a center estimate and (informal) error dispersion. Is it plausible that a person with typical travel time of 5 minutes experienced the same variation as another person with a typical time of 50 minutes? Consider an experiment to assess a typing training. Is it plausible that the dispersion of typing errors before the training is the same as after the training? In both cases, we would rather not expect constant variance and it is actually quite difficult to think of a process, where a strong change in average performance is not associated with a change in dispersion. The constant variance assumption, like the normality assumption is a usual suspect when approximating with linear models. We will come back to that down below. In a similar way, we can ask: can it be taken for granted that residual variance is constant when comparing two or more groups? Would you blindly assume that two rather different designs produce the same amount of spread around the average? It may be so, but one can easily think of reasons, why this might be different. We check the situation in the CGM of the BrowsingAB study. Do both design conditions have the same residual variance? Again, we extract the residuals, add them to the data set and produce a boxplot by Design condition: attach(BrowsingAB) BAB1 %&gt;% mutate(resid_CGM = residuals(M_CGM)) %&gt;% ggplot(aes(x = Design, y = resid_CGM)) + geom_boxplot() Both sets of residuals are reasonably symmetric, but it appears that design B produces more widely spread residuals. Something in the design causes individual performance to vary stronger from the population mean. The cause of this effect will be disclosed in @ref(differential_design_effects). (In essence, design B is rather efficient to use for younger users, whereas older users seem to have severe issues.) Visual checks of constant variance for factors is straight forward using common boxplots. For continuous predictors, such as age, requires a more uncommon graphical representation known as quantile plots. These are not the same as qq plots, but luckily are included with ggplot. BAB1 %&gt;% mutate(resid_age = residuals(M_age)) %&gt;% ggplot(aes(x = age, y = resid_age)) + geom_point() + geom_quantile() The quantile plot uses a smoothing algorithm (probably not unlike LOESS) to picture the trend of quantiles (25%, 50% and 75%). Here, the quantiles run almost horizontal and parallel, which confirms constant variance. Taking this as a starting point, we can evaluate more complex models, too. The grouped regression model on age and design just requires to create a grouped quantile plot. This looks best using facetting, rather than separating by colour: BAB1 %&gt;% mutate(resid_grm_1 = residuals(M_grm_1)) %&gt;% ggplot(aes(x = age_shft, y = resid_grm_1)) + facet_grid(~Design) + geom_point() + geom_quantile() This looks rather worrying. Especially with Design A, the residuals are not constant, but increase with age. In addition, we observe that residuals are not even centered at zero across the whole range. For design A, the residual distribution moves from positive centered to negative centered, design B vice versa. That also casts doubts on the validity of the LRM on age: these contrariwise trends seem to mix into an unsuspicious even distribution. It seems that a lot more has been going on in this study, than would be captured by any of these models. detach(BrowsingAB) Another model type we may want to check with quantile plots is the MRM. With two continuous predictors, one might be tempted to think of a 3-dimensional quantile plot, but this is not recommended. Rather, we can use a generalization of quantile plots, where the x-axis is not mapped to the predictor directly, but the predicted values \\(\\mu_i\\). We assess the residual variance on the MRM model on the AUP study, where resistance to fall for the active user paradox has been predicted by geekism tendencies (gex) and need-for-cognition (ncs): attach(AUP) AUP_1 %&gt;% mutate(resid_3 = residuals(M_3), mu_3 = predict(M_3)$center) %&gt;% ggplot(aes(x = mu_3, y = resid_3)) + geom_point() + geom_quantile() We observe a clear trend in quantiles, with residual dispersion increasing with predicted values. Generally, plotting residuals against predicted values can be done with any model, irrespectively of the number and types of predictors. However, interpretation is more limited than when plotting them against predictors directly. In fact, interpretation boils down to the intuition we introduced at the beginning of the section, that larger outcomes typically have larger dispersion. This is almost always a compelling assumption, or even a matter of underlying physics and, once again, a linear model may or may not be a reasonable approximation. Fortunately, when we turn towards @ref(generalized_linear_models), we will find that these models provide more reasonable defaults for the relationship between predicted values and dispersion. In contrast, residual variance per predictor allows to discover more surprising issues, such as conditional effects or heterogeneity in groups. detach(AUP) 8.1.2 Fitted responses analysis Frequently, the research question is how strong the effect of a predictor variable is on the response. This strength of effect is what coefficient table tell us. But, how do we know that the model actually fits the process under examination? We have seen several examples of models that do not align well with the observation, in particular when plain MPM were used in the presence of conditional effect. It is tempting to jump to the question, which of two (or more) models is best. I’ll reserve that for the final section [APA]. Here I will introduce technqiue to assess whether a model fits the data well, that is based on this model alone. More precisely, the method is based on fitted responses, which is the \\(\\mu_i\\) that appears in the model formula. We will see that, just like coefficients estimates, fitted responses can be extracted as CLU tables. In essence, by comparing fitted responses to observed responses \\(\\y_i\\), we can examine how well a model actually fits the data and identify find potential patterns in the data that the model ignores. Remember that all linear model assume that an observed value is composed of a structural component, the one that repeats across observations, and a Gaussian distributed error, as can best be seen in the classic notation: \\[ y_i = \\mu_i + \\epsilon_i\\\\ \\mu_i = \\beta_0 + \\beta_1 x_i\\\\ \\epsilon_i \\sim \\textrm{Gaus}(0, \\sigma) \\] The first line of the formula does the separation between structural part and pattern of randomness: by estimation, \\(\\mu_i\\) is the model’s idealized response and \\(\\epsilon_i\\) is the error, taking the form of a zero-centered Gaussian distribution. The purpose of investigating model fit is to find structural features in the obervations that are not rendered by the model at hand. We have already encountered multiple situations like this: BrowsingAB: Does an unconditional LRM suffice for the relationship between age and performance? IPump: Do both designs have the same learning progress, or do we need conditional effects? Uncanny: Does the cubic polynomial provide a right amount of flexibility (number of stationary points), or would a simpler model (a quadratic polynomial) or a more complex model render the relationship better? Before we come to the method, a warning: Fitted responses are more frequently called predicted values, and accordinly are extracted by the predict() function. This term is in two ways confusing: first, what values? Fair enough, but the second is more profound: The “predicted values” is suggesting that you can use them for forecasting future responses. Well, you can, but only after you tested forecasting accuracy. In [#APA] we will see that one can easily create a snug fitting model, without any value for forecasting. (And we will learn how to compare the forecasting ability of models.) As [McElreath] points out, one should rather call predicted values retrodictions, which is a term that avoids the second I equally like. because they are bound to the current data. I believe the better term is fitted responses, because it reminds us that we are talking about idealized responses under one model. That should keep imaginations in check. The match between idealized responses and the original observations is called model fit. And that is “fit” as in “fitted” and not as in “fitness”. We can always compute fitted responses by placing the estimated coefficients into the model formula. A more convenient way is to use the standard command predict() on the model object. The package bayr provides a tidy version, that produces tidy CLU tables. attach(BrowsingAB) T_pred_age_shft &lt;- predict(M_age_shft) T_pred_age_shft Obs center lower upper 48 191 99.0 281 70 187 95.5 273 95 178 92.0 270 125 184 92.1 275 141 189 100.9 279 There will always be as many fitted responses as there are responses and they come in the same order. They can be attached to the original data, which is very useful for evaluating model fit. BAB1 &lt;- BAB1 %&gt;% mutate(M_age_shft = T_pred_age_shft$center) Note that only the center estimate is extracted from the predicted and call the new variable like the model. Later, this will help to collect fitted responses from multiple models. The evaluation of model fit is a visual task (at this stage). We start with a plot of the raw data, together with a LOESS. BAB1 %&gt;% ggplot(aes(x = age, y = ToT)) + geom_point() + geom_smooth(aes(linetype = &quot;LOESS&quot;)) + geom_smooth(aes(y = M_age_shft, linetype = &quot;M_age_shft&quot;), se = F) + labs(linetype = &quot;fitted response&quot;) Note that the sequence of smooth geometries all use different sources for the y coordinate. The literal values for the color aesthetic produce the legend; the legend title is created by labs(). The only two features the LOESS smoother and the linear model have in common is their total upward trend and fitted responses at ages 20 and 50. The LOESS indicates some wobbliness in the response, with something that even could be a local maximum at around age 37. One has to be a little careful here, because LOESS is just another engine that produces a set of fitted responses. LOESS i a very flexible model, and as we will see in [APA], flexible models tend to over-fit, which means that they start to pull noise into the structural part. This results in less accurate forecasts. In conclusion, LOESS and LRM tell different stories and we cannot tell which one is closer to the truth without further investigation. Conditional effects are always among the suspects, when in comes to non-linearities. The wobbly relationship could be the result of a mixture of conditions. By pulling the Factor Design into the graph, we can assess how well the unconditional model fits both conditions. G_Design_age &lt;- BAB1 %&gt;% ggplot(aes(x = age, y = ToT, col = Design)) + geom_point() + geom_smooth(aes(linetype = &quot;LOESS&quot;), se = F) + labs(linetype = &quot;fitted response&quot;) G_Design_age + geom_smooth(aes(y = M_age_shft, linetype = &quot;M_age_shft&quot;), se = F) It turns out, that the MRM only fits for part of the obervations, namely those on design B and between age of 35 and 65. Most strikingly, the model seems to fit worst at age of 20, which we identified as a point of good prediction. As it turns out, this is just a good prediction in total, with strongly deviating local intercepts. The model seems in need for a conditional term, like in the following CGRM M_cgrm &lt;- BAB1 %&gt;% stan_glm(ToT ~ Design * age, data = .) BAB1$M_cgrm &lt;- predict(M_cgrm)$center G_Design_age %+% BAB1 + geom_smooth(aes(y = M_cgrm, linetype = &quot;M_cgrm&quot;)) The improved model now seems captures the overall increment by age in both conditions. Apart from the age-50 dip, the DesignA condition reasonably fitted. The model also predicts a cross-over point at age of 73, where both designs are equal. In contrast, the model cannot adequately render the association in design B, which appears inverse-sigmoid. These non-linear associations stem from a fancy psychological model I have put together at the end of the long night. Let us take a look at some real wobbles, instead. detach(BrowsingAB) The Uncanny Valley effect is all about non-linearity and we have seen in [Poly] how a complex curves can be captured by higher-degree polynomials. With every degree added to a polynomial, the model gets one more coefficient. It should be clear by now that models with more coefficients are more flexibel. In multi-factorial models, adding a conditional term lets all group means move freely, whereas the flexibility of a polynomial can be measured by how many stationary points are possible, because you need one for every peak and valley. Higher degree polynomials can do even more tricks, such as saddle points, that have a local slope of zero without changing direction. Mathur &amp; Reichling identified a cubic polynomial as the lowest degree that would render the Uncanny Valley effect, which has at least one local maximum and one local minimum (both are stationary points). In fact, they also conducted a formal model comparison, which approved that adding higher degrees does not make the model better. Such a formal procedure is introduced in [APA], whereas here we use visualizations of fitted responses to evaluate the possible models. In the following, the cubic model is compared to the simpler quadratic model. It could be, after all, that a parabole is sufficient to render the valley. On the other side of things, a polynomial model with the ridiculous degree 9 is estimated, just to see whether there is any chance a more complex model would further improve the fit. attach(Uncanny) M_poly_2 &lt;- UV_1 %&gt;% stan_glm(avg_like ~ poly(huMech, 2), data = .) M_poly_3 &lt;- UV_1 %&gt;% stan_glm(avg_like ~ poly(huMech, 3), data = .) M_poly_9 &lt;- UV_1 %&gt;% stan_glm(avg_like ~ poly(huMech, 9), data = .) sync_CE(Uncanny, M_poly_2, M_poly_3, M_poly_9) PP_poly &lt;- bind_rows(post_pred(M_poly_2), post_pred(M_poly_3), post_pred(M_poly_9)) T_fit_poly &lt;- predict(PP_poly) %&gt;% select(model, Obs, center) %&gt;% spread(model, center) UV_1 %&gt;% bind_cols(T_fit_poly) %&gt;% ggplot(aes(x = huMech)) + geom_point(aes(y = avg_like, col = &quot;observed&quot;)) + geom_smooth(aes(y = avg_like, col = &quot;observed&quot;), se = F) + geom_smooth(aes(y = M_poly_2, col = &quot;poly_2&quot;), se = F) + geom_smooth(aes(y = M_poly_3, col = &quot;poly_3&quot;), se = F) + geom_smooth(aes(y = M_poly_9, col = &quot;poly_9&quot;), se = F) + labs(col = &quot;Source&quot;) Note that: all posterior predictive distributions (PPD) are collected into one multi-model posterior predictions object (class tbl_post_pred) from the PP, multi-model CLUs are then created at once and turned into a wide table using spread, which can be attached to the original data. We observe that the two lower-degree polynomials deviate strongly from the observed pattern. The quadratic polynomial is a parabole and one could expect it to fit the valley part somewhat, but it does not. The cubic polynmial curve almost seems to snap into the right position. That is confirmed by observing that using a 9-degree polynomial barely changes the fitted curve, although, according to polynomial theory, this model could have as many as eight stationary points, which is a ridiculous amount of flexibility. This polynomial also sits perfectly snug with the LOESS (on the original observations), which means it is at least as flexible. These results confirm that [Mathur &amp; Reichling] got it just right with using a cubic polynomial. This model captures the salient features of the data, not more, but also not less. One could argue, that since the 9-degree polynomial makes almost the ame predictions as the cubic polynomial, it does no harm to always estimate a more flexible model. As we will see in [APA] this is problematic, as models with unnecessary flexibility tends to “over-fit”, which means seeing structure in the noise, which reduces forecasting accuracy. To sum it up, visual analysis with fitted responses is an effective way to discover shortcomings of a model with respect to the structural part. A possible strategy is to start with a basic model, that covers just the main research questions and explore how well it performs under different conditions. With metric predictors, fitted response analysis can uncover problems with the linearity assumption. 8.2 Model selection If one measures two predictors \\(x_1\\), \\(x_2\\) and one outcome variable \\(y\\), formally there exist four linear models to choose from: y ~ 1 (grand mean) y ~ x_1 (main effect 1) y ~ x_2 (main effect 2) y ~ x_1 + x_2 (both main effects) y ~ x_1 * x_2 (both main effects and interaction) For a data set with three predictors, the set of possible models is already 18. This section deals with how to choose the right one. Section [#fitted responses] has already shown that under-specified models can introduce severe biases. The opposite is when a model carries unnecessary parameters, which is called over-fitting and leads to an inferior predictive accuracy. The subsequent sections accept predictive accuracy as a legitimate measure for model comparison and introduces methods to measure predictive accuracy: simulation, cross validation, leave-one-out cross validation and information criteria. The final two sections show how model selection can be used to test hypothesis, without using the infamous p-value. 8.2.1 The problem of over-fitting In this chapter we have seen multiple times how a model that is too simple fails to align with the structure presnet in the data. For example, recall the IPump case, where an ordinal model was much more accurate in rendering the learning process than the simpler regression model. In several other cases, we have seen how introducing conditional effects improves model fit. At these examples, it is easy to see how omitting relevant predictors reduces the predictive accuracy of a model. Too sparse models produce inferior predictions, but that is only one side of the coin: Models that contain irrelevant predictors also produce inferior predictions. This is called over-fitting and can be understood better if we first recall, that a models job is to divide observed magnitudes into the structural part and the random part [#statmod]. The structural part is what all observations have in common, all future observations included. The structural part always is our best guess and the better a model separates structure from randomness, the more accurate our forecasts become. The process of separation is imperfect to some degree. Irrelevant predictors usually get center estimates close to zero in a linear model, but their posterior distribution (or credibility intervals) usually has its probability mass spread out over a range of non-zero values. The irrelevant parameter adds a degree of freedom which introduces additional uncertainty. As an analogy, an irrelevant parameter is to a model, what a loose wheel is to a car. Even on a perfectly flat road it will cause a wobbly driving experience. For further illustration, we simulate a small data set by drawing two variables from two Gaussian distributions. One variable is the (presumed) predictor, the other is the outcome and because they are completely unrelated, a GMM would be appropriate. But what happens if the researcher assumes that there is a linear relation and adds the irrelevant predictor to the model? attach(Chapter_LM) sim_overfit &lt;- function(n = 10, seed = 1317){ set.seed(seed) tibble(pred = rnorm(n = 10, mean = 0, sd = 1), outcome = rnorm(n = 10, mean = 2, sd = 1)) %&gt;% as_tbl_obs()} D_overfit &lt;- sim_overfit() M_gmm &lt;- stan_glm(outcome ~ 1, data = D_overfit, iter = 2000) M_lrm &lt;- stan_glm(outcome ~ 1 + pred, data = D_overfit, iter = 2000) P_overfit &lt;- bind_rows(posterior(M_gmm), posterior(M_lrm)) coef(P_overfit) model parameter fixef center lower upper M_gmm Intercept Intercept 2.255 1.684 2.840 M_lrm Intercept Intercept 2.255 1.609 2.894 M_lrm pred pred 0.018 -0.674 0.652 We first examine the coefficients. We observe that both models produce the same center estimates for Intercept. At the same time, the LRM predictor is centered at a point very close to zero. The good news is that irrelevant predictors usually do not add any biases. The credibility intervals are a different story. The most striking observation is that the LRM is very uncertain about the slope. The possibility of considerable positive or negative slopes is not excluded at all. Next to that, the intercept of the LRM bears more pronounced uncertainty, compared to the GMM. The following plot illustrates the mechanism behind over-fitting. It is created by extracting intercept and slope parameters from the posterior distributions and plot them as a bunch of linear functions. For the GMM, all slopes are set to Zero, but we observe that the LRM has visited many rather strong slopes. These extreme slopes are mostly caused by the extreme observations Five, Six and Eight, which the LRM tries to reach, while the GMM stays relatively contained, assigning most of these extreme values to the random part. Finally, by the distribution of slopes, the distribution of left end-points is pulled apart and that is what we see as an extra uncertainty in the intercept. P_overfit %&gt;% filter(type == &quot;fixef&quot;) %&gt;% select(model, iter, parameter, value) %&gt;% spread(key = parameter, value = value, fill = 0) %&gt;% filter( (iter %% 10) == 0 ) %&gt;% ggplot() + facet_wrap(~model) + geom_abline(aes(intercept = Intercept, slope = pred), alpha = .2) + geom_point(aes(x = pred, y = outcome), data = D_overfit, col = &quot;Red&quot;) + geom_label(aes(x = pred, y = outcome, label = Obs), data = D_overfit, col = &quot;Red&quot;) Every parameter that is added to a model, adds to it some amount of flexibility. When this parameter is influential within the structure, the extra flexibility improves the fit. When it is not, the extra flexibility grabs on too much randomness, with the consequence of reduced predictive accuracy. Model pruning is the process of discarding unnecessary parameters from a model until it reaches its maximum predictive accuracy. That is easy if you use simulated data, but in practice predictive accuracy can only be estimated by throwing new data at an estimated model. Nevertheless, unnecessary parameters in linear models can often sufficiently be identified by two simple rules (which actually are very similar to each other): The center estimate is close to Zero. If the parameter is removed, this causes little change in other parameters. In the following, I will introduce formal methods to model pruning. These can be used in more complex situations, such as pruning multi-level models or selecting an appropriate error distribution type [#temporal]. 8.2.2 Cross validation and LOO Recall that coefficients are tied to fitted responses by the structural part. Consequently, stronger uncertainty in coefficients causes stronger uncertainty of predictions. Are the predictions of LRM inferior to the parsimonous GMM? Since we have simulated this data set, the true population mean is known (\\(\\mu = 2\\)) and we can assess predictive accuracy by comparing the deviation of fitted responses from the true value. A standard way of summarizing the predictive accuracy of models is the root mean square error (RMSE), which we can compute from the posterior predictive distributions. RMSE &lt;- function(true, value) { se &lt;- (true - value)^2 mse &lt;- mean(se) rmse &lt;- sqrt(mse) rmse } PP_overfit &lt;- bind_rows(post_pred(M_gmm), post_pred(M_lrm)) %&gt;% left_join(D_overfit, by = &quot;Obs&quot;) PP_overfit %&gt;% group_by(model) %&gt;% summarize(RMSE = RMSE(true = 2, value)) model RMSE M_gmm 1.01 M_lrm 1.13 The LRM has a larger error by around 10%, which is a lot for just one additional parameter. The RMSE of the GMM is close to One, which is almost precisely the standard deviation of the simulation function; the GMM has found just the right amount of randomness. In practice, the central dilemma in evaluating predictive accuracy is that usually we do not know the real value. The best we can do, is use one data set to estimate the parameters and use a second data set to test how well the model predicts. This is called cross validation and it is the gold standard method for assessing predictive accuracy. Here, we can simulate the situation by using the simulation function one more time to produce future data, or more precisely: the data new to the model. D_new_data &lt;- sim_overfit(n = 100, seed = 1318) PP_cross_valid &lt;- bind_rows(post_pred(M_gmm, newdata = D_new_data), post_pred(M_lrm, newdata = D_new_data)) %&gt;% left_join(D_new_data, by = &quot;Obs&quot;) PP_cross_valid %&gt;% group_by(model) %&gt;% summarize(RMSE = RMSE(value, outcome)) model RMSE M_gmm 1.14 M_lrm 1.25 Waiting for new data before you can do model evaluation sounds awful, but new data is what cross validation requires. More precisely, cross validation only requires that the forecast data is not part of the sample you trained the model with. Psychometricians, for example, use the split half technique to assess the reliability of a test. The items of the test are split in half, one training set and one forecasting set. If the estimated participant scores correlate strongly, the test is called reliable. So, model evaluation can be done, by selecting on part of the data to train the model, i.e. estimate the coefficients, and try to forecast the other part of the data. However, data is precious and reserving half of it for forecasting will considerably be at the cost of certainty. Fortunately, nobody actually said it has to be half the data. Another method of splitting has become common, leave-one-out (LOO) cross validation. The idea is simple: Remove observation \\(i\\) from the data set. Estimate the model \\(M_{/i}\\). Predict observation \\(i\\) with Model \\(M_{/i}\\). Measure the predictive accuracy for observation \\(i\\). Repeat steps 1 to 4 until all observations have been left out and forecast once. The following code implements a generic function to run a LOO analysis using an arbitrary model. do_loo &lt;- function(data, F_fit, f_predict = function(fit, obs) post_pred(fit, newdata = obs)) { model_name &lt;- as.character(substitute(F_fit)) F_train_sample &lt;- function(obs) data %&gt;% slice(-obs) # Quosure F_test_obs &lt;- function(obs) data %&gt;% slice(obs) # Quosure F_post_pred &lt;- function(model, model_name, newdata, this_Obs) { post_pred(model = model, model_name = model_name, newdata = newdata) %&gt;% mutate(Obs = this_Obs)} out &lt;- tibble(Obs = 1:nrow(data), Model = model_name, Train_sample = map(Obs, F_train_sample), # training observations Test_obs = map(Obs, F_test_obs), # test observation Fitted = map(Train_sample, F_fit)) %&gt;% # model objects mutate(Post_pred = pmap(list(model = Fitted, newdata = Test_obs, model_name = Model, this_Obs = Obs), F_post_pred)) return(out) } Before we put do_loo to use, some notes on the programming seem in order. Despite its brevity, the function is highly generic in that it can compute leave-one-out scores no matter what model you throw at it. This is mainly achieved by using advanced techniques from functional programming: The argument f_fit takes an arbitrary function to estimate the model. This should work with all standard regression engines. The argument f_predict takes a function as argument that produces the predicted values for the removed observations. The default is a function based on predict from the bayr package, but this can be adjusted. The two functions that are defined inside do_loo are so-called quosures. Quosures are functions that bring their own copy of the data. They can be conceived as the functional programming counterpart to objects: Not the object brings the function, but the function brings its own data. The advantage is mostly computational as it prevents data to be copied every time the function is invoked. map is a meta function from package purrr. It takes a list of objects and applies an arbitrary function, provided as the second argument. map2 takes two parallel input lists and applies a function. Here the forecast is created by matching observations with the model they had been excluded from. The function output is created as a tibble, which is the tidy re-implementation of data frames. Different to original data.frame objects, tibbles can also store complex objects. Here, the outcome of LOO stores every single sample and estimated model, neatly aligned with its forecast value. Other than one might expect, the function does not return a single score for predictive accuracy, but a dataframe with inidividual forecasts. This is on purpose as there is more than one possible function to choose from for calculating a single accuracy score. The function also makes use of what is called non-standard evaluation. This is a very advanced programming concept in R. Suffice it to say that substitute() captures an expression, here this is the fitting function argument, without executing it, immediatly. Here the provided argument is converted to character and put as an identifier into the dataframe. That makes it very easy to use do_loo for multiple models, as we will see next. Since we want to compare two models, we define two functions, invoke do_loo twice and bind the results in one data frame. As the model estimation is done per observation, I dialed down the number of MCMC iterations a little bit to speed up the process: fit_GMM &lt;- function(sample) stan_glm(outcome ~ 1, data = sample, iter = 500) fit_LRM &lt;- function(sample) stan_glm(outcome ~ 1 + pred, data = sample, iter = 500) Loo &lt;- bind_rows(do_loo(D_overfit, fit_GMM), do_loo(D_overfit, fit_LRM)) Loo %&gt;% sample_n(5) sync_CE(Chapter_LM, Loo) ## [1] &quot;Loo&quot; This data frame Loo is somewhat unusual, because also it stores complex R objects, rather than atomic values. Doing a full LOO run is very computing expensive and therefore it makes a lot of sense to save all the models for potential later use. Model comparison is again based on the posterior predictive distribution, we are only interested in the posterior predictive distributions. The following code merges all posterior predictions into one multi-model posterior prediction table and joins it with the teh original observations. Now we can compute the RMSE and we even have the choice to do it on different levels. On a global level, the prediction errors of all observations are pooled, but we can also summarize the prediction error on observations level, or plot the predictive distribution. PP_Loo &lt;- bind_rows(Loo$Post_pred) %&gt;% left_join(D_overfit) %&gt;% rename(prediction = value) PP_Loo %&gt;% group_by(model) %&gt;% summarize(RMSE = RMSE(prediction, outcome)) model RMSE fit_GMM 1.33 fit_LRM 1.55 PP_Loo %&gt;% group_by(model, Obs) %&gt;% summarize(RMSE = RMSE(prediction, outcome)) %&gt;% spread(value = RMSE, key = model) Obs fit_GMM fit_LRM 1 1.14 1.21 2 1.09 1.18 3 1.20 1.23 4 1.15 1.26 5 1.64 1.95 6 1.54 1.88 7 1.12 1.50 8 1.85 2.30 9 1.16 1.32 10 1.21 1.25 PP_Loo %&gt;% ggplot(aes(x = as.factor(Obs), y = prediction, color = model)) + facet_grid(~model) + geom_violin() + geom_point(aes(y = outcome, col = &quot;Observed&quot;)) 8.2.3 Information Criteria So far, we have seen that the right level of parsimony is essential for good predictive accuracy. While LOO can be considered gold standard for assessing predictive accuracy, it has a severe downside. Estimating Bayesian models with MCMC is very computing intensive and for some models in this book, doing a single estimating is in the range of dozens of minutes to more than an hour. LOO estimates the model as many times as there are observations, which quickly leads to unbearable computing time for larger data sets. Information criteria account for goodness-of-fit, but also penalizes more flexible models. The oldest of all IC is the Akaike Information Criterion (AIC). Compared to its younger siblings, it is less broad, but its formula will be instructive to point out how goodness-of-fit and complexy are balanced within one formula. To represent goodness-of-fit, the AIC employs the deviance, which directly derives from the Likelihood (4.3.3). Model complexity is accounted for by a penalty term that is just two times the number of parameters \\(k\\). \\[ \\textrm{Deviance} = -2 \\log(p(y|\\hat\\theta)).\\\\ \\textrm{Penality} = 2k\\\\ AIC = D + 2k \\] By these two simple terms the AIC brings model fit and complexity into balance. Note that lower deviance is better and so is lower complexity. In effect, when comparing two models, the one with the lower AIC wins. As it grounds on the likelihood, it is routinely been used to compare models estimated by classic maximum likelihood estimation. The AIC formula is ridiculously simple and still has a solid foundation in mathematical information theory. It is easily computed, as model deviance is a by-product of parameter estimation. And if that was not enough, the AIC is an approximation of LOO cross-validation, beating it in computational efficiency. Still, the AIC has limitations. While it covers the full family of Generalized Linear Models [#GLM], it is not suited for Multi-level Models [#MLM]. The reason is that in multi-level models the degree of freedom (its flexibility) is no longer proportional to the formal number of parameters. Hence, the AIC penalty term is over-estimating. The Deviance Information Criterion (DIC) was the first generalization of the AIC to solve this problem by using an estimate for degrees of freedom. These ideas have more recently been refined into the Widely Applicable Information Criterion (WAIC). Like DIC this involves estimating the penalty term \\(p_\\text {WAIC}\\). In addition, WAIC makes use of the full posterior predictive distribution, which is results in the estimated log pointwise predictive density, \\(\\text{elpd}_\\text{WAIC}\\) as goodness-of-fit measure [#REF]. The standard implementation of WAIC is provided by package Loo, and works with all Rstanarm models (and Brms, [#GLM]). When invoked on a model, it returns all three estimates: attach(Chapter_LM) loo::waic(M_gmm) ## ## Computed from 4000 by 10 log-likelihood matrix ## ## Estimate SE ## elpd_waic -13.9 2.1 ## p_waic 1.6 0.6 ## waic 27.8 4.2 ## ## 1 (10.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. Compared to LOO-CV, the WAIC is blazing fast. However, the WAIC has two major down-sides: First, while the RMSE has a straight-forward interpretation as the standard deviation of the expected error, WAICs are unintelligable by themselves. They only indicate relative predictive accuracy, when multiple models are compared. Second, WAIC as an approximation can be wrong. Fortunately, as can be seen from the warning above, the WAIC command performs an internal check on the integrity of the estimate. When in doubt, the function recommends to try loo instead. This is not the full LOO-CV method, but another approximation: LOO-IC often is more reliable approximation of the real LOO-CV, than WAIC. It is slower than WAIC, but still has reasonable computation times. Again, the LOO-IC implementation features an extra safety feature, by checking the integrity of results and helping the user to fix problems. Loo_gmm &lt;- loo(M_gmm, cores = 1) Loo_lrm &lt;- loo(M_lrm, cores = 1) Loo_gmm ## ## Computed from 4000 by 10 log-likelihood matrix ## ## Estimate SE ## elpd_loo -14.0 2.1 ## p_loo 1.7 0.6 ## looic 28.0 4.3 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 9 90.0% 1135 ## (0.5, 0.7] (ok) 1 10.0% 359 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Loo_lrm ## ## Computed from 4000 by 10 log-likelihood matrix ## ## Estimate SE ## elpd_loo -16.3 3.0 ## p_loo 3.8 1.7 ## looic 32.5 6.0 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 6 60.0% 1644 ## (0.5, 0.7] (ok) 3 30.0% 481 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 1 10.0% 15 ## See help(&#39;pareto-k-diagnostic&#39;) for details. The interpretation of LOO-IC is the same as for all information criteria, the value with the smaller IC wins. What often confuses users of information criteria is when they see two ICs that are huge with only a tiny difference, like 1001317 and 1001320. Recall that information criteria all depend on the likelihood, but on a logarithmic scale. What is a difference on the logarithmic scale is a multiplier on the original scale of the likelihood, which is a product of probabilities. And a small difference on the log scale can be a respectable multiplicator: exp(1001320 - 1001317) ## [1] 20.1 8.2.4 Choosing response distributions In model pruning we compare the predictve accuracy of models that differ by the structural part. With modern evaluation criteria, there is more we can do to arrive at an optimal model. The methods for model comparison introduced so far also allow to compare Generalized Linear Models with different response distributions. For most outcome variables, choosing an appropriate response distribution is a straight-forward choice, based on just a few properties (continuous versus discrete, boundaries and overdispersion). In 7.3.2 the properties of ToT and RT data suggested that Gaussian error terms are inappropriate, because the eror distribution is highly skewed. While Gamma error distributions can accomodate some skew, this may not be sufficient, because the lower boundary of measures is strongly positive. From an informal analysis of these three models, we concluded that the exgaussian is most suited for ToT and RT outcomes, followed by Gamma, whereas the Gaussian showed a very poor fit. However, visual analysis of residuals on non-Gaussian models is notoriously difficult, due to the variance-mean relationship. Fortunately, the model evaluation criteria introduced so far, also cover comparisons of error distributions. The first analysis is The Hugme case experiment is a variant of the Stroop task and the outcomes are reaction times. In this case, WAIC failed to approximate predictive accuracy well, but to our convenience, the authors of package Loo provide a slightly less time efficient, but more accurate estimate, the LOO-IC, which is another approximation, not a real LOO-CV. One advantage of LOO-IC over WAIC is that it produces observation-level estimates, which can be checked individually. Often LOO-IC only misses out on a few observations, and the author of package Brms has added a fallback mechanism: by adding the argument reloo = TRUE, the problematic observations are refitted as real LOO-CVs. attach(Hugme) Loo_1_gau &lt;- loo(M_1_gau, reloo = TRUE) Loo_1_gam &lt;- loo(M_1_gam) Loo_1_exg &lt;- loo(M_1_exg, reloo = TRUE) loo_compare(Loo_1_gau, Loo_1_gam, Loo_1_exg) %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;Model&quot;) %&gt;% select(Model, looic) %&gt;% mutate(looic_diff = looic - min(looic)) Model looic looic_diff M_1_exg -12947 0 M_1_gam -11941 1006 M_1_gau -6479 6468 8.2.4.1 HERE detach(Hugme) The command loo_compare puts the results in one table and arranges the models by predictive accuracy. Unfortunately, it is not a tidy command, and a little extra tweaking is required to make it tidy. (loo_compare puts the models in row names, instead of using an explicit identifier variable). As expected, the Exgaussian error distribution makes for the best model. attach(CUE8) When the highly efficient WAIC reports unreliability, we traded some computing efficiency for reliability by switching too LOO-IC. If LOO-IC is unreliable for a few observations, these can be fitted by running a few real LOO-CV with reloo = T. This can get time expensive with more than a few suffering obervations. The authors of package Loo provide an alternative fall-back to reloo-ing: In k-fold cross validation a model is refit \\(k\\) times, always leaving out and predicting \\(N/k\\) observations. LOO cross validation actually is 1-fold cross validation, which results in fitting \\(N\\) models. 10-fold cross validation stands on middle ground by training the model ten times on a different 90% part of data and testing the model on the remaining 10%: F10_4_gau &lt;- kfold(M_4_gau, K = 10) F10_4_gam &lt;- kfold(M_4_gam, K = 10) F10_4_exg &lt;- kfold(M_4_exg, K = 10) loo_compare(F10_4_gau, F10_4_gam, F10_4_exg) %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;Model&quot;) %&gt;% mutate(kfoldic = -2 * elpd_kfold) %&gt;% select(Model, kfoldic) %&gt;% mutate(kfoldic_diff = kfoldic - min(kfoldic)) Model kfoldic kfoldic_diff M_4_gam 6587 0 M_4_exg 6765 178 M_4_gau 7159 572 detach(CUE8) Obviously, predictive accuracy can also be used to check for over-dispersion, just compare, for instance, a Poisson model to a Negbinomial model. I close this section with a more interesting comparison: In section [#dist_models] distributional models were introduced as a generalization of GLMs, that allow to also link predictors to variance and shape parameters, such as the Gaussian \\(\\sigma\\), the Beta \\(\\rho\\) and the two parameters of the Exgaussian distribution, \\(\\sigma\\) for dispersion and \\(\\beta\\) for skew. When applied to the participant level, we saw a strong inter-individual variance in skew and variance, which seemed to justified the model. However, distributional models on participant-level are very opulent and the question is whether this model truly performs better in terms of predictive accuracy. attach(Hugme) Loo_1_exg_dist &lt;- loo(M_1_exg_dist) loo_compare(Loo_1_exg, Loo_1_exg_dist) elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic M_1_exg_dist 0 0.0 7467 108 222 4.95 -14934 216 M_1_exg -994 47.5 6473 120 116 3.43 -12947 239 detach(Hugme) 8.2.5 Testing theories For everyone who has to publish in Social Science journals, this book has so far left one question unanswered: What is the Bayesian method for testing null hypotheses? To give the short answer: There are no p-values in this statistical framework, but you can always use credibility limits to say “the effect is different from Zero with a certainty larger than 95%”. If some pertinent reviewers want p-values, you can argue that a null-hypothesis test is really just model selection, but unfortunately no p-value tests are available for the models that you are using, and then you introduce one of the methods based on predictive accuracy. But, should you really do null hypothesis testing in design research? Well, if your research implies or even directly targets real costs, benefits and risks for real people, you should continue with quantifying impact factors. That can even go so far that an unnecessary predictor is left in the model, just to make its irrelevance transparent. Everyone whose final report draws any conclusion of how the results can be used for the better, must speak of quantities at some place. Still, it seems to make sense to test theories by comparing predictive accuracy of models. Recall the uncanny valley phenomenon. In chapter @ref(polynomial_regression) the non-linear relationship between human likeness of robot faces and emotional response was modelled as a 3-deg polynomial, at first, and we have reported a bunch of quantitative results, such as the position of the trough. We encountered the case again for an exmaination of goodness-of-fit [#fitted responses], which we examined informally. Our plots of fitted responses suggested that the cubic polynomial fitted than the simpler quadratic. A quadratic function produces a parabole and can therefore also can have a local minimum. But it cannot have the shoulder that makes the effect so abrupt. This abruptness is what has fascinated so many reseachers, but is it really true? attach(Uncanny) options(mc.cores = 1) Loo_poly_2 &lt;- loo(M_poly_2) Loo_poly_3 &lt;- loo(M_poly_3) loo_compare( Loo_poly_2, Loo_poly_3) elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic M_poly_3 0.00 0.00 71.5 4.78 4.36 0.685 -143 9.56 M_poly_2 -7.39 3.65 64.1 4.80 3.55 0.576 -128 9.61 The cubic polynomial has the better predictive accuracy. It suggests that the Uncanny Valley effect is due to a disruptive cognitive change, like when we suddenly become aware that we have been fooled. This is not a direct test of a given theory, but it favors all theories that contain an abrupt process. One of the theories that does not contain a disruptive process goes that the UV effect is caused by religious beliefs. Another theory explains the effect as a the startling when our mind suddenly becomes aware of a deception and re-adjusts. detach(Uncanny) "],
["reflections.html", "9 Reflections", " 9 Reflections Egan, D. Individual differences in human-computer interaction. In M. Helander, ed., Handbook of Human Computer interaction. Elsevier Science Publishers, Amsterdam, The Netherlands, 1988, 543–568.↩ "]
]
