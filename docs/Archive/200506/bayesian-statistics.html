<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>3 Elements of Bayesian statistics | New statistics for the design researcher</title>
  <meta name="description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works hard to make this world a better place.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="3 Elements of Bayesian statistics | New statistics for the design researcher" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works hard to make this world a better place." />
  <meta name="github-repo" content="schmettow/New_Stats" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Elements of Bayesian statistics | New statistics for the design researcher" />
  
  <meta name="twitter:description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works hard to make this world a better place." />
  

<meta name="author" content="Martin Schmettow">


<meta name="date" content="2019-10-30">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="design-research.html">
<link rel="next" href="getting-started-r.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/d3-3.3.8/d3.min.js"></script>
<script src="libs/dagre-0.4.0/dagre-d3.min.js"></script>
<link href="libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet" />
<script src="libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/chromatography-0.1/chromatography.js"></script>
<script src="libs/DiagrammeR-binding-1.0.0/DiagrammeR.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="part"><span><b>I Preparations</b></span></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#whom-this-book-is-for"><i class="fa fa-check"></i><b>1.1</b> Whom this book is for</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#the-empirical-design-researcher"><i class="fa fa-check"></i><b>1.1.1</b> The empirical design researcher</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#the-experimentalist"><i class="fa fa-check"></i><b>1.1.2</b> The experimentalist</a></li>
<li class="chapter" data-level="1.1.3" data-path="index.html"><a href="index.html#the-applied-researcher"><i class="fa fa-check"></i><b>1.1.3</b> The applied researcher</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#assumptions"><i class="fa fa-check"></i><b>1.2</b> Assumptions</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i><b>1.3</b> How to read this book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="design-research.html"><a href="design-research.html"><i class="fa fa-check"></i><b>2</b> Elements of quantitative design research</a><ul>
<li class="chapter" data-level="2.1" data-path="design-research.html"><a href="design-research.html#studies"><i class="fa fa-check"></i><b>2.1</b> Studies</a></li>
<li class="chapter" data-level="2.2" data-path="design-research.html"><a href="design-research.html#observations-and-measurestbc"><i class="fa fa-check"></i><b>2.2</b> Observations and measures[TBC]</a><ul>
<li class="chapter" data-level="2.2.1" data-path="design-research.html"><a href="design-research.html#interaction-sequences"><i class="fa fa-check"></i><b>2.2.1</b> Interaction sequences</a></li>
<li class="chapter" data-level="2.2.2" data-path="design-research.html"><a href="design-research.html#performance-measures"><i class="fa fa-check"></i><b>2.2.2</b> performance measures</a></li>
<li class="chapter" data-level="2.2.3" data-path="design-research.html"><a href="design-research.html#experience-tbd"><i class="fa fa-check"></i><b>2.2.3</b> experience [TBD]</a></li>
<li class="chapter" data-level="2.2.4" data-path="design-research.html"><a href="design-research.html#design-features-tbd"><i class="fa fa-check"></i><b>2.2.4</b> design features [TBD]</a></li>
<li class="chapter" data-level="2.2.5" data-path="design-research.html"><a href="design-research.html#the-human-factor-tbd"><i class="fa fa-check"></i><b>2.2.5</b> the human factor [TBD]</a></li>
<li class="chapter" data-level="2.2.6" data-path="design-research.html"><a href="design-research.html#situations-tbd"><i class="fa fa-check"></i><b>2.2.6</b> situations [TBD]</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="design-research.html"><a href="design-research.html#decision_making"><i class="fa fa-check"></i><b>2.3</b> Decision making under uncertainty</a><ul>
<li class="chapter" data-level="2.3.1" data-path="design-research.html"><a href="design-research.html#measuring-uncertainty"><i class="fa fa-check"></i><b>2.3.1</b> Measuring uncertainty</a></li>
<li class="chapter" data-level="2.3.2" data-path="design-research.html"><a href="design-research.html#betting-on-designs"><i class="fa fa-check"></i><b>2.3.2</b> Betting on designs</a></li>
<li class="chapter" data-level="2.3.3" data-path="design-research.html"><a href="design-research.html#case-e-commerce-ab"><i class="fa fa-check"></i><b>2.3.3</b> Case: e-commerce A/B</a></li>
<li class="chapter" data-level="2.3.4" data-path="design-research.html"><a href="design-research.html#case99"><i class="fa fa-check"></i><b>2.3.4</b> Case: 99 seconds</a></li>
<li class="chapter" data-level="2.3.5" data-path="design-research.html"><a href="design-research.html#prior-information"><i class="fa fa-check"></i><b>2.3.5</b> Prior information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html"><i class="fa fa-check"></i><b>3</b> Elements of Bayesian statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#descriptive_stats"><i class="fa fa-check"></i><b>3.1</b> Descriptive statistics</a><ul>
<li class="chapter" data-level="3.1.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#frequencies"><i class="fa fa-check"></i><b>3.1.1</b> Frequencies</a></li>
<li class="chapter" data-level="3.1.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#central-tendency"><i class="fa fa-check"></i><b>3.1.2</b> Central tendency</a></li>
<li class="chapter" data-level="3.1.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#dispersion"><i class="fa fa-check"></i><b>3.1.3</b> Dispersion</a></li>
<li class="chapter" data-level="3.1.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#associations"><i class="fa fa-check"></i><b>3.1.4</b> Associations</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayesian-probability-theory"><i class="fa fa-check"></i><b>3.2</b> Bayesian probability theory</a><ul>
<li class="chapter" data-level="3.2.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#some-set-theory"><i class="fa fa-check"></i><b>3.2.1</b> Some set theory</a></li>
<li class="chapter" data-level="3.2.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#probability"><i class="fa fa-check"></i><b>3.2.2</b> Probability</a></li>
<li class="chapter" data-level="3.2.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#likelihood"><i class="fa fa-check"></i><b>3.2.3</b> Likelihood</a></li>
<li class="chapter" data-level="3.2.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#certainty-as-probability"><i class="fa fa-check"></i><b>3.2.4</b> Certainty as probability</a></li>
<li class="chapter" data-level="3.2.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayes-theorem-and-the-dynamics-of-belief"><i class="fa fa-check"></i><b>3.2.5</b> Bayes theorem (and the dynamics of belief)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#statistical-models"><i class="fa fa-check"></i><b>3.3</b> Statistical models</a><ul>
<li class="chapter" data-level="3.3.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#predictions-and-likelihood"><i class="fa fa-check"></i><b>3.3.1</b> Predictions and likelihood</a></li>
<li class="chapter" data-level="3.3.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#distributions"><i class="fa fa-check"></i><b>3.3.2</b> Distributions: patterns of randomness</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayesian-estimation"><i class="fa fa-check"></i><b>3.4</b> Bayesian estimation</a></li>
<li class="chapter" data-level="3.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#what-is-wrong-with-classic-statistics-tbc"><i class="fa fa-check"></i><b>3.5</b> What is wrong with classic statistics? [TBC]</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="getting-started-r.html"><a href="getting-started-r.html"><i class="fa fa-check"></i><b>4</b> Getting started with R</a><ul>
<li class="chapter" data-level="4.1" data-path="getting-started-r.html"><a href="getting-started-r.html#setting-up-the-r-environment"><i class="fa fa-check"></i><b>4.1</b> Setting up the R environment</a><ul>
<li class="chapter" data-level="4.1.1" data-path="getting-started-r.html"><a href="getting-started-r.html#installing-cran-packages"><i class="fa fa-check"></i><b>4.1.1</b> Installing CRAN packages</a></li>
<li class="chapter" data-level="4.1.2" data-path="getting-started-r.html"><a href="getting-started-r.html#installing-packages-from-github"><i class="fa fa-check"></i><b>4.1.2</b> Installing packages from Github</a></li>
<li class="chapter" data-level="4.1.3" data-path="getting-started-r.html"><a href="getting-started-r.html#first_program"><i class="fa fa-check"></i><b>4.1.3</b> A first statistical program</a></li>
<li class="chapter" data-level="4.1.4" data-path="getting-started-r.html"><a href="getting-started-r.html#bibliographic-notes"><i class="fa fa-check"></i><b>4.1.4</b> Bibliographic notes</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="getting-started-r.html"><a href="getting-started-r.html#learning-r-a-primer"><i class="fa fa-check"></i><b>4.2</b> Learning R: a primer</a><ul>
<li class="chapter" data-level="4.2.1" data-path="getting-started-r.html"><a href="getting-started-r.html#objects"><i class="fa fa-check"></i><b>4.2.1</b> Objects</a></li>
<li class="chapter" data-level="4.2.2" data-path="getting-started-r.html"><a href="getting-started-r.html#vectors"><i class="fa fa-check"></i><b>4.2.2</b> Vectors</a></li>
<li class="chapter" data-level="4.2.3" data-path="getting-started-r.html"><a href="getting-started-r.html#classes"><i class="fa fa-check"></i><b>4.2.3</b> Classes</a></li>
<li class="chapter" data-level="4.2.4" data-path="getting-started-r.html"><a href="getting-started-r.html#functions"><i class="fa fa-check"></i><b>4.2.4</b> Functions</a></li>
<li class="chapter" data-level="4.2.5" data-path="getting-started-r.html"><a href="getting-started-r.html#operators"><i class="fa fa-check"></i><b>4.2.5</b> Operators</a></li>
<li class="chapter" data-level="4.2.6" data-path="getting-started-r.html"><a href="getting-started-r.html#storing-data-in-data-frames"><i class="fa fa-check"></i><b>4.2.6</b> Storing data in data frames</a></li>
<li class="chapter" data-level="4.2.7" data-path="getting-started-r.html"><a href="getting-started-r.html#import-export-and-archiving"><i class="fa fa-check"></i><b>4.2.7</b> Import, export and archiving</a></li>
<li class="chapter" data-level="4.2.8" data-path="getting-started-r.html"><a href="getting-started-r.html#case-environments"><i class="fa fa-check"></i><b>4.2.8</b> Case environments</a></li>
<li class="chapter" data-level="4.2.9" data-path="getting-started-r.html"><a href="getting-started-r.html#structuring-data"><i class="fa fa-check"></i><b>4.2.9</b> Structuring data</a></li>
<li class="chapter" data-level="4.2.10" data-path="getting-started-r.html"><a href="getting-started-r.html#data-transformation"><i class="fa fa-check"></i><b>4.2.10</b> Data transformation</a></li>
<li class="chapter" data-level="4.2.11" data-path="getting-started-r.html"><a href="getting-started-r.html#plotting-data"><i class="fa fa-check"></i><b>4.2.11</b> Plotting data</a></li>
<li class="chapter" data-level="4.2.12" data-path="getting-started-r.html"><a href="getting-started-r.html#fitting-regression-models"><i class="fa fa-check"></i><b>4.2.12</b> Fitting regression models</a></li>
<li class="chapter" data-level="4.2.13" data-path="getting-started-r.html"><a href="getting-started-r.html#knitting-statistical-reports"><i class="fa fa-check"></i><b>4.2.13</b> Knitting statistical reports</a></li>
<li class="chapter" data-level="4.2.14" data-path="getting-started-r.html"><a href="getting-started-r.html#exercises-1"><i class="fa fa-check"></i><b>4.2.14</b> Exercises</a></li>
<li class="chapter" data-level="4.2.15" data-path="getting-started-r.html"><a href="getting-started-r.html#bibliographic-notes-1"><i class="fa fa-check"></i><b>4.2.15</b> Bibliographic notes</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Models</b></span></li>
<li class="chapter" data-level="5" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>5</b> Linear models</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-models.html"><a href="linear-models.html#quantification-at-work-grand-mean-models"><i class="fa fa-check"></i><b>5.1</b> Quantification at work: grand mean models</a><ul>
<li class="chapter" data-level="5.1.1" data-path="linear-models.html"><a href="linear-models.html#talking-coefficient-tables"><i class="fa fa-check"></i><b>5.1.1</b> Talking coefficient tables</a></li>
<li class="chapter" data-level="5.1.2" data-path="linear-models.html"><a href="linear-models.html#working-with-the-posterior-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Working with the posterior distribution</a></li>
<li class="chapter" data-level="5.1.3" data-path="linear-models.html"><a href="linear-models.html#center-and-interval-estimates"><i class="fa fa-check"></i><b>5.1.3</b> Center and interval estimates</a></li>
<li class="chapter" data-level="5.1.4" data-path="linear-models.html"><a href="linear-models.html#random_walk"><i class="fa fa-check"></i><b>5.1.4</b> Do the random walk: Markov Chain Monte Carlo sampling</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="linear-models.html"><a href="linear-models.html#linear-regression"><i class="fa fa-check"></i><b>5.2</b> Walk the line: linear regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="linear-models.html"><a href="linear-models.html#transforming-measures"><i class="fa fa-check"></i><b>5.2.1</b> Transforming measures</a></li>
<li class="chapter" data-level="5.2.2" data-path="linear-models.html"><a href="linear-models.html#correlations"><i class="fa fa-check"></i><b>5.2.2</b> Correlations</a></li>
<li class="chapter" data-level="5.2.3" data-path="linear-models.html"><a href="linear-models.html#endlessly-linear"><i class="fa fa-check"></i><b>5.2.3</b> Endlessly linear</a></li>
<li class="chapter" data-level="5.2.4" data-path="linear-models.html"><a href="linear-models.html#exercises-2"><i class="fa fa-check"></i><b>5.2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="linear-models.html"><a href="linear-models.html#CGM"><i class="fa fa-check"></i><b>5.3</b> A versus B: Comparison of groups</a><ul>
<li class="chapter" data-level="5.3.1" data-path="linear-models.html"><a href="linear-models.html#dummy_variables"><i class="fa fa-check"></i><b>5.3.1</b> Not stupid: dummy variables</a></li>
<li class="chapter" data-level="5.3.2" data-path="linear-models.html"><a href="linear-models.html#contrasts"><i class="fa fa-check"></i><b>5.3.2</b> Getting it sharper with contrasts [TBC]</a></li>
<li class="chapter" data-level="5.3.3" data-path="linear-models.html"><a href="linear-models.html#sharper-on-the-fly-derived-quantities-tbd"><i class="fa fa-check"></i><b>5.3.3</b> Sharper on the fly: derived quantities [TBD]</a></li>
<li class="chapter" data-level="5.3.4" data-path="linear-models.html"><a href="linear-models.html#exercises-3"><i class="fa fa-check"></i><b>5.3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-models.html"><a href="linear-models.html#putting-it-all-together-multi-predictor-models"><i class="fa fa-check"></i><b>5.4</b> Putting it all together: multi predictor models</a><ul>
<li class="chapter" data-level="5.4.1" data-path="linear-models.html"><a href="linear-models.html#on-surface-multiple-regression-models"><i class="fa fa-check"></i><b>5.4.1</b> On surface: multiple regression models</a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-models.html"><a href="linear-models.html#crossover-multifactorial-models-tbc"><i class="fa fa-check"></i><b>5.4.2</b> Crossover: multifactorial models [TBC]</a></li>
<li class="chapter" data-level="5.4.3" data-path="linear-models.html"><a href="linear-models.html#line-by-line"><i class="fa fa-check"></i><b>5.4.3</b> Line-by-line: regression in groups [TBC]</a></li>
<li class="chapter" data-level="5.4.4" data-path="linear-models.html"><a href="linear-models.html#fitting-observations"><i class="fa fa-check"></i><b>5.4.4</b> Fitting observations</a></li>
<li class="chapter" data-level="5.4.5" data-path="linear-models.html"><a href="linear-models.html#residual-analysis"><i class="fa fa-check"></i><b>5.4.5</b> Residual analysis</a></li>
<li class="chapter" data-level="5.4.6" data-path="linear-models.html"><a href="linear-models.html#resid_predictive_power"><i class="fa fa-check"></i><b>5.4.6</b> Assessing predictive power</a></li>
<li class="chapter" data-level="5.4.7" data-path="linear-models.html"><a href="linear-models.html#empirical-versus-statistical-control"><i class="fa fa-check"></i><b>5.4.7</b> Empirical versus statistical control</a></li>
<li class="chapter" data-level="5.4.8" data-path="linear-models.html"><a href="linear-models.html#exercises-4"><i class="fa fa-check"></i><b>5.4.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-models.html"><a href="linear-models.html#interaction-effects"><i class="fa fa-check"></i><b>5.5</b> Interaction effects</a><ul>
<li class="chapter" data-level="5.5.1" data-path="linear-models.html"><a href="linear-models.html#differential_design_effects"><i class="fa fa-check"></i><b>5.5.1</b> Users differ: differential design effects</a></li>
<li class="chapter" data-level="5.5.2" data-path="linear-models.html"><a href="linear-models.html#hitting-the-boundaries-of-saturation"><i class="fa fa-check"></i><b>5.5.2</b> Hitting the boundaries of saturation</a></li>
<li class="chapter" data-level="5.5.3" data-path="linear-models.html"><a href="linear-models.html#more-than-the-sum-amplification"><i class="fa fa-check"></i><b>5.5.3</b> More than the sum: amplification</a></li>
<li class="chapter" data-level="5.5.4" data-path="linear-models.html"><a href="linear-models.html#theoretically-interesting-interaction-effects-edit"><i class="fa fa-check"></i><b>5.5.4</b> Theoretically interesting interaction effects [EDIT]</a></li>
<li class="chapter" data-level="5.5.5" data-path="linear-models.html"><a href="linear-models.html#exercises-5"><i class="fa fa-check"></i><b>5.5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="linear-models.html"><a href="linear-models.html#polynomial_regression"><i class="fa fa-check"></i><b>5.6</b> Doing the rollercoaster: polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multilevel-models.html"><a href="multilevel-models.html"><i class="fa fa-check"></i><b>6</b> Multilevel models</a><ul>
<li class="chapter" data-level="6.1" data-path="multilevel-models.html"><a href="multilevel-models.html#thinking-multi-level"><i class="fa fa-check"></i><b>6.1</b> Thinking multi-level</a></li>
<li class="chapter" data-level="6.2" data-path="multilevel-models.html"><a href="multilevel-models.html#the-human-factor-intercept-random-effects"><i class="fa fa-check"></i><b>6.2</b> The Human Factor: Intercept random effects</a></li>
<li class="chapter" data-level="6.3" data-path="multilevel-models.html"><a href="multilevel-models.html#slope_RE"><i class="fa fa-check"></i><b>6.3</b> Slope random effects: variance in change [TBC]</a></li>
<li class="chapter" data-level="6.4" data-path="multilevel-models.html"><a href="multilevel-models.html#designs-and-other-non-human-populations"><i class="fa fa-check"></i><b>6.4</b> Designs and other non-human populations</a></li>
<li class="chapter" data-level="6.5" data-path="multilevel-models.html"><a href="multilevel-models.html#crossover"><i class="fa fa-check"></i><b>6.5</b> Crossover: Testing Egans assumption</a></li>
<li class="chapter" data-level="6.6" data-path="multilevel-models.html"><a href="multilevel-models.html#measuring-entities-psychometric-applications-tbc"><i class="fa fa-check"></i><b>6.6</b> Measuring entities: psychometric applications [TBC]</a></li>
<li class="chapter" data-level="6.7" data-path="multilevel-models.html"><a href="multilevel-models.html#nested-random-effects"><i class="fa fa-check"></i><b>6.7</b> Nested random effects</a></li>
<li class="chapter" data-level="6.8" data-path="multilevel-models.html"><a href="multilevel-models.html#what-are-random-effects-on-pooling-and-shrinkage"><i class="fa fa-check"></i><b>6.8</b> What are random effects? On pooling and shrinkage</a></li>
<li class="chapter" data-level="6.9" data-path="multilevel-models.html"><a href="multilevel-models.html#re_correlations"><i class="fa fa-check"></i><b>6.9</b> Growing with random effects correlations [TBD]</a></li>
<li class="chapter" data-level="6.10" data-path="multilevel-models.html"><a href="multilevel-models.html#average-neverage-a-pledoyer-for-within-subject-research"><i class="fa fa-check"></i><b>6.10</b> Average? Neverage! A pledoyer for within-subject research</a></li>
<li class="chapter" data-level="6.11" data-path="multilevel-models.html"><a href="multilevel-models.html#exercises-7"><i class="fa fa-check"></i><b>6.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="GLM.html"><a href="GLM.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="GLM.html"><a href="GLM.html#debunking-the-gaussian-linear-model"><i class="fa fa-check"></i><b>7.1</b> Debunking the Gaussian linear model</a><ul>
<li class="chapter" data-level="7.1.1" data-path="GLM.html"><a href="GLM.html#assuming-linearity"><i class="fa fa-check"></i><b>7.1.1</b> Assuming linearity</a></li>
<li class="chapter" data-level="7.1.2" data-path="GLM.html"><a href="GLM.html#assuming-normal-distribution-of-randomness"><i class="fa fa-check"></i><b>7.1.2</b> Assuming Normal distribution of randomness</a></li>
<li class="chapter" data-level="7.1.3" data-path="GLM.html"><a href="GLM.html#assume-constant-variance"><i class="fa fa-check"></i><b>7.1.3</b> Assuming constant variance of randomness</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="GLM.html"><a href="GLM.html#glm_concepts"><i class="fa fa-check"></i><b>7.2</b> Elements of Generalized Linear Models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="GLM.html"><a href="GLM.html#re-linking-linearity-relinking_linearity-tbc"><i class="fa fa-check"></i><b>7.2.1</b> Re-linking linearity (#relinking_linearity) [TBC]</a></li>
<li class="chapter" data-level="7.2.2" data-path="GLM.html"><a href="GLM.html#choosing-patterns-of-randomness-choosing_randomness"><i class="fa fa-check"></i><b>7.2.2</b> Choosing patterns of randomness (#choosing_randomness)</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="GLM.html"><a href="GLM.html#case-user-testing-infusion-pumps"><i class="fa fa-check"></i><b>7.3</b> Case: user testing infusion pumps</a></li>
<li class="chapter" data-level="7.4" data-path="GLM.html"><a href="GLM.html#count_data"><i class="fa fa-check"></i><b>7.4</b> Count data</a><ul>
<li class="chapter" data-level="7.4.1" data-path="GLM.html"><a href="GLM.html#poisson-regression"><i class="fa fa-check"></i><b>7.4.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="GLM.html"><a href="GLM.html#logistic-aka-binomial-regression-logistic_regression"><i class="fa fa-check"></i><b>7.4.2</b> Logistic (aka Binomial) regression (#logistic_regression)</a></li>
<li class="chapter" data-level="7.4.3" data-path="GLM.html"><a href="GLM.html#modelling-overdispersion"><i class="fa fa-check"></i><b>7.4.3</b> Modelling overdispersion</a></li>
<li class="chapter" data-level="7.4.4" data-path="GLM.html"><a href="GLM.html#exercises-8"><i class="fa fa-check"></i><b>7.4.4</b> Exercises:</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="GLM.html"><a href="GLM.html#measures_of_time"><i class="fa fa-check"></i><b>7.5</b> Measures of time</a><ul>
<li class="chapter" data-level="7.5.1" data-path="GLM.html"><a href="GLM.html#exponential-regression"><i class="fa fa-check"></i><b>7.5.1</b> Exponential regression</a></li>
<li class="chapter" data-level="7.5.2" data-path="GLM.html"><a href="GLM.html#gamma-regression"><i class="fa fa-check"></i><b>7.5.2</b> Gamma regression</a></li>
<li class="chapter" data-level="7.5.3" data-path="GLM.html"><a href="GLM.html#exgaussian-regression"><i class="fa fa-check"></i><b>7.5.3</b> ExGaussian regression</a></li>
<li class="chapter" data-level="7.5.4" data-path="GLM.html"><a href="GLM.html#literature"><i class="fa fa-check"></i><b>7.5.4</b> Literature</a></li>
<li class="chapter" data-level="7.5.5" data-path="GLM.html"><a href="GLM.html#exercises-9"><i class="fa fa-check"></i><b>7.5.5</b> Exercises:</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="GLM.html"><a href="GLM.html#rating-scales"><i class="fa fa-check"></i><b>7.6</b> Rating scales</a><ul>
<li class="chapter" data-level="7.6.1" data-path="GLM.html"><a href="GLM.html#ordered-logistic-regression"><i class="fa fa-check"></i><b>7.6.1</b> Ordered logistic regression</a></li>
<li class="chapter" data-level="7.6.2" data-path="GLM.html"><a href="GLM.html#beta-regression"><i class="fa fa-check"></i><b>7.6.2</b> Beta regression</a></li>
<li class="chapter" data-level="7.6.3" data-path="GLM.html"><a href="GLM.html#distributional-models"><i class="fa fa-check"></i><b>7.6.3</b> Distributional models</a></li>
<li class="chapter" data-level="7.6.4" data-path="GLM.html"><a href="GLM.html#exercises-10"><i class="fa fa-check"></i><b>7.6.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="GLM.html"><a href="GLM.html#robust-models"><i class="fa fa-check"></i><b>7.7</b> Robust Models</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">New statistics for the design researcher</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian_statistics" class="section level1">
<h1><span class="header-section-number">3</span> Elements of Bayesian statistics</h1>
<p>As human beings we make our decisions on what has happened to us earlier in time. For example, we trust a person or a company more, when we can look back at a series of successful transactions. And we have remarkable skills to recall what has just happened, but also what happened yesterday or years ago. By integrating over all the evidence, we form a view of the world we forage. When evidence is abundant, we believe vigorously experience a feeling of certainty, or lack of doubt. That is not to deny, that in a variety of situations, the boundedness of human cognition kick in and we becaome terrible decision makers. This is for a variety of psychological reasons, to name just a few:</p>
<ul>
<li>forgetting evidence</li>
<li>the primacy effect: recent events get more weight</li>
<li>confirmation bias: evidence that supports a belief is actively sought for, counter-evidence gets ignored.</li>
<li>the hindsight bias: once a situation has taken a certain outcome, we believe that it had to happen that way.</li>
</ul>
<p>The very aim of scientific research is to avoid the pitfalls of our minds and act as rational as possible by translating our theory into a formal model, gathering evidence in an unbiased way and weigh the evidence by formal procedures. When a statistic is reported together with the strength of evidence, this is conventionally called an <em>inferential statistic</em>. In applied research, real world decisions depend on the evidence, which has two aspects: first, the strength of effects and the level of certainty we have reached.</p>
<p>Bayesian inferential statistics grounds on the idea of accumulating evidence through and through. <em>Certainty</em> (or belief or credibility or credence) in Bayesian statistics is formalized as a <em>probability scale (0 = impossible, 1 = certain)</em>. Evidence accumulates by two mechanisms, the successive observations in a data set and what has already been learned in the past. When new data arrives and is analyzed, a transition occurs from what you new before, <em>prior belief</em>, to what you know after seeing the data, <em>posterior belief</em>. In other words: by data the current belief gets an update.</p>
<p>In the following the essential concepts of statistics and Bayesian analysis will be introduced.</p>
<div id="descriptive_stats" class="section level2">
<h2><span class="header-section-number">3.1</span> Descriptive statistics</h2>
<p>In empirical research we systematically gather observations. Observations of the same kind are usually subsumed as variables. A set of variables that have been gathered on the same sample are called a data set, which typically is a table with variables in columns. In the most general meaning, <em>a statistic</em> is a single number that somehow represents relevant features of a data set. Statistics fall into several broad classes that answer certain types of questions:</p>
<ul>
<li>frequency: how many measures of a certain kind can be found in the data set?</li>
<li>central tendency: do measures tend to be located left (weak) or right (strong) on a scale?</li>
<li>dispersion: are measures close together or widely distributed along the scale?</li>
<li>association: does one variable X tend to change when another variable Y changes?</li>
</ul>
<div id="frequencies" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Frequencies</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">attach</span>(Sec99)</code></pre></div>
<p>The most basic statistics of all probably is the number of observations on a variable <span class="math inline">\(x\)</span>, usually denoted by <span class="math inline">\(n_{x}\)</span>. The number of observations is a rough indicator for the amount of data that has been gathered, which is directly linked to the level of certainty we can reach.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(Ver20<span class="op">$</span>ToT)</code></pre></div>
<pre><code>## [1] 100</code></pre>
<p>The number of observations is not as trivial as it may appear at first. In particular, it is usually not the same as the sample size, for two reasons: First, most studies employ repeated measures to some extent. You may have invited <span class="math inline">\(n_{Part}\)</span> participants to your lab, but on each participant you have obtained several measures of the same kind. When every participant is tested on, let’s say, five tasks, the amount of data obtained gets larger. Second, taking a valid measure can always fail for a variety of reasons, reasulting in <em>missing values</em>. For example, in the 99 seconds study, it has happened, that a few participants missed to fill in their age on the intake form. The researcher is left fewer measures of age <span class="math inline">\(n_{age}\)</span> than there were participants.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">sum</span>(<span class="op">!</span><span class="kw">is.na</span>(x))
<span class="kw">N</span>(Ver20<span class="op">$</span>age)</code></pre></div>
<pre><code>## [1] 97</code></pre>
<p>Another important issue is the distribution of measures across groups in a data set. Again, the number of observations in a group is linked to the certainty we can gain on that group. Furthermore, it is sometimes important to have the distribution match the proportions in the population, as otherwise biases may occur.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Ver20 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Gender) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="kw">n</span>())</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Gender</th>
<th align="right">n()</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">female</td>
<td align="right">59</td>
</tr>
<tr class="even">
<td align="left">male</td>
<td align="right">41</td>
</tr>
</tbody>
</table>
<p>The table above shows so called absolute frequencies. When comparing frequencies by groups, it often is more appropriate to report <em>relative frequencies</em> or <em>proportions</em>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_Gender &lt;-<span class="st"> </span><span class="kw">N</span>(Ver20<span class="op">$</span>Gender)

Ver20 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(Gender) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">proportion =</span> <span class="kw">n</span>()<span class="op">/</span>n_Gender)</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Gender</th>
<th align="right">proportion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">female</td>
<td align="right">0.59</td>
</tr>
<tr class="even">
<td align="left">male</td>
<td align="right">0.41</td>
</tr>
</tbody>
</table>
<p>Summarizing frequencies of metric measures, such as ToT or number of errors is useful, too. However, a complication arises by the fact that continuous measures do not naturally fall into groups. Especially in duration measures no two measures are exactly the same.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(<span class="kw">unique</span>(Ver20<span class="op">$</span>Gender))</code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(<span class="kw">unique</span>(Ver20<span class="op">$</span>ToT))</code></pre></div>
<pre><code>## [1] 100</code></pre>
<p>The answer to this problem is <em>binning</em>: the scale of measurement is divided into a number of adjacent sections, called bins, and all measures that fall into one bin are counted. For example, we could use bins of 10 seconds and assess whether the bin with values larger than 90 and smaller or equal to 100 is representative in that it contains a large proportion of values. As the histogram reveals, it is not very representative.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bin &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">bin_width =</span> <span class="dv">10</span>) <span class="kw">floor</span>(x<span class="op">/</span>bin_width) <span class="op">*</span><span class="st"> </span>bin_width
n_ToT &lt;-<span class="st"> </span><span class="kw">N</span>(Ver20<span class="op">$</span>ToT)

Ver20 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">bin =</span> <span class="kw">bin</span>(ToT)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(bin) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop_ToT =</span> <span class="kw">n</span>()<span class="op">/</span>n_ToT) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bin, <span class="dt">y =</span> prop_ToT)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>()</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-7-1.png" width="66%" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Ver20 %&gt;% </span>
<span class="co">#   ggplot(aes(x = ToT)) +</span>
<span class="co">#   geom_histogram(binwidth = 10)</span></code></pre></div>
<p>Strictly spoken, grouped and binned frequencies are not one statistic, but a vector of statistics. A plot that shows the density of values in a sequence of bins is called a histogram. It approximates what we will later get to know more closely as a <em>distribution</em>.</p>
</div>
<div id="central-tendency" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Central tendency</h3>
<p>Reconsider Jane <a href="design-research.html#case99">2.3.4</a>. When asked about whether users can complete a transaction within 99, she looked at the population average of her measures. The population average is what we call the <em>(arithmetic) mean</em>. The mean is computed by summing over all measures and divide by the number of observations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mean &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">sum</span>(x)<span class="op">/</span><span class="kw">length</span>(x)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(Ver20<span class="op">$</span>ToT)</code></pre></div>
<pre><code>## [1] 106</code></pre>
<p>Imagine a competitor would go to court. Not an expert in that matter, my humble opinion is that one of the first questions to be regarded probably is: what does “rent a car in 99 seconds” actually promise? And here are some other ways to interpret the same slogan:</p>
<p>“<em>50% (or more) of users</em> can rent a car in 99 seconds”. This is called the <em>median</em>. The median is computed by bringing all measures into an order and collect the element right in the center, or the mean of the center pair of values when the number of observations is even.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">median &lt;-<span class="st"> </span><span class="cf">function</span>(x){ 
  n &lt;-<span class="st"> </span><span class="kw">length</span>(x)
  center &lt;-<span class="st"> </span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">%/%</span><span class="dv">2</span>
    <span class="cf">if</span> (n<span class="op">%%</span><span class="dv">2</span> <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) 
        <span class="kw">sort</span>(x, <span class="dt">partial =</span> center)[center]
    <span class="cf">else</span> <span class="kw">mean</span>(<span class="kw">sort</span>(x, <span class="dt">partial =</span> center <span class="op">+</span><span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">1</span>)[center <span class="op">+</span><span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">1</span>])
}</code></pre></div>
<p>Actually, the median is a special case of so called <em>quantiles</em>. The court could decide that 50% of users is too lenient as a criterion and could demand that 75% percent of users must complete the task within 99 seconds for the slogan to be considered valid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quantile</span>(Ver20<span class="op">$</span>ToT, .<span class="dv">75</span>)</code></pre></div>
<pre><code>## 75% 
## 125</code></pre>
<p>A common pattern in distributions of measures is that a majority of observations accumulate in the center region. The point of highest density of a distribution is called the <em>mode</em>. In other words: the mode is the region (or point) that is most likely to occur. For continuous measures this once again poses the problem that every value is unique. Sophisticated procedures exist to smooth over this inconvenience, but by binning we can construct an approximation of the mode: just choose the center of the bin with highest frequency.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mode &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">bin_width =</span> <span class="dv">10</span>) {
  bins &lt;-<span class="st"> </span><span class="kw">bin</span>(x, bin_width)
  bins[<span class="kw">which.max</span>(<span class="kw">tabulate</span>(<span class="kw">match</span>(x, bins)))] <span class="op">+</span><span class="st"> </span>bin_width<span class="op">/</span><span class="dv">2</span>
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Ver20</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="right">Part</th>
<th align="right">ToT</th>
<th align="right">age</th>
<th align="left">Gender</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">146.1</td>
<td align="right">36</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">2</td>
<td align="right">88.1</td>
<td align="right">41</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">3</td>
<td align="right">115.9</td>
<td align="right">34</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">4</td>
<td align="right">124.0</td>
<td align="right">39</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">5</td>
<td align="right">117.1</td>
<td align="right">44</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">6</td>
<td align="right">101.8</td>
<td align="right">NA</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">7</td>
<td align="right">150.3</td>
<td align="right">40</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">8</td>
<td align="right">102.2</td>
<td align="right">41</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">9</td>
<td align="right">165.6</td>
<td align="right">42</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">10</td>
<td align="right">103.1</td>
<td align="right">48</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="right">11</td>
<td align="right">144.1</td>
<td align="right">37</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="right">12</td>
<td align="right">173.6</td>
<td align="right">55</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">13</td>
<td align="right">13</td>
<td align="right">63.3</td>
<td align="right">37</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">14</td>
<td align="right">14</td>
<td align="right">96.6</td>
<td align="right">40</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">15</td>
<td align="right">15</td>
<td align="right">101.0</td>
<td align="right">48</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">16</td>
<td align="right">16</td>
<td align="right">124.1</td>
<td align="right">43</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">17</td>
<td align="right">17</td>
<td align="right">96.5</td>
<td align="right">32</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">18</td>
<td align="right">18</td>
<td align="right">25.3</td>
<td align="right">47</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">19</td>
<td align="right">19</td>
<td align="right">31.8</td>
<td align="right">NA</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">20</td>
<td align="right">20</td>
<td align="right">144.6</td>
<td align="right">47</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">21</td>
<td align="right">21</td>
<td align="right">95.8</td>
<td align="right">44</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">22</td>
<td align="right">22</td>
<td align="right">51.6</td>
<td align="right">24</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">23</td>
<td align="right">23</td>
<td align="right">99.8</td>
<td align="right">46</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">24</td>
<td align="right">24</td>
<td align="right">141.4</td>
<td align="right">44</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">25</td>
<td align="right">25</td>
<td align="right">161.9</td>
<td align="right">45</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">26</td>
<td align="right">26</td>
<td align="right">92.1</td>
<td align="right">47</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">27</td>
<td align="right">27</td>
<td align="right">97.3</td>
<td align="right">50</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">28</td>
<td align="right">28</td>
<td align="right">52.1</td>
<td align="right">33</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">29</td>
<td align="right">29</td>
<td align="right">118.8</td>
<td align="right">36</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">30</td>
<td align="right">30</td>
<td align="right">85.8</td>
<td align="right">42</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">31</td>
<td align="right">31</td>
<td align="right">118.7</td>
<td align="right">38</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">32</td>
<td align="right">32</td>
<td align="right">126.1</td>
<td align="right">53</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">33</td>
<td align="right">33</td>
<td align="right">136.1</td>
<td align="right">37</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">34</td>
<td align="right">34</td>
<td align="right">86.7</td>
<td align="right">42</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">35</td>
<td align="right">35</td>
<td align="right">120.1</td>
<td align="right">46</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">36</td>
<td align="right">36</td>
<td align="right">53.5</td>
<td align="right">29</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">37</td>
<td align="right">37</td>
<td align="right">81.5</td>
<td align="right">33</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">38</td>
<td align="right">38</td>
<td align="right">79.5</td>
<td align="right">36</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">39</td>
<td align="right">39</td>
<td align="right">32.6</td>
<td align="right">26</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">40</td>
<td align="right">40</td>
<td align="right">106.1</td>
<td align="right">36</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">41</td>
<td align="right">41</td>
<td align="right">111.2</td>
<td align="right">36</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">42</td>
<td align="right">42</td>
<td align="right">94.2</td>
<td align="right">36</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">43</td>
<td align="right">43</td>
<td align="right">127.7</td>
<td align="right">38</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">44</td>
<td align="right">44</td>
<td align="right">83.2</td>
<td align="right">41</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">45</td>
<td align="right">45</td>
<td align="right">64.0</td>
<td align="right">46</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">46</td>
<td align="right">46</td>
<td align="right">118.0</td>
<td align="right">39</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">47</td>
<td align="right">47</td>
<td align="right">80.7</td>
<td align="right">27</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">48</td>
<td align="right">48</td>
<td align="right">148.3</td>
<td align="right">43</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">49</td>
<td align="right">49</td>
<td align="right">92.1</td>
<td align="right">48</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">50</td>
<td align="right">50</td>
<td align="right">124.7</td>
<td align="right">42</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">51</td>
<td align="right">51</td>
<td align="right">114.7</td>
<td align="right">26</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">52</td>
<td align="right">52</td>
<td align="right">81.5</td>
<td align="right">32</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">53</td>
<td align="right">53</td>
<td align="right">152.3</td>
<td align="right">51</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">54</td>
<td align="right">54</td>
<td align="right">124.3</td>
<td align="right">46</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">55</td>
<td align="right">55</td>
<td align="right">107.7</td>
<td align="right">45</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">56</td>
<td align="right">56</td>
<td align="right">113.3</td>
<td align="right">39</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">57</td>
<td align="right">57</td>
<td align="right">125.4</td>
<td align="right">39</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">58</td>
<td align="right">58</td>
<td align="right">107.7</td>
<td align="right">33</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">59</td>
<td align="right">59</td>
<td align="right">15.2</td>
<td align="right">27</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">60</td>
<td align="right">60</td>
<td align="right">113.5</td>
<td align="right">52</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">61</td>
<td align="right">61</td>
<td align="right">94.0</td>
<td align="right">30</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">62</td>
<td align="right">62</td>
<td align="right">110.6</td>
<td align="right">39</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">63</td>
<td align="right">63</td>
<td align="right">122.5</td>
<td align="right">45</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">64</td>
<td align="right">64</td>
<td align="right">147.0</td>
<td align="right">50</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">65</td>
<td align="right">65</td>
<td align="right">83.2</td>
<td align="right">57</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">66</td>
<td align="right">66</td>
<td align="right">144.1</td>
<td align="right">45</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">67</td>
<td align="right">67</td>
<td align="right">115.1</td>
<td align="right">29</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">68</td>
<td align="right">68</td>
<td align="right">136.2</td>
<td align="right">43</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">69</td>
<td align="right">69</td>
<td align="right">132.6</td>
<td align="right">49</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">70</td>
<td align="right">70</td>
<td align="right">126.6</td>
<td align="right">47</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">71</td>
<td align="right">71</td>
<td align="right">73.7</td>
<td align="right">54</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">72</td>
<td align="right">72</td>
<td align="right">102.3</td>
<td align="right">48</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">73</td>
<td align="right">73</td>
<td align="right">123.7</td>
<td align="right">NA</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">74</td>
<td align="right">74</td>
<td align="right">76.4</td>
<td align="right">60</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">75</td>
<td align="right">75</td>
<td align="right">88.7</td>
<td align="right">42</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">76</td>
<td align="right">76</td>
<td align="right">122.4</td>
<td align="right">42</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">77</td>
<td align="right">77</td>
<td align="right">128.0</td>
<td align="right">43</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">78</td>
<td align="right">78</td>
<td align="right">118.9</td>
<td align="right">51</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">79</td>
<td align="right">79</td>
<td align="right">78.4</td>
<td align="right">41</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">80</td>
<td align="right">80</td>
<td align="right">72.0</td>
<td align="right">43</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">81</td>
<td align="right">81</td>
<td align="right">150.4</td>
<td align="right">48</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">82</td>
<td align="right">82</td>
<td align="right">112.7</td>
<td align="right">42</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">83</td>
<td align="right">83</td>
<td align="right">107.7</td>
<td align="right">47</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">84</td>
<td align="right">84</td>
<td align="right">101.4</td>
<td align="right">49</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">85</td>
<td align="right">85</td>
<td align="right">69.2</td>
<td align="right">30</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">86</td>
<td align="right">86</td>
<td align="right">123.4</td>
<td align="right">39</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">87</td>
<td align="right">87</td>
<td align="right">98.5</td>
<td align="right">38</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">88</td>
<td align="right">88</td>
<td align="right">99.5</td>
<td align="right">46</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">89</td>
<td align="right">89</td>
<td align="right">133.0</td>
<td align="right">43</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">90</td>
<td align="right">90</td>
<td align="right">129.7</td>
<td align="right">42</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">91</td>
<td align="right">91</td>
<td align="right">146.8</td>
<td align="right">43</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">92</td>
<td align="right">92</td>
<td align="right">90.7</td>
<td align="right">41</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="right">93</td>
<td align="right">93</td>
<td align="right">124.5</td>
<td align="right">37</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">94</td>
<td align="right">94</td>
<td align="right">146.7</td>
<td align="right">49</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">95</td>
<td align="right">95</td>
<td align="right">71.7</td>
<td align="right">29</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">96</td>
<td align="right">96</td>
<td align="right">79.2</td>
<td align="right">42</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">97</td>
<td align="right">97</td>
<td align="right">71.0</td>
<td align="right">39</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="right">98</td>
<td align="right">98</td>
<td align="right">61.2</td>
<td align="right">50</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="right">99</td>
<td align="right">99</td>
<td align="right">107.4</td>
<td align="right">41</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="right">100</td>
<td align="right">100</td>
<td align="right">124.6</td>
<td align="right">49</td>
<td align="left">male</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Ver20 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(
    <span class="dt">mean_ToT   =</span> <span class="kw">mean</span>(ToT),
    <span class="dt">median_ToT =</span> <span class="kw">median</span>(ToT),
    <span class="dt">mode_ToT =</span> <span class="kw">mode</span>(ToT))</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">mean_ToT</th>
<th align="right">median_ToT</th>
<th align="right">mode_ToT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">106</td>
<td align="right">108</td>
<td align="right">145</td>
</tr>
</tbody>
</table>
<p>The table above shows the three statistics for central tendency side-by-side. Mean and median are close together. This is frequently the case, but not always. When the distribution of measures is completely symmetric mean and median perfectly coincide. In section @(distributions) we will encounter distributions that are not symmetric. The more a distribution is skewed, the stronger the difference between mean and median increases.</p>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-15-1.png" width="66%" /></p>
<p>To be more precise: for left skewed distributions the mean is strongly influenced by few, but extreme, values in the left tail of the distribution. The median only counts the number of observations to both sides and is not influenced by how extreme these values are. Therefore, it is located more to the right. The mode does not regard any values other than those in the densest region and just marks that peak. The same principles hold for right-skewed distributions.</p>
<p>To summarize, the mean is the most frequently used measure of central tendency, one reason being that it is a so called <em>sufficient statistic</em>, meaning that it exploits the full information present in the data. The median is frequently used when extreme measures are a concern. The mode is the point in the distribution that is most typical.</p>
</div>
<div id="dispersion" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Dispersion</h3>
<p>In a symmetric distribution with exactly one peak, mean and mode coincide and the mean represents the most typical value. For a value being more typical does not mean it is very typical. That depends on how the measures are dispersed over the whole range. In the figure below, the center value of the narrow distribution contains 60% of all measures, as compared to 40% in the wide distribution, and is therefore more representative.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_disp &lt;-
<span class="st">  </span><span class="kw">tribble</span>(<span class="op">~</span>y, <span class="op">~</span>narrow, <span class="op">~</span>wide,
        <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>,
        <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>,
        <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">4</span>,
        <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">2</span>,
        <span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(Distribution, frequency, <span class="op">-</span>y) 


D_disp <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> y, 
             <span class="dt">y =</span> frequency)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(Distribution <span class="op">~</span><span class="st"> </span>.) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>()</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-16-1.png" width="66%" /></p>
<p>A very basic way to describe dispersion of a distribution is to report the <em>range</em> between the two extreme values, <em>minimum</em> and <em>maximum</em>. These are easily computed by sorting all values and selecting the first and the last element. Coincidentally, they are also special cases of quantiles, namely the 0% and 100% quantiles.</p>
<p>The range statistic only uses just these two values and therefore does not fully represent the amount of dispersion. A statistic for dispersion that does so is the <em>variance</em>, which is the mean of squared deviatons from the mean. Squaring the deviations always produces a positive value, but makes variance difficult to interpret. The <em>standard deviation</em> is the square root of variance. By reversing the square the standard deviation is on the same scale as the original measures and their mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">min &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">sort</span>(x)[<span class="dv">1</span>]
max &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">quantile</span>(x, <span class="dv">1</span>)
range &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">max</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(x)
var &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">mean</span>((<span class="kw">mean</span>(x) <span class="op">-</span><span class="st"> </span>x)<span class="op">^</span><span class="dv">2</span>)
sd  &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">sqrt</span>(<span class="kw">var</span>(x))

Ver20 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="kw">min</span>(ToT),
            <span class="kw">max</span>(ToT),
            <span class="kw">range</span>(ToT),
            <span class="kw">var</span>(ToT),
            <span class="kw">sd</span>(ToT))</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">min(ToT)</th>
<th align="right">max(ToT)</th>
<th align="right">range(ToT)</th>
<th align="right">var(ToT)</th>
<th align="right">sd(ToT)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">15.2</td>
<td align="right">174</td>
<td align="right">158</td>
<td align="right">966</td>
<td align="right">31.1</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">detach</span>(Sec99)</code></pre></div>
</div>
<div id="associations" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Associations</h3>
<p>Are elderly users slower at navigating websites? How does reading speed depend on font size? Is the result of an intelligence test independent from gender?</p>
<p>A majority of research deals with associations between variables and the present section introduces some statistics to describe them. Variables represent properties of the objects of research and fall into two categories: <em>Metric variables</em> represent a measured property, such as speed, height, money or perceived satisfaction. <em>Categorial variables</em> put observations (or objects of research) into non-overlapping groups, such as experimental conditions, persons who can program or cannot, type of education etc. Consequently, associations between any two variables fall into precisely one of three cases:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>categorial</th>
<th>metric</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>categorial</td>
<td>frequency cross tables</td>
<td>differences in mean</td>
</tr>
<tr class="even">
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>metric</td>
<td>classification</td>
<td>covariance, correlation</td>
</tr>
<tr class="even">
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>All forms of associations derive directly from statistics we have already encountered. That is obvious for when one variable is categorial and a little more subtle for two metric variables.</p>
<p>When there are only categories in the game, but no measures, the only way to compare is by frequencies. To illustrate the categorial-categorial case, consider a study to assess the safety of two syringe infusion pump designs, called Legacy and Novel. All participants of the study are asked to perform a typical sequence of operation and it is recorded whether the sequence was completed correctly or not.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">attach</span>(IPump)

D_agg <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(Session <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Design, completion) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">frequency =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(completion, frequency)</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Design</th>
<th align="right">FALSE</th>
<th align="right">TRUE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Legacy</td>
<td align="right">21</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="left">Novel</td>
<td align="right">22</td>
<td align="right">3</td>
</tr>
</tbody>
</table>
<p>Besides the troubling result that incorrect completion is the rule, not the exception, there is almost no difference between the two designs. Note that in this study, both professional groups were even in number. If that is not the case, it is preferred to report proportions within a group. Note how every row sums up to <span class="math inline">\(1\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_agg <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(Session <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Design, completion) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">frequency =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Design) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">frequency =</span> frequency<span class="op">/</span><span class="kw">sum</span>(frequency)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(completion, frequency)</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Design</th>
<th align="right">FALSE</th>
<th align="right">TRUE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Legacy</td>
<td align="right">0.84</td>
<td align="right">0.16</td>
</tr>
<tr class="even">
<td align="left">Novel</td>
<td align="right">0.88</td>
<td align="right">0.12</td>
</tr>
</tbody>
</table>
<p>In a similar manner, associations between groups and metric variables are reported by group means. In the case of the two infusion pump designs, the time spent to complete the sequence is compared by the following table. And as you can see, adding a comparison of variance is no hassle.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_agg <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(Session <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Design) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">mean_ToT =</span> <span class="kw">mean</span>(ToT),
            <span class="dt">sd_ToT =</span> <span class="kw">sd</span>(ToT))</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Design</th>
<th align="right">mean_ToT</th>
<th align="right">sd_ToT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Legacy</td>
<td align="right">151.0</td>
<td align="right">62.2</td>
</tr>
<tr class="even">
<td align="left">Novel</td>
<td align="right">87.7</td>
<td align="right">33.8</td>
</tr>
</tbody>
</table>
<p>For associations between two metric variables, <em>covariance</em> is a commonly employed statistic for the association between two variables and derives directly from variance, with the difference that the square of deviations is replaced by the multiplication of two deviations. For example, we may want to assess whether there is any relationship between years of experience and ToT:</p>
<p><span class="math display">\[
\textrm{cov}_{XY} = \frac{1}{n} \sum_{i=1}^n (x_i - E(X)) (y_i - E(Y))
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cov &lt;-<span class="st"> </span><span class="cf">function</span>(x, y)
  <span class="kw">mean</span>((x <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)) <span class="op">*</span><span class="st"> </span>(y <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y, <span class="dt">na.rm =</span> T)), <span class="dt">na.rm =</span> T)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cov</span>(D_agg<span class="op">$</span>experience, D_agg<span class="op">$</span>ToT)</code></pre></div>
<pre><code>## [1] 271</code></pre>
<p>When at large the deviations go into the same direction from the respective mean, covariance becomes positive. When the deviations primarily move in opposite direction it is negative. When the picture is mixed, covariance will stay close to zero. The three plots below illustrate three covariances, a strongly positive, a weakly positive and a moderate negative one.</p>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-26-1.png" width="66%" /><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-26-2.png" width="66%" /></p>
<p>The graphs highlight the deviation from the population mean of each variables. A strong covariance emerges whenever there is a strong tendency to deviate in either one direction. In fact, the illustration introduces a geometric interpretation: every data point stretches a rectangle which has an area of the product of the two deviations, just like in the formula of covariance.</p>
<p>As intuitive the idea of covariance is, as unintelligible is the statistic itself for reporting results. Covariance is not a pure measure of association, but is contaminated by the dispersions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. For that reason, two covariances can hardly be compared. The <em>Pearson correlation coefficient</em> <span class="math inline">\(r\)</span> solves the problem by rescaling a covariance by the two standard deviations:</p>
<p><span class="math display">\[
r_XY = \frac{\textrm{cov}_{XY}}{\textrm{sd}_X \textrm{sd}_Y}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cor &lt;-<span class="st"> </span><span class="cf">function</span>(x, y)
  <span class="kw">cov</span>(x, y)<span class="op">/</span>(<span class="kw">sd</span>(x, <span class="dt">na.rm =</span> T) <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(y, <span class="dt">na.rm =</span> T))

<span class="kw">cor</span>(D_psychomet<span class="op">$</span>MRS_<span class="dv">1</span>, D_psychomet<span class="op">$</span>MRS_<span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.952</code></pre>
<p>Due to the standardization of dispersion, <span class="math inline">\(r\)</span> lets the researcher interpret strength of association independent of scale of measurement. More precisely, <span class="math inline">\(r\)</span> will always be in the interval <span class="math inline">\([-1,1]\)</span>. That makes it the perfect choice for several purposes.</p>
<p>In the field of psychometrics, correlations are ubiquotously employed to represent <em>reliability</em> and <em>validity</em> of tests. For example, when the long-term career of an individual is dependent on the score of an assessment, it would be inexceptable if test scores differed strongly from day to day. <em>Test-retest stability</em> is one form to measure reliability and it is just the correlation of the same test taken on different days. For example, we could ask whether mental rotation speed as measured by the mental rotation task is a stable, such that we can use it for long-term predictions of whether someone will become a good surgeon. A reliability of <span class="math inline">\(.95\)</span> will probably satisfy most psychometricians. Validity of a test means that it predicts what it was intended for. For example, we could ask how well the ability of a person to become a minimally invasive surgeon depends on spatial cognitive abilities, like mental rotation speed. Validity could be assessed by taking performance scores from exercises in a surgery simulator and do the correlation with mental rotation speed. A correlation of <span class="math inline">\(r = .5\)</span> would indicate that mental rotation speed as measured by the task has rather limited validity. Another form is called <em>discriminant validity</em> and is about how specific a measure is. Imagine another test is already part of the assessment suite. This test aims to measure another aspect of spatial cognition, namely the capacity of the visual-spatial working memory (e.g., the Corsi block tapping task). If both tests are as specific as they claim to be, we would expect a particularly low correlation.</p>
<!-- #28 -->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_psychomet <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>GGally<span class="op">::</span><span class="kw">ggpairs</span>()</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-29-1.png" width="66%" /></p>
<p>Correlations allow psychometricians to employ absolute standards for the quality of measures. In exploratory analysis, one often seeks to get a broad overview of how a bunch of variables is associated. Creating a correlation table of all variables is no hassle and allows to get a broad picture of the situation. Correlations are ubitutous in data analysis, but have limitations: First, a correlation only uncovers linear trends, whereas the association between two variables can take any conceivable form. The validity of correlations depends on how salient the feature of linear trend is. In the example below, <span class="math inline">\(Y_1\)</span> reveals a strong parabolic form, which results in zero correlation. The curvature of an exponentially rising function is only captured insufficiently. For that reason, I recommend that correlations are always cross-checked by a scatterplot.</p>
<p>Another situation where covariances and correlations fail is when there simply is no variance. It is almost trivial, but for observing how a variable <span class="math inline">\(Y\)</span> changes when <span class="math inline">\(X\)</span> moves is that both variables <em>vary</em>, there is no co-variance without variance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(<span class="dt">x =</span>  (<span class="dv">0</span><span class="op">:</span><span class="dv">100</span>)<span class="op">/</span><span class="dv">10</span>,
           <span class="dt">y_1 =</span> <span class="kw">rnorm</span>(<span class="dv">101</span>, <span class="kw">exp</span>(x)<span class="op">/</span><span class="dv">100</span>, x <span class="op">*</span><span class="st"> </span><span class="dv">2</span>),
           <span class="dt">y_2 =</span> <span class="kw">rnorm</span>(<span class="dv">101</span>, (x <span class="op">-</span><span class="st"> </span><span class="dv">5</span>)<span class="op">^</span><span class="dv">2</span>, <span class="dv">3</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggpairs</span>(<span class="dt">lower=</span><span class="kw">list</span>(<span class="dt">continuous=</span><span class="st">&quot;smooth&quot;</span>))</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-30-1.png" width="66%" /></p>
<!-- 31
With covariance and correlations we covered linear trends between metric variables. But, how would we represent associations between groups and metric variables?
-->
<div id="here" class="section level4">
<h4><span class="header-section-number">3.1.4.1</span> [HERE]</h4>
<!--


+ There are at least some users who can (which clearly is the case). That would be the *minimum value*.
+ Half of the users or more can, which is a statistic called the *median*.
+ Most users can in around 99s, which is called the *mode*.

All the above, including the  mean, are numbers that summarize the observations. A single number that summarizes a set of observations is called *a statistic*, formally it is just

$s = f_s(D)$

with $s$ being a statistic on data $D$, defined by the function $f_s$. Characterizing a data set by appropriate statistics is called *descriptive statistics*. 

Most commonly used statistics fall in three classes: cardinality, central tendency (or location) of data and data dispersion. Cardinality statistics give the number of units of a class in a data set. The most common cardinalities are  the number of observed performance measures $n_{Obs}$, the number of individual participants $n_{Part}$ in the study. 

Reporting the calculated mean of time-on-task in two designs is descriptive. It summarizes the data set in a reasonable way. It is informative in so far as one learns which design to prefer, but coarsely. Consider a case where two designs were just slight variations, come at precisely the same costs, and the researcher has collected performance measures on five subjects per group, without the option of inviting more participants.
-->
</div>
</div>
</div>
<div id="bayesian-probability-theory" class="section level2">
<h2><span class="header-section-number">3.2</span> Bayesian probability theory</h2>
<!--Probability is an elusive concept and a source of mental suffering and heated debates, especially when people use intuition. The reason is that in the human mind, probability can be rooted in two different ways: in frequentist thinking, probability is represented by relative frequencies, whereas in Bayesian school of thought it is the level of certainty for some event to happen.-->
<!-- This is a Bayesian book and the following considerations are meant to convince the reader that the Bayesian use of probability has a wider domain of application than the frequentist. I also see this as one of the more rare situations where introducing some formalism is supportive in that it dissolves the subtle preoccupations that often accompany intuitive (or, I'd rather say: exemplified) understanding. In addition, the mathematical definition is bare of any real world notions, like observed frequencies or experienced certainties and  (figuratively speaking) makes the different perspectives converge. The algebraic definition of probability is given by the three Kolmogorov axioms. Before we come to that, let me undertake one intermediate step: introducing some set-theoretic concepts, which at the same time is meant as a deep bow to the discipline of thought that is so awe inspiring on the one hand a source of mental suffering for many on the other hand. -->
<p>First, be reminded mathematics is empty. In its purest form, it does not require or have any link to the real world. It often helps, to associate a mathematical system with some more real world ideas or illustrations, for example, I remember how my primary school teacher introduced the sum of two numbers as removing elements from one stack and place it on another, until the first stack is empty, piece by piece. Later, I found this to be an exact embodidment of how the sum follows from the Peano axioms, that define the set of natural numbers. Sometimes, a mathematical theory just describes trivial real world intuitions in just more complicated ways, like the Peano axioms. Sometimes a mathematical theory describe real world phenomena that we have no intuition about at all. A classic example is Einstein’s General Relativity Theory, which assumes a curved, rather than straight space. Earlier, Isaac Newton has introduced his Mechanics, grounding on the idea of a Euclidian space, where three dimensions form rectangular cubes, just as we know them. In order to formalize his theory, Einstein used the mathematical theory of Rieman spaces, which were discovered in the 19th century, but initially made no sense to anyone outside mathematics.</p>
<p>Fortunately, the math of probability is more on the intuitive side of things. The most tangible interpretation is that the probability of an event to happen, say throwing a Six with a dice, coincides with the relative frequency of a Six in a (very long) sequence of throws. This is called the <em>frequentist interpretation</em> of probability and this is how probability will be introduced in the following. While thinking in terms of relative frequency in long running sequences is rather intuitive, it has limitations. Not all events we want to assign a probability can be imagined as a long running sequence, among those are:</p>
<ul>
<li>the probability that a space ship will safely reach Mars (there’s only this one attempt)</li>
<li>the probability that a theory is more true than another (there’s only this pair)</li>
<li>the probability that your house burns down (you only have this one)</li>
</ul>
<p>Therefore, I will finally introduce the <em>Bayesian interpretation</em> of probability, which is the certainty in one’s belief. While this might be less tangible, it is compatible with the mathematical notion of probability and it is immensly useful in scientific thinking, as progress in science can be described as updating our beliefs about the world.</p>
<div id="some-set-theory" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Some set theory</h3>
<p>The mathematical concept of probability can most intuitively be approached by thinking in terms od relative frequency in long-running sequences. Actually, it is not even required to think of a sequence (where events have an order). It suffices to assume a set of events.</p>
<p>A mathematical <em>set</em> is a collection of elements taken from a domain (or universe, more dramatically). These can either be defined by stating all the elements, like <span class="math inline">\(S = \{\textrm{red}, \textrm{yellow}, \textrm{green}, \textrm{off}\}\)</span> or by a characterizing statement, like:</p>
<p><span class="math inline">\(S := \textrm{possible states of a Dutch traffic light}\)</span></p>
<p>The elements should be clearly identified, but need not have a particular order. (If they do, this is called an <em>ordered set</em>, the set of natural numbers is an example). Sets can have all possible sizes, which is called the <em>cardinality</em> of a set:</p>
<ul>
<li>empty like “all opponents who can defeat Chuck Norris”, <span class="math inline">\(\{\}\)</span> or <span class="math inline">\(\oslash\)</span></li>
<li>finite (and countable) like the states of a traffic light</li>
<li>infinite, but countable, like the natural numbers <span class="math inline">\(N\)</span></li>
<li>infinite, uncountable, like the real numbers <span class="math inline">\(R\)</span></li>
</ul>
<p>You may wonder now, whether you would ever need such a strange concept as uncaountable infinite sets in your down-to-earth design research. Well, the set of primary interest in every design study is the possible outcomes. Sometimes, these are finite, like <span class="math inline">\(\{\textrm{success}, \textrm{failure}\}\)</span>, but when you measure durations or distances, you enter the realm of real numbers. We will set this issue aside for the moment and return to it later in the context of continuous distributions of randomness.</p>
<p>In order to introduce the mathematical concept of probability, we first have to understand some basic operations on sets. For an illustration, imagine a validation study for a medical infusion pump, where participants were given a task and the outcome was classified by the following three criteria:</p>
<ul>
<li>was the task goal achieved successfully?</li>
<li>was the task completed timely (e.g., one minute or below)?</li>
<li>were there any operation errors along the way with potentially harmful consequences?</li>
</ul>
<p>Note how the the data table makes use of logical values to denote their membership to a set. Based on these three criteria, we can extract subsets of observations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sets)</code></pre></div>
<!-- 33 -->
<!-- 34 add set notation-->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">All      &lt;-<span class="st"> </span><span class="kw">as.set</span>(D_sets<span class="op">$</span>Obs)
Success  &lt;-<span class="st"> </span><span class="kw">as.set</span>(<span class="kw">filter</span>(D_sets, Success)<span class="op">$</span>Obs)
Harmful  &lt;-<span class="st"> </span><span class="kw">as.set</span>(<span class="kw">filter</span>(D_sets, Harm)<span class="op">$</span>Obs)
Timely   &lt;-<span class="st"> </span><span class="kw">as.set</span>(<span class="kw">filter</span>(D_sets, Timely)<span class="op">$</span>Obs)</code></pre></div>
<p>The first basic set operator is the <em>complementary set</em>. It selects all elements from the domain that are not part of a given set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Failure  &lt;-<span class="st"> </span>All <span class="op">-</span><span class="st"> </span>Success
Harmless &lt;-<span class="st"> </span>All <span class="op">-</span><span class="st"> </span>Harmful
Delayed  &lt;-<span class="st"> </span>All <span class="op">-</span><span class="st"> </span>Timely</code></pre></div>
<p>Once there is more than one set in the game, set operators can be used to create all kinds of new sets. First, the <em>union</em> of two sets collects the elements of two separate sets into one new set, for example, the set of all tasks that were failure or delayed (or both):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Failure <span class="op">|</span><span class="st"> </span>Delayed</code></pre></div>
<pre><code>## {1L, 2L, 3L, 4L, 5L, 6L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
##  27L, 28L, 29L, 30L}</code></pre>
<p>Another commonly used set operator is the <em>intersect</em>, which produces a set that contains only those elements present in both original sets, like the set of timely and succesful task completions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Success <span class="op">&amp;</span><span class="st"> </span>Timely</code></pre></div>
<pre><code>## {7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L}</code></pre>
<p>The <em>set difference</em> removes elements of one set from another, for example the set of all successful observations with no harmful side effects:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Success <span class="op">-</span><span class="st"> </span>Harmless</code></pre></div>
<pre><code>## {}</code></pre>
<p>It turns out that all succesful observations are also harmless. That indicates that the set of successful events is a <em>subset</em> of harmless events. The subset operator differs from those discussed so far, in that it does not produce a new set, but a truth value (also called logical or Boolean). A distinction is made between subsets and <em>proper subsets</em>, where one set contains all elements of another, but never the other way round. If two sets are mutual subsets (and therefore improper), they are <em>equal</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># subset</span>
Success <span class="op">&lt;=</span><span class="st"> </span>Harmless</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># proper subset</span>
Success <span class="op">&lt;</span><span class="st"> </span>Harmless</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set equality</span>
Success <span class="op">==</span><span class="st"> </span>Harmless</code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Success <span class="op">==</span><span class="st"> </span>(All <span class="op">-</span><span class="st"> </span>Failure)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>The example above demonstrates another important element of set theory: the empty set, which has the special property of being a subset of all other set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set</span>() <span class="op">&lt;</span><span class="st"> </span>Success</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Among other uses, the empty set is important for intersections. It may happen that two sets do not share any elements at all. It would be problematic, if the intersect operator only worked if common elements truly existed. In such a case, the intersection of two sets is the empty set. Sets that have an empty intersection are called <em>disjunct sets</em> and complementary sets are a special case. The package Sets, which defines all operators on sets so far is lacking a dedicated function for disjunctness, but this is easily defined using the intersect function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">is_disjunct &lt;-<span class="st"> </span><span class="cf">function</span>(x, y) <span class="kw">set_is_empty</span>(x <span class="op">&amp;</span><span class="st"> </span>y)
<span class="kw">is_disjunct</span>(Success, Harmful)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>So far, we have only seen sets of atomic elements, where all elements are atomic, i.e. they are not sets themselves. We can easily conceive a set that has elements that are sets. The set of sets that are defined by the three performance criteria and their complementary sets is an obvious example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set</span>(Success, Failure, Harmful, Harmless, Timely, Delayed)</code></pre></div>
<pre><code>## {&lt;&lt;set(9)&gt;&gt;, &lt;&lt;set(10)&gt;&gt;, &lt;&lt;set(12)&gt;&gt;, &lt;&lt;set(18)&gt;&gt;, &lt;&lt;set(20)&gt;&gt;,
##  &lt;&lt;set(21)&gt;&gt;}</code></pre>
<p>For the introduction of probability, we need two concepts related to sets o sets: First, a <em>partition of a set</em> is a set of non-empty subsets such that every element is assigned to exactly one subset. The subsets of successes and its conplementary set, all failures, is such a partition. Second, the <em>power set</em> is the set of all possible subsets in a set. Even with a rather small set of 20 elements, this is getting incredibly large, so let’s see it on a smaller example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S &lt;-<span class="st"> </span><span class="kw">set</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)
P &lt;-<span class="st"> </span><span class="kw">set_power</span>(S)</code></pre></div>
<p>The power set is tantamount for the definition of probability that follows, because it has two properties: first, for every subset of <code>S</code> it also contains the complementary set. That is called <em>closed under complementarity</em>. Second, for every pair of subsets of <code>S</code>, <code>P</code> it also contains the union, it is <em>closed under union</em>. In the same way, power sets are also <em>closed under intersection</em>. Generally, all sets of subsets that fulfill these three requirements are called <em><span class="math inline">\(\Sigma\)</span> algebras</em>. The mathematical theory of <span class="math inline">\(\Sigma\)</span> algebras is central for the mathematical definition of measures. Loosely speaken, a measure is a mapping from the domain of empirical observations to the domain of numbers, such that certain operations in the domain of measurement have their counterparts in the numerical domain. Probabilities are measures and in the next section we will see how numerical operations on probabilities relate to set operations in a <span class="math inline">\(\Sigma\)</span> algebra.</p>
<!--
A simple example of that is operations on rods of different lengths. Putting them  
-->
</div>
<div id="probability" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Probability</h3>
<p>Before we get to a formal presentation of probability, we can develop the idea on the fictional validation study introduced in the previous section. Performance of participants was classified by the three two-level criteria, success, harm and timeliness. Every recorded outcome therefore falls into one of eight possible sets and a purposeful way to summarize the results of the study would be relative frequencies (<span class="math inline">\(\pi\)</span>, <code>pi</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N_sets &lt;-<span class="st"> </span><span class="kw">nrow</span>(D_sets)
D_freq &lt;-
<span class="st">  </span>D_sets <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Success, Harm, Timely) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">complete</span>(Success, Harm, Timely, <span class="dt">fill =</span> <span class="kw">list</span>(<span class="dt">n =</span> <span class="dv">0</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># adds empty events</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pi =</span> n<span class="op">/</span><span class="kw">sum</span>(n))

D_freq</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Success</th>
<th align="left">Harm</th>
<th align="left">Timely</th>
<th align="right">n</th>
<th align="right">pi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FALSE</td>
<td align="left">FALSE</td>
<td align="left">FALSE</td>
<td align="right">1</td>
<td align="right">0.033</td>
</tr>
<tr class="even">
<td align="left">FALSE</td>
<td align="left">FALSE</td>
<td align="left">TRUE</td>
<td align="right">2</td>
<td align="right">0.067</td>
</tr>
<tr class="odd">
<td align="left">FALSE</td>
<td align="left">TRUE</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="right">0.100</td>
</tr>
<tr class="even">
<td align="left">FALSE</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="right">6</td>
<td align="right">0.200</td>
</tr>
<tr class="odd">
<td align="left">TRUE</td>
<td align="left">FALSE</td>
<td align="left">FALSE</td>
<td align="right">6</td>
<td align="right">0.200</td>
</tr>
<tr class="even">
<td align="left">TRUE</td>
<td align="left">FALSE</td>
<td align="left">TRUE</td>
<td align="right">12</td>
<td align="right">0.400</td>
</tr>
<tr class="odd">
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">FALSE</td>
<td align="right">0</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="right">0</td>
<td align="right">0.000</td>
</tr>
</tbody>
</table>
<p>Let’s examine on an abstract level, what has happened here:</p>
<ol style="list-style-type: decimal">
<li>The set of events has been partitioned into eight disjunct subsets</li>
<li>All subsets got a real number assigned, by the operation of relative frequencies, that is between (and including) zero and one.</li>
<li>The sum of these numbers is one.</li>
</ol>
<p>The common mathematical theory of probability assumes a set of outcomes <span class="math inline">\(\Omega\)</span> and a <span class="math inline">\(\Sigma\)</span> algebra <span class="math inline">\(F\)</span> on <span class="math inline">\(\Omega\)</span>, like the power set. An element <span class="math inline">\(E\)</span> of <span class="math inline">\(F\)</span> is called an <em>event</em>. Note the difference between outcomes, which are singular outcomes, and events, which are sets of outcomes, like the eight outcome categories above. Also note that these eight sets are a partition of <span class="math inline">\(\Omega\)</span>, but not a <span class="math inline">\(\Sigma\)</span>-algebra. However, we can easily construct a <span class="math inline">\(\Sigma\)</span>-algebra by adding all possible unions and intersections. Or we use the power set of outcomes in the data set right-away.</p>
<p>The <em>first Kolmogorov axiom</em> states that a probability is a non-negative real number assigned to every event. Take a look at the table above, to see that this is satisfied for the relative frequencies.</p>
<p>While the first axiom defines a lower border of zero for a probability measure, the <em>second Kolmogorov axiom</em> cares for the upper limit (although somewhat indirectly) by stating that the set of all observations <span class="math inline">\(\Omega\)</span> (which is an element of <span class="math inline">\(F\)</span>) is assigned a probability of one. In the table of relative frequencies that is not yet covered, but we can easily do so:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_sets <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># no group_by</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">pi =</span> <span class="kw">n</span>()<span class="op">/</span>N_sets) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">c</span>()</code></pre></div>
<pre><code>## $pi
## [1] 1</code></pre>
<p>So far, the theory only cared for assigning numbers to events (subsets), but provides no means to operate on probabilites. The <em>third Kolmogorov axiom</em> establishes a relation between the union operator on sets and the sum operator on probabilities by stating that the probability of a union of disjunct events is the sum of the individual probabilities. We can approve this to be true for the relative frequencies. For example, is the set of all successful observations is the union of successful timely observations. Indeed, the relative frequency of all successful events is the sum of the two and satisfies the third axiom:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_sets <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Success) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pi =</span> n<span class="op">/</span><span class="kw">sum</span>(n))</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Success</th>
<th align="right">n</th>
<th align="right">pi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FALSE</td>
<td align="right">12</td>
<td align="right">0.4</td>
</tr>
<tr class="even">
<td align="left">TRUE</td>
<td align="right">18</td>
<td align="right">0.6</td>
</tr>
</tbody>
</table>
<p>The Kolmogorov axioms establish a probability measure and lets us do calculations on disjunct subsets. That would be a meager toolbox to do calculations with probabilities. What about all the other set operators and their possible counterparts in the realm of numbers? It is one of greatest wonders of the human mind that the rich field of reasoning about probabilities spawns from just these three axioms and a few set theoretic underpinnings. To give just one example, we can derive that the probability of the complement of a set <span class="math inline">\(A\)</span> is <span class="math inline">\(P(\Omega/A) = 1 - P(A)\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>From set theory follows that a set <span class="math inline">\(A\)</span> and its complement <span class="math inline">\(\Omega/A\)</span> are disjunct, hence axiom 3 is applicable: <span class="math inline">\(P(A \cup \Omega/A) = P(A) + P(\Omega/A)\)</span></p></li>
<li><p>From set theory follows that a set <span class="math inline">\(A\)</span> and its complement <span class="math inline">\(\Omega/A\)</span> form a partition on <span class="math inline">\(\Omega\)</span>. Using axiom 2, we can infer: <span class="math display">\[\begin{aligned}
A \cup \Omega/A = \Omega\\ 
\Rightarrow P(A) + P(\Omega/A) = P(\Omega) = 1\\ 
\Rightarrow P(\Omega/A) = 1 - P(A)
\end{aligned}\]</span></p></li>
</ol>
<!-- However, we may want to calculate the probability of events that are not disjunct, speaking of sets that is they overlap. If we just add -->
<p>The third axiom tells us how to deal with probabilities, when events are disjunct. As we have seen, it applies for defining more general events. How about the opposite direction, calculating probabilities of more special events? In our example, two rather general events are Success and Timely, whereas the intersection event Success <em>and</em> Timely is more special. The probability of two events occuring together is called <em>joint probability</em> <span class="math inline">\(P(\textrm{Timely} \cap \textrm{Success})\)</span>. The four joint probabilities on the two sets and their complements are shown in the following table.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_sets <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Success, Timely) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">pi =</span> <span class="kw">n</span>()<span class="op">/</span>N_sets) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Success</th>
<th align="left">Timely</th>
<th align="right">pi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FALSE</td>
<td align="left">FALSE</td>
<td align="right">0.133</td>
</tr>
<tr class="even">
<td align="left">FALSE</td>
<td align="left">TRUE</td>
<td align="right">0.267</td>
</tr>
<tr class="odd">
<td align="left">TRUE</td>
<td align="left">FALSE</td>
<td align="right">0.200</td>
</tr>
<tr class="even">
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="right">0.400</td>
</tr>
</tbody>
</table>
<p>As joint probability asks for simultaneous occurance it treats both involved sets symmetrically: <span class="math inline">\(P(\textrm{Timely} \cap \textrm{Success}) = P(\textrm{Successes} \cap \textrm{Timely})\)</span>. What if you are given one piece of information first, such as “this was a successful outcome” and you have to guess the other “Was it harmful?”. That is called <em>conditional probability</em> and in this case, we even have 100% certainty in our guess, because an observation was only rated a success if there was no harm. But we can easily think of more gradual ways to make a better guess.</p>
<p><span class="math inline">\(P(\textrm{Harmful}|\textrm{Success})\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_sets <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(Success) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Harm) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pi =</span> n<span class="op">/</span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Harm</th>
<th align="right">n</th>
<th align="right">pi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FALSE</td>
<td align="right">18</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<!-- 38 examples gradual conmditional probability-->
<p>Perhaps, there is a relationship between Timely and Harm in the manner of a speed-accuracy trade-off. In the manner of a speed-accuracy trade-off, there could be a relationship between Timely and Harm. Participants who rush through the task are likely to make more harmful errors. We would then expect a different distribution of probability of harm by whether or not task completion was timely.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_sets <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(Timely, Harm) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pi =</span> n<span class="op">/</span><span class="kw">sum</span>(n)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Timely</th>
<th align="left">Harm</th>
<th align="right">n</th>
<th align="right">pi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FALSE</td>
<td align="left">FALSE</td>
<td align="right">7</td>
<td align="right">0.7</td>
</tr>
<tr class="even">
<td align="left">FALSE</td>
<td align="left">TRUE</td>
<td align="right">3</td>
<td align="right">0.3</td>
</tr>
<tr class="odd">
<td align="left">TRUE</td>
<td align="left">FALSE</td>
<td align="right">14</td>
<td align="right">0.7</td>
</tr>
<tr class="even">
<td align="left">TRUE</td>
<td align="left">TRUE</td>
<td align="right">6</td>
<td align="right">0.3</td>
</tr>
</tbody>
</table>
<p>See how conditional probabilities sum up to one <em>within</em> their condition. In this case, the conditional probabilities for harm are the same for successes and failures. As a consequence, it is also the same as the overall probability, hence:</p>
<p><span class="math display">\[\begin{aligned}
P(\textrm{Harm} | \textrm{Timely}) = P(\textrm{No harm} | \textrm{Timely}) =
P(\textrm{Timely})
\end{aligned}\]</span></p>
<p>This sitation is called <em>independence of events</em> and it means that knowing about one variable does not help in guessing the other. In statistics, <em>conditional probability</em> is an important concept. In particular, it will carry us away from the set theoretic interpretation towards the Bayesian interpretation of probability as states of knowledge.</p>
<!-- #39 elaborate the role of independence -->
<ul>
<li>joint probability</li>
<li>conditional probability (events in a sequence –&gt; knowledge of A is knowledge of B)</li>
<li>independence</li>
<li>Bayes theorem</li>
</ul>
<p>Other definitions and basic numerical operations on probabilities can be inferred in similar ways.</p>
<!-- 35 -->
<p>The basic operations on probability then give rise to more advanced laws of probability:</p>
<!-- 36 -->
<!--The two endpoints of the probability scale we can call the *impossible event*, $P(A) = 0$ and the *inevitable event* $P(A) = 1$. For example, it is impossible that any task a researcher throws at a participant is completed in a negative time: $P(ToT < 0) = 0$. At the same time, it is certain that the participant either correctly completes the task or not, $P(\textrm{correct } or \textrm{ incorrect}) = 1$. You may find the latter example trivial, but, in fact it points to another element of probability, namely, *the probability of all possible mutually exclusive events sums to 1*, for example:

+ tomorrow it rains or is doesn't
+ all people carrying zero or more Y chromosomes
+ the duration is a positive number

Have you noticed the frequent occurance of the word *or* in the above examples? In probability space, this little word precisely denotes an event 

On the other hand, *and* means that two events occur together. 


+ 
+ probabilities of independent events multiply


What are those events in design research and how do we assign them those numbers called probabilities? -->
<!--32-->
<!--To some confusion, but also underlining the emptiness of math, several types of measures are true probabilities themselves. Consider a validation study for a medical device. A sample of nurses is asked to complete a series of similar tasks. The performance measure of interest is the *relative frequency $\pi$* of failures. Generally, classifications of outcomes are probabilities, when the classification is unambiguous and exhaustive, such that every event falls into exactly one class. When the criterion for success is crisp enough, these requirements are fulfilled. They are also fulfilled by the relative frequencies by which you see green, yellow, red or dark when approaching a Dutch traffic light.

Especially for readers with some theoretical grounding in frequentist statistics it may be helpful to see that also continuous variables can be probabilities, namely proportions. Imagine a novel behavioural screening test on alcohol influence. The client is asked to walk 10 meters on a curved path. Using a set of smart sensors, that continuously assess whether the clients center of gravity is inside or outside the path. The *proportions* of relative distances inside or outside the path are probabilities (without being countable events).-->
</div>
<div id="likelihood" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Likelihood</h3>
<p>[…]</p>
<p>Consider the following example: A standard dice is rolled twice. How likely is an outcome of two times Six? We use probability theory: It is the same dice and the results of the rolls are independent. The probability of rolling a Six on either attempt is <span class="math inline">\(p(y_1 = \textrm{Six}) = p(y_2 = \textrm{Six}) = {1 \over 6}\)</span>. Therefore the joint probability is just the product:</p>
<p><span class="math display">\[
P(y_1 = \textrm{Six} \textrm{ and } y_2 = \textrm{Six}) \\= 
P(y_1 = \textrm{Six}) \times P(y_2 = \textrm{Six})= {1 \over 36}
\]</span></p>
<p>The joint probability of all data points is called the <em>Likelihood</em> and generalizes to situations where the probabilities of events are not the same, for example: What is the probability of the first dice being an Four, the second a five and the third a Three or a Six?</p>
<p><span class="math display">\[
P(y_1 = \textrm{Four} \textrm{ and } y_2 = \textrm{Five} \textrm{ and } y_3 = \textrm{Three} \textrm{ or } \textrm{Six}\\
=  {1 \over 6}
\times \frac 1 6 \times \frac 1 3 = \frac 1 {108}
\]</span></p>
<p>Notice how the likelihood gets smaller in the second example. In fact, likelihoods are products of numbers between zero and one and therefore become smaller with every observation that is added. In most empirical studies, the number of observations is much larger than two or three and the likelihood becomes inexpressibly small. Consider the following results from 16 rolls. In order to distinguish the base probability for any side from derived probabilities, we call it <span class="math inline">\(\pi = \frac 1 6\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">42</span>)
Events &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;One&quot;</span>, <span class="st">&quot;Two&quot;</span>, <span class="st">&quot;Three&quot;</span>, <span class="st">&quot;Four&quot;</span>, <span class="st">&quot;Five&quot;</span>, <span class="st">&quot;Six&quot;</span>)
Result &lt;-<span class="st"> </span><span class="kw">sample</span>(Events, <span class="dv">16</span>, <span class="dt">replace =</span> T)
pi =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">6</span>
Likelihood &lt;-<span class="st"> </span>pi<span class="op">^</span><span class="kw">length</span>(Result)</code></pre></div>
<p>The likelihood of this result, given that it is a fair dice, is <span class="math inline">\(\frac 1 6^{16} = 3.545\times 10^{-13}\)</span>. Therefore, one usually reports the <em>logarithm of the likelihood (log-likelihood)</em>. This results in “reasonable” negative numbers. Why negative? Because all Likelihoods are fractions of One (the identity element of multiplication), which results in a negative logarithm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(logLik &lt;-<span class="st"> </span><span class="kw">log</span>(Likelihood))</code></pre></div>
<pre><code>## [1] -28.7</code></pre>
<!-- The negative log likelihood is central concept in Bayesian and classic model estimation, as we will see. When estimating a model, a likelihood is computed a myriad of times. With a little mathematical trick, the negative log-likelihood can be calculated as sum of logarithms, instead of a logarithm of products. -->
<!-- $$ -->
<!-- \log(a \times b) = \log(a) + log(b) -->
<!-- $$ -->
<!-- ```{r} -->
<!-- -sum(rep(log(1/6), 16)) -->
<!-- ``` -->
<!-- For the human mind, summing over numbers is much easier than products. And the same goes for computers. The following code measures the computing time (ms) for a likelihood of ten thousand dice rolls. On my computer, summing over logarithms almost cuts the computing time in half. -->
<!-- ```{r} -->
<!-- Rolls <- rep(1/6, 10000) -->
<!-- bind_rows( -->
<!--   microbenchmark::microbenchmark(-prod(Rolls)), -->
<!--   microbenchmark::microbenchmark(-sum(log(Rolls))) -->
<!-- ) -->
<!-- ``` -->
<p>The dice rolling example above has a twist. assumed that we may enter <span class="math inline">\(\pi = {1 \over 6}\)</span>, because we <em>believe</em> that it is a fair dice, without further notice. In other words, we needed no data, because of overwhelming prior knowledge (or theory, if you will). Imagine we have been called in to uncover fraud with biased dices in a casino. There is suspicion, that the chance of rolling a Six is lower than <span class="math inline">\(1 \over 6\)</span>. So, what is the (most likely) chance of rolling a Six? With the help of a dice rolling robot, the following 6000 rolls have been recorded.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_Rolls &lt;-<span class="st"> </span><span class="dv">6000</span>

Biased_dice &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">Side =</span> <span class="kw">as_factor</span>(Events),
         <span class="dt">pi =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">6</span> <span class="op">+</span><span class="st"> </span>.<span class="dv">02</span>, <span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">6</span>, <span class="dv">4</span>), <span class="dv">1</span><span class="op">/</span><span class="dv">6</span> <span class="op">-</span><span class="st"> </span>.<span class="dv">02</span>))
  
  
  
<span class="kw">set.seed</span>(<span class="dv">41</span>)
Rolls &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">Roll =</span> <span class="dv">1</span><span class="op">:</span>n_Rolls,
                   <span class="dt">Result =</span> <span class="kw">sample</span>(Biased_dice<span class="op">$</span>Side, 
                                   <span class="dt">prob    =</span> Biased_dice<span class="op">$</span>pi, 
                                   <span class="dt">size    =</span> n_Rolls, 
                                   <span class="dt">replace =</span> T))

<span class="kw">head</span>(Rolls)</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">Roll</th>
<th align="left">Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">Three</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">Six</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">Five</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">One</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">Two</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left">Six</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Rolls <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Result)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>()</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-54-1.png" width="66%" /></p>
<p>The result is shown in the figure above. For simplicity we just focus on rolling Six. If we have no prior assumptions about the dice, the estimated probability is simply the relative frequency of Six.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Rolls <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Result) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">pi =</span> <span class="kw">n</span>()<span class="op">/</span>n_Rolls)</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Result</th>
<th align="right">pi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">One</td>
<td align="right">0.189</td>
</tr>
<tr class="even">
<td align="left">Two</td>
<td align="right">0.177</td>
</tr>
<tr class="odd">
<td align="left">Three</td>
<td align="right">0.163</td>
</tr>
<tr class="even">
<td align="left">Four</td>
<td align="right">0.160</td>
</tr>
<tr class="odd">
<td align="left">Five</td>
<td align="right">0.164</td>
</tr>
<tr class="even">
<td align="left">Six</td>
<td align="right">0.147</td>
</tr>
</tbody>
</table>
<p>In this case, we can simply note down that the <em>most likely value</em> is <span class="math inline">\(\pi_{Six} =.147\)</span>, which is lower than the fair 0.167. But, note the slight ruggedness of the bar chart. Not a single bar is read as exactly <span class="math inline">\(1 \over 6\)</span>, so the deviation of Six could have happened by chance. One way to approach this question is comparing the likelihoods <span class="math inline">\(P(\text{Result = Six}|\pi = {1 \over 6})\)</span> and <span class="math inline">\(P(\text{Result = Six}|\pi = {.147})\)</span>. For that purpose, we create a new event variable <code>Six</code>, that indicates whether a roll is a Six (<code>TRUE</code>) or not (<code>FALSE</code>). Further, a <em>distribution function</em> is required that assigns these events their probabilities. Distribution functions can take very complicated forms [REF statistical models], but in the case here it is the rather simple <em>Bernoulli distribution</em>.</p>
<p><span class="math display">\[
d_\text{Bern}(\text{Six}|p) = 
  \begin{cases}
    \text{Six} ,&amp; p\\
    \text{not Six},&amp; 1 - p
  \end{cases}
\]</span></p>
<p>The log-likelihood function of the Bernoulli distribution is just the sum of log-probabilities across all Rolls <span class="math inline">\(y_i\)</span>.</p>
<p><span class="math display">\[
LL_\text{Bern}(\pi) = \sum_i{\log(d_\text{Bern}(\text{Six},\pi))}
\]</span></p>
<p>Now, we can determine the ratio of likelihoods, conditional on <span class="math inline">\(\pi\)</span>. Recall that with logarithmic transformations, what was a ratio becomes a difference.</p>
<p><span class="math display">\[
LR = \exp(LL_\text{Bern}(.147) - LL_\text{Bern}(\frac 1 6)
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Rolls &lt;-<span class="st">  </span>Rolls <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Six =</span> (Result <span class="op">==</span><span class="st"> &quot;Six&quot;</span>))

dbern &lt;-<span class="st"> </span><span class="cf">function</span>(y, pi) <span class="kw">if_else</span>(y, pi, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>pi)
LL_bern &lt;-<span class="st"> </span><span class="cf">function</span>(pi) <span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">dbern</span>(Rolls<span class="op">$</span>Six, pi)))

pi_fair =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">6</span>
pi_est  =<span class="st"> </span>.<span class="dv">147</span>

<span class="kw">exp</span>(<span class="kw">LL_bern</span>(pi_est) <span class="op">-</span><span class="st"> </span><span class="kw">LL_bern</span>(pi_fair))</code></pre></div>
<pre><code>## [1] 4846</code></pre>
<p>Now, recall what a likelihood is: the probability of the observed data, under a certain model. Here, the data is almost 5000 times more likely with <span class="math inline">\(p = .147\)</span>. In classic statistics, such likelihood ratios are routinely been used for comparison of models.</p>
<p>Previously, I have indicated that the relative frequency gives us the most likely value for parameter <span class="math inline">\(\pi\)</span> (the case of a Bernoulli distributed variable), the <em>maximum likelihood estimate (MLE)</em>. The MLE is that point in the parameter range (here <span class="math inline">\([0;1]\)</span>), with the maximum likelihood. It is the point, where the data is most likely. In a similar way, the mean of Normal distributed measures is the maximum likelihood estimate for the distribution parameter <span class="math inline">\(\mu\)</span>. But, more advanced models do not have such a closed form, i.e., a formula that you can solve. Therefore, parameter estimation in classic statistics heavily grounds on numerical procedures to find maximum likelihood estimates, which I will now outline for illustrative purposes:</p>
<p>Notice that the probability function <span class="math inline">\(d_\text{Bern}(y_i, \pi)\)</span> has two parameters, the result of a roll and the base probability. The likelihood function, in contrast, only has the parameter <span class="math inline">\(\pi\)</span>, whereas the data is “buried” inside. This is by convention in order indicate that the likelihood function takes the data as fixed and is meant to explore the range of values for the parameter. By varying the parameter and reading the resulting likelihood of data, we can numerically interpolate the MLE. The most basic numerical interpolation method is a grid search, which starts at the left boundary of parameter range, zero in this case, and walks in small steps along a grid to the right boundary (one). By convention, maximum likelihood estimation is performed by <em>minimizing the negative log-likelihood</em>. For the convenience, the following likelihood function has been vectorized, to make it work smoothly in a tidy processing chain.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">LL_bern &lt;-<span class="st"> </span><span class="cf">function</span>(pi) <span class="kw">map_dbl</span>(pi, <span class="cf">function</span>(x) <span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">dbern</span>(Rolls<span class="op">$</span>Six, x))))

<span class="kw">LL_bern</span>(<span class="kw">c</span>(.<span class="dv">1</span>, .<span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] -2572 -2563</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">LL_grid &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">pi =</span> <span class="kw">seq</span>(.<span class="dv">01</span>, .<span class="dv">99</span>, <span class="dt">by =</span> .<span class="dv">01</span>)) <span class="op">%&gt;%</span><span class="st"> </span>## the grid
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">nlogLik =</span> <span class="op">-</span><span class="kw">LL_bern</span>(pi),
         <span class="dt">rank =</span> <span class="kw">min_rank</span>(nlogLik),
         <span class="dt">MLE =</span> (rank <span class="op">==</span><span class="st"> </span><span class="dv">1</span>))

LL_grid  <span class="op">%&gt;%</span><span class="st">          </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> pi, <span class="dt">y =</span> nlogLik)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> <span class="kw">filter</span>(LL_grid, MLE), <span class="dt">color =</span> <span class="st">&quot;Red&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_text</span>(<span class="dt">data =</span> <span class="kw">filter</span>(LL_grid, MLE), <span class="dt">label =</span> <span class="st">&quot;MLE&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;Red&quot;</span>, <span class="dt">nudge_y =</span> <span class="op">-</span><span class="dv">500</span>)</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-57-1.png" width="66%" /></p>
<p>Because we use the <em>negative</em> log-liklihood, the value for <span class="math inline">\(\pi\)</span> with maximum likelihood is the minimum of the likelihood curve. Here, <span class="math inline">\(pi_\text{MLE} = 0.15\)</span>, which is very close to the relative frequency estimate. It is always possible to get even more accurate by using a finer grid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(MLE &lt;-<span class="st"> </span><span class="kw">filter</span>(LL_grid, MLE))</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">pi</th>
<th align="right">nlogLik</th>
<th align="right">rank</th>
<th align="left">MLE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.15</td>
<td align="right">2507</td>
<td align="right">1</td>
<td align="left">TRUE</td>
</tr>
</tbody>
</table>
<p>In classic statistics MLE is one of the most common methods for estimating parameters from models. However, most of the time, data is more abundant and there is more than one parameter. It is possible tow extend the grid method to as many parameters as the model contains by just creating multi-dimensional grids. However, already a two-dimensional grid of rather coarse <span class="math inline">\(100 \times 100\)</span> would require the computation of 10.000 likelihoods. Classic statisticians have therefore developed optimization methods to identify the MLE with as few runs as possible. Soon, we will turn our attention to Bayesian estimation, where the Likelihood plays a central role in estimation, too [REF]. Another appliction of the Likelihood is introduced in chapter <a href="#GOF"><strong>??</strong></a>, where it defines the deviance criterion for model goodness-of-fit.</p>
</div>
<div id="certainty-as-probability" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Certainty as probability</h3>
<p>Inferential statistics serves rational decision making under uncertainty by attaching information on the <em>level of certainty</em> to a parameter of interest. A central difference between frequentist and Bayesian statistical theory is how the elusive concept of <em>certainty</em> emerges.</p>
<p>Let us for a moment return to how probability was axiomatically defined by Kolmogorov’s axioms. The axioms themselves only speak of relations between sets and ptobability. At no point do these axioms operate on frequencies (such as the cardinality of sets). The axioms are not constructive in the way that they show you how to calculate a probability in the real world. They just define the requirements for any number to be called a probability. It is our intuition that makes it almost inevitable to think of probability of a set <span class="math inline">\(A\)</span> as relative frequencies, i.e. the ratio of the cardinality of A divided by the cardinality of the universal set <span class="math inline">\(\Omega\)</span>. Relative frequency precisely is the frequentist defintion of probability:</p>
<p><span class="math display">\[
P(A) = {|A| \over |\Omega|}
\]</span></p>
<p>In frequentist statistics, a common way to express ones level of certainty about a parameter (say, the population mean) is the <em>confidence interval</em>, which is expressed as two endpoints:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">attach</span>(Sec99)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Ver20 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">lm</span>(ToT <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> .) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">confint</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">2.5 %</th>
<th align="right">97.5 %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td align="right">99.8</td>
<td align="right">112</td>
</tr>
</tbody>
</table>
<p>The 95% confidence interval is defined by assuming an (infinite) set of replications of the very same experiment and using relative frequencies: <em>The 95% confidence interval is constructed in such a way that, if the same experiment were repeated an infinite number of times, in 95% of these repetitions the true value is contained in the interval.</em></p>
<p>And similar it goes for the infamous p-value: <em>A result is called statistically significant on level <span class="math inline">\(\alpha = .05\)</span>, if drawing from the null distribution (an infinite number of times) will produce the observed result or a larger result in no more than 5% of cases.</em></p>
<p>You are not alone when you lack intuition of what the definition says and when you feel at unease about where all these experiments are supposed to come from. (Most of the time they are just imaginations.) As much as I usually embrace parsimony, it must not kill intuition. Also, the Kolmogorov axioms can be satisfied by other interpretations of probability. In Bayesian statistics, the level of certainty is expressed as a <em>proposition about one’s state of knowledge</em>, like : <em>Based on my data I am 95% sure that there is a difference</em>. Equating level of certainty with probability directly, without taking the detour via relative frequencies, may be a little lax, but it leads to remarkably intuitive statements on uncertainty. In Bayesian statistics the <em>credibility interval</em> is defined as: <em>With a probability of 95%, the true value is contained.</em> Since there seems to be no external criterion (such as a series of experiments, imagined or not), Bayesian statistics often faced the criticism of being subjective. In fact, if we imagine a certainty of 95% as some number in the researchers mind, that might be true. But, it is quite easy to grasp certainty as an objective quantity, when we assume that there is something at stake for the researcher and that she aims for a rational decision. In the previous chapter I have illustrated this idea by the example of carrying an umbrella with you (or not) and the 99 seconds claim. Generally, it helps to imagine any such situation as a gamble: if you bet 1 EUR that the true population mean is outside the 95% credibility interval, as a rational person I would put 19 EUR against.</p>
<p>In effect, the Bayesian certainty is a probability in mathematical terms, without the necessity to implement it as a relative frequency. That liberates our reasoning from the requirement to think of long-running series, when this makes little sense to us. Much of the time, we have only this one shot. In the next section we will see how probability theory is used to operate on certainties using Bayes famous theorem.</p>
</div>
<div id="bayes-theorem-and-the-dynamics-of-belief" class="section level3">
<h3><span class="header-section-number">3.2.5</span> Bayes theorem (and the dynamics of belief)</h3>
<p>In Bayesian statistics, certainty (or strength of belief, state of knowledge, credibility) is assumed to follow all rules of probability theory. We may still think of it in terms of frequencies or illustrate it as a gamble, whatever fits the context better. What sets Bayesian statistics apart is how it emphasizes the <em>dynamics of certainty</em>, like in the real world:</p>
<ul>
<li>Someone sceptical of climate change may change their view after one very hot summer.</li>
<li>Everyone believed strongly in Newton’s Mechanics, but some people got curious when it turned out that light speed is constant.</li>
</ul>
<p>Note how these examples all have the same dynamics: a prior belief is updated when new data arrives. These are precisely the three elements of Bayesian statistics:</p>
<ol style="list-style-type: decimal">
<li>the <em>prior belief</em> is what you believe before (climate change is a myth)</li>
<li>the <em>likelihood</em> is what you learn by observation (hot summer)</li>
<li>the <em>posterior belief</em> is your adjusted believe after seeing the data (maybe not such a myth)</li>
</ol>
<p>Before we get to a more formal definition of the dynamics of knowledge, Bayes theorem, let us explore this idea a little. First of all, it seems that updating knowledge is the primary purpose of any central nervous system, including its sensors (which provide the data).</p>
<p>The data is usually the present observation or study. But, that does not exclude that prior knowledge grounds on data, too. In experimental Psychology researchers entertain themselves repetitions of the very same experimental paradigm, with slight variations maybe. For example, in the famous Stroop effect, participants have to name the ink color of a word. When the word is itself a color word and refers to a different color, response times typically increase. This effect has been replicated in many dozens of published studies and, probably, thousands of student experiments. Cumulative evidence is so strong that, would repeat the experiment another time and find the reverse effect, no one would seriously take this as a debunk. This extreme example illustrates another principle that follows from Bayes rule:</p>
<blockquote>
<p>Today’s posterior is tomorrow’s prior.</p>
</blockquote>
<p>There is no principled difference between prior and posterior. They are just levels of belief (credences) at different points in time. Both differ in strength: prior knowledge can be firm when it rests on an abundance of past evidence. The same holds for the likelihood in the present data: the more observations, the stronger the evidence. This is why larger sample sizes are usually preferred. Prior belief and likelihood can be congruent or contradict each other. When they are congruent, prior belief is just strengthened by the data. When they are contradicting each other, prior belief is over-ruled to some degree. That degree depends on how much data there is.</p>
<!--#25-->
<p>Whatever happens in the individual case, it is generally accepted that scientific progress is incremental over large periods of time. Under this perspective the idea of updating one’s belief is even trivial. It is common sense, that if you are too uncertain about a situation you better gather more information. Once you have reached a satisfactory level of certainty, you proceed to act (or publish). However, as readers with a very firm background in classic statistics may know, once you have settled down on a sample size (and a level of significance), you absolutely must test precisely this number of participants. Stopping early (because you have reached the desired p-value) or adding to the sample is strictly forbidden. Doesn’t that bother you under the perspective of accumulative evidence? It should bother you that incremental collection of evidence is impossible when doing frequentist statistics. On the other hand, in Bayesian statistics, accumulation of evidence is a core feature.</p>
<p>Neither is there a way to express one’s prior belief when doing a t-test, nor may you continuously adjust your sample size until satisfactory certainty is reached. In the fictional example of the Stroop task, the classic data analysis pretends as if no one has ever done such an experiment before. At the same time, it is strictly forbidden to invite further participants to the lab, when the test results point into the right direction, but evidence is still to weak. If you planned the study with, say, <span class="math inline">\(N = 20\)</span>, this is what you must do, no less no more. If you reach your goal with less participants, you must continue testing. If you are unsatisfied with the level of certainty, the only permissable action is dump your data and start over from zero. The denial incremental knowledge is commonly counter-intuitive and for a researcher it is a millstone around the neck.</p>
<p>Reconsider Jane and Andrew. What did they know about the current state of affairs when running a particular session. Using some time-on-task measures they disproved the claim “rent a car in 99 seconds”. Recall how precisely the question was phrased: on average, users had to be able to complete the transaction in 99 seconds. The statistic of interest is the mean. This was debunked with almost no effort, by calculating:</p>
<p><span class="math display">\[\hat M_{ToT} = 1 \over n * \sum{ToT} = 105.975\]</span></p>
<p>But how about the updated slogan: “rent a car in 111 seconds”. Can we be sure it holds, when someone repeats the study? We can only to a degree. It could still happen, that the belligerent competitor comes to a different result, just because they have a different sample of participants. Even if they would test a fully matching sample of participants, the measures will differ, simply because an array of smaller and larger impact factors is continuously hitting the central and peripheral nervous systems of your participants. The result is randomness in the data. Fortunately, randomness is often found to be well-behaved in that recurrent patterns emerge. These patterns are called distributions of randomness and I will introduce a whole bunch of them later in this chapter @().</p>
<p>Let us finally see how Bayesian statistics formalizes the transition from prior belief to posterior belief. Although it may not seem obvious at first, but everything rests on one rather simple result from probability theory, <em>Bayes’ theorem</em>:</p>
<p><span class="math display">\[
P(A|B) = { P(A)P(B|A) \over P(B)}
\]</span></p>
<p>This theorem emerges from formal probability theory (and therefore is neither Bayesian nor frequentist). As so often with math, the theorem is easiest understood when put into context. Standing on the shoulder of giants, let me explain it by the example of a medical screening test:</p>
<p>In the 1980, the human immunodeficiency virus (HIV) was discovered and since then has become a scourge for humanity. Given that the first outbursts of the desease raged among homosexual men, it is not really surprising that a few conservative politicians quickly called for action an proposed a mandatory test for everyonefor the results to be registered in a central data base. With some stretch it may just be justifiable to store (and use) the information that someone is carrying such a dangerous desease. The problem is with those people who do not carry it, but could be mis-diagnosed. These are called <em>false-positives</em>. The power of a screening test has been assessed by examining samples of participants where it is fully known whether someone is a carrier of the virus <span class="math inline">\(C+\)</span> or not <span class="math inline">\(C-\)</span>. The result is a <em>specificity</em> of 95%, meaning that 95% of <span class="math inline">\(C-\)</span> are diagnosed correctly (<span class="math inline">\(P(T-|C-)\)</span>), and a <em>sensitivity</em> of 99%, meaning that 99% with <span class="math inline">\(C+\)</span> are diagnosed correctly (<span class="math inline">\(P(T+|C+)\)</span>). The question that Bayes’ theorem can answer in such a situation is <em>How many citizens would be registered as HIV carrying, although they are not?</em>. For this to work, we must also know the probability that someone randomly chosen from the population is a carrier (<span class="math inline">\(P(C+)\)</span>) and the proportion of positive test results <span class="math inline">\(P(T+)\)</span>.</p>
<!-- # Explain marginal probability p(A) = p(B)p(A|B) + p(-B)p(A|-B), weighted average of probabilities -->
<p><span class="math display">\[
\begin{aligned}
P(C+) &amp;&amp;= .0001\\
P(C-) &amp;= 1 - P(C+) &amp;= .9999\\
P(T+|C+) &amp;&amp;= .99\\
P(T-|C+) &amp;= 1 - P(T-|C+) &amp;= .01\\
P(T-|C-) &amp;&amp;= .95\\
P(T+|C-) &amp;= 1 - P(T-|C-) &amp;= .05\\
P(T+) &amp;= P(C+)P(T+|C+) + P(C-)P(T+|C-) &amp;\approx .05\\
P(T-) &amp;= 1 - P(T+) &amp;\approx .95 
\end{aligned}
\]</span></p>
<p>Once again, how do these numbers arise? The first, <span class="math inline">\(P(C+)\)</span>) is the proportions of HIV carriers in the whole population. If you have no test at all, that is your best guess for whether someone random has the virus, <em>your prior knowledge</em>. Then, the validation study of the test provides us with more <em>data</em>. The study examined the outcome of the test (<span class="math inline">\(T+\)</span> or <span class="math inline">\(T-\)</span>) in two groups of participants, those that were knowingly carriers <span class="math inline">\(C+\)</span> and those that were not <span class="math inline">\(C-\)</span>. This is where all the conditional probabilities come from. Finally, we need the expected proportion of positive test results <span class="math inline">\(P(T+)\)</span>, which we compute as, sort of, a weighted average over the two conditions. Because <span class="math inline">\(C-\)</span> dominates the population, this is approximately the same as <span class="math inline">\(P(T+|C-)\)</span>. What matters in the present, that is when the test is put to use on random people.</p>
<ul>
<li>collateral damage: which proportion of the population will be registered, with all possible consequences, although they are non-carriers? That is: <span class="math inline">\(P(C-|T+)\)</span></li>
<li>intercourse risk: if a random sexual candidate shows you a negative test results, what is the remaining risk that they are carriers? That is: <span class="math inline">\(P(C+|T-)\)</span></li>
</ul>
<p>These two questions can be answered by using Bayes’ theorem. By combining prior knowledge and data in the prescribed form, we obtain for collateral damage:</p>
<p><span class="math display">\[
\begin{aligned}
P(C-|T+) &amp;= {P(C-)P(T+|C-) \over P(T+)}\\
&amp;\approx {.9999 \times .05} \over .05\\
&amp;\approx .9999
\end{aligned}
\]</span></p>
<p>This would be a disaster. The data base would be filled mostly with non-carriers. Let us see whether the test is safe in practice? The probability that somehone has the virus despite a negative test result is:</p>
<p><span class="math display">\[
\begin{aligned}
P(C+|T-) &amp;= {P(C+)P(T-|C+) \over P(T-)}\\
&amp;\approx {.0001 \times .01 \over .95}\\
&amp;\approx .000001
\end{aligned}
\]</span> We see a strong asymmetry in how useful the test is in the two situations. Specificity of the test is rather low and stands no chance against the over-whelming prevalence of non-carriers. In the second use case, prevalence and high test sensitivity work in the same direction, which results in a fantastic low risk to err.</p>
<p>Again, you probably are not alone, if you lack intuition, here: As Gerd Gigerenzer argues from an evolutionary perspective, our brain is capable of understanding Bayes’ theorem if it sees it in terms of <em>frequencies</em>. Isn’t that ironic? Gigerenzer argues that our brains might even be Bayesian machines that operate on frequencies in our personal history with encounters of this and that type. In fact, there were frequencies in the first place, when the evaluation study was conducted. Specificity and sensitivity are statistics, i.e. summaries of <em>2x2 frequency tables</em>.</p>
<p>We simulate the situation with a population of 100.000.000:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_HIV &lt;-<span class="st"> </span><span class="dv">100000000</span>

<span class="kw">set.seed</span>(<span class="dv">42</span>)
D_HIV_pop  &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">Carrier =</span> <span class="kw">as.logical</span>(<span class="kw">rbinom</span>(n_HIV, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">prob =</span> .<span class="dv">0001</span>))) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prob =</span> <span class="kw">if_else</span>(Carrier, .<span class="dv">99</span>, .<span class="dv">05</span>),
         <span class="dt">Test =</span> <span class="kw">as.logical</span>(<span class="kw">rbinom</span>(n_HIV, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">prob =</span> prob))) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Carrier, Test) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">N =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">print</span>()</code></pre></div>
<pre><code>## # A tibble: 4 x 3
## # Groups:   Carrier [2]
##   Carrier Test         N
##   &lt;lgl&gt;   &lt;lgl&gt;    &lt;int&gt;
## 1 FALSE   FALSE 94991624
## 2 FALSE   TRUE   4998456
## 3 TRUE    FALSE      108
## 4 TRUE    TRUE      9812</code></pre>
<div id="here-1" class="section level4">
<h4><span class="header-section-number">3.2.5.1</span> [HERE]</h4>
<p>The table below comes in long format and computes the posterior probability in just two steps. First, it takes the frequencies of all positive and negative tests. These are the <em>margin sums</em> of Test.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_HIV_pop <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Test) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">margin_Test =</span> <span class="kw">sum</span>(N)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">posterior_prob =</span> N <span class="op">/</span>margin_Test,
         <span class="dt">log_posterior_prob =</span> <span class="kw">log</span>(posterior_prob)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">print</span>()</code></pre></div>
<pre><code>## # A tibble: 4 x 6
##   Carrier Test         N margin_Test posterior_prob log_posterior_prob
##   &lt;lgl&gt;   &lt;lgl&gt;    &lt;int&gt;       &lt;int&gt;          &lt;dbl&gt;              &lt;dbl&gt;
## 1 FALSE   FALSE 94991624    94991732     1.000             -0.00000114
## 2 FALSE   TRUE   4998456     5008268     0.998             -0.00196   
## 3 TRUE    FALSE      108    94991732     0.00000114       -13.7       
## 4 TRUE    TRUE      9812     5008268     0.00196           -6.24</code></pre>
<!-- # correct numbers in formulas -->
<p>Now, we clearly see how it happens: In this massive evaluation study, only 100 persons were carriers, due to the low prevalence of the virus. Only 1 in 949906 is a carrier with a negative test result. At the same time, almost everybody who has a positive test is not a carrier, just because almost everybody is a non-carrier. To be fair, part of the magic is an assumption we made: the evaluation study is fully representative as it reflects the true distribution of carriers in the full population. Probably, a real study would be conducted with a biased sample, say 100 carriers and 100 non-carriers. That is not a problem, if we first adjust for prevalence and then carry out the same procedure:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TestEval &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">tribble</span>(<span class="op">~</span>Carrier,   <span class="op">~</span>Test,  <span class="op">~</span>prevalence, <span class="op">~</span>N,
          T,          T,      .<span class="dv">0001</span>,       <span class="dv">99</span>,      
          T,          F,      .<span class="dv">0001</span>,        <span class="dv">1</span>,
          F,          T,      .<span class="dv">9999</span>,        <span class="dv">5</span>,
          F,          F,      .<span class="dv">9999</span>,       <span class="dv">95</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Test) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">margin_Test =</span> <span class="kw">sum</span>(N)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prob =</span> (N <span class="op">/</span>margin_Test)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">print</span>()</code></pre></div>
<pre><code>## # A tibble: 4 x 6
##   Carrier Test  prevalence     N margin_Test   prob
##   &lt;lgl&gt;   &lt;lgl&gt;      &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;
## 1 TRUE    TRUE      0.0001    99         104 0.952 
## 2 TRUE    FALSE     0.0001     1          96 0.0104
## 3 FALSE   TRUE      1.000      5         104 0.0481
## 4 FALSE   FALSE     1.000     95          96 0.990</code></pre>
<!-- find correct term for testAbundnc -->
<p><span class="math display">\[\text{posterior}\ \propto \text{prior}\times\text{likelihood}\]</span></p>
<!-- #24 -->
<p>Note that <span class="math inline">\(\propto\)</span> here means <em>proportional to</em>. In very plain words this is:</p>
<blockquote>
<p>what you believe now is a combination of what you knew before and what you have just seen in the data.</p>
</blockquote>
<!--


So, if everybody would just do descriptive statistics, we were done, here: Identify the quantitative summary that suits your situation and compute it. However, what happens the field of statistics strongest contribution is that the level of uncertainty can also be quantified. 


Inferential statistics is required, whenever there is imperfect evidence and when there is skin-in-the-game.


This uncertainty arises from incomplete knowledge: only a small fraction of potential users have been tested, whereas the claim is about the whole population. Notice that I have not written the sample mean as $M$, but put a "hat" on it. That is to denote that the sample mean is an *estimate* for the population mean. The term estimate usually denotes that it is reasonable (intuitive, as well strictly mathematical) to assume that the statistic obtained from the sample is useful for making claims about the whole population. At the same time, every sample of users carries only partial information on the whole population, such that the estimate is imperfect.

A consequence of imperfection is that when another sample is drawn, one usually does not obtain the precise same estimate. The fluctuation of samples from an unknown population is what classic frequentist statistics draws upon. In the case of Jane and Andrew, a frequentist statistician would ask the question:

> How certain can you be that in the population of users $M_{ToT} <= 99$?

For frequentist thinkers the idea of the sample is central and the mathematical underpinning rests on an experiment of thought: how would all other possible samples look like? Here, following the Bayesian approach, and the fluctuation in sample statistics we consider a consequence of uncertainty. All of them carrying incomplete information, and inferential Bayesian statistics centers around full quantification of uncertainty. As we have seen, uncertainty about a future ivent to occur, is crucial for decision making on rational grounds.


-->
</div>
</div>
</div>
<div id="statistical-models" class="section level2">
<h2><span class="header-section-number">3.3</span> Statistical models</h2>
<p>It is a scientific principle that every event to happen has its causes (from the same universe). The better these causes are understood, the better will be all predictions of what is going to happen the next moment, given that one knows the laws of physics. <em>Laplace demon</em> is a classic experiment of thought on the issue: the demon is said to have perfect knowledge of laws of physics and about the universe’s current state. Within naturalistic thinking, the demon should be able to perfectly predict what is going to happen next. Of course, such an entity could never exist, because it were actually a computer that matches the universe in size. In addition, there are limits to how precisely we can measure the current state, although physicist and engineers have pushed this very far.</p>
<p>When Violet did her experiment to prove the superiority of design B, the only two things she knew about the state of affairs was that the participant sitting in front of her is member of a very loose group of people called the “typical user” and the design her or she was exposed to. That is painstakingly little to pin down the neural state of affairs. Her lack of knowledge is profound but still not a problem as the research question was gross, too, not asking for more than the difference in <em>average</em> duration. Instead, imagine Violet and a collegue had invented a silly game where they both guess the time-on-task of individual participants. Who comes closest wins. As both players are clever people, they do not just randomly announce numbers, but let themselves guide by data of previous sessions. A very simple but reasonable approach would be to always guess the average ToT in all previous sessions. As gross as this is, it qualifies as a model, more precisely the grand mean model [LM]. The model explains all observations by the population mean.</p>
<p>Of course, Violet would never expect her grand mean model to precisely predict the outcome of a session. Still, imagine a device that has perfect knowledge of the car rental website, the complete current neural state of a the participant and the physical environment both are in. The device would also have a complete and valid psychological theory. With this device, Jane could always make a perfect prediction of the outcome. Unfortunately, real design researchers are far from Laplace demoism. Routinely borrowing instruments from social sciences, precision of measurement is humble and the understanding of neural processes during web navigation is highly incomplete. Participants vary in many complex ways in their neural state and a myriad of <em>small unrelated forces (SMURF)</em> can push or hinder the user towards completion.</p>
<p>Laplace demon has perfect knowledge of all SMURF trajectories and therefore can produce a perfect prediction. Violet is completely ignorant of any SMURFs and her predictions will be off many times. A common way to conceive this situation is that observed values <span class="math inline">\(y_i\)</span> are composed of the <em>expected value</em> under the model <span class="math inline">\(\mu_i\)</span> and a <em>random part</em>, <span class="math inline">\(\epsilon_i\)</span></p>
<p><span class="math display">\[y_i = \mu_i + \epsilon_i\]</span></p>
<p>Generally, statistical models consist of these two parts: the <em>likelihood</em> to describe the association between predictors and expected values and the random part, which describes the overall influence of the unexplained SMURFs.</p>
<div id="predictions-and-likelihood" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Predictions and likelihood</h3>
<p>The likelihood function states the dependency of outcome on the predictor variables. The dependency can be a complex mathematical function of multiple predictors, or as simple as the population average. A common likelihood function is the linear function. For example, in their guessing game, Violet could try to improve her population model, by also taking age of participants into account. Older people tend to be slower. Violet creates a plot from past records. The ellipsoid form of the point cloud indicates that ToT is somehow depending on age. Violet draws a straight line with an upward slope to approximate the relationship. It seems that 30 year old persons have an average ToT of around 90 seconds, which increases to around 120 seconds for 50 year olds. Arithmetically, this is an increase of around 1.5 seconds per year of age.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Sec99<span class="op">$</span>Ver20 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> ToT)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> F)</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-66-1.png" width="66%" /></p>
<p>Violet can use this information to improve her gambling. Instead of stoically calling the population mean, she uses a linear function as predictor: $90 + ( - 30) 1.5 $. In Bayesian statistics, this is called a <em>likelihood function</em> and the general form for a single linear likelihood function is:</p>
<p><span class="math display">\[\mu_i = \beta_0 + \beta_1x_{1i}\\\]</span></p>
<p>Likelihood functions connect the <em>expected value</em> <span class="math inline">\(\mu\)</span> with <em>observed variables</em> <span class="math inline">\(x_{i1}, x_{i2}, ..., x_{ik}\)</span>, and (to be estimated) parameters, e.g. <span class="math inline">\(\beta_0, \beta_1\)</span>. The likelihood function is often called the <em>deterministic part</em> of a model, because its prediction strictly depends on the observed values and the predictors, but nothing else. For example, two persons of age 30 will always be predicted to use up 90 seconds. Apparently, this is not the case for real data.</p>
<p>The linear model is very common in statistical modelling, but likelihoods can basically take all mathematical forms. For example:</p>
<ul>
<li>the grand mean model, Violet used before: <span class="math inline">\(\mu_i = \beta_0\)</span></li>
<li>two predictors with a linear relationship: <span class="math inline">\(\mu_i = \beta_0 + \beta_1x_{1i} + \beta_1x_{2i}\\\)</span></li>
<li>a parabolic relationship: <span class="math inline">\(\mu_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i}^2\)</span></li>
<li>a nonlinear learning curve: <span class="math inline">\(\mu_i = \beta_\textrm{asym} (1 + \exp(-\beta_\textrm{rate}(x_\textrm{training} + \beta_\textrm{pexp})))\)</span></li>
<li>the difference between groups A and B, where <span class="math inline">\(x_1\)</span> is a membership (dummy) variable coded as <span class="math inline">\(A\rightarrow x_1:=0, B\rightarrow x_1:=1\)</span>: <span class="math inline">\(\mu_i = \beta_0 + \beta_1x_{1i}\)</span></li>
</ul>
<p>In the vast majority of cases, the likelihood function is the interesting part of the model, where researchers transform their theoretical considerations or practical questions into a mathematical form. The parameters of the likelihood function are being estimated and answer the urging questions, such as:</p>
<ul>
<li>Is the design efficient enough? (<span class="math inline">\(\beta_0\)</span>)</li>
<li>By how much does performance depend on age? (<span class="math inline">\(\beta_1\)</span>)</li>
<li>Under which level of arousal does performance peak? (determining the staionary point of the parabola)</li>
<li>How fast people learn by training (<span class="math inline">\(\beta_\textrm{rate}\)</span>)</li>
<li>By how much design B is better than A (<span class="math inline">\(\beta_1\)</span>)</li>
</ul>
<p>A subtle, but noteworthy feature of likelihood functions is that <span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(x_i\)</span> have indicators <span class="math inline">\(i\)</span>. Potentially, every observation <span class="math inline">\(i\)</span> has their own realization of predictors and gets a unique expected value, whereas the parameters <span class="math inline">\(\beta_0, \beta_1\)</span> asf. are single values that apply for all observations at once. In fact, we can conceive statistical models as operating on multiple levels, where there is always the two: the observation level and the population level. When introducing multi-level models, we will see how this principle extends to more than these two levels. Another related idea is that parameters summarizes patterns found in data. Any summary implies repetition and that is what the likelihood expresses: the pattern that repeats across observations and is therefore predictable.</p>
</div>
<div id="distributions" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Distributions: patterns of randomness</h3>
<!--
+ spell out R code hidden in figures and formulas to teach Rs distribution commands along the way
+ change trials to attempts for binomial distributions to not confuse with experimental trails
- discrete or continuous
- cumulative probability
- sums to one
- density intervals
- probability and density
+ range of support
+ parameters
- location and dispersion
+ in R
-->
<p>The random part of a statistical model is what changes between observation and is not predictable. When using the grand mean model, the only information we are using is that the person is from the target population. Everything else is left to the unobserved SMURFs and that goes into the random part of the model. Fortunately, SMURFs don’t work completely arbitrary. Frequently, recognizable patterns of ramdomness emerge. These patterns can be formulated mathematically as probability and density distributions. A probability distribution is typically characterized as a probability mass function that assigns <em>probabilities to outcomes</em>, such as:</p>
<ul>
<li>probability of <em>task success</em> is <span class="math inline">\(.81\)</span></li>
<li>probability of <em>99 seconds or better</em> is <span class="math inline">\(.22\)</span></li>
<li>probability of all SMURFs together pushing a persons <em>IQ beyond 115</em> is around <span class="math inline">\(.33\)</span></li>
</ul>
<div id="probability-distributions" class="section level4">
<h4><span class="header-section-number">3.3.2.1</span> Probability distributions</h4>
<p>Probability distributions are mathematical functions that assign probabilities to the outcome of a measured variable <span class="math inline">\(y\)</span>. Consider a participant who is asked to complete three tasks of constant difficulty, such that there is a chance of <span class="math inline">\(30\)</span> percent for each one to be solved. The outcome variable of interest is the number of correct completions (0, 1, 2 or 3). Under idealized consitions, the following random distribution gives the probability of every possible outcome.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_three_tasks &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">y =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">4</span>,
             <span class="dt">outcome =</span> <span class="kw">as.character</span>(y),
             <span class="dt">probability =</span> <span class="kw">dbinom</span>(y, <span class="dt">size =</span> <span class="dv">3</span>, <span class="dt">prob =</span> <span class="fl">0.3</span>),
             <span class="dt">cumul_prob     =</span> <span class="kw">pbinom</span>(y, <span class="dt">size =</span> <span class="dv">3</span>, <span class="dt">prob =</span> <span class="fl">0.3</span>))


D_three_tasks <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> outcome, <span class="dt">y =</span> probability)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">fill =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylim</span>(<span class="dv">0</span>,<span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-67-1.png" width="66%" /></p>
<p>Further, we observe that the most probable outcome is exactly one correct task, which occurs with a probability of <span class="math inline">\(P(y = 1) = 0.441\)</span>. At the same time, there is ample possibility for all failures, <span class="math inline">\(P(y = 0) = 0.343\)</span>. We may also look at <em>combined events</em>, say the probability for less than two correct. That is precisely the sum <span class="math inline">\(P(y \leq 1) = P(y = 0) + P(y = 1) = 0.784\)</span>.</p>
<p>We can bundle basic events by adding up the probabilities. An extreme case of that is the universal event that includes all possible outcomes. You can say with absolute certainty that the outcome is, indeed, between zero and three and certainty means the probability is 1, or: <span class="math inline">\(P(0 \leq y \leq 3) = 1\)</span>. As a matter of fact, all probability (and density) distributions fulfill that property.</p>
<p>More precisely, the area under the PMF must be exactly one and that brings us directly to a second form of characterizing the random distribution: the <em>cumulative distribution distribution (CDF)</em> renders the probability for the outcome to be smaller or equal to <span class="math inline">\(y\)</span>. In the case of discrete outcomes, this is just stacking (or summing) over all outcomes, just as we did for <span class="math inline">\(P(y\leq1)\)</span> above. The CDF of the three-tasks example is shown in the graph below. We recognize the left starting point, which is exactly <span class="math inline">\(P(y = 0)\)</span> and observe large jump to <span class="math inline">\(P(y \leq 1)\)</span>. Finally, at <span class="math inline">\(y \leq 3\)</span> the function reaches the upper limit of 1, which is full certainty.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_three_tasks <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> y, <span class="dt">y =</span> cumul_prob)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">ylim</span>(<span class="dv">0</span>,<span class="dv">1</span>)</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-68-1.png" width="66%" /></p>
</div>
<div id="density-distributions" class="section level4">
<h4><span class="header-section-number">3.3.2.2</span> Density distributions</h4>
<p>In the three-tasks example, reading and recombining probabilities is like counting blocks and stacking them upon each other. This is how most children learn basic arithmetics. when the outcome measure is <em>continuous</em>, rather than discrete, some high school math is required. The most common continuous measure is probably durations. As we will see, durations take quite tricky random patterns, so for the sake of simplicity, consider the distribution of intelligence quotients (IQ). Strictly spoken, the IQ is <em>not</em> continuous, as one usually only measures and reports whole number scores. Still, for instructional purposes, assume that the IQ is given in arbitrary precision, be it <span class="math inline">\(114.9\)</span>, <span class="math inline">\(100.0001\)</span> or <span class="math inline">\(\pi * 20\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_IQ &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">IQ =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">200</span>,
                   <span class="dt">density =</span> <span class="kw">dnorm</span>(IQ, <span class="dv">100</span>, <span class="dv">15</span>),
                   <span class="dt">cdf =</span> <span class="kw">pnorm</span>(IQ, <span class="dv">100</span>, <span class="dv">15</span>),
                   <span class="dt">SE =</span> (IQ <span class="op">&gt;</span><span class="st"> </span><span class="dv">85</span>) <span class="op">*</span><span class="st"> </span>(IQ <span class="op">&lt;</span><span class="st"> </span><span class="dv">115</span>) <span class="op">*</span><span class="st"> </span>density,
                   <span class="dt">PDF_085 =</span> (IQ <span class="op">&lt;</span><span class="st"> </span><span class="dv">85</span>) <span class="op">*</span><span class="st"> </span>density,
                   <span class="dt">PDF_115 =</span> (IQ <span class="op">&lt;</span><span class="st"> </span><span class="dv">115</span>) <span class="op">*</span><span class="st"> </span>density)

D_IQ <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> IQ, <span class="dt">y =</span> density)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_area</span>()</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-69-1.png" width="66%" /></p>
<p>We observe that the most likely IQ is 100 and that almost nobody reaches scores higher than 150 or lower than 50. But, how likely is it to have an IQ of exactly 100? Less than you might think! With continuous measures, we can no longer think in blocks that have a certain area. In fact, the probability of having an IQ of <em>exactly</em> <span class="math inline">\(100.00...0\)</span> is exactly zero. The block of IQ = 100 is infinitely narrow and therefore has an area of zero. Generally, with continuous outcome variables, We can no longer read probabilities directly. Therefore, probability mass distributions don’t apply, but the association between outcome and probability is given by what is called <em>probability density functions</em>. What PDFs share withg PMFs is that the area under the curve is always exactly one. They differ in that PDFs return a density for every possible outcome, which by itself is not as useful as probability, but can be converted into probabilities.</p>
<p>Practically, nobody is really interested in infinite precision. When asking <em>“what is the probability of IQ = 100?”</em>, the answer is <em>“zero”</em>, but what was really meant was: <em>“what is the probability of an IQ in a close interval around 100?”</em>. Once we speak of intervals, we clearly have areas larger than zero. The graph below shows the area in the range of 85 to 115.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_IQ <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> IQ, <span class="dt">y =</span> density)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_area</span>(<span class="kw">aes</span>(<span class="dt">y =</span> SE))</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-70-1.png" width="66%" /></p>
<p>But, how large is this area exactly? As the distribution is curved, we can no longer simply count virtual blocks. Recall that the CDF gives the probability mass, i.e. the area under the curve, for outcomes up to a chosen point. Continuous distributions have CDFs, too, and the the graph below shows the CDF for the IQs. We observe how the curve starts to rise from zero at around 50, has its steepest point at 100, just to slow down and run against 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_IQ <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> IQ, <span class="dt">y =</span> cdf)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-71-1.png" width="66%" /> Take a look at the following graph. It shows the two areas <span class="math inline">\(IQ \leq 85\)</span> and <span class="math inline">\(IQ \leq 115\)</span>. The magic patch in the center is just the desired interval.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">D_IQ <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> IQ, <span class="dt">y =</span> density)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_area</span>(<span class="kw">aes</span>(<span class="dt">y =</span> PDF_<span class="dv">085</span>, <span class="dt">fill =</span> <span class="st">&quot;P(IQ &lt; 85)&quot;</span>), <span class="dt">alpha =</span> .<span class="dv">4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_area</span>(<span class="kw">aes</span>(<span class="dt">y =</span> PDF_<span class="dv">115</span>, <span class="dt">fill =</span> <span class="st">&quot;P(IQ &lt; 115)&quot;</span>), <span class="dt">alpha =</span> .<span class="dv">4</span>)</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-72-1.png" width="66%" /></p>
<p>And here the CDF comes into the play. To any point of the PDF, the CDF yields the area up to this point and we can compute the area of the interval by simple subtraction:</p>
<p><span class="math display">\[
\begin{aligned}
P(IQ \leq 115) &amp;&amp;= 0.159
P(IQ \leq 85) &amp;&amp;= 0.841
P(85 \leq IQ \leq 115) &amp;= P(IQ \leq 115) - P(IQ \leq 85) &amp;= 0.683
\end{aligned}
\]</span></p>
<p>Probability and density distributions usually are expressed as mathematical functions. For example, the function for the case of task completion is the binomial distribution, which gives the probability for <span class="math inline">\(y\)</span> successes in <span class="math inline">\(k\)</span> trials when the success rate is <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
Pr(y|p,k) = {k  \choose y}p^y(1-p)^{k-y}
\]</span></p>
<p>In most cases where the binomial distribution applies, base probability <span class="math inline">\(p\)</span> is the parameter of interest, whereas the number of of trials is known beforehand and therefore does not require estimation. For that reason, the binomial distribution is commonly taken as a one-parameter distribution. When discussing the binomial in more detail, we will learn that <span class="math inline">\(p\)</span> determines the location of the distribution, as well as how widely it is dispersed (preview Figure XY). The distribution that approximated the IQs is called the Gaussian distribution (or Normal). The Gaussian distribution function takes two parameters, <span class="math inline">\(mu\)</span> determines the location of the distribution, say average IQ being 98, 100 or 102 and <span class="math inline">\(sigma\)</span> which gives the dispersion, independently (preview Figure XY).</p>
</div>
<div id="location-and-dispersion" class="section level4">
<h4><span class="header-section-number">3.3.2.3</span> Location and dispersion</h4>
<p><em>Location</em> and <em>dispersion</em> are two immediate properties of plotted distributions that have intuitive interpretations. Location of a distribution usually reflects where the most typical values come to lie (100 in the IQ example). When an experimenter asks for the difference of two designs in ToT, this is purely about location. Dispersion can either represent <em>uncertainty</em> or <em>variation</em> in a population, depending on the research design and statistical model. The most common interpretation is uncertainty. The basic problem with dispersion is that spreading out a distribution influences <em>how typical</em> the most typical values are. The fake IQ data basically is a perfect Gaussian distribution with a mean of 100 and a standard deviation of 15. The density of this distribution at an IQ of 100 is 0.027. If IQs had a standard deviation of 30, the density at 100 would fall to 0.013. If you were in game to guess an unfamiliar persons IQ, in both cases 100 would be the best guess, but you had a considerable higher chance of being right, when dispersion is low.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(<span class="dt">IQ =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">200</span>,
           <span class="dt">density =</span> <span class="kw">dnorm</span>(IQ, <span class="dv">100</span>, <span class="dv">30</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> IQ, <span class="dt">y =</span> density)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_area</span>()</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-73-1.png" width="66%" /></p>
<p>The perspective of uncertainty routinely occurs in the experimental comparison of conditions, e.g. design A compared to design B. What causes experimenters worry is when the <em>residual distributions</em> in their models is widely spread. Roughly speaking, residuals are the variation that is not predicted by the model. The source of this variation is unknown and usually called <em>measurement error</em>. It resides in the realm of the SMURFs. With stronger measurement error dispersion, the two estimated locations get less certainty assigned, which blurs the difference between the two.</p>
<p>The second perspective on dispersion is that it indicates <em>variation by a known source</em>. Frequently, this source is differences between persons. The IQ is an extreme example of this, as these tests are purposefully designed to have the desired distribution. In chapter [LMM] we will encounter several sources of variation, but I am really concerned about human variation, mostly. Commonly, experimental researchers are obsessed by differences in location, which to my mind confuses “the most typical” with “in general”. Only when variation by participants is low, this gradually becomes the same. We will re-encounter this idea when turning to multi-level models.</p>
<p>Most distributions routinely used in statistics have one or two parameters. Generally, if there is one parameter this determines both, location and dispersion, whereas two-parameter distributions can vary location and dispersion independently, to some extent. The Gaussian distribution is a special case as <span class="math inline">\(\mu\)</span> purely does location, whereas <span class="math inline">\(\sigma\)</span> is just dispersion. With common two-parametric distributions, both parameters influence location and dispersion in more or less twisted ways. For example, mean and variance of a two-parametric binomial distributions both depend on chance of success <span class="math inline">\(p\)</span> and number of trials <span class="math inline">\(k\)</span>, as <span class="math inline">\(\textrm{M} = pk\)</span> and <span class="math inline">\(\textrm{Var} = kp(1-p)\)</span>.</p>
</div>
<div id="range-of-support-and-skewness" class="section level4">
<h4><span class="header-section-number">3.3.2.4</span> Range of support and skewness [### ]</h4>
<p>In this book I advocate the thoughtful choice of distributions rather than doing batteries of goodness-of-fit to confirm that one of them, the Gaussian, is an adequate approximation. It usual is trivial to determine whether a measure is discrete (like everything that is counted) or (quasi)continuous and that is the most salient feature of distributions. A second, nearly as obvious, feature of any measure is its range. Practically all physical measures, such as duration, size or temperature have natural lower bounds, which typically results in scales of measurement which are non-negative. Counts have a lower boundary, too (zero), but there can be a known upper bound, such as the number of trials. Statistical distributions can be classified the same way: having no bounds (Gaussian, t), one bound (usually the lower, Poisson, exponential) or two bounds (binomial, beta).</p>
</div>
<div id="data-generating-process" class="section level4">
<h4><span class="header-section-number">3.3.2.5</span> Data generating process</h4>
<p>Many dozens of PMFs and PDFs are known in statistical science and are candidates to choose from. First orientation grounds on superficial characteristics of measures, such as discrete/continuous or range, but that is sometimes not sufficient. For example, the pattern of randomness in three-tasks falls into a binomial distribution only, when all trials have the same chance of success. If the tasks are very similar in content and structure, learning is likely to happen and the chance of success differs between trials. Using the binomial distribution when chances are not constant leads to severely mistaken statistical models.</p>
<p>For most distributions, strict mathematical definitions exist for under which circumstances randomness takes this particular pattern. Frequently, there is one or more natural phenomena that accurately fall into this pattern, such as the number of radioactive isotope cores decaying in a certain interval (Poisson distributed) or … . This is particularly the case for the canonical four random distributions that follow. Why are these canonical? The pragmatic answer is: they cover the basic types of measures: chance of success in a number of trials (binomial), counting (Poisson) and continuous measures (exponential, Gaussian).</p>
<!--
However, 


Imagine, standing slightly elevated in front of a moving crowd and choose one pedestrian to follow with your eyes. The task is easier when the target person is tall, moves straight on while the rest of the crowd is moving in random curly patterns. This method also works to identify the waiter in a crowded bar. 


The theorists answer is twofold: first, they are exponential .... second, they are lowest entropy.




Random distributions: 

+ discrete or continuous
+ probability and density
+ range of support
+ parameters
+ location and dispersion
+ data generating process
-->
</div>
<div id="binomial_dist" class="section level4">
<h4><span class="header-section-number">3.3.2.6</span> Binomial distributions</h4>
<p>A very basic performance variable in design research is task success. Think of devices in high risk sitations such as medical infusion pumps in surgery. These devices are remarkably simple, giving a medication at a certain rate into the bloodstream for a given time. Yet, they are operated by humans under high pressure and must therefore be extremely error proof in handling. Imagine, the European government would set up a law that manufacturers of medical infusion pump must prove a 90% error-free operation in routine tasks. A possible validation study could be as follows: a sample of <span class="math inline">\(N = 30\)</span> experienced nurses are invited to a testing lab and asked to complete ten standard tasks with the device. The number of error-free task completions per nurse is the recorded performance variable tpo validate the 90% claim. Under somewhat idealized conditions, namely that all nurses have the same proficiency with the device and all tasks have the success chance of 90%, the outcome follows a <em>Binomial distribution</em> and the results could look like the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="kw">tibble</span>(<span class="dt">succs =</span> <span class="kw">rbinom</span>(<span class="dv">30</span>, <span class="dv">10</span>, .<span class="dv">9</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> succs)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">11</span>))</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-74-1.png" width="66%" /></p>
<p>Speaking about the Binomial distribution in terms of <em>successes in a number of attempts</em> is common. As a matter of fact, <em>any</em> binary classification of outcomes is amenable for Binomial modelling, like on/off, red/blue, male/female. Imagine, Jane’s big boss needs a catchy phrase for an investor meeting. Together they decide that the return rate of customers could be a good measure, translating into a statement such as <em>eighty percent of customers come back</em>. To prove (or disprove) the claim, Jane uses the customer data base and divides all individuals into two groups: those who have precisely one record and those who returned (no matter how many times). This process results in a distribution, that has two possible outcomes: : <span class="math inline">\(0\)</span> for one-timers and <span class="math inline">\(1\)</span> for returners. This is in fact, a special case of the Binomial distribution with <span class="math inline">\(k = 1\)</span> attempts. Examples are given in the first row of the figure.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mascutils<span class="op">::</span><span class="kw">expand_grid</span>(<span class="dt">k =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">20</span>), 
                       <span class="dt">p =</span> <span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>), 
                       <span class="dt">succs =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">20</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">probability =</span> <span class="kw">dbinom</span>(succs, k, p)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> succs, <span class="dt">y =</span> probability)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(k <span class="op">~</span><span class="st"> </span>p)</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-75-1.png" width="66%" /></p>
<p>A Binomial distributions has two parameters: <span class="math inline">\(p\)</span> is the chance of success and <span class="math inline">\(k\)</span> is the number of attempts. <span class="math inline">\(p\)</span> is a probability and therefore can take values in the range from zero to one. With larger <span class="math inline">\(p\)</span> the distribution moves to the right. The mean of Binomial distributions is the probability scaled by number of attempts, <span class="math inline">\(M = kp\)</span>. Logically, there cannot be more successes then <span class="math inline">\(k\)</span>, but with larger <span class="math inline">\(k\)</span> the distribution gets wider. The variance is the odds scaled by number of attempts, <span class="math inline">\(\textrm{Var} = kp(1-p)\)</span>. As mean and variance depend on the exact same parameters, they cannot be set independently. In fact, the relation <span class="math inline">\(Var = M(1-p)\)</span> is parabolic, so that variance is largest at <span class="math inline">\(p = .5\)</span>, but decreases towards both boundaries. A Binomial distribution with, say <span class="math inline">\(k=10\)</span> and <span class="math inline">\(p = .4\)</span> always has mean <span class="math inline">\(4\)</span> and variance <span class="math inline">\(2.4\)</span>. This means, in turn, that an outcome with a mean of <span class="math inline">\(4\)</span> and a variance of <span class="math inline">\(3\)</span> is not Binomially distributed. This occurs frequently, when the success rate is not identical across trials. A common solution is to use hierarchical distributions, where the parameter <span class="math inline">\(p\)</span> itself is distributed, rather than fixed. A common distribution for <span class="math inline">\(p\)</span> is the <em>beta</em> distribution and the <em>logitnormal</em> distribution is an alternative.</p>
<p>The Binomial distribution has two boundaries, zero below and number of attempts <span class="math inline">\(k\)</span> above. While a lower boundary of zero is often natural, one cannot always speak of a number of attempts. For example, the number of times a customer returns to the car rental website does not yield a natural interpretation of number of attempts. Rather, one could imagine the situation as that any moment is an opportunity to hire a car. At the same time, every single moment has a very, very small chance that a car is hired, indeed. Under these conditions, an infinite (or painstakingly large) number of opportunities and a very low rate, the random pattern is neatly summarized by <em>Poisson distributions</em>.</p>
</div>
<div id="poisson_dist" class="section level4">
<h4><span class="header-section-number">3.3.2.7</span> Poisson distributions</h4>
<p>Some counting processes have no natural upper limit like the number of trials in a test. In design research, a number of measures are such <em>unbounds counts</em>:</p>
<ul>
<li>number of erronous actions</li>
<li>frequency of returns</li>
<li>behavioural events, e.g. showing explorative behaviour</li>
<li>physiological events, such as number of peaks in galvanic skin response</li>
</ul>
<p>These measures can often be modelled as <em>Poisson distributed</em>. A useful way to think of unbound counts, is that they can happen at every moment, but with a very small chance. Think of a longer interaction sequence of a user with a system, where errors are recorded. It can be conceived as an almost infinite number of opportunities to err, with a very small chance of something to happen. The Poisson distribution is a so called limiting case of the binomial distributions, with infinite <span class="math inline">\(k\)</span> and infinitely small <span class="math inline">\(p\)</span>. Of course, such a situation is completely ideal. Yet, Poisson distributions fit such situations well enough.</p>
<p>Poisson distributions possess only one parameter <span class="math inline">\(\lambda\)</span> (lambda), that is strictly positive and determines mean and variance of the distribution alike: <span class="math inline">\(\lambda = M = \textrm{Var}\)</span>. As a matter of fact, there cannot be massively dispersed distributions close to zero, nor narrow ones in the far. Owing to the lower boundary, Poisson distributions are <em>asymmetric</em>, with the left tail always being steeper. Higher <span class="math inline">\(\lambda\)</span>s push the distribution away from the boundary and the skew diminishes. It is commonly practiced to approximate counts in the high numbers by <em>normal distributions</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mascutils<span class="op">::</span><span class="kw">expand_grid</span>(<span class="dt">lambda =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>),
                       <span class="dt">count =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">20</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">probability =</span> <span class="kw">dpois</span>(count, lambda)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> count, <span class="dt">y =</span> probability)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(lambda<span class="op">~</span>.)</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-76-1.png" width="66%" /></p>
<p>The linkage between mean and variance is very strict. Only a certain amount of randomness can be contained. If there is more randomness, and that is almost certainly so, Poisson distributions are not appropriate. One speaks of <em>overdispersion</em> in such a case.</p>
<p>Consider a very simple video game, <em>subway smurfer</em>, where the player jumps and runs a little blue avatar on the roof of a train and catches items passing by. Many items have been placed into the game, but catching a single one is very difficult. The developers are aware that a too low success rate would demotivate players as much as when the gane is made to easy. In this experiment, only one player is recorded, and in wonderful ways this player never suffers from fatigue, nor does he get better with training. The player plays a 100 times and records the catches after every run. In this idealized situation, the distribution of catches would, indeed, follow a Poisson distribution, as in the figure below.</p>
<p>Consider a variation of the experiment with 100 players doing one game and less restrictive rules. Players come differently equipped to perform visual search tasks and coordinate actions at high speeds. They are tested at different times of the day and by chance feel a bit groggy or energized. The chance of catching varies between players, which violates the assumption that was borrowed from the Binomial, a constant chance <span class="math inline">\(p\)</span>. The extra variation is seen in the wider of the two distributions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## <span class="al">FIXME</span>




<span class="kw">tibble</span>(<span class="dt">lambda =</span> <span class="dv">20</span>,
           <span class="dt">ci =</span> <span class="kw">rpois</span>(<span class="dv">100</span>, <span class="kw">log</span>(lambda)),
           <span class="dt">ci_ovdsp =</span> <span class="kw">rpois</span>(<span class="dv">100</span>, <span class="kw">log</span>(lambda <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">100</span>)))
           ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> ci)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">fill =</span> <span class="st">&quot;single participant&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">x =</span> ci_ovdsp, <span class="dt">fill =</span> <span class="st">&quot;multi participants&quot;</span>), <span class="dt">alpha =</span> .<span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">fill=</span><span class="st">&quot;Count distributions&quot;</span>, <span class="dt">x =</span> <span class="st">&quot;outcome&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;density&quot;</span>)</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-77-1.png" width="66%" /></p>
<p>Poisson distributions’ lower boundary can cause trouble: the measure at hand is truly required to include the lower bound. A person can perform a sequence with no errors, catch zero items or have no friends on facebook. But, you cannot complete an interaction sequence in zero steps or have a conversation with less than two statements. Fortunately, once a count measure has a lower boundary right of zero, the offset is often available, such as the minimum necessary steps to complete a task. In such a case, the number of errornous steps can be derived and used as a measure, instead:</p>
<p><span class="math display">\[
\textrm{\#errors} = \textrm{\#steps} - \textrm{\#neccessary steps}
\]</span></p>
<p>Another lower bound problem arises, when there are hurdles. In traffic research, the frequency of use public transport certainly is an interesting variable. A straight-forward assessment would be to ask bus passengers “How many times have you taken the bus the last five days?”. This clearly is a count measure, but it cannot be zero, because the person is sitting in the bus right now. This could be solved by a more inclusive form of inquiry, such as approaching random households. But, the problem is deeper: actually, the whole population is of two classes, those who use public transport and those who don’t.</p>
</div>
<div id="exponential-distribution-tbc" class="section level4">
<h4><span class="header-section-number">3.3.2.8</span> Exponential distribution [TBC]</h4>
<p>Exponential distributions apply for measures of duration. Exponential distributions have the same generating process as Poisson distributions, except, that the <em>duration for an event to happen</em> is the variable of interest, rather than events in a given time. The same idealized conditions of a completely unaffected subway smurfer player and constant catchability of items, the duration between any two catches is exponentially distributed. In more general, the chance for an event to happen is the same at any moment, completely independent of how long one has been waiting for it. For this property, the exponential distribution is called <em>memoryless</em>.</p>
<p>Durations are common measures in design research, most importantly, time-on-task and reaction time. Unfortunately, the exponential distribution is a poor approximation of the random pattern found in duration measures. That is for two reasons: first, the exponential distribution shares with Poisson, that it does not allow variance between participants. Second, the distribution always starts at zero, whereas human reactions always include some basic processing, and be this just the velocity of signals passing nerve cells, which is far below speed of sound (in air).</p>
</div>
<div id="gamma-distribution-tbd" class="section level4">
<h4><span class="header-section-number">3.3.2.9</span> Gamma distribution [TBD]</h4>
</div>
<div id="normal-distributions" class="section level4">
<h4><span class="header-section-number">3.3.2.10</span> Normal distributions</h4>
<p>The best known distributions are <em>normal distributions</em> or <em>Gaussian distributions</em>. These distributions arise mathematically under the assumption of a myriad of small unrelated forces (SMURF) pushing performance (or any other outcome) up or down. As SMURFs work in all directions independently, their effects often average out and the majority of observations stays clumped together in the center, more or less.</p>
<p>Normal distributions have two parameters: <span class="math inline">\(\mu\)</span> marks the center and mean of the distribution. The linear models introduced later are aiming at predicting <span class="math inline">\(\mu\)</span>. The second parameter <span class="math inline">\(\sigma\)</span> represents the dispersion of the random pattern. When randomness is pronounced, the center of the distribution gets less mass assigned, as the tails get wider.</p>
<p>Different to Poisson and Binomial distributions, mean and variance of the distribution can be set independently and overdispersion is never an issue.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>)), <span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>), <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">colour =</span> <span class="st">&quot;Normal( 0, 1)&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="op">-</span><span class="fl">1.5</span>, <span class="dt">sd =</span> <span class="fl">0.5</span>), <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">colour =</span> <span class="st">&quot;Normal(-1.5, 0.5)&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="fl">0.5</span>, <span class="dt">sd =</span> <span class="dv">2</span>), <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">colour =</span> <span class="st">&quot;Normal(0.5,1.5)&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">colour=</span><span class="st">&quot;Normal distributions&quot;</span>, <span class="dt">x =</span> <span class="st">&quot;outcome&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;density&quot;</span>)</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-80-1.png" width="66%" /></p>
<p>Normal distributions have the compelling interpretation of summarizing the effect of SMURFs. They serve to capture randomness in a broad class of regression models and other statistical approaches. The problem with normal distributions is that they only capture the pattern of randomness under two assumption. The first assumption is that the outcome is continuous. While that holds for duration as a measure of performance, it would not hold for counting the errors a user makes. The second assumption is that the SMURFs are truly additive, like the forces add up, when two pool balls collide. This appears subtle at first, but it has the far reaching consequence that the outcome variable must have an infinite range in both directions, which is impossible.</p>
<p>The normal distribution is called “normal”, because people normally use it. Of course not. It gots its name for a deeper reason, commonly known (and held with awe) as the <em>central limit theorem</em>. Basically, this theorem proves what we have passingly observed at binomial and Poisson distributions: the more they move to the right, the more symmetric they get. The central limit theorem proves that, in the long run, a wide range of distributions are indistinguishable from the normal distribution. In practice, infinity is relative. In some cases, it is reasonable to trade in some fidelity for convenience and good approximations make effective statisticians. As a general rule, the normal distribution approximates other distributions well, when the majority of measures stay far from the natural boundaries. That is the case in experiments with very many attempts and moderate chances(e.g. signal detection experiments), when counts are in the high numbers (number of clicks in a complex task) or with long durations. However, these rules are no guarantee and careful model criticism is essential.</p>
<p>Measurement is prime and specialized (non-central-limit) distributions remain the first recommendation for capturing measurement errors. The true salvation of normal distributions is their application in <em>multi-level models</em>. While last century statistics was reigned by questions of location, and variance considered nuisance, new statistics care for variation. Most notably, amount of variation in a population is added as a central idea in multi-level modelling, which is commonly referred to as <em>random effects</em>. These models can become highly complex and convenience is needed more than ever. Normal distributions tie things together in multi-level models, as they keep location and dispersion apart, tidy.</p>
<p>The dilemma is then solved with the introduction of <em>generalized linear models</em>, which is a framework for using linear models with appropriate error distributions. Fortunately, MLM and GLM work seemlessly together. With MLM we can conveniently build graceful likelihood models, using normal distributions for populations. The GLM part is a thin layer to get the measurement scale right and choose the right error distribution, just like a looking glass.</p>
</div>
<div id="t-distribution-tbd" class="section level4">
<h4><span class="header-section-number">3.3.2.11</span> t distribution [TBD]</h4>
<!--
#### Plug-in distributions [TBD]

The five canonical random distributions match the basic type of measurements, they make strict assumptions on the data generating process  A majority of data does not meet these conditions. A routine problem is that binomial and Poisson assume that the chance for an event to occur is strictly constant. Take the number of successful tasks in . A Poisson distribution with $\lambda = M = \textrm{Var}$ emerges only, if all users in the study have the same chance of errors. Could that be the case?

In GLMM is can happen that a normal distribution sits underneath a Poisson distribution. That is not a plugin distribution, strictly, but a related concept.



-->
</div>
<div id="exercises" class="section level4">
<h4><span class="header-section-number">3.3.2.12</span> Exercises:</h4>
<ol style="list-style-type: decimal">
<li>Google it: speed of sound and signal velocity in nerve cells.</li>
</ol>
</div>
</div>
</div>
<div id="bayesian-estimation" class="section level2">
<h2><span class="header-section-number">3.4</span> Bayesian estimation</h2>
<p>Frequentist statistics falls short on recognizing that research is incremental. Bayesian statistics embraces the idea of gradual increase in certainty when (new) data arrives. Why has it not been adopted earlier? The reason is that Bayesian estimation was computationally unfeasible. The seemingly innocent multiplication of prior and likelihood results in a complex integral, which in most cases has no analytic solution. If you have enjoyed a classic statistics education, you may remember how the computation of sum of squares (explained and residual) can be done by paper and pencil in reasonable time. And that is precisely how statistical computations has been performed before the advent of electronic computing machinery. In the frequentist statistical framework (some call it a zoo), ingenious mathematicians have developed procedures that were rather efficient to compute. That made statistical data analysis possible in those times. It came at costs, though:</p>
<ol style="list-style-type: decimal">
<li>procedures make more or less strong assumptions, limiting their applicability.</li>
<li>procedures are asymptotically accurate with inference being accurate at large sample sizes only</li>
<li>common researchers do not understand crucial elements, for example how the F distribution is derived</li>
</ol>
<p>Expensive computation is in the past. Modern computers can simulate realistic worlds in real time and the complex integrals in Bayesian statistics they solve hands down. When analytical solutions do not exist, the integrals can still be solved using numerical procedures. Numerical procedures have been used in frequentist statistics, too, for example the iterative least squares algorithm applies for Generalized Linear Models, Newton-Rapson optimizer can be used to find the maximum likelihood estimate and boot-strapping produces accurate confidence limits. However, these procedures are too limited as they fail for highly multidimensional problems as they are common in advanced regression models.</p>
<p>Most Bayesian estimation engines these days ground on a numerical procedure called <em>Markov-Chain Monte-Carlo (MCMC)</em> sampling. This method differs from the earlier mentioned in that it basically is a random walk. The closest frequentist counterpart to MCMC is the boot strapping algorithm, which draws many samples from data and computes the estimates many times. In some way, MCMC turns this upside down, by randomly drawing possible parameter values and computing the posterior probability many times. Similar to boot strapping, the basic MCMC algorithm is so simple, it can be explained on half a page and implemented with 25 lines of code. Despite its simplicity, the MCMC algorithm is applicable to practically all statistical problems one can imagine. Being so simple and generic at the same time must come at some costs. The downside of MCMC sampling still is computing time. Models with little data and few variables, like the Rainfall case above, are estimated within a few minutes. Linear-mixed effects models, which we will encounter later in this book, can take hours and large psychometric models, can take up to a few days of processor time.</p>
<p>The particular merit of the MCMC algorithm is that it not only delivers accurate point estimates in almost any situation, it produces the full <em>posterior probability distribution</em>. This lets us characterize a parameters magnitude and degree of (un)certainty. Let’s run an analysis on the 20 rainfall observations to see how this happens.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">attach</span>(Rainfall)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M_<span class="dv">1</span> &lt;-<span class="st"> </span>
<span class="st">  </span>Rain <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">stan_glm</span>(rain <span class="op">~</span><span class="st"> </span>cloudy <span class="op">-</span><span class="dv">1</span>, 
           <span class="dt">family =</span> binomial,
           <span class="dt">data =</span> .)</code></pre></div>
<p>What the estimation does, is to calculate the <em>posterior distribution</em> from the observations. The <em>posterior distribution</em> contains the probability (more precisely: the <em>density</em>) for all possible values of the parameter in question. The following density plot represents our belief about the parameter <span class="math inline">\(P(rain|cloudy)\)</span> after we have observed twenty days:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">posterior</span>(M_<span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(type <span class="op">==</span><span class="st"> &quot;fixef&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">chance_of_rain =</span> <span class="kw">plogis</span>(value)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> chance_of_rain, <span class="dt">fill =</span> parameter, <span class="dt">col =</span> parameter)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> .<span class="dv">5</span>)</code></pre></div>
<p><img src="Bayesian_statistics_files/figure-html/umbrella_post-1.png" width="66%" /></p>
<p>From the posterior distribution, we can deduct all kinds of summary statistics, such as:</p>
<ol style="list-style-type: decimal">
<li>the most likely value for a parameter in question is called the <em>posterior mode</em> and is the same as the <em>maximum likelihood estimate</em> when prior knowledge is absent.</li>
<li>the average of parameter values, weighted by their probability is called the <em>posterior mean</em></li>
<li>a defined range to express 95% (or any other level of) certainty is the <em>95% credibility interval</em></li>
</ol>
<p>We can also make non-standard evaluations on the posterior distribution, for example: How certain is it that <span class="math inline">\(P(rain|cloudy) &lt; 0.7\)</span>? We’ll demonstrate the use of this in the next section.</p>
<p>Coming back to MCMC: how is this distribution actually produced. In plain words, MCMC makes a random walk through parameter space. Regions where the true value is more likely to be are just visited more often. The posterior distribution plots above are really just marginal frequencies. Note how the gray connecting lines show the jumps in the MCMC random walk.</p>
<p><img src="Bayesian_statistics_files/figure-html/unnamed-chunk-84-1.png" width="66%" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">detach</span>(Rainfall)</code></pre></div>
</div>
<div id="what-is-wrong-with-classic-statistics-tbc" class="section level2">
<h2><span class="header-section-number">3.5</span> What is wrong with classic statistics? [TBC]</h2>
<p>The short version of the story is this: in face of typically noisy data, there is always the risk that experimental data supports a certain hypothesis, seemingly. If the researcher calls the results supportive, although they were just due to randomness, this is called a <em>false alarm</em>. Many scientific disciplines have therefore commited to keep the overall number of false alarms under a threshold, for example 5%.</p>
<p>Thze even shorter version is that voluntary commitment failed. The true rate of false alarms is about a magnitude higher.</p>
<p>The p-value […] Imagine giant replication study that showed that 35% of published psychology studies are not replicable. In fact, such a study exists and it has far-reaching consequences. […] [REF] shows that rejection bias is one reason: studies that have not reached the <em>magic .05 level</em> of significance, have a lower chance of publication. [REF] sees as a reason that author’s implicitly trick the system by <em>the garden of forking paths</em> strategy. The research project collects an array of variables, followed by a fishing expediture. Theory is conventiently considered last, but written up in a pseudo-a prior way.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="design-research.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="getting-started-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["New_Stats.pdf", "New_Stats.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
