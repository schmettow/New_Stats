<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Generalized Linear Models | New statistics for design researchers</title>
  <meta name="description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works to make this world an easier place." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Generalized Linear Models | New statistics for design researchers" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works to make this world an easier place." />
  <meta name="github-repo" content="schmettow/New_Stats" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Generalized Linear Models | New statistics for design researchers" />
  
  <meta name="twitter:description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works to make this world an easier place." />
  

<meta name="author" content="Martin Schmettow" />


<meta name="date" content="2021-03-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mlm.html"/>
<link rel="next" href="wwm.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="NewStats.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="part"><span><b>I Preparations</b></span></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#whom-for"><i class="fa fa-check"></i><b>1.1</b> Whom this book is for</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#quant-design-research"><i class="fa fa-check"></i><b>1.2</b> Quantitative design research</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#what-new-stats"><i class="fa fa-check"></i><b>1.3</b> What is New Statistics?</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#how-to-use"><i class="fa fa-check"></i><b>1.4</b> How to use this book</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#routes"><i class="fa fa-check"></i><b>1.4.1</b> Routes</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#classroom"><i class="fa fa-check"></i><b>1.4.2</b> In the classroom</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#rosetta"><i class="fa fa-check"></i><b>1.4.3</b> The stone of Rosetta</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#thank-you-and-supplementary-readings"><i class="fa fa-check"></i><b>1.5</b> Thank you and supplementary readings</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="gsr.html"><a href="gsr.html"><i class="fa fa-check"></i><b>2</b> Getting started with R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="gsr.html"><a href="gsr.html#setting-up-r"><i class="fa fa-check"></i><b>2.1</b> Setting up the R environment</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="gsr.html"><a href="gsr.html#install-cran"><i class="fa fa-check"></i><b>2.1.1</b> Installing CRAN packages</a></li>
<li class="chapter" data-level="2.1.2" data-path="gsr.html"><a href="gsr.html#install-github"><i class="fa fa-check"></i><b>2.1.2</b> Installing packages from Github</a></li>
<li class="chapter" data-level="2.1.3" data-path="gsr.html"><a href="gsr.html#first-program"><i class="fa fa-check"></i><b>2.1.3</b> A first statistical program</a></li>
<li class="chapter" data-level="2.1.4" data-path="gsr.html"><a href="gsr.html#bibliographic-notes"><i class="fa fa-check"></i><b>2.1.4</b> Bibliographic notes</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="gsr.html"><a href="gsr.html#r-primer"><i class="fa fa-check"></i><b>2.2</b> Learning R: a primer</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="gsr.html"><a href="gsr.html#objects"><i class="fa fa-check"></i><b>2.2.1</b> Assigning and calling Objects</a></li>
<li class="chapter" data-level="2.2.2" data-path="gsr.html"><a href="gsr.html#vectors"><i class="fa fa-check"></i><b>2.2.2</b> Vectors</a></li>
<li class="chapter" data-level="2.2.3" data-path="gsr.html"><a href="gsr.html#object-types"><i class="fa fa-check"></i><b>2.2.3</b> Basic object types</a></li>
<li class="chapter" data-level="2.2.4" data-path="gsr.html"><a href="gsr.html#operators-functions"><i class="fa fa-check"></i><b>2.2.4</b> Operators and functions</a></li>
<li class="chapter" data-level="2.2.5" data-path="gsr.html"><a href="gsr.html#data-frames"><i class="fa fa-check"></i><b>2.2.5</b> Storing data in data frames</a></li>
<li class="chapter" data-level="2.2.6" data-path="gsr.html"><a href="gsr.html#import-export"><i class="fa fa-check"></i><b>2.2.6</b> Import, export and archiving</a></li>
<li class="chapter" data-level="2.2.7" data-path="gsr.html"><a href="gsr.html#case-env"><i class="fa fa-check"></i><b>2.2.7</b> Case environments</a></li>
<li class="chapter" data-level="2.2.8" data-path="gsr.html"><a href="gsr.html#structuring-data"><i class="fa fa-check"></i><b>2.2.8</b> Structuring data</a></li>
<li class="chapter" data-level="2.2.9" data-path="gsr.html"><a href="gsr.html#data-transformation"><i class="fa fa-check"></i><b>2.2.9</b> Data transformation</a></li>
<li class="chapter" data-level="2.2.10" data-path="gsr.html"><a href="gsr.html#plotting"><i class="fa fa-check"></i><b>2.2.10</b> Plotting data</a></li>
<li class="chapter" data-level="2.2.11" data-path="gsr.html"><a href="gsr.html#fitting"><i class="fa fa-check"></i><b>2.2.11</b> Fitting regression models</a></li>
<li class="chapter" data-level="2.2.12" data-path="gsr.html"><a href="gsr.html#knitting"><i class="fa fa-check"></i><b>2.2.12</b> Knitting statistical reports</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="gsr.html"><a href="gsr.html#lib_gsr"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ebs.html"><a href="ebs.html"><i class="fa fa-check"></i><b>3</b> Elements of Bayesian statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ebs.html"><a href="ebs.html#decision-making"><i class="fa fa-check"></i><b>3.1</b> Rational decision making in design research</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ebs.html"><a href="ebs.html#measuring-uncertainty"><i class="fa fa-check"></i><b>3.1.1</b> Measuring uncertainty</a></li>
<li class="chapter" data-level="3.1.2" data-path="ebs.html"><a href="ebs.html#benchmarking-designs"><i class="fa fa-check"></i><b>3.1.2</b> Benchmarking designs</a></li>
<li class="chapter" data-level="3.1.3" data-path="ebs.html"><a href="ebs.html#comparing-designs"><i class="fa fa-check"></i><b>3.1.3</b> Comparison of designs</a></li>
<li class="chapter" data-level="3.1.4" data-path="ebs.html"><a href="ebs.html#prior-knowledge"><i class="fa fa-check"></i><b>3.1.4</b> Prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ebs.html"><a href="ebs.html#observations-measures"><i class="fa fa-check"></i><b>3.2</b> Observations and measures</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ebs.html"><a href="ebs.html#interaction-seq"><i class="fa fa-check"></i><b>3.2.1</b> Interaction sequences</a></li>
<li class="chapter" data-level="3.2.2" data-path="ebs.html"><a href="ebs.html#perf-measures"><i class="fa fa-check"></i><b>3.2.2</b> Performance measures</a></li>
<li class="chapter" data-level="3.2.3" data-path="ebs.html"><a href="ebs.html#satisfaction-and-other-feelings"><i class="fa fa-check"></i><b>3.2.3</b> Satisfaction and other feelings</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ebs.html"><a href="ebs.html#descriptive-stats"><i class="fa fa-check"></i><b>3.3</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ebs.html"><a href="ebs.html#frequencies"><i class="fa fa-check"></i><b>3.3.1</b> Frequencies</a></li>
<li class="chapter" data-level="3.3.2" data-path="ebs.html"><a href="ebs.html#central-tendency"><i class="fa fa-check"></i><b>3.3.2</b> Central tendency</a></li>
<li class="chapter" data-level="3.3.3" data-path="ebs.html"><a href="ebs.html#dispersion"><i class="fa fa-check"></i><b>3.3.3</b> Dispersion</a></li>
<li class="chapter" data-level="3.3.4" data-path="ebs.html"><a href="ebs.html#associations"><i class="fa fa-check"></i><b>3.3.4</b> Associations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ebs.html"><a href="ebs.html#bayes-prob-theory"><i class="fa fa-check"></i><b>3.4</b> Bayesian probability theory</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ebs.html"><a href="ebs.html#set-theory"><i class="fa fa-check"></i><b>3.4.1</b> Some set theory</a></li>
<li class="chapter" data-level="3.4.2" data-path="ebs.html"><a href="ebs.html#probability"><i class="fa fa-check"></i><b>3.4.2</b> Probability</a></li>
<li class="chapter" data-level="3.4.3" data-path="ebs.html"><a href="ebs.html#likelihood"><i class="fa fa-check"></i><b>3.4.3</b> Likelihood</a></li>
<li class="chapter" data-level="3.4.4" data-path="ebs.html"><a href="ebs.html#bayes-freq-prob"><i class="fa fa-check"></i><b>3.4.4</b> Bayesian and frequentist probability</a></li>
<li class="chapter" data-level="3.4.5" data-path="ebs.html"><a href="ebs.html#bayes-theorem"><i class="fa fa-check"></i><b>3.4.5</b> Bayes theorem</a></li>
<li class="chapter" data-level="3.4.6" data-path="ebs.html"><a href="ebs.html#dynamics-belief"><i class="fa fa-check"></i><b>3.4.6</b> Bayesian dynamics of belief</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ebs.html"><a href="ebs.html#statmod"><i class="fa fa-check"></i><b>3.5</b> Statistical models</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="ebs.html"><a href="ebs.html#structural-part"><i class="fa fa-check"></i><b>3.5.1</b> The structural part</a></li>
<li class="chapter" data-level="3.5.2" data-path="ebs.html"><a href="ebs.html#distributions"><i class="fa fa-check"></i><b>3.5.2</b> Distributions: shapes of randomness</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="ebs.html"><a href="ebs.html#bayes-estimation"><i class="fa fa-check"></i><b>3.6</b> Towards Bayesian estimation</a></li>
<li class="chapter" data-level="3.7" data-path="ebs.html"><a href="ebs.html#priors-defaults"><i class="fa fa-check"></i><b>3.7</b> On priors and defaults</a></li>
<li class="chapter" data-level="3.8" data-path="ebs.html"><a href="ebs.html#further-readings"><i class="fa fa-check"></i><b>3.8</b> Further readings</a></li>
</ul></li>
<li class="part"><span><b>II Models</b></span></li>
<li class="chapter" data-level="4" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>4</b> Basic Linear models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lm.html"><a href="lm.html#gmm"><i class="fa fa-check"></i><b>4.1</b> Quantification at work: grand mean models</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="lm.html"><a href="lm.html#random-walk"><i class="fa fa-check"></i><b>4.1.1</b> Do the random walk: Markov Chain Monte Carlo sampling</a></li>
<li class="chapter" data-level="4.1.2" data-path="lm.html"><a href="lm.html#likelihood-random-term"><i class="fa fa-check"></i><b>4.1.2</b> Likelihood and random term</a></li>
<li class="chapter" data-level="4.1.3" data-path="lm.html"><a href="lm.html#posterior-dist"><i class="fa fa-check"></i><b>4.1.3</b> Working with the posterior distribution</a></li>
<li class="chapter" data-level="4.1.4" data-path="lm.html"><a href="lm.html#clu"><i class="fa fa-check"></i><b>4.1.4</b> Center and interval estimates</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lm.html"><a href="lm.html#lrm"><i class="fa fa-check"></i><b>4.2</b> Walk the line: linear regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="lm.html"><a href="lm.html#transform-measures"><i class="fa fa-check"></i><b>4.2.1</b> Transforming measures</a></li>
<li class="chapter" data-level="4.2.2" data-path="lm.html"><a href="lm.html#correlations"><i class="fa fa-check"></i><b>4.2.2</b> Correlations</a></li>
<li class="chapter" data-level="4.2.3" data-path="lm.html"><a href="lm.html#endless-linear"><i class="fa fa-check"></i><b>4.2.3</b> Endlessly linear</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lm.html"><a href="lm.html#factorial-models"><i class="fa fa-check"></i><b>4.3</b> Factorial Models</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="lm.html"><a href="lm.html#cgm"><i class="fa fa-check"></i><b>4.3.1</b> A versus B: Comparison of groups</a></li>
<li class="chapter" data-level="4.3.2" data-path="lm.html"><a href="lm.html#dummy"><i class="fa fa-check"></i><b>4.3.2</b> Not stupid: dummy variables</a></li>
<li class="chapter" data-level="4.3.3" data-path="lm.html"><a href="lm.html#treatment-contrasts"><i class="fa fa-check"></i><b>4.3.3</b> Treatment contrast coding</a></li>
<li class="chapter" data-level="4.3.4" data-path="lm.html"><a href="lm.html#amm"><i class="fa fa-check"></i><b>4.3.4</b> Absolute Means Model</a></li>
<li class="chapter" data-level="4.3.5" data-path="lm.html"><a href="lm.html#ofm"><i class="fa fa-check"></i><b>4.3.5</b> Ordered Factorial Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mpm.html"><a href="mpm.html"><i class="fa fa-check"></i><b>5</b> Multi-predictor models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mpm.html"><a href="mpm.html#mrm"><i class="fa fa-check"></i><b>5.1</b> On surface: multiple regression models</a></li>
<li class="chapter" data-level="5.2" data-path="mpm.html"><a href="mpm.html#mfm"><i class="fa fa-check"></i><b>5.2</b> Crossover: multifactorial models</a></li>
<li class="chapter" data-level="5.3" data-path="mpm.html"><a href="mpm.html#grm"><i class="fa fa-check"></i><b>5.3</b> Line-by-line: grouped regression models</a></li>
<li class="chapter" data-level="5.4" data-path="mpm.html"><a href="mpm.html#cfxm"><i class="fa fa-check"></i><b>5.4</b> Conditional effects models</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="mpm.html"><a href="mpm.html#cmrm"><i class="fa fa-check"></i><b>5.4.1</b> Conditional multiple regression</a></li>
<li class="chapter" data-level="5.4.2" data-path="mpm.html"><a href="mpm.html#cmfm"><i class="fa fa-check"></i><b>5.4.2</b> Conditional multifactorial models</a></li>
<li class="chapter" data-level="5.4.3" data-path="mpm.html"><a href="mpm.html#saturation"><i class="fa fa-check"></i><b>5.4.3</b> Saturation: hitting the boundaries</a></li>
<li class="chapter" data-level="5.4.4" data-path="mpm.html"><a href="mpm.html#amplification"><i class="fa fa-check"></i><b>5.4.4</b> Amplification: more than the sum</a></li>
<li class="chapter" data-level="5.4.5" data-path="mpm.html"><a href="mpm.html#cfx-theory"><i class="fa fa-check"></i><b>5.4.5</b> Conditional effects and design theory</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="mpm.html"><a href="mpm.html#prm"><i class="fa fa-check"></i><b>5.5</b> Doing the rollercoaster: polynomial regression models</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="mpm.html"><a href="mpm.html#test-stat"><i class="fa fa-check"></i><b>5.5.1</b> Make yourself a test statistic</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="mpm.html"><a href="mpm.html#further-readings-1"><i class="fa fa-check"></i><b>5.6</b> Further readings</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mlm.html"><a href="mlm.html"><i class="fa fa-check"></i><b>6</b> Multilevel models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mlm.html"><a href="mlm.html#intercept-re"><i class="fa fa-check"></i><b>6.1</b> The Human Factor: Intercept random effects</a></li>
<li class="chapter" data-level="6.2" data-path="mlm.html"><a href="mlm.html#slope-re"><i class="fa fa-check"></i><b>6.2</b> Multi-level linear regression: variance in change</a></li>
<li class="chapter" data-level="6.3" data-path="mlm.html"><a href="mlm.html#thinking-multi-level"><i class="fa fa-check"></i><b>6.3</b> Thinking multi-level</a></li>
<li class="chapter" data-level="6.4" data-path="mlm.html"><a href="mlm.html#universality"><i class="fa fa-check"></i><b>6.4</b> Testing universality of theories</a></li>
<li class="chapter" data-level="6.5" data-path="mlm.html"><a href="mlm.html#non-human-populations"><i class="fa fa-check"></i><b>6.5</b> Non-human populations and cross-overs</a></li>
<li class="chapter" data-level="6.6" data-path="mlm.html"><a href="mlm.html#nested-re"><i class="fa fa-check"></i><b>6.6</b> Nested random effects</a></li>
<li class="chapter" data-level="6.7" data-path="mlm.html"><a href="mlm.html#pool-shrink"><i class="fa fa-check"></i><b>6.7</b> What are random effects? On pooling and shrinkage</a></li>
<li class="chapter" data-level="6.8" data-path="mlm.html"><a href="mlm.html#psychometrics"><i class="fa fa-check"></i><b>6.8</b> Psychometrics and design-o-metric models</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="mlm.html"><a href="mlm.html#coverage"><i class="fa fa-check"></i><b>6.8.1</b> Coverage</a></li>
<li class="chapter" data-level="6.8.2" data-path="mlm.html"><a href="mlm.html#reliability"><i class="fa fa-check"></i><b>6.8.2</b> Reliability</a></li>
<li class="chapter" data-level="6.8.3" data-path="mlm.html"><a href="mlm.html#validity"><i class="fa fa-check"></i><b>6.8.3</b> Validity</a></li>
<li class="chapter" data-level="6.8.4" data-path="mlm.html"><a href="mlm.html#designometrix"><i class="fa fa-check"></i><b>6.8.4</b> Towards Design-o-metrix</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="mlm.html"><a href="mlm.html#further-readings-2"><i class="fa fa-check"></i><b>6.9</b> Further readings</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="glm.html"><a href="glm.html#elements-glm"><i class="fa fa-check"></i><b>7.1</b> Elements of Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="glm.html"><a href="glm.html#relinking-linearity"><i class="fa fa-check"></i><b>7.1.1</b> Re-linking linearity</a></li>
<li class="chapter" data-level="7.1.2" data-path="glm.html"><a href="glm.html#choosing-randomness"><i class="fa fa-check"></i><b>7.1.2</b> Choosing patterns of randomness</a></li>
<li class="chapter" data-level="7.1.3" data-path="glm.html"><a href="glm.html#mean-var-rel"><i class="fa fa-check"></i><b>7.1.3</b> Mean-variance relationship</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="glm.html"><a href="glm.html#count-data"><i class="fa fa-check"></i><b>7.2</b> Count data</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="glm.html"><a href="glm.html#pois-reg"><i class="fa fa-check"></i><b>7.2.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="glm.html"><a href="glm.html#logistic-reg"><i class="fa fa-check"></i><b>7.2.2</b> Logistic (aka Binomial) regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="glm.html"><a href="glm.html#overdispersion"><i class="fa fa-check"></i><b>7.2.3</b> Modelling overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="glm.html"><a href="glm.html#duration-measures"><i class="fa fa-check"></i><b>7.3</b> Duration measures</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="glm.html"><a href="glm.html#exp-gam-reg"><i class="fa fa-check"></i><b>7.3.1</b> Exponential and Gamma regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="glm.html"><a href="glm.html#exgauss-reg"><i class="fa fa-check"></i><b>7.3.2</b> ExGaussian regression</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="glm.html"><a href="glm.html#rating-scales"><i class="fa fa-check"></i><b>7.4</b> Rating scales</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="glm.html"><a href="glm.html#ord-logist-reg"><i class="fa fa-check"></i><b>7.4.1</b> Ordered logistic regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="glm.html"><a href="glm.html#beta-reg"><i class="fa fa-check"></i><b>7.4.2</b> Beta regression</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="glm.html"><a href="glm.html#distributional-models"><i class="fa fa-check"></i><b>7.5</b> Beyond mean: distributional models</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="glm.html"><a href="glm.html#item-anchoring"><i class="fa fa-check"></i><b>7.5.1</b> Item-level anchoring in rating scales</a></li>
<li class="chapter" data-level="7.5.2" data-path="glm.html"><a href="glm.html#part-employment"><i class="fa fa-check"></i><b>7.5.2</b> Participant-level employment of scale</a></li>
<li class="chapter" data-level="7.5.3" data-path="glm.html"><a href="glm.html#part-skew"><i class="fa fa-check"></i><b>7.5.3</b> Participant-level skew in reaction times</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="glm.html"><a href="glm.html#further-readings-3"><i class="fa fa-check"></i><b>7.6</b> Further readings</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="wwm.html"><a href="wwm.html"><i class="fa fa-check"></i><b>8</b> Working with models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="wwm.html"><a href="wwm.html#model-criticism"><i class="fa fa-check"></i><b>8.1</b> Model criticism</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="wwm.html"><a href="wwm.html#residual-analysis"><i class="fa fa-check"></i><b>8.1.1</b> Residual analysis</a></li>
<li class="chapter" data-level="8.1.2" data-path="wwm.html"><a href="wwm.html#fitted-responses"><i class="fa fa-check"></i><b>8.1.2</b> Fitted responses analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="wwm.html"><a href="wwm.html#model-comp"><i class="fa fa-check"></i><b>8.2</b> Model comparison</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="wwm.html"><a href="wwm.html#overfitting"><i class="fa fa-check"></i><b>8.2.1</b> The problem of over-fitting</a></li>
<li class="chapter" data-level="8.2.2" data-path="wwm.html"><a href="wwm.html#cross-validation"><i class="fa fa-check"></i><b>8.2.2</b> Cross validation and LOO</a></li>
<li class="chapter" data-level="8.2.3" data-path="wwm.html"><a href="wwm.html#ic"><i class="fa fa-check"></i><b>8.2.3</b> Information Criteria</a></li>
<li class="chapter" data-level="8.2.4" data-path="wwm.html"><a href="wwm.html#model-selection"><i class="fa fa-check"></i><b>8.2.4</b> Model selection</a></li>
<li class="chapter" data-level="8.2.5" data-path="wwm.html"><a href="wwm.html#choose-dist"><i class="fa fa-check"></i><b>8.2.5</b> Comparing response distributions</a></li>
<li class="chapter" data-level="8.2.6" data-path="wwm.html"><a href="wwm.html#testing-theories"><i class="fa fa-check"></i><b>8.2.6</b> Testing hypotheses</a></li>
<li class="chapter" data-level="8.2.7" data-path="wwm.html"><a href="wwm.html#bayes-factor"><i class="fa fa-check"></i><b>8.2.7</b> A note on Bayes Factor</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="wwm.html"><a href="wwm.html#further-readings-4"><i class="fa fa-check"></i><b>8.3</b> Further readings</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="cases.html"><a href="cases.html"><i class="fa fa-check"></i><b>A</b> Cases</a>
<ul>
<li class="chapter" data-level="A.1" data-path="cases.html"><a href="cases.html#real-cases"><i class="fa fa-check"></i><b>A.1</b> Real cases</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="cases.html"><a href="cases.html#hugme"><i class="fa fa-check"></i><b>A.1.1</b> Hugme</a></li>
<li class="chapter" data-level="A.1.2" data-path="cases.html"><a href="cases.html#cue8"><i class="fa fa-check"></i><b>A.1.2</b> CUE8</a></li>
<li class="chapter" data-level="A.1.3" data-path="cases.html"><a href="cases.html#uncanny"><i class="fa fa-check"></i><b>A.1.3</b> Uncanny Valley</a></li>
<li class="chapter" data-level="A.1.4" data-path="cases.html"><a href="cases.html#ipump"><i class="fa fa-check"></i><b>A.1.4</b> IPump</a></li>
<li class="chapter" data-level="A.1.5" data-path="cases.html"><a href="cases.html#sleepstudy"><i class="fa fa-check"></i><b>A.1.5</b> Case Sleepstudy</a></li>
<li class="chapter" data-level="A.1.6" data-path="cases.html"><a href="cases.html#egan"><i class="fa fa-check"></i><b>A.1.6</b> Egan</a></li>
<li class="chapter" data-level="A.1.7" data-path="cases.html"><a href="cases.html#mmn"><i class="fa fa-check"></i><b>A.1.7</b> Case: Millers Magic Number</a></li>
<li class="chapter" data-level="A.1.8" data-path="cases.html"><a href="cases.html#aup"><i class="fa fa-check"></i><b>A.1.8</b> AUP</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="cases.html"><a href="cases.html#synthetic-data"><i class="fa fa-check"></i><b>A.2</b> Synthetic data sets</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="cases.html"><a href="cases.html#rainfall"><i class="fa fa-check"></i><b>A.2.1</b> Rainfall</a></li>
<li class="chapter" data-level="A.2.2" data-path="cases.html"><a href="cases.html#sec99"><i class="fa fa-check"></i><b>A.2.2</b> 99 seconds</a></li>
<li class="chapter" data-level="A.2.3" data-path="cases.html"><a href="cases.html#rational"><i class="fa fa-check"></i><b>A.2.3</b> Rational</a></li>
<li class="chapter" data-level="A.2.4" data-path="cases.html"><a href="cases.html#browsingab"><i class="fa fa-check"></i><b>A.2.4</b> BrowsingAB</a></li>
<li class="chapter" data-level="A.2.5" data-path="cases.html"><a href="cases.html#headache"><i class="fa fa-check"></i><b>A.2.5</b> Headache</a></li>
<li class="chapter" data-level="A.2.6" data-path="cases.html"><a href="cases.html#reading"><i class="fa fa-check"></i><b>A.2.6</b> Reading time</a></li>
<li class="chapter" data-level="A.2.7" data-path="cases.html"><a href="cases.html#argame"><i class="fa fa-check"></i><b>A.2.7</b> AR_game</a></li>
<li class="chapter" data-level="A.2.8" data-path="cases.html"><a href="cases.html#sleep"><i class="fa fa-check"></i><b>A.2.8</b> Sleep</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">New statistics for design researchers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="glm" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Generalized Linear Models</h1>
<p>In the preceding chapters we got acquainted with the linear model as an extremely flexible tool to represent dependencies between predictors and outcome variables. We saw how factors and covariates gracefully work together and how complex research designs can be captured by multi-level random effects. It was all about specifying an appropriate (and often sophisticated) right-hand side of the regression formula, the predictor term. In contrast, little space has been dedicated to the outcome variables, except that sometimes we used log-transformed outcomes to accommodate the Gaussian error term. That is now going to change, and we will start by examining the assumptions that are associated with the outcome variable.</p>
<p>A particular question is probably lurking in the minds of readers with classic statistics training: What happened to the process of checking assumptions on ANOVA (and alike) and where are all the neat tests that supposedly check for Normality, constant variance and such? The Gaussian linear model, which we used throughout <a href="lm.html#lm">4</a> and <a href="mlm.html#mlm">6</a>, shares these assumptions, the three crucial assumptions being:</p>
<ol style="list-style-type: decimal">
<li><em>Linearity</em> of the association between predictors and outcome variable.</li>
<li><em>Gaussian distribution</em> of responses</li>
<li><em>constant variance</em> of response distribution</li>
</ol>
<p>In the next section we will review these assumptions and lead them ad absurdum. Simply put, with real world outcome measures there is no such thing as Gaussian distribution and true linearity. Checking assumptions on a model that you know is inappropriate, seems a futile exercise, unless better alternatives are available, and that is the case: with <em>Generalized Linear Models</em> (GLMs) we extend our regression modeling framework once again, this time focusing on the outcome variables and their shape of randomness.</p>
<p>As we will see, GLMs solves some common problems with linearity and gives us more choices on the shape of randomness. To say that once and for all: What GLMs do not do is relax the assumptions of linear models. And because I have met at least one seasoned researcher who divided the world of data into two categories, “parametric data,” that meets ANOVA assumptions, and “non-parametric data” that does not, let me get this perfectly straight: <em>data is neither parametric nor non-parametric</em>. Instead, data is the result of a process that distributes measures in some form and a good model aligns to this form. Second, a <em>model is parametric</em>, when the statistics it produces have a useful interpretations, like the intercept is the group mean of the reference group and the intercept random effect represents the variation between individuals. All models presented in this chapter (and this book) fulfill this requirement and <em>all are parametric</em>. There may be just one counter-example, which is polynomial regression <a href="mpm.html#prm">5.5</a>, which we used for its ability to render non-monotonic curves. The polynomial coefficients have no interpretation in terms of the cognitive processes leading to the Uncanny Valley. However, as we have seen in <a href="mpm.html#test-stat">5.5.1</a>, they can easily be used to derive meaningful parameters, such as the positions of shoulder and trough. A clear example of a non-parametric method is the Mann-Withney U-test, which compares the sums of ranks between groups, which typically has no useful interpretation.</p>
<p>The GLM framework rests on two extensions that bring us a huge step closer to our precious data. The first one is the <em>link function</em> a mathematical trick that establishes linearity in many situations. The second is to select a <em>shape of randomness</em> that matches the type of outcome variable, and removes the difficult assumption of constant variance. After we established the elements of the GLM framework <a href="glm.html#elements-glm">7.1</a>, I will introduce a good dozen of model families, that leaves little reason to ever fall back to the Gaussian distributions and data transformations, let alone unintelligible non-parametric procedures. As we will see, there almost always is a clear choice right at the beginning that largely depends on the properties of the response variable, for example:</p>
<ul>
<li><em>Poisson LM</em> is the first choice for outcome variables that are counted (with no upper limit), like number of errors.<br />
</li>
<li><em>Binomial (aka logistic) LM</em> covers the case of successful task completion, where counts have an upper boundary.</li>
</ul>
<p>These two GLM families have been around for more many decades in statistical practice, and they just found a new home under the GLM umbrella. For some other types of outcome variables good default models have been lacking, such as rating scale responses and time-on-task and reaction times. Luckily, with recent developments in Bayesian regression engines the choice of random distributions has become much broader and now also covers distribution families that are suited for these very common types of measures. For RT and ToT, I will suggest exponentially-modified Gaussian <em>(ExGauss)</em> models or, to some extent, <em>Gamma</em> models. For rating scales, where responses fall into a few ordered categories, <em>ordinal logistic regression</em> is a generally accepted approach, but for (quasi)continuous rating scales I will introduce a rather novel approach, <em>Beta regression</em>.</p>
<p>Too many choices can be a burden, but as we will see, most of the time the appropriate model family is obvious. For the impatient readers, here is the recipe: Answer the following three questions about the outcome variable and follow (Figure <a href="glm.html#fig:glm-decision">7.1</a>).</p>
<ol style="list-style-type: decimal">
<li>Is the outcome variable discrete or continuous?</li>
<li>What are the lower and upper boundaries of outcome measures?</li>
<li>Can we expect over-dispersion?</li>
</ol>
<div class="figure"><span id="fig:glm-decision"></span>
<img src="Illustrations/GLM_decision_chart.png" alt="Decision chart for Generalized Linear Models" width="90%" />
<p class="caption">
Figure 7.1: Decision chart for Generalized Linear Models
</p>
</div>
<p>To make it even easier, it is practically always adequate and safe to answer Yes to the third question (over-dispersion). Based on these questions, the graph below identifies the correct distribution family and you can jump right to respective section, if you need a quick answer. In the following section (<a href="glm.html#elements-glm">7.1</a>)), I will provide a general explanation of why GLMs are needed and how they are constructed by choosing a response distribution (<a href="glm.html#choosing-randomness">7.1.2</a>) and a link function (<a href="glm.html#relinking-linearity">7.1.1</a>). The remainder of the chapter is organized by <em>types of measures</em> that are typical for design research: count data (<a href="glm.html#count-data">7.2</a>), duration measures (<a href="glm.html#duration-measures">7.3</a>)) and rating scales (<a href="glm.html#rating-scales">7.4</a>). Together with chapter <a href="mlm.html#mlm">6</a>, this introduces the family of models called <em>Generalized Multi-level Linear Models (GMLM)</em>, which covers a huge variety of research situations. The chapter closes with a brief introduction to an even mightier class of models: GMLMs still have certain limitations. One of them is that they are all about estimating average performance. <em>Distributional models</em> are one further step of abstraction and they apply when the research is concerned with variance, actually (<a href="glm.html#distributional-models">7.5</a>).</p>
<div id="elements-glm" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Elements of Generalized Linear Models</h2>
<p>GLM is a <em>framework for modelling</em> that produces a <em>family of models</em> (Figure <a href="glm.html#fig:glm-decision">7.1</a>). Every member of this family uses a specific <em>link functions</em> to establish linearity and a particular <em>distribution</em>, that has an adequate shape and mean-variance relationship.</p>
<p>Sometimes GLMs are mistaken as a way to relax assumptions of linear models, (or even called non-parametric). They are definitely not! Every member makes precise assumptions on the level of measurement and the shape of randomness. One can even argue that Poisson, Binomial and exponential regression are stricter than Gaussian, as they use only one parameter, with the consequence of a tight association between variance and mean. A few members of GLM are classic: Poisson, Binomial (aka logistic) and exponential regression have routinely been used before they were united under the hood of GLM. These and a few others are called <em>canonical</em> GLMs, as they possess some convenient mathematical properties, that made efficient estimation possible, back in the days of limited computing power.</p>
<p>For a first understanding of Generalized Linear Models, you should know that linear models are one family of Generalized Linear Models, which we call a Gaussian linear model. The three crucial assumptions of Gaussian linear models are encoded in the model formula:</p>
<p><span class="math display">\[
\begin{aligned}
\mu_i &amp;=\beta_0+ \beta_1 x_{1i}+ \dots +\beta_k x_{ki}\\ 
y_i &amp;\sim \textrm{Gaus}(\mu_i,\sigma)
\end{aligned}
\]</span></p>
<p>The first term, we call the structural part and it represents the systematic quantitative relations we expect to find in the data. When it is a sum of products, like above, we call it linear. <em>Linearity</em> is a frequently under-regarded assumption of linear models and it is doomed to fail <a href="glm.html#relinking-linearity">7.1.1</a>. The second term defines the pattern of randomness and it hosts two further assumptions: <em>Gaussian distribution</em> and <em>constant error variance</em> of the random component. The latter might not seem obvious, but is given by the fact that there is just a single value for the standard error <span class="math inline">\(\sigma\)</span>.</p>
<p>In classic statistics education, the tendency is still to present these assumptions as preconditions for a successful ANOVA or linear regression. The very term <em>pre</em>condition suggest, that they need to be checked upfront and the classic statisticians are used to deploy a zoo of null hypothesis tests for this purpose, although it is widely held among statisticians that this practice is illogical. If an assumptions seems to be violated, let’s say Normality, researchers then often turn to non-parametric tests. Many also just continue with ANOVA and add some shameful statements to the discussion of results or humbly cite one research paper that claims ANOVAs robustness to this or that violation.</p>
<p>The parameters of a polynomial model usually don’t have a direct interpretation. However, we saw that useful parameters, such as the minimum of the curve, can be derived. Therefore, polynomial models are sometimes called <em>semiparametric</em>. As an example for a <em>non-parametric</em> test, the Mann-Whitney <em>U</em> statistic is composed of the number of times observations in group A are larger than in group B. The resulting sum <em>U</em> usually bears little relation to any real world process or question. Strictly speaking, the label non-parametric has nothing to do with ANOVA assumptions. It refers to the usefulness of parameters. A research problem, where <em>U</em> as the sum of wins has a useful interpretation could be that in some dueling disciplines, such as Fencing, team competitions are constructed by letting every athlete from a team duel every member of the opponent team. Only under such circumstances would we call the <em>U</em>-test parametric.</p>
<div id="relinking-linearity" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Re-linking linearity</h3>
<p>Linearity means that we increase the predictor by a fixed unit, the outcome will follow suit by a constant amount. In the chapter on Linear Models <a href="lm.html#lm">4</a>, we encountered several situations where linearity was violated.</p>
<ul>
<li>In <a href="lm.html#ofm">4.3.5</a> increasing amount of training by one session did result in rather different amount of learning. In effect, we used not an LRM, but ordered factors to estimate the learning curves.</li>
<li>In <a href="mpm.html#saturation">5.4.3</a>, two pills did not reduce headache by the sum of each oill alone. We used conditional effects when two or more interventions improve the same process in a non-linear way.</li>
<li>and in <a href="mpm.html#prm">5.5</a> we used polynomials to estimate wildly curved relationships</li>
</ul>
<p>The case of Polynomial regression is special in two ways: first, the curvature itself is of theoretical interest (e.g. finding the “trough” of the Uncanny Valley effect). Second, a polynomial curve (of second degree or more) is no longer monotonously increasing (or decreasing). In contrast, learning curves and saturation effects have in common that in both situations outcome steadily increases (or decreases) when we add more to the predictor side. There just is a limit to performance, which is reached asymptotically (which means the curve never really flattens).</p>
<p>For the learning process in the IPump study we earlier used an OFM with stairways coding to account for this non-linearity (@ref(#ofm)), but that has one disadvantage. From a practical perspective it would interesting to know, how performance improves when practice continues. What would be performance in (hypothetical) sessions 4, 5 and 10. Because the OFM just makes up one estimate for every level, there is no way to get predictions beyond the observed range.</p>
<p>With an LRM, the slope parameter applies to all steps, which gives us the possibility of deriving predictions beyond the observed range. To demonstrate this on the deviations from optimal path, the following code estimates a plain LRM and then injects some new (virtual) data to get <em>extrapolations</em> beyond the observed three tasks. Note that the virtual data comes without a response variable, as this is what the model provides (Table <a href="glm.html#tab:relink-1">7.1</a>).</p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="glm.html#cb542-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(IPump)</span></code></pre></div>
<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb543-1"><a href="glm.html#cb543-1" aria-hidden="true" tabindex="-1"></a>M_LRM_1 <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(deviations <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> session,</span>
<span id="cb543-2"><a href="glm.html#cb543-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_Novel</span>
<span id="cb543-3"><a href="glm.html#cb543-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="glm.html#cb544-1" aria-hidden="true" tabindex="-1"></a>D_extra <span class="ot">&lt;-</span></span>
<span id="cb544-2"><a href="glm.html#cb544-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb544-3"><a href="glm.html#cb544-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">session =</span> <span class="fu">as.integer</span>(<span class="fu">c</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">9</span>)),</span>
<span id="cb544-4"><a href="glm.html#cb544-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">range =</span> <span class="fu">if_else</span>(session <span class="sc">&lt;</span> <span class="dv">3</span>,</span>
<span id="cb544-5"><a href="glm.html#cb544-5" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;observed&quot;</span>, <span class="st">&quot;extrapolated&quot;</span></span>
<span id="cb544-6"><a href="glm.html#cb544-6" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb544-7"><a href="glm.html#cb544-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb544-8"><a href="glm.html#cb544-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tbl_obs</span>()</span>
<span id="cb544-9"><a href="glm.html#cb544-9" aria-hidden="true" tabindex="-1"></a>D_extra </span></code></pre></div>
<table>
<caption><span id="tab:relink-1">Table 7.1: </span>Virtual data for generating out-of-range predictions</caption>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="right">session</th>
<th align="left">range</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="left">observed</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1</td>
<td align="left">observed</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">2</td>
<td align="left">observed</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">3</td>
<td align="left">extrapolated</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">4</td>
<td align="left">extrapolated</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">5</td>
<td align="left">extrapolated</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">6</td>
<td align="left">extrapolated</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">7</td>
<td align="left">extrapolated</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">8</td>
<td align="left">extrapolated</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">9</td>
<td align="left">extrapolated</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb545-1"><a href="glm.html#cb545-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(M_LRM_1,</span>
<span id="cb545-2"><a href="glm.html#cb545-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">newdata =</span> D_extra</span>
<span id="cb545-3"><a href="glm.html#cb545-3" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb545-4"><a href="glm.html#cb545-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(D_extra) <span class="sc">%&gt;%</span></span>
<span id="cb545-5"><a href="glm.html#cb545-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(</span>
<span id="cb545-6"><a href="glm.html#cb545-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> session, <span class="at">y =</span> center,</span>
<span id="cb545-7"><a href="glm.html#cb545-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper,</span>
<span id="cb545-8"><a href="glm.html#cb545-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> range</span>
<span id="cb545-9"><a href="glm.html#cb545-9" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb545-10"><a href="glm.html#cb545-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>() <span class="sc">+</span></span>
<span id="cb545-11"><a href="glm.html#cb545-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">&quot;impossible predictions below&quot;</span>), <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb545-12"><a href="glm.html#cb545-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb545-13"><a href="glm.html#cb545-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb545-14"><a href="glm.html#cb545-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;deviations&quot;</span>, <span class="at">col =</span> <span class="cn">NULL</span>)</span></code></pre></div>
<div class="figure"><span id="fig:relink-2"></span>
<img src="Generalized_Linear_Models_files/figure-html/relink-2-1.png" alt="Trying to predict future performance by a linear model produces inpossible predictions" width="90%" />
<p class="caption">
Figure 7.2: Trying to predict future performance by a linear model produces inpossible predictions
</p>
</div>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>If we use a continuous linear model to predict future outcomes of a learning curve, negative values are eventually produced, which is impossible. Non-linearity is not just a problem with learning curves, but happens to all outcomes that have natural lower or upper boundaries. All known outcome variables in the universe have boundaries, just to mention velocity and temperature (on spacetime the jury is still out). It is an inescapable, that all our ephemeral measures in design research have boundaries and strictly cannot have linear associations:</p>
<ul>
<li>Errors and other countable incidences are bounded at zero</li>
<li>Rating scales are bounded at the lower and upper extreme item</li>
<li>Task completion has a lower bound of zero and the number of tasks as an upper bound.</li>
<li>Temporal measures formally have lower bound of zero, but psychologically, the lower bound always is a positive number.</li>
</ul>
<p>The strength of the linear term is its versatility in specifying multiple relations between predictor variables and outcome. It’s Achilles heel is that it assumes measures without boundaries.
Generalized linear models use a simple mathematical trick that keeps a linear terms, but confines the fitted responses to the natural boundaries of the measures. In linear models, the linear term is mapped directly to fitted responses <span class="math inline">\(\mu_i\)</span>:</p>
<p><span class="math display">\[
\mu_i = \beta_0 + x_{1i}\beta_1
\]</span></p>
<p>In GLMs, an additional layer sits between the fitted response <span class="math inline">\(\mu\)</span> and the linear term: The <em>linear predictor</em> <span class="math inline">\(\theta\)</span> has the desired range of <span class="math inline">\([-\infty; \infty]\)</span> and is linked directly to the linear term. In turn, we choose a <em>link function</em> <span class="math inline">\(\phi\)</span> that up-scales the bounded range of measures (<span class="math inline">\(\mu\)</span>). The inverse of the link function (<span class="math inline">\(\phi^{-1}\)</span>) is called the mean function and it does the opposite by down-scaling the linear predictor to the range of measures.</p>
<p><span class="math display">\[
\begin{aligned}
\theta_i &amp;\in [-\infty; \infty]\\
\theta_i &amp;= \beta_0 + x_{1i} \beta_1\\ 
\theta_i &amp;= \phi( \mu_i)\\
\mu_i &amp;= \phi^{-1}(\theta_i)
\end{aligned}
\]</span>
The question is: what mathematical function transforms a bounded space into an unbounded? A link function <span class="math inline">\(\phi\)</span> must fulfill two criteria:</p>
<ol style="list-style-type: decimal">
<li>mapping from the (linear) range <span class="math inline">\([-\infty; \infty]\)</span> to the range of the response, e.g. <span class="math inline">\([0; \infty]\)</span>.</li>
<li>be monotonically increasing, such that the order is preserved</li>
</ol>
<p>A monotonically increasing function always preserves the order, such that the following holds for a link function.</p>
<p><span class="math display">\[
\theta_i &gt; \theta_j \rightarrow \phi(\theta_i) &gt; \phi(\theta_j) \rightarrow \mu_i &gt; \mu_j
\]</span></p>
<p>It would be devastating if a link function would not preserve order, but there is another useful side-effect of monotony: if a function <span class="math inline">\(\phi\)</span> is monotonous, then there exists an inverse function <span class="math inline">\(\phi^{-1}\)</span>, which is called the mean function, as it transforms back to the fitted responses <span class="math inline">\(\mu_i\)</span>. For example, <span class="math inline">\(x^2\)</span> is not monotonous and its inverse, <span class="math inline">\(\sqrt{x}\)</span>, produces <em>two</em> results (e.g. <span class="math inline">\(\sqrt{x} = [2, -2]\)</span>) and therefore is not a even a function, strictly speaking.</p>
<p>A typical case is count variables with a lower boundary of zero and no upper bound. The <em>logarithm</em> is a function that maps positive numbers to the linear range <span class="math inline">\([-\infty; \infty]\)</span>, in the way that numbers smaller than One become negative.</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="glm.html#cb547-1" aria-hidden="true" tabindex="-1"></a><span class="fu">log</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, .<span class="dv">5</span>))</span></code></pre></div>
<pre><code>## [1]  0.693  0.000 -0.693</code></pre>
<p>The logarithm has the <em>exponential</em> function as a counterpart, which bends the linear range back into the boundaries. Other measures, like success rates or rating scales, have lower and upper boundaries. A suitable pair of functions is the <em>logit</em> link function and the <em>logistic</em> mean function (Figure <a href="glm.html#fig:relink-3">7.3</a>).</p>
<div class="figure"><span id="fig:relink-3"></span>
<img src="Generalized_Linear_Models_files/figure-html/relink-3-1.png" alt="Log and logit link functions expand the bounded range of measures. Mean functions do the reverse." width="90%" />
<p class="caption">
Figure 7.3: Log and logit link functions expand the bounded range of measures. Mean functions do the reverse.
</p>
</div>
<p>Using the link function comes at a cost: the linear coefficients <span class="math inline">\(\beta_i\)</span> is losing its interpretation as increment-per-unit and no longer has a natural interpretation. Later, we will see that logarithmic and logit scales gain an intuitive interpretation when parameters are exponentiated, <span class="math inline">\(\textrm{exp}(\beta_i)\)</span> (@ref(speaking-multipliers and <a href="glm.html#talking-odds">7.2.2.3</a>).</p>
<p>Who needs a well-defined link between observations and fitted responses? Applied design researchers do when predictions are their business. In the IPump study it is compelling to ask: “how will the nurse perform in session 4?” or “When will he reach error-free operation?” In <a href="glm.html#learning-curves">7.2.1.2</a> we will see a non-linear learning process becoming an almost straight line on the logarithmic scale.</p>
</div>
<div id="choosing-randomness" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Choosing patterns of randomness</h3>
<p>The second term of a linear model, <span class="math inline">\(y_i \sim Norm(\mu_i, \sigma)\)</span> states that the observed values are drawn from Gaussian distributions (<a href="ebs.html#gauss-dist">3.5.2.6</a>). But Gaussian distributions have the same problem as the linearity assumption: the range is <span class="math inline">\([-\infty; \infty]\)</span>.</p>
<!-- in fact, a Gaussian distribution can only be a reasonable approximation when the measures are far off the boundaries of measures and the error is much smaller than the predicted values. -->
<p>The problem can be demonstrated by simulating observations, using a Gaussian pattern of randomness, and see how this fails to produce realistic data. Imagine a study comparing a novel and a legacy interface design for medical infusion pumps. The researchers let trained nurses perform a single task on both devices and count the errors. Assuming, the average number of errors per tasks is <span class="math inline">\(\mu_L = 3\)</span> for the legacy device and <span class="math inline">\(\mu_N = 1.2\)</span> for the novel device, with standard deviation of <span class="math inline">\(\sigma = .8\)</span>. We simulate data from a factorial linear model, ashown in Figure <a href="glm.html#fig:random-1">7.4</a>.</p>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb549-1"><a href="glm.html#cb549-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">84</span>)</span>
<span id="cb549-2"><a href="glm.html#cb549-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">80</span></span>
<span id="cb549-3"><a href="glm.html#cb549-3" aria-hidden="true" tabindex="-1"></a>D_pumps_sim <span class="ot">&lt;-</span></span>
<span id="cb549-4"><a href="glm.html#cb549-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb549-5"><a href="glm.html#cb549-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">Design =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;L&quot;</span>, <span class="st">&quot;N&quot;</span>), N <span class="sc">/</span> <span class="dv">2</span>),</span>
<span id="cb549-6"><a href="glm.html#cb549-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">mu =</span> <span class="fu">if_else</span>(Design <span class="sc">==</span> <span class="st">&quot;L&quot;</span>, <span class="dv">3</span>, <span class="fl">1.2</span>),</span>
<span id="cb549-7"><a href="glm.html#cb549-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">errors =</span> <span class="fu">rnorm</span>(N, mu, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb549-8"><a href="glm.html#cb549-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb549-9"><a href="glm.html#cb549-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tbl_obs</span>()</span></code></pre></div>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb550-1"><a href="glm.html#cb550-1" aria-hidden="true" tabindex="-1"></a>D_pumps_sim <span class="sc">%&gt;%</span></span>
<span id="cb550-2"><a href="glm.html#cb550-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> errors)) <span class="sc">+</span></span>
<span id="cb550-3"><a href="glm.html#cb550-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span>Design) <span class="sc">+</span></span>
<span id="cb550-4"><a href="glm.html#cb550-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">20</span>) <span class="sc">+</span></span>
<span id="cb550-5"><a href="glm.html#cb550-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="dv">0</span>)) <span class="sc">+</span></span>
<span id="cb550-6"><a href="glm.html#cb550-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb550-7"><a href="glm.html#cb550-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">colour =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:random-1"></span>
<img src="Generalized_Linear_Models_files/figure-html/random-1-1.png" alt="Simulation with Gaussian error terms produces impossible values." width="90%" />
<p class="caption">
Figure 7.4: Simulation with Gaussian error terms produces impossible values.
</p>
</div>
<p>We immediately see, that simulation with Gaussian distributions is inappropriate: a substantial number of simulated observations is <em>negative</em>, which strictly makes no sense for error counts. The pragmatic and impatient reader may suggest to adjust the standard deviation (or move the averages up) to make negative values less unlikely. That would be a poor solution as Gaussian distributions support the full range of real numbers, no matter how small the variance is (but not zero). There is always a chance of negative simulations, as tiny as it may be. Repeatedly running the simulation until <code>pumps</code> contains exclusively positive numbers (and zero), would compromise the idea of random numbers itself. We can simply conclude that any model that assumes normally distributed errors must be wrong when the outcome is bounded below or above, which means: always.</p>
<p>Recall how linearity is gradually bent when a magnitude approaches its natural limit. A similar effect occurs for distributions. Distributions that respect a lower or upper limit get squeezed like chewing gum into a corner when approaching the boundaries. Review the sections on Binomial <a href="ebs.html#binomial-dist">3.5.2.3</a> and Poisson distributions <a href="ebs.html#poisson-dist">3.5.2.4</a> for illustrations. As a matter of fact, a lot of real data in design research is skewed that way, making Gaussian distributions a poor fit. The only situation where Gaussian distributions are reasonable approximations is when the outcomes are far off the boundaries. An example of that is the approximation of Binomial outcomes (lower and upper bound), when the probability of success is around 50%. That is also the only point, where a Binomial distribution is truly symmetric.</p>
<p>In contrast, a common misconception is that the Gaussian distribution is a getting better at approximation, when sample sizes are large. This is simply wrong. What really happens, is that increasing the number of observations renders the true distribution more clearly.</p>
<p>In chapter <a href="ebs.html#distributions">3.5.2</a> a number of random distributions were introduced, together with conditions of when they arise. The major criteria were related to properties of the outcome measure: how it is bounded and whether it is discrete (countable) or continuous.
Generalized Linear Models give the researcher a larger choice for modeling the random component and Table <a href="glm.html#tab:glm-distributions">7.2</a> lists some common candidates.</p>
<table>
<caption><span id="tab:glm-distributions">Table 7.2: </span>Canonical probability distribution families by type of measures</caption>
<thead>
<tr class="header">
<th align="left">boundaries</th>
<th align="left">discrete</th>
<th align="left">continuous</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">unbounded</td>
<td align="left"></td>
<td align="left">Normal</td>
</tr>
<tr class="even">
<td align="left">lower</td>
<td align="left">Poisson</td>
<td align="left">Exponential</td>
</tr>
<tr class="odd">
<td align="left">lower and upper</td>
<td align="left">Binomial</td>
<td align="left">Beta</td>
</tr>
</tbody>
</table>
<p>That is not to say that these five are the only possible choices. Many dozens of statistical distributions are known and these five are just making the least assumptions on the shape of randomness in their class (mathematicians call this <em>maximum entropy distributions</em>). In fact, we will soon discover that real data frequently violates principles of these distributions. For example, count measures in behavioral research typically show more error variance than is allowed by Poisson distributions. As we will see in <a href="glm.html#overdispersion">7.2.3</a>, Poisson distribution can still be used in such cases with some additional tweaks borrowed from multi-level modeling (observation-level random effects).</p>
<p>Response times in design research are particularly “misbehaved,” as they do not have their lower boundary at zero, but at the lowest human possible time to solve the task. The complication arises that most continuous distributions have a lower boundary of exactly zero. In case of response times, we will take advantage of the fact, that modern Bayesian estimation engines support a larger range of distributions than ever seen before. The <code>stan_glm</code> regression engine has been designed with downwards compatibility in mind, which is why it does not include newer distributions. In contrast, the package <code>brms</code> is less hampered by legacy and gives many more choices, such as the Exponential-Gaussian distribution for ToT.</p>
</div>
<div id="mean-var-rel" class="section level3" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Mean-variance relationship</h3>
<p>The third assumption of linear models is rooted in the random component term as well. Recall, that there is just one parameter <span class="math inline">\(\sigma\)</span> for the dispersion of randomness and that any Gaussian distribution’s dispersion is exclusively determined by <span class="math inline">\(\sigma\)</span>. That is more of a problem as it may sound, at first. In most real data, the dispersion of randomness depends on the location, as can be illustrated by the following simulation.</p>
<p>Imagine a survey on commuter behavior that asks the following questions:</p>
<ol style="list-style-type: decimal">
<li>How long is your daily route?</li>
<li>How long does it <em>typically</em> take to go to work?</li>
<li>What are the maximum and minimum travel times you remember?</li>
</ol>
<p>If we simulate such data from a linear model, the relationship between length of route and travel time would look like a evenly wide band, which is due to the constant variance (Figure <a href="glm.html#fig:mvr-1">7.5</a>).</p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="glm.html#cb551-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb551-2"><a href="glm.html#cb551-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb551-3"><a href="glm.html#cb551-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Obs =</span> <span class="fu">as.factor</span>(<span class="dv">1</span><span class="sc">:</span>N),</span>
<span id="cb551-4"><a href="glm.html#cb551-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">km =</span> <span class="fu">runif</span>(N, <span class="dv">2</span>, <span class="dv">40</span>),</span>
<span id="cb551-5"><a href="glm.html#cb551-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">min =</span> <span class="fu">rnorm</span>(N, km <span class="sc">*</span> <span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb551-6"><a href="glm.html#cb551-6" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb551-7"><a href="glm.html#cb551-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> km, <span class="at">y =</span> min)) <span class="sc">+</span></span>
<span id="cb551-8"><a href="glm.html#cb551-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb551-9"><a href="glm.html#cb551-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_quantile</span>(<span class="at">quantiles =</span> <span class="fu">c</span>(.<span class="dv">25</span>, .<span class="dv">5</span>, .<span class="dv">75</span>))</span></code></pre></div>
<div class="figure"><span id="fig:mvr-1"></span>
<img src="Generalized_Linear_Models_files/figure-html/mvr-1-1.png" alt="A Gaussian linear simulation of travel times (min) depending on distance (km) results in an unrealistic mean-variance relationship." width="90%" />
<p class="caption">
Figure 7.5: A Gaussian linear simulation of travel times (min) depending on distance (km) results in an unrealistic mean-variance relationship.
</p>
</div>
<p>What is unrealistic is that persons who live right around the corner experience the same range of possible travel times than people who drive dozens of kilometers. That does not seem right.</p>
<p>Gaussian distributions are a special case, because most other distributions do not have constant variance. For example, a Gamma distribution takes two parameters, shape <span class="math inline">\(alpha\)</span> and scale <span class="math inline">\(tau\)</span> and both of them influence mean and variance of the distribution, such that the error variance increases by the square of the mean (Figure <a href="glm.html#fig:mvr-2">7.6</a>))</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;\sim \textrm{Gamma}(\alpha, \theta)\\ 
E(Y) &amp;= \alpha \theta\\
\textrm{Var}(Y) &amp;= \alpha \theta^2\\
\textrm{Var}(Y) &amp;= E(Y) \theta
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="glm.html#cb552-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb552-2"><a href="glm.html#cb552-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">km =</span> <span class="fu">runif</span>(<span class="dv">100</span>, <span class="dv">2</span>, <span class="dv">40</span>),</span>
<span id="cb552-3"><a href="glm.html#cb552-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">min =</span> <span class="fu">rgamma</span>(<span class="dv">100</span>, <span class="at">shape =</span> km <span class="sc">*</span> .<span class="dv">5</span>, <span class="at">scale =</span> <span class="dv">4</span>)</span>
<span id="cb552-4"><a href="glm.html#cb552-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb552-5"><a href="glm.html#cb552-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> km, <span class="at">y =</span> min)) <span class="sc">+</span></span>
<span id="cb552-6"><a href="glm.html#cb552-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb552-7"><a href="glm.html#cb552-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_quantile</span>(<span class="at">quantiles =</span> <span class="fu">c</span>(.<span class="dv">25</span>, .<span class="dv">5</span>, .<span class="dv">75</span>))</span></code></pre></div>
<div class="figure"><span id="fig:mvr-2"></span>
<img src="Generalized_Linear_Models_files/figure-html/mvr-2-1.png" alt="A Gamma simulation of travel times (min) depending on distance (km) results in a realistic mean-variance relationship." width="90%" />
<p class="caption">
Figure 7.6: A Gamma simulation of travel times (min) depending on distance (km) results in a realistic mean-variance relationship.
</p>
</div>
<p>A similar situation arises for count data. When counting user errors, we would expect a larger variance for complex tasks and interfaces, e.g. writing an article in a word processor, as compared to the rather simple situation like operating a medical infusion pump. For count data, the Poisson distribution is often a starting point and for Poisson distributed variables, mean and variance are both exactly determined by the Poisson rate parameter <span class="math inline">\(\lambda\)</span>, and therefore strictly connected to each other. Figure <a href="glm.html#fig:mvr-3">7.7</a> shows hypothetical data from two tasks with very different error rates.</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;\sim \textrm{Poisson}(\lambda)\\
\textrm{Var}(Y) &amp;= E(Y) = \lambda
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb553-1"><a href="glm.html#cb553-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb553-2"><a href="glm.html#cb553-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">Task =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;writing article&quot;</span>, <span class="st">&quot;using infusion pump&quot;</span>), <span class="dv">50</span>),</span>
<span id="cb553-3"><a href="glm.html#cb553-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">errors =</span> <span class="fu">rpois</span>(<span class="dv">100</span>,</span>
<span id="cb553-4"><a href="glm.html#cb553-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">lambda =</span> <span class="fu">if_else</span>(Task <span class="sc">==</span> <span class="st">&quot;writing article&quot;</span>,</span>
<span id="cb553-5"><a href="glm.html#cb553-5" aria-hidden="true" tabindex="-1"></a>      <span class="dv">200</span>, <span class="dv">8</span></span>
<span id="cb553-6"><a href="glm.html#cb553-6" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb553-7"><a href="glm.html#cb553-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb553-8"><a href="glm.html#cb553-8" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb553-9"><a href="glm.html#cb553-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Task, <span class="at">y =</span> errors)) <span class="sc">+</span></span>
<span id="cb553-10"><a href="glm.html#cb553-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb553-11"><a href="glm.html#cb553-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_jitter</span>()</span></code></pre></div>
<div class="figure"><span id="fig:mvr-3"></span>
<img src="Generalized_Linear_Models_files/figure-html/mvr-3-1.png" alt="Mean-variance relationship of Poisson distributed data  with two groups" width="90%" />
<p class="caption">
Figure 7.7: Mean-variance relationship of Poisson distributed data with two groups
</p>
</div>
<p>Not by coincidence, practically all distributions with a lower boundary have variance increase with the mean. Distributions that have two boundaries, like binomial or beta distributions also have a mean-variance relationship, but a different one. For binomial distributed variables, mean and variance are determined as follows:</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;\sim \textrm{Binom}(p, k)\\
E(Y) &amp;= p k\\
\textrm{Var}(Y) &amp;= p (1-p) k\\
\textrm{Var}(Y) &amp;= E(Y)(1-p)
\end{aligned}
\]</span></p>
<p>To see this, imagine a study that examines the relationship between user expertise (for the convenience on a scale of zero to one) and success rate on ten tasks. The result is a cigar-like shape, like in Figure <a href="glm.html#fig:mvr-4">7.8</a>. For binomial distributions, variance gets largest, when the chance of success is centered at <span class="math inline">\(p = .5\)</span>. This is very similar for other distributions with two boundaries, such as beta and logit-Gaussian distributions.</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="glm.html#cb554-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb554-2"><a href="glm.html#cb554-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">expertise =</span> <span class="fu">runif</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb554-3"><a href="glm.html#cb554-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">successes =</span> <span class="fu">rbinom</span>(<span class="dv">1000</span>, <span class="dv">25</span>, expertise)</span>
<span id="cb554-4"><a href="glm.html#cb554-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb554-5"><a href="glm.html#cb554-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> expertise, <span class="at">y =</span> successes)) <span class="sc">+</span></span>
<span id="cb554-6"><a href="glm.html#cb554-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<div class="figure"><span id="fig:mvr-4"></span>
<img src="Generalized_Linear_Models_files/figure-html/mvr-4-1.png" alt="Cigar shaped mean-variance relationship of Binomial data" width="90%" />
<p class="caption">
Figure 7.8: Cigar shaped mean-variance relationship of Binomial data
</p>
</div>
<p>In conclusion, real distributions are typically asymmetric and have mean and variance linked. Both phenomena are tightly linked to the presence of boundaries. Broadly, the deviation from symmetry gets worse when observations are close to the boundaries (e.g. low error rates), whereas differences in variance is more pronounced when the means are far apart from each other.</p>
<p>Still, using distributions that are not Gaussian sometimes carries minor complications. Gaussian distributions have the convenient property that the amount of randomness is directly expressed as the parameter <span class="math inline">\(\sigma\)</span>. That allowed us to compare the fit of two models A and B by comparing <span class="math inline">\(\sigma_A\)</span> and <span class="math inline">\(\sigma_B\)</span>. In random distributions with just one parameter, the variance of randomness is fixed by the location (e.g. Poisson <span class="math inline">\(\lambda\)</span> or Binomial <span class="math inline">\(p\)</span>). For distributions with more than one parameter, dispersion of randomness typically is a function of two (or more) parameters, as can be seen in the formulas above. For example, Gamma distributions have two parameters, but both play a role in location and dispersion.</p>
<p>Using distributions with entanglement of location and dispersion seems to be a step back, but frequently it is necessary to render a realistic association between location of fitted responses and amount of absolute randomness. Most distributions with a lower bound (e.g. Poisson, exponential and Gamma) increase variance with mean, whereas double bounded distributions (beta and binomial) typically have maximum variance when the distribution is centered and symmetric. For the researcher this all means that the choice of distribution family determines the shape of randomness <em>and</em> the relation between location and variance.</p>
<p>The following sections are organized by type of typical outcome variable (counts, durations and rating scales). Each section first introduces a one-parametric model (e.g. Poisson). A frequent problem with these models is that the location-variance relation is too strict. When errors are more widely dispersed than is allowed, this is called over-dispersion and one can either use a trick borrowed from multi-level models, observation-level random effects @(olre) or select a two-parametric distribution class (e.g., Negative-Binomial).</p>
</div>
</div>
<div id="count-data" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Count data</h2>
<p>Gaussian distributions assume that the random variable under investigation is continuous. For measures, such as time, it is natural and it can be a reasonable approximation for all measures with fine-grained steps, such as average scores of self-report scales with a large number of items. Other frequently used measures are clearly, i.e. naturally, discrete, in particular everything that is counted. Examples are: number of errors, number of successfully completed tasks or the number of users. Naturally, count measures have a lower bound and sometimes this is zero (or can be made zero by simple transformations). A distinction has to be made, though, for the upper bound. In some cases, there is no well defined upper bound, or it is very large (e.g. number of visitors on a website) and Poisson regression applies. In other cases, the upper bound is determined by the research design. A typical case in design research is the number of tasks in a usability study. When there is an upper bound, Binomial distributions apply, which is called logistic regression.</p>
<div id="pois-reg" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Poisson regression</h3>
<p>When the outcome variable is the result of a counting process with no obvious upper limit, Poisson regression applies.
In brief, Poisson regression has the following attributes:</p>
<ol style="list-style-type: decimal">
<li>The outcome variable is bounded at zero (and that must be a possible outcome, indeed).</li>
<li>The linear predictor is on a logarithmic scale, with the exponential function being the inverse.</li>
<li>Randomness follows a Poisson distribution.</li>
<li>Variance of randomness increases linearly with the mean.</li>
</ol>
<p>The link function is the logarithm, as it transforms from the non-negative range of numbers to real numbers (<a href="glm.html#relinking-linearity">7.1.1</a>). For a start, we have a look at a Poisson GMM. Recall the smart smurfer game from section <a href="ebs.html#poisson-dist">3.5.2.4</a>. Imagine that in an advanced level of the game <!-- #99 -->, items are well hidden from the player and therefore extremely difficult to catch. To compensate for the decreased visibility of items, every level carries an abundance of them. In fact, the goal of the designers is that visibility and abundance are so carefully balanced that, on average, a player finds three items. We simulate a data set for one player repeating the level 30 times (Figure <a href="glm.html#fig:poisreg-1">7.9</a>) and run our first Poisson model, which is a plain GMM.</p>
<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb555-1"><a href="glm.html#cb555-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6</span>)</span>
<span id="cb555-2"><a href="glm.html#cb555-2" aria-hidden="true" tabindex="-1"></a>D_Pois <span class="ot">&lt;-</span></span>
<span id="cb555-3"><a href="glm.html#cb555-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb555-4"><a href="glm.html#cb555-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Obs =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">30</span>,</span>
<span id="cb555-5"><a href="glm.html#cb555-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">items_found =</span> <span class="fu">rpois</span>(<span class="dv">30</span>, <span class="at">lambda =</span> <span class="fl">3.4</span>)</span>
<span id="cb555-6"><a href="glm.html#cb555-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb555-7"><a href="glm.html#cb555-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb555-8"><a href="glm.html#cb555-8" aria-hidden="true" tabindex="-1"></a>D_Pois <span class="sc">%&gt;%</span></span>
<span id="cb555-9"><a href="glm.html#cb555-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> items_found)) <span class="sc">+</span></span>
<span id="cb555-10"><a href="glm.html#cb555-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>()</span></code></pre></div>
<div class="figure"><span id="fig:poisreg-1"></span>
<img src="Generalized_Linear_Models_files/figure-html/poisreg-1-1.png" alt="Data sampled from a Poisson distribution" width="90%" />
<p class="caption">
Figure 7.9: Data sampled from a Poisson distribution
</p>
</div>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="glm.html#cb556-1" aria-hidden="true" tabindex="-1"></a>M_Pois <span class="ot">&lt;-</span></span>
<span id="cb556-2"><a href="glm.html#cb556-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(items_found <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb556-3"><a href="glm.html#cb556-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> poisson,</span>
<span id="cb556-4"><a href="glm.html#cb556-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> D_Pois</span>
<span id="cb556-5"><a href="glm.html#cb556-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb557-1"><a href="glm.html#cb557-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(M_Pois)</span></code></pre></div>
<table>
<caption><span id="tab:poisreg-2">Table 7.3: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">type</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">object</td>
<td align="left">fixef</td>
<td align="left">Intercept</td>
<td align="right">1.31</td>
<td align="right">1.12</td>
<td align="right">1.49</td>
</tr>
</tbody>
</table>
<p>Poisson distributions have only one parameter <span class="math inline">\(\lambda\)</span> (lambda), which has a direct interpretation as the expected mean and variance of the distribution. On the contrary, the regression coefficient is on a logarithmic scale to ensure it has no boundaries. To reverse to the scale of measurement, we use the exponential function as the <em>mean function</em> <a href="glm.html#relinking-linearity">7.1.1</a>:</p>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="glm.html#cb558-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(M_Pois, <span class="at">mean.func =</span> exp)</span></code></pre></div>
<table>
<caption><span id="tab:poisreg-3">Table 7.4: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">type</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">object</td>
<td align="left">fixef</td>
<td align="left">Intercept</td>
<td align="right">3.72</td>
<td align="right">3.07</td>
<td align="right">4.44</td>
</tr>
</tbody>
</table>
<p>The exponentiated coefficient can now be interpreted as the expected number of items found per session. Together with the credibility limits it would allow the conclusion that the items are slightly easier to find than three per session. Before we move on to more complex Poisson models, let’s take a look of the formalism of the Poisson GMM:</p>
<p><span class="math display">\[
\begin{aligned}
\theta_i &amp;= \beta_0\\
\mu_i &amp;= \exp(\theta_i)\\
y_i &amp;\sim \textrm{Pois}(\mu_i)
\end{aligned}
\]</span></p>
<p>In linear models, the first equation used to directly relate fitted responses <span class="math inline">\(\mu_i\)</span> to the linear term. As any linear term is allowed to have negative results, this could lead to problems in the last line, because Poisson <span class="math inline">\(\lambda\)</span> is strictly non-negative. <em>Linear predictor</em> <span class="math inline">\(\theta_i\)</span> is taking those punches from the linear term and hands it over to the fitted responses <span class="math inline">\(\mu_i\)</span> via the exponential function. This function takes any number and returns a positive number, and that makes it safe for the last term that defines the pattern of randomness.</p>
<div id="speaking-multipliers" class="section level4" number="7.2.1.1">
<h4><span class="header-section-number">7.2.1.1</span> Speaking multipliers</h4>
<p>To demonstrate the interpretation of coefficients other than the intercept (or absolute group means), we turn to the more complex case of the infusion pump study. In this study, the deviations from a normative path were counted as a measure for error-proneness. In the following regression analysis, we examine the reduction of deviations by training sessions as well as the differences between the two devices. As we are interested in the improvement from first to second session and second to third, successive difference contrasts apply (<a href="lm.html#treatment-contrasts">4.3.3</a>).</p>
<div class="sourceCode" id="cb559"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb559-1"><a href="glm.html#cb559-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(IPump)</span></code></pre></div>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="glm.html#cb560-1" aria-hidden="true" tabindex="-1"></a>M_dev <span class="ot">&lt;-</span></span>
<span id="cb560-2"><a href="glm.html#cb560-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glmer</span>(deviations <span class="sc">~</span> Design <span class="sc">+</span> session <span class="sc">+</span> session<span class="sc">:</span>Design <span class="sc">+</span></span>
<span id="cb560-3"><a href="glm.html#cb560-3" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span> <span class="sc">+</span> Design <span class="sc">+</span> session <span class="sc">|</span> Part) <span class="sc">+</span></span>
<span id="cb560-4"><a href="glm.html#cb560-4" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span> <span class="sc">+</span> Design <span class="sc">|</span> Task) <span class="sc">+</span></span>
<span id="cb560-5"><a href="glm.html#cb560-5" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span> <span class="sc">|</span> Obs), <span class="do">## observation-level ramdom effect</span></span>
<span id="cb560-6"><a href="glm.html#cb560-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> poisson,</span>
<span id="cb560-7"><a href="glm.html#cb560-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_pumps</span>
<span id="cb560-8"><a href="glm.html#cb560-8" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>Note that in order to account for over-dispersion, observation-level random effect (<code>1|Obs</code>) has been used, see <a href="glm.html#overdispersion">7.2.3</a>. For the current matter, we can leave that alone and inspect population-level coefficients (Table <a href="glm.html#tab:poisreg-4">7.5</a>).</p>
<div class="sourceCode" id="cb561"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb561-1"><a href="glm.html#cb561-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(M_dev)</span></code></pre></div>
<table>
<caption><span id="tab:poisreg-4">Table 7.5: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">0.831</td>
<td align="right">0.244</td>
<td align="right">1.406</td>
</tr>
<tr class="even">
<td align="left">DesignNovel</td>
<td align="right">-1.555</td>
<td align="right">-2.364</td>
<td align="right">-0.785</td>
</tr>
<tr class="odd">
<td align="left">session</td>
<td align="right">-0.234</td>
<td align="right">-0.335</td>
<td align="right">-0.133</td>
</tr>
<tr class="even">
<td align="left">DesignNovel:session</td>
<td align="right">-0.074</td>
<td align="right">-0.243</td>
<td align="right">0.084</td>
</tr>
</tbody>
</table>
<p>These coefficients are on a logarithmic scale and cannot be interpreted right away. By using the exponential mean function, we reverse the logarithm and obtain Table <a href="glm.html#tab:poisreg-5">7.6</a>.</p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="glm.html#cb562-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(M_dev, <span class="at">mean.func =</span> exp)</span></code></pre></div>
<table>
<caption><span id="tab:poisreg-5">Table 7.6: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">2.297</td>
<td align="right">1.277</td>
<td align="right">4.081</td>
</tr>
<tr class="even">
<td align="left">DesignNovel</td>
<td align="right">0.211</td>
<td align="right">0.094</td>
<td align="right">0.456</td>
</tr>
<tr class="odd">
<td align="left">session</td>
<td align="right">0.791</td>
<td align="right">0.715</td>
<td align="right">0.876</td>
</tr>
<tr class="even">
<td align="left">DesignNovel:session</td>
<td align="right">0.928</td>
<td align="right">0.784</td>
<td align="right">1.087</td>
</tr>
</tbody>
</table>
<p>Like in the GMM, the intercept now has the interpretation as the number of deviations with the legacy design in the first session. However, all the other coefficients are no longer summands, but <em>multiplicative</em>. It would therefore be incorrect to speak of them in terms of differences.</p>
<p><span class="math display">\[
\begin{aligned}
\mu_i &amp;= \exp(\beta_0 + x_1\beta_1 + x_2\beta_2) \\
&amp;= \exp(\beta_0) \exp(x_1\beta_1) \exp(x_2\beta_2)
\end{aligned}
\]</span></p>
<p>Actually, it is rather unnatural to speak of error reduction in terms of differences, as we did in previous chapters. If we would say “With the novel interface 1.8 fewer errors are being made,” that means nothing. 1.8 fewer than what? Instead, the following statements make perfect sense:</p>
<ol style="list-style-type: decimal">
<li>In the first session, the novel design produces 2.297 <em>times</em> the deviations than with the legacy design.</li>
<li>For the legacy design, every new training session reduces the number of deviations <em>by factor</em> 0.791<br />
</li>
<li>The reduction rate per training session of the novel design is *92.843% as compared to the legacy design.</li>
</ol>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>To summarize: reporting coefficients on the linearized scale is not useful. We are not tuned to think in logarithmic terms and any quantitative message would get lost. By applying the mean function, we get back to the original scale. As it turns out, what was a sum of linear terms, now becomes a multiplication. For this reason, Poisson regression has often been called a <em>multiplicative model</em>. Another name is <em>log-linear model</em>, which attests that on the log-scale the model is linear. As we will see next, that is perfectly true for the learning process: on the log scale, the training sessions in the IPump study are associated by a constant decrease in deviations from optimal path.</p>
</div>
<div id="learning-curves" class="section level4" number="7.2.1.2">
<h4><span class="header-section-number">7.2.1.2</span> Linearizing learning curves</h4>
<p>The Achilles heel of Gaussian linear models is the linearity assumption. All measures in this universe are finite, which means that all processes eventually hit a boundary. Linearity is an approximation that works well if you stay away from the boundaries. If you can’t, saturation effects happen and that means you have to add interaction effects or ordered factors to your model. Unless, you go multiplicative.</p>
<p>Classic Mechanics assumes that the a space rocket accelerates linearly with the energy production by its thrusters. Every Joule you burn increases your current speed <span class="math inline">\(\beta_0\)</span> by <span class="math inline">\(\beta_1\)</span> km/h. The problem with this assumption is that when you are already close to speed of light and the next thrust pushes you beyond the border. In the real world, <span class="math inline">\(E = Mc^2\)</span> holds an every thrust increases the energy, and therefore also its drag, <span class="math inline">\(\beta_0\)</span> is not constant. The advantage of multiplicative model is that it does not cross the boundary between positive and negative.</p>
<!-- In practice, we are more concerned about crossing the Zero and get negative predictions. In fact, multiplications have no such problem. A product is possible  $\beta_0x_1\beta_1 $ is positive, otherwise  -->
<!-- For a summation of linear coefficients, like  -->
<!-- $x + 10 + 10 + 10 ... $ -->
<!-- . is To use the dramatic example of speed-of-light:  -->
<p>In the <em>additive</em> linear model, the learning curve is non-linear and we had to use an ordered factor model. Learning curves are characterized by running against an asymptote, which is the level of maximum level of achievable performance.</p>
<p>As we will see now, the clumsy OFM (<a href="lm.html#ofm">4.3.5</a>) can be replaced by a log-linear regression model, with just one slope coefficient. The idea of replacing the OFM with a *linear__ized__ regression model (LzRM)<em>, is attractive. For one, with such a model we can obtain valid </em>forecasts* of the learning process. And second, the LzRM is more <em>parsimonous</em> <a href="wwm.html#overfitting">8.2.1</a>. For any sequence length, an LzRM just needs two parameters: intercept and slope, whereas the OFM requires one coefficient per session.</p>
<p>As it happens, learning curves often follow the <em>exponential law of practice</em>. Basically, that means that the performance increase is defined as <em>rate</em>, rather than a difference. In a sentence that would be something like: Every training session reduces the number of errors by 20%. When initial errors are 100, then after <span class="math inline">\(n\)</span> sessions it is:</p>
<p>ToT = <span class="math inline">\(100 \times .8^n\)</span></p>
<p>Exponential functions make pretty good learning curves and they happen to be the mean function of Poisson regression. This leads to the following simulation of a learning experiment. This simulation takes a constant step size of <span class="math inline">\(\log(.8) = -0.223\)</span> on the log-linearized scale, resulting in a reduction of 20% per session.</p>
<div class="figure"><span id="fig:poisreg-6"></span>
<img src="Generalized_Linear_Models_files/figure-html/poisreg-6-1.png" alt="Exponential learning curve becoming linear under the log link function" width="90%" />
<p class="caption">
Figure 7.10: Exponential learning curve becoming linear under the log link function
</p>
</div>
<p>While the linear predictor scale is a straight line, the response scale clearly is a curve-of-diminishing returns. That opens up the possibility that learning the novel pump design also has a constant difference on the linearized scale, which would mena a constant rate on the original scale. In the following, we estimate two Poisson models, one linearized OFM (OzFM) (with stairway dummies <a href="lm.html#ofm">4.3.5</a>) and one LzRM. Then we will assess the model fit (using fitted responses). If the learning process is linear on the log scale, we can expect to see the following:</p>
<ol style="list-style-type: decimal">
<li>The two step coefficients of the OzFM become similar (they were wide apart for ToT).</li>
<li>The slope effect of the LzRM is the same as the step sizes.</li>
<li>Both models fit similar initial performance (intercepts)</li>
</ol>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="glm.html#cb564-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(IPump)</span></code></pre></div>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="glm.html#cb565-1" aria-hidden="true" tabindex="-1"></a>D_agg <span class="ot">&lt;-</span></span>
<span id="cb565-2"><a href="glm.html#cb565-2" aria-hidden="true" tabindex="-1"></a>  D_agg <span class="sc">%&gt;%</span></span>
<span id="cb565-3"><a href="glm.html#cb565-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb565-4"><a href="glm.html#cb565-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Step_1 =</span> <span class="fu">as.integer</span>(session <span class="sc">&gt;=</span> <span class="dv">1</span>),</span>
<span id="cb565-5"><a href="glm.html#cb565-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">Step_2 =</span> <span class="fu">as.integer</span>(session <span class="sc">&gt;=</span> <span class="dv">2</span>)</span>
<span id="cb565-6"><a href="glm.html#cb565-6" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb566"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb566-1"><a href="glm.html#cb566-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Ordered  regression model</span></span>
<span id="cb566-2"><a href="glm.html#cb566-2" aria-hidden="true" tabindex="-1"></a>M_pois_cozfm <span class="ot">&lt;-</span></span>
<span id="cb566-3"><a href="glm.html#cb566-3" aria-hidden="true" tabindex="-1"></a>  D_agg <span class="sc">%&gt;%</span></span>
<span id="cb566-4"><a href="glm.html#cb566-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(deviations <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> Design <span class="sc">+</span> Step_1<span class="sc">:</span>Design <span class="sc">+</span> Step_2<span class="sc">:</span>Design,</span>
<span id="cb566-5"><a href="glm.html#cb566-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">data =</span> .</span>
<span id="cb566-6"><a href="glm.html#cb566-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb566-7"><a href="glm.html#cb566-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb566-8"><a href="glm.html#cb566-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Linear regression model</span></span>
<span id="cb566-9"><a href="glm.html#cb566-9" aria-hidden="true" tabindex="-1"></a>M_pois_clzrm <span class="ot">&lt;-</span></span>
<span id="cb566-10"><a href="glm.html#cb566-10" aria-hidden="true" tabindex="-1"></a>  D_agg <span class="sc">%&gt;%</span></span>
<span id="cb566-11"><a href="glm.html#cb566-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(deviations <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> Design <span class="sc">+</span> session<span class="sc">:</span>Design, <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">data =</span> .)</span></code></pre></div>
<p>For the question of a constant rate of learning, we compare the one linear coefficient of the regression model with the two steps of the ordered factor model (Figure <a href="glm.html#fig:poisreg-9">7.11</a>):</p>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="glm.html#cb567-1" aria-hidden="true" tabindex="-1"></a>T_fixef <span class="ot">&lt;-</span></span>
<span id="cb567-2"><a href="glm.html#cb567-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(</span>
<span id="cb567-3"><a href="glm.html#cb567-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">posterior</span>(M_pois_cozfm),</span>
<span id="cb567-4"><a href="glm.html#cb567-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">posterior</span>(M_pois_clzrm)</span>
<span id="cb567-5"><a href="glm.html#cb567-5" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb567-6"><a href="glm.html#cb567-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(<span class="at">mean.func =</span> exp) <span class="sc">%&gt;%</span></span>
<span id="cb567-7"><a href="glm.html#cb567-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">separate</span>(fixef, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">&quot;Design&quot;</span>, <span class="st">&quot;Learning_unit&quot;</span>), <span class="at">sep =</span> <span class="st">&quot;:&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb567-8"><a href="glm.html#cb567-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">model =</span> <span class="fu">if_else</span>(<span class="fu">str_detect</span>(model, <span class="st">&quot;lz&quot;</span>),</span>
<span id="cb567-9"><a href="glm.html#cb567-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Continuous Model&quot;</span>, <span class="st">&quot;Ordered Factor Model&quot;</span></span>
<span id="cb567-10"><a href="glm.html#cb567-10" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">%&gt;%</span></span>
<span id="cb567-11"><a href="glm.html#cb567-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(Learning_unit)) <span class="sc">%&gt;%</span></span>
<span id="cb567-12"><a href="glm.html#cb567-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(Design, Learning_unit, model) <span class="sc">%&gt;%</span></span>
<span id="cb567-13"><a href="glm.html#cb567-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">discard_redundant</span>()</span>
<span id="cb567-14"><a href="glm.html#cb567-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb567-15"><a href="glm.html#cb567-15" aria-hidden="true" tabindex="-1"></a>T_fixef <span class="sc">%&gt;%</span></span>
<span id="cb567-16"><a href="glm.html#cb567-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(</span>
<span id="cb567-17"><a href="glm.html#cb567-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> Learning_unit, <span class="at">y =</span> center, <span class="at">col =</span> Design,</span>
<span id="cb567-18"><a href="glm.html#cb567-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper</span>
<span id="cb567-19"><a href="glm.html#cb567-19" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb567-20"><a href="glm.html#cb567-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_crossbar</span>(<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>, <span class="at">width =</span> .<span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb567-21"><a href="glm.html#cb567-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(. <span class="sc">~</span> model, <span class="at">scales =</span> <span class="st">&quot;free_x&quot;</span>) <span class="sc">+</span></span>
<span id="cb567-22"><a href="glm.html#cb567-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;rate of learning&quot;</span>, <span class="at">scales =</span> <span class="st">&quot;free_x&quot;</span>) <span class="sc">+</span></span>
<span id="cb567-23"><a href="glm.html#cb567-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fl">1.2</span>)</span></code></pre></div>
<div class="figure"><span id="fig:poisreg-9"></span>
<img src="Generalized_Linear_Models_files/figure-html/poisreg-9-1.png" alt="Learning rate estimates from a log-linearized continuous model and an OFM." width="90%" />
<p class="caption">
Figure 7.11: Learning rate estimates from a log-linearized continuous model and an OFM.
</p>
</div>
<p>With the Poisson OFM the learning rates are very similar for both steps, which means the learning rate is almost constant and taking one learning step as a unit is justified. Furthermore, the learning rate appears to also be almost constant across designs. If that is true, one implication is that the novel design is superior in many aspects, accelerated learning may not be one of them. The other implication is that we no longer need two learning rate parameters (session). The final model in this section is the most simple one, it even no longer contains conditional effects. Table <a href="glm.html#tab:poisreg-10">7.7</a> can now be summarized in three simple sentences:</p>
<ol style="list-style-type: decimal">
<li>On average there are 26 deviations with Legacy in the first session.</li>
<li>Novel reduces deviations to 25% at every stage of learning.</li>
<li>Per session, deviations are reduced by around 25%.</li>
</ol>
<div class="sourceCode" id="cb568"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb568-1"><a href="glm.html#cb568-1" aria-hidden="true" tabindex="-1"></a>M_pois_lzrm <span class="ot">&lt;-</span></span>
<span id="cb568-2"><a href="glm.html#cb568-2" aria-hidden="true" tabindex="-1"></a>  D_agg <span class="sc">%&gt;%</span></span>
<span id="cb568-3"><a href="glm.html#cb568-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(deviations <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Design <span class="sc">+</span> session,</span>
<span id="cb568-4"><a href="glm.html#cb568-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">data =</span> .</span>
<span id="cb568-5"><a href="glm.html#cb568-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb569-1"><a href="glm.html#cb569-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_pois_lzrm, <span class="at">mean.func =</span> exp)</span></code></pre></div>
<table>
<caption><span id="tab:poisreg-10">Table 7.7: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">b_Intercept</td>
<td align="left">Intercept</td>
<td align="right">26.250</td>
<td align="right">24.508</td>
<td align="right">27.989</td>
</tr>
<tr class="even">
<td align="left">b_DesignNovel</td>
<td align="left">DesignNovel</td>
<td align="right">0.247</td>
<td align="right">0.221</td>
<td align="right">0.276</td>
</tr>
<tr class="odd">
<td align="left">b_session</td>
<td align="left">session</td>
<td align="right">0.773</td>
<td align="right">0.731</td>
<td align="right">0.815</td>
</tr>
</tbody>
</table>
<!-- In the previous section, we have seen that the log-link of Poisson (and other) models makes us speak in multipliers, which actually make more sense. In this section we have seen, how such linearized model produces a much simpler model, that is as the same time more powerful as it allows to extrapolate the process. For reporting results that is great news; you don't have to explain ordinal factorial models or conditional effects and you can even tell the future to some extent. In \@ref(model-selection), we will come back to this case and actually demonstrate that a unconditional model with two coefficients beats all the more complex models in predictive accuracy. -->
<!-- ```{r} -->
<!-- T_pred <- -->
<!--   bind_rows( -->
<!--     post_pred(M_pois_ofm), -->
<!--     post_pred(M_pois_clzrm) -->
<!--   ) %>% -->
<!--   predict(scale = "lin_pred") %>% -->
<!--   left_join(D_Novel, by = "Obs") -->
<!-- T_pred %>% -->
<!--   ggplot(aes(x = Session, y = center, ymin = lower, ymax = upper, col = model)) + -->
<!--   geom_point(size = 3) + -->
<!--   geom_errorbar(width = .2, position = "dodge") -->
<!-- + -->
<!--   geom_jitter(data = D_Novel, aes(y = deviations)) -->
<!-- ``` -->
<p>For reporting results that is great news; you don’t have to explain ordinal factorial models or conditional effects and you can even tell the future to some extent. In <a href="wwm.html#model-selection">8.2.4</a>, we will come back to this case and actually demonstrate that a unconditional model with two coefficients beats all the more complex models in predictive accuracy.</p>
<!-- We will return to this case in section \@ref(ic) and show that the unconditional model, with just one learning rate parameter, cannot just extrapolate, but also does that with better predictive accuracy than the two more complex models.  -->
<!-- The bottom line is that log-linearized models are very useful when saturation effects are involved. That allows for models that are not only much leaner, but can also produce *forecasts* for future sessions. Again, we inject fabricated data (predictors only) into the model, extract the fitted responses and plot the results: -->
<!-- ```{r poisreg-11} -->
<!-- D_forecast <-  -->
<!--   expand_grid(session = c(0:10), -->
<!--               Design = c("Novel", "Legacy"), -->
<!--               Part = 1:50) %>%  -->
<!--   as_tbl_obs() %>%  -->
<!--   mutate(Session = as.factor(session + 1)) -->
<!-- T_pred <-  -->
<!--   post_pred(M_pois_lzrm, newdata = D_forecast) %>%  -->
<!--   left_join(D_forecast, by = "Obs") %>%  -->
<!--   group_by(Design, Session) %>%  -->
<!--   summarize(mean_deviations = mean(value)) -->
<!-- T_pred %>%  -->
<!--   mutate(linetype = if_else(as.numeric(Session) <= 3, "retrospective", "forecast")) %>%  -->
<!--   ggplot(aes(x = Session, col = Design, y = mean_deviations)) + -->
<!--   geom_line(aes(group = Design)) -->
<!-- ``` -->
<p>Normally, fitted responses are just retrospective. Here, we extrapolate the learning curve by fake data and obtain real <em>forecasts</em>. We can make more interesting comparison of the two devices. For example, notice that initial performance with Novel is around five deviations. With Legacy this level is reached only in the seventh session. We can say that the Novel design is always seven sessions of training ahead of Legacy.</p>
<p>The conclusion is that log-linearized scales can reduce or even entirely remove saturation effects, such that we can go with a simpler models, that are easier to explain and potentially more useful. Potentially, because we can not generally construct parametric learning curves with log-linear models. The crucial property here is that the lower bound is Zero. Some measures have a positive lower bound, which is constant and known, and can be translated to a lower bound of Zero. For example, path length, the minimum number of steps to find something on the internet is One, Path length can just shifted by One, e.g. <code>mutate(addional_steps = steps + 1)</code> to create a lower bound of Zero. This is different for Time-on-Task, which always has a strictly positive lower bound, which we don’t know and which probably varies between individuals. Learning curves that approach strictly positive asymptotes have the following mathematical form:</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="glm.html#cb570-1" aria-hidden="true" tabindex="-1"></a><span class="co"># devtools::install_github(&quot;schmettow/asymptote&quot;)</span></span>
<span id="cb570-2"><a href="glm.html#cb570-2" aria-hidden="true" tabindex="-1"></a>asymptote<span class="sc">::</span>ARY</span></code></pre></div>
<pre><code>## perf ~ ampl * exp(-rate * trial) + asym
## &lt;environment: namespace:asymptote&gt;</code></pre>
<p>The offset to Zero is in the summand <code>asym</code>, and because it is a summand this term cannot be linearized in a straight-forward manner. For general learning curves a truly non-linear model is required, not just a linearized. This can be constructed with the Brm engine, but is beyond the scope of this book.</p>
<!-- $$y_i = \phi\exp(\rho + x) + \omega$$ -->
<!-- where $\omega$ is the positive asymptote -->
<!-- ```{r} -->
<!-- # devtools::install_github("schmettow/asymptote") -->
<!-- asymptote::ARY -->
<!-- ``` -->
<!-- count errors, deviations or additional steps are great measures for experiments on learning. By model comparison against an OFM you first must assess whether the learning rate is approximately constant. If it is, then a log-linearized model renders a valid learning curve and can be used to predict future performance. When two designs are learned new, as in the IPump case, the difference can be interpreted as saved amount of training. Another interesting application would be how much training is needed for a novel design to actually beat a default design that users are highly trained with. -->
<!-- An interesting application arises, when a novel design is learned and compared against a default device, that users are highly trained with. A naive A/B study would come to te conclusion that the default design is much better. With learning curves,  would An interesting application of learning curves is to find out  -->
<!-- #### Monotony and quantiles -->
<!-- The  transformation of coefficients to the original scale has been applied to the point and range estimates as produced by the `fixef` command, that is *after* summarizing the posterior distribution. One may wonder if this is valid.  Would we get the same estimates when applying the mean function to all draws of the MCMC chain and then summarize? The general answer is that applying the mean function after summarizing is allowed if the summary function is invariant under the exponential function. <!-- 101 -> -->
<!-- For all GLM, the link and mean functions are monotonically increasing, with the consequence that the order of observations is preserved. Formally, for any two MCMC iterations $i$ and  $j$ for a parameter $\beta_i$: -->
<!-- $$ -->
<!-- \beta_{1i} < \beta_{1j} \rightarrow \exp(\beta_{1i}) < \exp(\beta_{1j}) -->
<!-- $$ -->
<!-- Recall that throughout this book, center and interval estimates have been obtained by simple quantiles, marking the points where 2.5%, 50% and 97.5% of all iterations are smaller. Order does not change with monotonous transformations, if 2.5% (50%, 97.5%) of draws are smaller on the linear scale, they will still be after applying the mean function. Quantiles are not affected by monotonous transformation and transformation after summary is therefore valid. Some researchers prefer the mode of the posterior to represent its center location. The mode is the point of highest density and does not rely on ranks, it is therefore even invariant under all transformations that preserve identity. -->
<!-- This is different for higher order methods for obtaining point and interval estimates. Most notably the mean and the highest posterior density intervals are not invariant to mean functions. When using those, the mean function must be applied before summarizing the posterior, which is inconvenient and inconsistent. -->
<!-- #### Zero inflation -->
<!-- #### ZAP ME -->
<!-- Imagine a study that examines the frequency of visits to the social media website Fakebook <-- 102 ->. While other researchers already set out to find predictive factors, like extrovert personality and the like, we are here interested in the frequency of daily use.   -->
<!-- ```{r opts.label = "invisible"} -->
<!-- attach(Chapter_GLM) -->
<!-- ``` -->
<!-- ```{r sim_ZI, ops.label = "rtut"} -->
<!-- sim_zi <- function( -->
<!--   beta_0  = 2.5, -->
<!--   N_Part  = 120, -->
<!--   p_zero  = .2, # proportion of non-users -->
<!--   sd_Part = 0.8, # individual differences (lp scale) -->
<!--   seed    = 23 # parameters passed on to simulate_1 -->
<!-- ){ -->
<!--   set.seed(seed) -->
<!--   tibble(Part = 1:N_Part, -->
<!--              theta = rnorm(N_Part, beta_0, sd_Part), -->
<!--              mu = exp(theta), -->
<!--              is_user = rbinom(N_Part, 1, 1 - p_zero),  -->
<!--              visits = 0 + is_user *rpois(N_Part, mu)) -->
<!-- } -->
<!-- D_zi <- sim_zi() -->
<!-- D_zi %>%  -->
<!--   ggplot(aes(x = visits)) + -->
<!--   geom_histogram() -->
<!-- ``` -->
<!-- ```{r ZI, opts.label = "future"} -->
<!-- M_zi <- -->
<!--   D_zi %>% brm(visits ~ 1 + (1|Part) , family = zero_inflated_poisson, data = .) -->
<!-- M_zi -->
<!-- sync_CE(Chapter_GLM, M_zi) -->
<!-- ``` -->
<!-- ```{r opts.label = "invisible"} -->
<!-- detach(Chapter_GLM) -->
<!-- sync_CE(Chapter_GLM, sim_zi, D_zi) -->
<!-- ``` -->
</div>
</div>
<div id="logistic-reg" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Logistic (aka Binomial) regression</h3>
<p>In the last section, we have seen how Poisson regression applies, when outcome variables are count numbers. More precisely, Poisson regression applies to count data, when there is no upper limit to counts (or if this limit is extremely large, as in the Smart Smurfer example). When the outcome variable is counts, but an upper limit exists and is known, <em>logistic regression</em> is an appropriate model. Such a situation often arises, when the counts are successes in a fixed number of trials. Logistic regression has the following properties:</p>
<ol style="list-style-type: decimal">
<li>The outcome variable has a zero lower bound and a fixed upper bound, e.g. number of trials <span class="math inline">\(k\)</span>.</li>
<li>The linear predictors are on a <em>logit scale</em> also called <em>log-odds</em>, which is reversed by a <em>logistic function</em>.</li>
<li>The random component follows a <em>binomial distribution</em>.</li>
<li>Due to the former, the variance of randomness is largest at <span class="math inline">\(\mu = 0.5\)</span> or <span class="math inline">\(\eta = 1\)</span> and declines towards both boundaries, taking a characteristic cigar shape.</li>
</ol>
<p>Logistic regression applies for discrete outcomes, just like Poisson regression. The difference is that logistic regression has a finite number of possible outcomes, which is the number of trials plus one (no successes). In the following section, I will first introduce logistic regression for when there is only one trial per observation, with two possible outcomes. That is called <em>dichotomous outcomes</em>. Dichotomous outcomes are not limited to the Zero or One, Miss or Success, but apply to all outcomes that fall into two classes. In the subsequent section, we will look at logistic regression for when there is more than one trial. The most difficult part of logistic regression is to report the estimated coefficients in an intelligible manner, which will be covered in the final section.</p>
<div id="dich-outcomes" class="section level4" number="7.2.2.1">
<h4><span class="header-section-number">7.2.2.1</span> Dichotomous outcomes</h4>
<p>The most simple form of successes-in-trials measure is when there is only one trial. This is called a dichtotomous variable, and that is very common:</p>
<ul>
<li>a user is successful at a task, or fails</li>
<li>a visitor returns to a website or does not</li>
<li>a usability problem is discovered or remains unseen</li>
<li>a driver brakes just in time or crashes</li>
<li>a customer recommends a product to a friend or does not</li>
<li>a user starts searching on a website by keyword or by traversing links</li>
</ul>
<p>Often, dichotomous outcome variables have a quantitative notion in the sense of more or less desirable. When the outcome casts a positive light on the design, by convention it is coded as 1, otherwise 0. But, the dichotomy can also be two equal alternatives, such as whether a user starts a web inquiry by keyword search or by following a link. Let’s take this as an example.</p>
<p>Research on search strategies of web users revealed that they are quite ecclectic regarding their method to find a piece of information. In particular, most people use keyword search and link navigation at occasion. Web users are also known to be impatient companions, who build a first judgment very quickly and swiftly turn to a competitor’s site, when the first impression leaves something to be desired. Therefore, it can be valuable to know what method the majority of users prefer, initially.</p>
<p>For this purpose, we can classify users by what method they start with when given a search task during a usability study. As there exist only two options, keyword search or following links, we can capture the outcome in a dichotomous response variable. Below is the simulation of a small data set, where 40% of users initially prefer keyword search (Table <a href="glm.html#tab:logreg-1">7.8</a>)).</p>
<!-- extremely impatient companions. They scan a page for visual features, rather than reading [REF: high school students information mall]. Visitors of websites build their first judgement in a time as short as 17ms [REF: Tuch presentation time]. For e-commerce that is a highly important fact to know about their customers and nowadays practically all commercial websites shine with a pleasing visual appearance. But, how would one measure the gratitude of a visitor who actually used the website and may have something to tell beyond visual appeal? -->
<!-- Let us consider an example: early research on foraging strategies of web users revealed that they are extremely impatient companions. They scan a page for visual features, rather than reading [REF: high school students information mall]. Visitors of websites build their first judgement in a time as short as 17ms [REF: Tuch presentation time]. For e-commerce that is a highly important fact to know about their customers and nowadays practically all commercial websites shine with a pleasing visual appearance. But, how would one measure the gratitude of a visitor who actually used the website and may have something to tell beyond visual appeal? -->
<!-- A simple measure for gratitude simply is when a visitor returns to buy more. This is usually a highly available measure, too, as any skilled web administrators can distill such data from the server logfiles with little effort.  -->
<!-- First, all unique visitors are extracted and if the same visitor returns within a given period of time, this is coded as a success (one) or otherwise failure (zero). We simulate such a data set:  -->
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb572-1"><a href="glm.html#cb572-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb572-2"><a href="glm.html#cb572-2" aria-hidden="true" tabindex="-1"></a>D_web <span class="ot">&lt;-</span></span>
<span id="cb572-3"><a href="glm.html#cb572-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">init_keyword =</span> <span class="fu">rbinom</span>(<span class="dv">100</span>, <span class="dv">1</span>, .<span class="dv">4</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb572-4"><a href="glm.html#cb572-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tbl_obs</span>()</span>
<span id="cb572-5"><a href="glm.html#cb572-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb572-6"><a href="glm.html#cb572-6" aria-hidden="true" tabindex="-1"></a>D_web</span></code></pre></div>
<table>
<caption><span id="tab:logreg-1">Table 7.8: </span>Data set with 2 variables, showing 8 of 100 observations.</caption>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="right">init_keyword</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">18</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">21</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">24</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">40</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">55</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">57</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">69</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">82</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>For estimating the proportion of the two classes of users, we run a logistic regression grand mean model and inspect the coefficient table. Note that <em>logistic</em> regression is called so by its mean function (inverse link), not its shape of randomness. For counts with lower and upper border, the <em>binomial family</em> applies.</p>
<div class="sourceCode" id="cb573"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb573-1"><a href="glm.html#cb573-1" aria-hidden="true" tabindex="-1"></a>M_web <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(init_keyword <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb573-2"><a href="glm.html#cb573-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_web,</span>
<span id="cb573-3"><a href="glm.html#cb573-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> binomial</span>
<span id="cb573-4"><a href="glm.html#cb573-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="glm.html#cb574-1" aria-hidden="true" tabindex="-1"></a><span class="fu">clu</span>(M_web)</span></code></pre></div>
<table>
<caption><span id="tab:logreg-2">Table 7.9: </span>Parameter estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">-0.167</td>
<td align="right">-0.582</td>
<td align="right">0.255</td>
</tr>
</tbody>
</table>
<p>Clearly, the Intercept parameter is not a proportion, as that forbids negative values. Like with Poisson regression, the coefficient is on a linearized scale with infinite range. It is the <em>logit</em> functions that inflates the response range <span class="math inline">\(\mu_i \in [0;1]\)</span> to the linear predictor <em>linear predictor</em> scale <span class="math inline">\(\eta_i \in [-\infty; \infty]\)</span>.</p>
<p><span class="math display">\[
\eta_i = \textrm{logit}(\mu_i) = \log \frac{\mu_i} {1-\mu_i}
\]</span></p>
<p>Note that the fraction <span class="math inline">\(\frac{\mu_i} {1-\mu_i}\)</span> is the proportion of keyword search divided by the proportion of following links and is called an <em>odds</em>. The logit function is therefore often called <em>log-odds</em>. In section <a href="glm.html#talking-odds">7.2.2.3</a>, we will see how we can report logistic regression results as odds. In the case of our simple GMM, we can directly report the results as proportions, which requires to apply the <em>mean function</em>, which is the inverse of the logit, also known as the <em>logistic function</em>:</p>
<p><span class="math display">\[
\mu_i = \textrm{logit}^{-1}(\eta_i) = \frac{\exp \eta_1} {\exp \eta_i + 1}
\]</span></p>
<!-- \@ref(logit_logist) shows link and mean functions side-by-side. -->
<!-- ```{r logit_logist} -->
<!-- grid.arrange( -->
<!--   ggplot(data.frame(mu=c(0, 1)), aes(x = mu)) +  -->
<!--     stat_function(fun = mascutils::logit) + -->
<!--     xlab(expression(mu)) + ylab(expression(eta)) + -->
<!--     ggtitle("logit link function"), -->
<!--   ggplot(data.frame(eta=c(-5, 5)), aes(x = eta)) +  -->
<!--     stat_function(fun = mascutils::inv_logit) +  -->
<!--     xlab(expression(mu)) + ylab(expression(eta)) + -->
<!--     ggtitle("logistic mean function"), -->
<!--   nrow = 1) -->
<!-- ``` -->
<p>In GMM, <span class="math inline">\(\eta_i = \beta_0\)</span> and we can directly obtain the estimated proportion by applying the logistic function to the Intercept. The <code>clu</code> command lets you pass on a mean function, resulting in Table <a href="glm.html#tab:logreg-3">7.10</a>.</p>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb575-1"><a href="glm.html#cb575-1" aria-hidden="true" tabindex="-1"></a><span class="fu">posterior</span>(M_web) <span class="sc">%&gt;%</span> <span class="fu">clu</span>(<span class="at">mean.func =</span> inv_logit)</span></code></pre></div>
<table>
<caption><span id="tab:logreg-3">Table 7.10: </span>Parameter estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">0.458</td>
<td align="right">0.359</td>
<td align="right">0.563</td>
</tr>
</tbody>
</table>
<p>From the GMM we retrieve one estimate that reflects the <em>proportion</em> to start by keyword search. That works for absolute group mean, but most of the time, logistic regression coefficients are <em>exponentiated</em> and read as <em>odds</em> <a href="glm.html#talking-odds">7.2.2.3</a>.</p>
<p>As a side note, proportions could also be called probabilities, like “with 40% probability a user starts by keyword search.” However, I urge anyone to avoid speaking of logistic regression coefficients as probabilities. While mathematically this is correct, for the audience it can easily cause confusion with certainty or, beware of this, the p-value.</p>
<p>The apt reader may have noticed that the returners data set has been simulated with an exact return rate of 40%. Despite the sample size of 100, the center estimate seems rather off and hampered by considerable uncertainty. In computer science jargon, every dichotomous observation accounts to a <em>bit</em>, which is the smallest amount of information possible. Because the information of a single dichotomous observation is so sparse, large samples are important when dealing with dichotomous outcomes. Large samples can mean testing many users, or giving every user more than one trial.</p>
</div>
<div id="successes-trials" class="section level4" number="7.2.2.2">
<h4><span class="header-section-number">7.2.2.2</span> Successes in a number of trials</h4>
<p>If we repeatedly observe a dichotomous response, we can summarize the results as <em>successes-in trials</em>, like:</p>
<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb576-1"><a href="glm.html#cb576-1" aria-hidden="true" tabindex="-1"></a>responses <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb576-2"><a href="glm.html#cb576-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sum</span>(responses), <span class="st">&quot;successes in&quot;</span>, <span class="fu">length</span>(responses), <span class="st">&quot;trials&quot;</span>)</span></code></pre></div>
<pre><code>## 4 successes in 6 trials</code></pre>
<p>Imagine we had conducted an extended version of the previous experiment, where users get set of ten tasks and we observe their initial behavior every time they open a new website. As such tasks sometimes take very long, it may also happen that a participant cannot finish all ten tasks within time. That means, we potentially have a different number of attempts per participant, which we simulate as Binomial random numbers (Table <a href="glm.html#tab:logreg-6">7.11</a>).</p>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb578-1"><a href="glm.html#cb578-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb578-2"><a href="glm.html#cb578-2" aria-hidden="true" tabindex="-1"></a>D_web_ex <span class="ot">&lt;-</span></span>
<span id="cb578-3"><a href="glm.html#cb578-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb578-4"><a href="glm.html#cb578-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">trials =</span> <span class="fu">round</span>(<span class="fu">runif</span>(<span class="dv">100</span>, <span class="dv">7</span>, <span class="dv">10</span>), <span class="dv">0</span>),</span>
<span id="cb578-5"><a href="glm.html#cb578-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">init_keyword =</span> <span class="fu">rbinom</span>(<span class="dv">100</span>, trials, .<span class="dv">4</span>),</span>
<span id="cb578-6"><a href="glm.html#cb578-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">init_link =</span> trials <span class="sc">-</span> init_keyword</span>
<span id="cb578-7"><a href="glm.html#cb578-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb578-8"><a href="glm.html#cb578-8" aria-hidden="true" tabindex="-1"></a>  mascutils<span class="sc">::</span><span class="fu">as_tbl_obs</span>()</span>
<span id="cb578-9"><a href="glm.html#cb578-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb578-10"><a href="glm.html#cb578-10" aria-hidden="true" tabindex="-1"></a>D_web_ex</span></code></pre></div>
<table>
<caption><span id="tab:logreg-6">Table 7.11: </span>Data set with 4 variables, showing 8 of 100 observations.</caption>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="right">trials</th>
<th align="right">init_keyword</th>
<th align="right">init_link</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">14</td>
<td align="right">8</td>
<td align="right">3</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="right">24</td>
<td align="right">10</td>
<td align="right">4</td>
<td align="right">6</td>
</tr>
<tr class="odd">
<td align="right">25</td>
<td align="right">7</td>
<td align="right">4</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="right">34</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">61</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="right">62</td>
<td align="right">10</td>
<td align="right">4</td>
<td align="right">6</td>
</tr>
<tr class="odd">
<td align="right">92</td>
<td align="right">7</td>
<td align="right">4</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="right">98</td>
<td align="right">9</td>
<td align="right">2</td>
<td align="right">7</td>
</tr>
</tbody>
</table>
<!-- Recall the fictional jump-and-run game  *smart smurfer* in \@ref(poisson_dist): the goal of the game is that players collect items and for the user experience it is crucial that this is neither too difficult nor too easy. Imagine, that for adjusting the difficulty level, the developers conduct a quick evaluation study, where they place a number of items (trials) in the game and the success rate of a single player is observed in a series of 15 game sessions. We simulate such a data set: -->
<!-- Per session the player has a number of opportunities for collecting an item, which makes it a repeated measures situation. One might expect that we need to include random effects into the model. Later we will see that this is necessary when the sessions were observed on a sample of players with different abilities. However, as long as one can reasonably assume the chance of catching an item to be constant across all sessions, plain logistic regression can deal with *successes in multiple trials*.  -->
<p>In order to estimate a model on the proportion of successes in a number of trials, somehow this needs to be specified. That is done indirectly via the number of “failures,” in this case this is the number of times a link was followed, rather than a search query. The response side of the model formula takes this in as an array with two columns, which is generally constructed as <code>cbind(successes, failures)</code>. We estimate a Binomial GMM and extract the coefficient as a proportion (Table <a href="glm.html#tab:logreg-7">7.12</a>):</p>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb579-1"><a href="glm.html#cb579-1" aria-hidden="true" tabindex="-1"></a>M_web_ex <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(<span class="fu">cbind</span>(init_keyword, init_link) <span class="sc">~</span> <span class="dv">1</span>, <span class="co"># &lt;--</span></span>
<span id="cb579-2"><a href="glm.html#cb579-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> binomial,</span>
<span id="cb579-3"><a href="glm.html#cb579-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_web_ex</span>
<span id="cb579-4"><a href="glm.html#cb579-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb580"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb580-1"><a href="glm.html#cb580-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(M_web_ex,</span>
<span id="cb580-2"><a href="glm.html#cb580-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">mean.func =</span> inv_logit</span>
<span id="cb580-3"><a href="glm.html#cb580-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<table>
<caption><span id="tab:logreg-7">Table 7.12: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">type</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">object</td>
<td align="left">fixef</td>
<td align="left">Intercept</td>
<td align="right">0.406</td>
<td align="right">0.374</td>
<td align="right">0.442</td>
</tr>
</tbody>
</table>
<p>With a ten-fold of data, as compared to the dichotomous model, the estimate is much closer to the real value and the credibility intervals tightened up, too. By using the inverse logit, we can readily report the results as proportions. Again, make no mistake, this really only works for GMMs and AMMs. when effects come into play and additional coefficients are being estimated, reporting proportions does no longer work. Instead, we have to learn to talk odds.</p>
<p>Recall, how we characterized tasks as populations and introduced multi-level models with task-level coefficients (<a href="mlm.html#non-human-populations">6.5</a>). It could appear as if a multiple-trials Binomial model is an alternative to multi-level modeling. It is not! First of all, there are no task-level estimates. In contrast, the above model assumes that chance of success is constant across tasks. If these are not simple experimental tasks, all quite the same, this assumption most is violated. More specifically, recall that the variance of Binomial distributions is fixed by the mean. If tasks vary, this will create <em>over-dispersion</em> ((<a href="glm.html#overdispersion">7.2.3</a>)).</p>
</div>
<div id="talking-odds" class="section level4" number="7.2.2.3">
<h4><span class="header-section-number">7.2.2.3</span> Talking odds</h4>
<p>When presenting results of a statistical analysis, the linear predictor is likely to cause trouble, at least when the audience is interested in real quantities. Coefficients on a logit-linearized scale have only very general intuition:</p>
<ul>
<li>zero marks a 50% chance</li>
<li>positive values increase the chance, negative decrease</li>
<li>bigger effects have larger absolute values</li>
</ul>
<p>That is sufficient for purely ranking predictors by relative impact (if on a comparable scale of measurement), or plain hypothesis testing, but it does not connect well with quantities a decision maker is concerned with. Let’s see this at the example of the infusion pump study, where some relevant questions for the evaluation of failures are</p>
<ol style="list-style-type: decimal">
<li>What is the expected frequency of failure on first use?</li>
<li>The novel design reduces failures, but is it sufficient?</li>
<li>Is frequency of failures sufficiently reduced after two training sessions?</li>
</ol>
<p>In the comparison of two medical infusion pumps (@ref(slope_RE)) 25 nurses completed a set of eight tasks repeatedly over three sessions. In @ref(slope_RE) a multi-level model was estimated on the workload outcome. It is tempting to apply the same structural model to success in task completion, using binomial random patterns and logit links.</p>
<pre><code>completion ~ Design*Session + (Design*Session|Part) + (Design*Session|Task)</code></pre>
<p>Such a model is practically impossible to estimate, because dichotomous variables are so scarce in information. Two populations encounter each other in the model: participants and tasks, with 6 observations per combination (6 bit). We should not expect to get reasonably certain estimates on that level and, in fact, the chains will not even mix well. The situation is a little better on the population level: every one of the six coefficients is estimated on 400 bit of raw information. We compromise here by estimating the full model on population level and do only intercept random effects to account for gross differences between participants and tasks (Table <a href="glm.html#tab:logreg-9">7.13</a>).</p>
<div class="sourceCode" id="cb582"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb582-1"><a href="glm.html#cb582-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(IPump)</span></code></pre></div>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="glm.html#cb583-1" aria-hidden="true" tabindex="-1"></a>M_cmpl <span class="ot">&lt;-</span></span>
<span id="cb583-2"><a href="glm.html#cb583-2" aria-hidden="true" tabindex="-1"></a>  D_pumps <span class="sc">%&gt;%</span></span>
<span id="cb583-3"><a href="glm.html#cb583-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glmer</span>(completion <span class="sc">~</span> Design <span class="sc">*</span> Session <span class="sc">+</span></span>
<span id="cb583-4"><a href="glm.html#cb583-4" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span> <span class="sc">|</span> Part) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Task),</span>
<span id="cb583-5"><a href="glm.html#cb583-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> binomial,</span>
<span id="cb583-6"><a href="glm.html#cb583-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> .</span>
<span id="cb583-7"><a href="glm.html#cb583-7" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb584"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb584-1"><a href="glm.html#cb584-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(M_cmpl)</span></code></pre></div>
<table>
<caption><span id="tab:logreg-9">Table 7.13: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">1.317</td>
<td align="right">0.131</td>
<td align="right">2.522</td>
</tr>
<tr class="even">
<td align="left">DesignNovel</td>
<td align="right">0.403</td>
<td align="right">0.080</td>
<td align="right">0.719</td>
</tr>
<tr class="odd">
<td align="left">Session2-1</td>
<td align="right">0.692</td>
<td align="right">0.138</td>
<td align="right">1.237</td>
</tr>
<tr class="even">
<td align="left">Session3-2</td>
<td align="right">-0.079</td>
<td align="right">-0.635</td>
<td align="right">0.457</td>
</tr>
<tr class="odd">
<td align="left">DesignNovel:Session2-1</td>
<td align="right">-0.301</td>
<td align="right">-1.093</td>
<td align="right">0.494</td>
</tr>
<tr class="even">
<td align="left">DesignNovel:Session3-2</td>
<td align="right">0.286</td>
<td align="right">-0.546</td>
<td align="right">1.079</td>
</tr>
</tbody>
</table>
<p>The result is one absolute group mean, the Intercept, and five effects, which are mean differences on the logit-linearized scale <span class="math inline">\(\eta_i\)</span>. If we want to report absolute group means, we can use the inverse logit function to obtain proportions, but for that we have to <em>first do the linear combination followed by the transformation</em>, for example:</p>
<ul>
<li>the completion rate in the first legacy session is 0.789</li>
<li>in novel/session 1: <code>logist(Intercept + DesignNovel)</code> = 0.848</li>
<li>in novel/session 2: <code>logist(Intercept + DesignNovel + Session2-1 + DesignNovel:Session2-1)</code> = 0.892</li>
<li>in legacy/session 3: <code>logist(Intercept + DesignNovel + Session2-1)</code> = 0.873</li>
</ul>
<p>Above we have used the inverse-logit (aka logistic) mean function to elevate the absolute group means to proportions. This is an intuitive scale, but unfortunately, the mean function does not apply to individual effects. It is for example, <em>incorrect</em> to apply it like: “the novel pumps proportion of failures in the first session increases by <code>logist(DesignNovel)</code> = 0.6.”</p>
<p>Log-odds are compound function. The inner part of the function, the <em>odds</em>, are the chance of success divided by the chance of failure. Especially in the anglo-american culture, odds are a rather common way to express ones chances in a game, say:</p>
<ul>
<li>odds are 1 against 1 that the coin flip produces Head. If you place €1 on Head, I put €1 on tail.</li>
<li>odds are 1 against 12 that Santa wins the dog race. If you place 1€ on Santa, I place €12 against.</li>
<li>46% on Red. 54% on Blue</li>
</ul>
<p>Reversing only the logarithm produces odds, as in Table <a href="glm.html#tab:logreg-10">7.14</a></p>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb585-1"><a href="glm.html#cb585-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(M_cmpl, <span class="at">mean.func =</span> exp)</span></code></pre></div>
<table>
<caption><span id="tab:logreg-10">Table 7.14: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">3.733</td>
<td align="right">1.140</td>
<td align="right">12.45</td>
</tr>
<tr class="even">
<td align="left">DesignNovel</td>
<td align="right">1.497</td>
<td align="right">1.083</td>
<td align="right">2.05</td>
</tr>
<tr class="odd">
<td align="left">Session2-1</td>
<td align="right">1.999</td>
<td align="right">1.148</td>
<td align="right">3.44</td>
</tr>
<tr class="even">
<td align="left">Session3-2</td>
<td align="right">0.924</td>
<td align="right">0.530</td>
<td align="right">1.58</td>
</tr>
<tr class="odd">
<td align="left">DesignNovel:Session2-1</td>
<td align="right">0.740</td>
<td align="right">0.335</td>
<td align="right">1.64</td>
</tr>
<tr class="even">
<td align="left">DesignNovel:Session3-2</td>
<td align="right">1.331</td>
<td align="right">0.579</td>
<td align="right">2.94</td>
</tr>
</tbody>
</table>
<p>But is it legitimate to apply the transformation on individual coefficients in order to speak of changes of odds? The following arithmetic law tells that what is a sum on the log-odds scale, is multiplication on the scale of odds:</p>
<p><span class="math display">\[
\exp(x + y) = \exp(x)\exp(y)
\]</span></p>
<p>Consequently, we may speak of changes of odds using <em>multiplicative language</em>:</p>
<ul>
<li>If you place €100 on failure in the next task with the legacy design in session 1, I place €373.303 on success.</li>
<li>The odds of success with the novel design increase by <em>factor</em> 1.497. Now, I would place <span class="math inline">\(373.303 \times 1.497\)</span> = €558.835 on success.</li>
<li>On success with the novel design in session 2, I would place <span class="math inline">\(373.303 \times 1.497 \times 1.999 \times 0.74\)</span> = €826.809 on success.</li>
</ul>
<p>Once, we have transformed the coefficients to the odds scale, we can read coefficients as multipliers and speak of them in hard currency.</p>
<div class="sourceCode" id="cb586"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>To summarize: Logistic regression applies when the basic observations falls into two classes. For any research design involving such outcomes, repetition is highly recommended, and outcomes can be summarized into successes-in-trials. Reporting coefficients on the logit scale is only useful when nobody is interested in intelligible effects sizes. How to report the results depends on the research question. If one is interested in proportions per group, the inverse logit applies to the absolute group means and this can be easily understood. If one wants to talk about effects or differences, such as the amount of improvement with a novel design, only the logarithm is reversed, and effects are reported as odds. Depending on the audience, this may be more or less intuitive, but it can always be embedded in a wager for illustration.</p>
<p>Logistic regression is known in many areas of application, as well as for some interesting extensions.</p>
<ul>
<li>In epidemiology research, logistic regression is the indispensable tool for several central outcomes, such as hospitalization, mortality, infection and recovery.</li>
<li>In psychometrics, the famous Rasch model applies for measuring a persons ability by the number of correct answers in a test. A Rasch model is just a cross-classified multilevel logistic regression <a href="mlm.html#designometrix">6.8.4</a>.</li>
<li>If the outcome is a classification with more than two classes, <em>multi-nomial regression</em> is an extension of logistic regression.</li>
<li>In section <a href="lm.html#ofm">4.3.5</a>, we will encounter <em>ordinal logistic regression</em>, which applies for classifications with an order, such as responses on Likert scales.</li>
</ul>
<p>One frequent problem when using logistic regression on successes-in-trials outcomes is that the assumption of a Binomial shape of randomness is violated by <em>over-dispersion</em>. Like Poisson distributions, Binomial distributions have a variance tightly linked to the mean, but frequently there is more variance than allowed, for example when tasks or test items vary in difficulty. In the following section two solutions to the problem are introduces: <em>beta-binomial regression</em> and <em>observation-level random effects</em>.</p>
<!--What is reported here, are the coefficients on the *linear predictor scale*, with the *logit* as link function. 
-->
<!-- \@ref(gross_task_completion) is chilling. Successful completion is the exception rather than the rule. Besides that, there is little difference between designs and sessions. For the mere sake of illustration we run a first logistic regression:





The Design factor is centered at zero and is highly uncertain. There simply is no clear conclusion possible. However, if Binomial distribution is underlying the logistic regression, should we not expect probability of success? Clearly that is not the case, as the lower credibility limits are negative. 



<!-- here: Infusion pump -->
<!-- For the two logistic GMMs, it seems one can short-circuit $\eta$ and state $\mu = \textrm{logist}(\eta)$ right away. This is not so, but a mere consequence of them only being the intercept $\mu = \beta_0$. It is also legitimate for absolute group mean models (AGMs), but not for any other coefficients with contrast settings. In order to find a specific predicted value, the mean function is applied to the linear combination of coefficients. For example, the 

However, in the final example of logistic regression we will see an alternative way to speak of logistic coefficients, *odds*, which allows transformation of individual coefficients.



<!--What is reported here, are the coefficients on the *linear predictor scale*, with the *logit* as link function. The logit is also called *log odds*: $\textrm{logit}(x) = \log(p(1-p))$. The inner part of the function, the *odds* are the chance of success divided by the chance of failure. That is a rather common way to express ones chances in a bet, say: "odds are 1 against 12 that Santa wins the dog race". If you place 1€ on Santa, I place €12 against.

If the coefficients are log odds, than we can convert them to odds by the inverse of the logarithm, the exponential function, like in the following call of `fixef`:



If you place €1000 on the next sequence with the legacy design being successful (which is the reference group), I place €103 against. What would be the odds of a successful sequence with the novel design? There are two ways to calculate. The first way is to do the respective linear combination on the linear predictor estimates, and than apply the exponential transformation:

$$\textrm(Odds(success|Novel)) = \exp(\beta_{0} + \beta_1) =
\exp(0) = 
1$$

The second way makes use of the following arithmetic law:

$$\exp(x + y) = \exp(x)\exp(y)$$

Once, we have transformed the linear predictor coefficients to the odds scale, we receive the combination of odds by multiplication:

$$\textrm(Odds(success|Novel)) = \exp(\beta_{0})\exp(\beta_1)) =
1$$
-->
<!--
Now we simulate a data set with the same research design, but the outcome variable is successes in ten tasks. For instance, the designs are three websites and participants are given ten information retrieval tasks.

Because the research design is the same, we can keep the two object data frames. The only issue is that $\beta$ and $\theta$ are on the scale of the linear predictor $\eta$, which is linked to fitted responses $\mu$ by the *logit link function*:

$$\eta = \log(\mu/(1-\mu))$$ 

fitted responses $\mu$ are computed from $\eta$ through the so-called *mean function*, which is the inverse of the logit, the *logistic tranformation*:

$$\mu = \exp(\eta)/{1 + \exp(eta)}$$ 

Note that values of \mu from the Gaussian data set above, are in the range of 300 - 350, which would result in chances of success of virtually 100%. To get more realistic values, we first divide $\eta$ by 100. 


from the experiment on Visual EDA:

Logistic regression coefficients are on a logit scale. For the interpretation, we want to back-transform to the original scale, which is: probability of the assumption to be rejected. In order to make statements on the probability scale, we backtransform using the inverse function of the logit, the logistic function:
mu_x = exp(eta_x)/(1 + exp(eta_x))
eta in the above formula is called the "linear predictor". The linear predictor is the value you get when you do operations on the coefficients. The most simple application is computing the overall probability for a rejection. This is represented by the Intercept parameter. In the table below, this parameter accounts to practically zero, on the logit scale. Back-transforming to the probability scale gives exactly a chance of 50%:
exp(0)/(1 + exp(0)) = 1/2
 So, an intercept of zero always means that chances are equal, which is the same as saying: the coefficient does not add any information. The same is true for parameters: the closer a coefficient is to zero, the less does it influence the prediction of rejection. Like with general linear models, parameters close to zero have "no effect" and can even be removed to simplify a model (we do not do that here).
Note, that the intercept in our model represents the situation that skew is 0 and N = 0. A more useful baseline rate for rejection rate would be at skew = 0 and the smallest sample size N = 10.
We can compute any prediction we want by combining the parameters with the predictors. But, the other coefficients in the model represent differences, not absolute values. Therefore, when predicting a certain event, let's say rejection at a skew = 0 and N = 50, we first have to compute the linear predictor eta, then transform to get predicted value mu. 
eta = 0.042 + Skew * 0 + Sampel size * 10
mu = logist(eta)
To give another example, the chance of rejection at skew = .5 and N = 50 is computed as:
eta = 0.042 + Skew * 0.5 + Sampel size * 50
mu = logist(eta)
Watch out! It is a severe mistake to transform coefficients individually, then do the linear combination.
eta = logist(Skew * 0.5) + logist(Sampel size * 50)
In any case, it is much preferred to report the estimates on the probability scale, as this is right-away interpretable. Credibility limits can be reported on the linear predictor scale. 

Group effects indicate how homogeneous the overall pattern (Table 1) is on individual units in the data set. Relevant units here are the participant and stimuli. For exanple, the relatively large coefficient skew means that people differ a lot in how much their response is influenced by skew (which is desireable): The strong effect on stimuli means that (after taking skew and N into account) stimuli differ a lot in which response they provoke. It sounds a bit paradox, but that means that stimuli differ systematically: there exist properties other then skew and N, that trigger a response.

-->
</div>
</div>
<div id="overdispersion" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Modelling overdispersion</h3>
<!-- With count data, we usually expect the variance of randomness to be tied to the location (or magnitude). The Poisson distribution  is very strict in the sense that the variance equals the mean.  -->
<p>Poisson and binomial distributions are one-parameter distributions. As there is only one parameter, it is impossible to choose location and dispersion independently. In effect, both properties are tightly entangled. For Poisson distributions they are even the same.</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;\sim \textrm{Poisson} (\lambda) \\
\textrm {Var}(Y) &amp;= \textrm {Mean}(Y) = \lambda
\end{aligned}
\]</span></p>
<p>For binomial distributions, mean and variance both depend on probability <span class="math inline">\(p\)</span> and are entangled in cigar shaped form, as the dispersion shrinks when approaching the lower or upper boundaries. Binomial variance is also affected by the number of trials <span class="math inline">\(k\)</span>, but that hardly matters as the value of <span class="math inline">\(k\)</span> is usually not up for estimation, but known.</p>
<p><span class="math display">\[ 
\begin{aligned}
Y &amp;\sim \textrm{Binom} (p, k) \\
\textrm {Mean}(Y) &amp;= kp \\
\textrm {Var}(Y) &amp;= kp(1 - p) 
\\&amp;= \textrm {Mean}(Y) (1-p)\\
\end{aligned}
\]</span></p>
<p>In real data, we often see similar relationships between variance and mean, except that variance is inflated by some additional positive factor, which is called <em>overdispersion</em>. Poisson or Binomial distribution cannot render inflated data, and using them on over-dispersed data is a serious mistake. Fortunately, there exist two solutions to the problem, which I will introduce in the following three sections. In the first two sections, we will replace the one-parameter distribution with a <em>two-parameter distribution</em>, where the second parameter represents the variance inflation. The second method is to use <em>observation-level random effects</em>, which draws from the multi-level modeling toolbox.</p>
<p>Let me give you an example to illustrate the two methods. It is common saying that some people attract mosquito bites more than others. But is that really true? A simple lab experiment could be done to test the “Sweet Blood” theory. A sample of participants are exposed to a pack of mosquitoes under carefully controlled conditions (time of day, environmental condition, hungriness of mosquitoes). We don’t know the mechanisms that makes the blood sweeter, and hence cannot measure it. In the simulation below, it is just assumed that there is a such a property, but in a real study we would not know.</p>
<p>The following simulation function works by using a two-parameter distribution, that have the same properties as Poisson (or binomial) distributions. Negative-binomial distributions are discrete distributions with a lower bound of zero, just like Poisson distributions. They also have the same location parameter <span class="math inline">\(\mu\)</span>, but a new parameter <code>size</code>, which re-scales the scale of measurement. When the scale of measurement is down-scaled, the distribution becomes relatively wider. When size approaches infinity, we are left with a plain Poisson variance. The following data simulation samples Sweet-blood data from a negative-binomial distribution with a size of 3 (Table <a href="glm.html#tab:logreg-10a">7.15</a>).</p>
<div class="sourceCode" id="cb587"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb587-1"><a href="glm.html#cb587-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb587-2"><a href="glm.html#cb587-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">400</span></span>
<span id="cb587-3"><a href="glm.html#cb587-3" aria-hidden="true" tabindex="-1"></a>avg_sweet <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb587-4"><a href="glm.html#cb587-4" aria-hidden="true" tabindex="-1"></a>size <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb587-5"><a href="glm.html#cb587-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb587-6"><a href="glm.html#cb587-6" aria-hidden="true" tabindex="-1"></a>Sweet_blood_nbin <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb587-7"><a href="glm.html#cb587-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">Method =</span> <span class="st">&quot;NegBinomial&quot;</span>,</span>
<span id="cb587-8"><a href="glm.html#cb587-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">bites =</span> <span class="fu">rnbinom</span>(</span>
<span id="cb587-9"><a href="glm.html#cb587-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> N,</span>
<span id="cb587-10"><a href="glm.html#cb587-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">mu =</span> avg_sweet,</span>
<span id="cb587-11"><a href="glm.html#cb587-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> size</span>
<span id="cb587-12"><a href="glm.html#cb587-12" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb587-13"><a href="glm.html#cb587-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb587-14"><a href="glm.html#cb587-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb587-15"><a href="glm.html#cb587-15" aria-hidden="true" tabindex="-1"></a>Sweet_blood_nbin <span class="sc">%&gt;%</span></span>
<span id="cb587-16"><a href="glm.html#cb587-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="fu">mean</span>(bites), <span class="fu">var</span>(bites)) </span></code></pre></div>
<table>
<caption><span id="tab:logreg-10a">Table 7.15: </span>Overdispersed counts can be sampled from a Negative-Binomial distribution.</caption>
<thead>
<tr class="header">
<th align="right">mean(bites)</th>
<th align="right">var(bites)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">6</td>
<td align="right">19.2</td>
</tr>
</tbody>
</table>
<p>The next simulation first creates an observation-level score for blood sweetness, which in real data would not be known to the researcher; it is therefore similar to a random effect. A property called Sweetness is first sampled from a Gaussian distribution. The result is exponentiated to achieve positive numbers and plugged into the Poisson random number generator. Table <a href="glm.html#tab:logreg-11">7.16</a> shows how variance exceeds the mean of the so the produced responses.</p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb588-1"><a href="glm.html#cb588-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb588-2"><a href="glm.html#cb588-2" aria-hidden="true" tabindex="-1"></a>sd <span class="ot">&lt;-</span> .<span class="dv">5</span></span>
<span id="cb588-3"><a href="glm.html#cb588-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb588-4"><a href="glm.html#cb588-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb588-5"><a href="glm.html#cb588-5" aria-hidden="true" tabindex="-1"></a>Sweet_blood_olre <span class="ot">&lt;-</span></span>
<span id="cb588-6"><a href="glm.html#cb588-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb588-7"><a href="glm.html#cb588-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">Method =</span> <span class="st">&quot;OLRE&quot;</span>,</span>
<span id="cb588-8"><a href="glm.html#cb588-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">sweetness =</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="fu">log</span>(avg_sweet), <span class="at">sd =</span> sd),</span>
<span id="cb588-9"><a href="glm.html#cb588-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">bites =</span> <span class="fu">rpois</span>(N, <span class="at">lambda =</span> <span class="fu">exp</span>(sweetness))</span>
<span id="cb588-10"><a href="glm.html#cb588-10" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb588-11"><a href="glm.html#cb588-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb588-12"><a href="glm.html#cb588-12" aria-hidden="true" tabindex="-1"></a>Sweet_blood_olre <span class="sc">%&gt;%</span></span>
<span id="cb588-13"><a href="glm.html#cb588-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="fu">mean</span>(bites), <span class="fu">var</span>(bites)) </span></code></pre></div>
<table>
<caption><span id="tab:logreg-11">Table 7.16: </span>Overdispersed counts can be sampled as Gaussian deviations on the linearized scalle, aka observation-level random effects.</caption>
<thead>
<tr class="header">
<th align="right">mean(bites)</th>
<th align="right">var(bites)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">6.67</td>
<td align="right">20.3</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb589"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb589-1"><a href="glm.html#cb589-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb589-2"><a href="glm.html#cb589-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">Method =</span> <span class="st">&quot;Poisson&quot;</span>,</span>
<span id="cb589-3"><a href="glm.html#cb589-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">bites =</span> <span class="fu">rpois</span>(N, avg_sweet)</span>
<span id="cb589-4"><a href="glm.html#cb589-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb589-5"><a href="glm.html#cb589-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(</span>
<span id="cb589-6"><a href="glm.html#cb589-6" aria-hidden="true" tabindex="-1"></a>    Sweet_blood_nbin,</span>
<span id="cb589-7"><a href="glm.html#cb589-7" aria-hidden="true" tabindex="-1"></a>    Sweet_blood_olre</span>
<span id="cb589-8"><a href="glm.html#cb589-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb589-9"><a href="glm.html#cb589-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> bites)) <span class="sc">+</span></span>
<span id="cb589-10"><a href="glm.html#cb589-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb589-11"><a href="glm.html#cb589-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(Method <span class="sc">~</span> .)</span></code></pre></div>
<div class="figure"><span id="fig:logreg-13"></span>
<img src="Generalized_Linear_Models_files/figure-html/logreg-13-1.png" alt="Overdispersed samples (Negbinomial, OLRE) compared to Poisson samples of same average." width="90%" />
<p class="caption">
Figure 7.12: Overdispersed samples (Negbinomial, OLRE) compared to Poisson samples of same average.
</p>
</div>
<p>When building a model for over-dispersed count data, the process of simulating it is simply reversed, for both methods. Either we choose a more flexible distribution, or we estimate the residuals on the linearized scale. The first method has the advantage of being leaner. Only one parameter is added, whereas OLRE results in one linearized residual for every observation. The advantage of OLRE is more of a conceptual kind. Not only is it appealing for researchers who are familiar with multi-level models, it also produces estimates similar to the well-known residuals. Furthermore, it also works with regression engines that do not cover the two-parameter distribution. In fact, the beta-binomial family is the matching two-parameter distribution for Binomial variables, but is not supported out-of-the-box by any regression engine I am aware of. In <a href="glm.html#betabin-reg">7.2.3.2</a>, we will see how user-defined distributions can be added to the Brm engine.</p>
<!-- Overdispersion practically always happens in studies involving objects with complex dynamics, such as the human mind. Two solutions exist for overdispersed count data: we can either switch to a two-parameter response distribution, that gives variance more flexibility (see table below). -->
<!-- <!-- 46 make work with LaTeX -->
<p>–&gt;</p>
<!-- ```{r echo = F} -->
<!-- readxl::read_excel("Illustrations/GLM_distributions.xlsx", sheet = "plugin") -->
<!-- ``` -->
<!-- The alternative is to introduce an observation-level random effect, which is just like the Gaussian distributed variable Sweetness in the simulation above. -->
<!-- #### HERE -->
<div id="negbin-reg" class="section level4" number="7.2.3.1">
<h4><span class="header-section-number">7.2.3.1</span> Negative-binomial regression for overdispersed counts</h4>
<p>When Poisson regression is used for overdispersed count data, the model will produce accurate center estimates, but the credibility limits will be too narrow. The model suggests better certainty than there is. To explain that in simple terms: The model “sees” the location of a measure, which makes it seek errors in a region with precisely that variance. There will be many measures outside the likely region, but the model will hold on tight, regard these as (gradual) outliers and give them less weight. A solution to the problem is using a matching response distribution with <em>two parameters</em>. A second parameter usually gives variance of the distribution more flexibility, although only Gaussian models can set it completely independent of location.</p>
<!-- For the Poisson case (i.e. counts without an upper limit) *negative binomial distributions* do the job. This distribution is a so-called *mixture distributions*. In mixture distributions, the parameter of the "outer" distribution is not constant, but allowed to vary by a distribution itself. Under this perspective, negative binomial distribution is equivalent to a Poisson distribution, if we let parameter $\lambda$ follow a gamma distribution, like this: -->
<!-- ```{r logreg-13, fig.cap = ""} -->
<!-- rnegbinom <- function(n, mu, size){ -->
<!--   shape <- size -->
<!--   scale <- mu/size -->
<!--   lambdas <- rgamma(n, shape = shape, scale = scale) -->
<!--   rpois(n, lambda = lambdas) -->
<!-- } -->
<!-- rnegbinom(1000, mu = 3, size = 2) %>%  qplot(bins = 20) -->
<!-- ``` -->
<!-- The figure below shows a negative binomial distribution and Poisson distribution with the same mean. The additional parameter `size` virtually shrinks the scale of measuremet, which makes the distribution appear wider. -->
<!-- ```{r} -->
<!-- tibble(x = 0:15, -->
<!--            nbinom = dnbinom(x, mu = 3, size = 2), -->
<!--            poisson  = dpois(x, lambda = 3)) %>%  -->
<!--   gather(distribution, prob, -x) %>%  -->
<!--   ggplot(aes(x = x, y = prob)) + -->
<!--   facet_grid(distribution ~ .) + -->
<!--   geom_col(position = "dodge") -->
<!-- ``` -->
<p>In <a href="glm.html#learning-curves">7.2.1.2</a> we have seen how log-linearization can accommodate learning curves, using a Poisson model. It is very likely that this data is over-dispersed and that the Poisson model was not correct. To demonstrate overdispersion, we estimate the linearized learning curve one more time, with a negative-binomial pattern of randomness. Figure <a href="glm.html#fig:negbinreg-1">7.13</a> shows the coefficient estimates next to coefficients from the Poisson model</p>
<div class="sourceCode" id="cb590"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb590-1"><a href="glm.html#cb590-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(IPump)</span></code></pre></div>
<div class="sourceCode" id="cb591"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb591-1"><a href="glm.html#cb591-1" aria-hidden="true" tabindex="-1"></a>M_negbin_lzrm <span class="ot">&lt;-</span></span>
<span id="cb591-2"><a href="glm.html#cb591-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(deviations <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Design <span class="sc">+</span> session,</span>
<span id="cb591-3"><a href="glm.html#cb591-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> D_agg,</span>
<span id="cb591-4"><a href="glm.html#cb591-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;negbinomial&quot;</span></span>
<span id="cb591-5"><a href="glm.html#cb591-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb592"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb592-1"><a href="glm.html#cb592-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb592-2"><a href="glm.html#cb592-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_pois_lzrm),</span>
<span id="cb592-3"><a href="glm.html#cb592-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_negbin_lzrm)</span>
<span id="cb592-4"><a href="glm.html#cb592-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb592-5"><a href="glm.html#cb592-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(type <span class="sc">==</span> <span class="st">&quot;fixef&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb592-6"><a href="glm.html#cb592-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">clu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb592-7"><a href="glm.html#cb592-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> center, <span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper, <span class="at">x =</span> model)) <span class="sc">+</span></span>
<span id="cb592-8"><a href="glm.html#cb592-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>fixef, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>) <span class="sc">+</span></span>
<span id="cb592-9"><a href="glm.html#cb592-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_crossbar</span>(<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:negbinreg-1"></span>
<img src="Generalized_Linear_Models_files/figure-html/negbinreg-1-1.png" alt="Comparing credibility intervals of a Poisson and Neg-Binomial models" width="90%" />
<p class="caption">
Figure 7.13: Comparing credibility intervals of a Poisson and Neg-Binomial models
</p>
</div>
<p>We observe that the center estimates are precisely the same. Over-dispersion usually does not bias the location of an estimate. But, credibility limits are much wider with an underlying negative-binomial distribution. A full parameter table would also show the Neg-Binomial model additional parameter <code>phi</code>, controlling over-dispersion relative to a Poisson distribution as:</p>
<p><span class="math display">\[
\textrm{Variance} := \mu + \mu^2/\phi
\]</span></p>
<p>Due to the reciprocal term, the <em>smaller</em> <span class="math inline">\(\phi\)</span> gets, the <em>more</em> overdispersion had to be accounted for. From this formula alone it may seem that neg-binomial distributions could also account for under-dispersion, when we allow negative values. But, in most implementations <span class="math inline">\(\phi\)</span> must be non-negative. That is rarely a problem, as under-dispersion only occurs under very special circumstances. Over-dispersion in count variables in contrast, is very common, if not ubiquitous. Negative-binomial regression solves the problem with just one additional parameter, which typically need not be interpreted. Reporting on coefficients uses the same principle as in plain Poisson regression: inversion by exponentiation and speaking multiplicative.</p>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<!-- ```{r} -->
<!-- L_ovdsp_1 <- waic(M_ovdsp_1) -->
<!-- W_ovdsp_3 <- waic(M_ovdsp_2) -->
<!-- ``` -->
</div>
<div id="betabin-reg" class="section level4" number="7.2.3.2">
<h4><span class="header-section-number">7.2.3.2</span> Beta-binomial regression for successes in trials</h4>
<p>Beta-binomial regression follows a similar pattern as negative-binomial. A two parameter distribution allows to scale up the variance relative to a binomial model <a href="glm.html#logistic-reg">7.2.2</a>. A beta-binomial distribution is a mixed distribution, created by replacing binomial parameter <span class="math inline">\(p\)</span> by a <span class="math inline">\(beta distribution\)</span>, with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:</p>
<div class="sourceCode" id="cb594"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb594-1"><a href="glm.html#cb594-1" aria-hidden="true" tabindex="-1"></a>rbetabinom <span class="ot">&lt;-</span> <span class="cf">function</span>(n, size, a, b) {</span>
<span id="cb594-2"><a href="glm.html#cb594-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbinom</span>(n, size, <span class="fu">rbeta</span>(n, a, b))</span>
<span id="cb594-3"><a href="glm.html#cb594-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb594-4"><a href="glm.html#cb594-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb594-5"><a href="glm.html#cb594-5" aria-hidden="true" tabindex="-1"></a><span class="fu">rbetabinom</span>(<span class="dv">1000</span>, <span class="dv">10</span>, <span class="dv">1</span>, <span class="dv">2</span>) <span class="sc">%&gt;%</span> <span class="fu">qplot</span>()</span></code></pre></div>
<div class="figure"><span id="fig:betabin-1"></span>
<img src="Generalized_Linear_Models_files/figure-html/betabin-1-1.png" alt="Sampling from a Beta-Bionomial distribution" width="90%" />
<p class="caption">
Figure 7.14: Sampling from a Beta-Bionomial distribution
</p>
</div>
<!-- Predictions and interpretation of coefficients of negative-binomial and beta-binomial models are just as with their counterparts, using the same link functions. There is just a tiny difference: a single parameter has been added to the model, which modifies the dispersion. In a standard analysis these parameters have very little meaning, even less than the standard error in a Gaussian model. No misunderstanding: these parameters are *not* the constant standard deviation of residuals. They act as scalers for the "natural" dispersion at any point. -->
<p>The Brms regression engine currently does not implement the beta-binomial family. That is a good opportunity to applaud the author of the Brms package for his ingenious architecture, which allows custom families to be defined by the user. The only requirement is that the distribution type is implemented in Stan <span class="citation">(Carpenter et al. 2017)</span>, which is the underlying general-purpose engine behind Brms. The following code is taken directly from the Brms documentation and adds the beta-binomial family.</p>
<div class="sourceCode" id="cb595"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb595-1"><a href="glm.html#cb595-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define a custom beta-binomial family</span></span>
<span id="cb595-2"><a href="glm.html#cb595-2" aria-hidden="true" tabindex="-1"></a>beta_binomial2 <span class="ot">&lt;-</span> <span class="fu">custom_family</span>(</span>
<span id="cb595-3"><a href="glm.html#cb595-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;beta_binomial2&quot;</span>,</span>
<span id="cb595-4"><a href="glm.html#cb595-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">dpars =</span> <span class="fu">c</span>(<span class="st">&quot;mu&quot;</span>, <span class="st">&quot;phi&quot;</span>),</span>
<span id="cb595-5"><a href="glm.html#cb595-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">links =</span> <span class="fu">c</span>(<span class="st">&quot;logit&quot;</span>, <span class="st">&quot;log&quot;</span>), <span class="at">lb =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">0</span>),</span>
<span id="cb595-6"><a href="glm.html#cb595-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&quot;int&quot;</span>, <span class="at">vars =</span> <span class="st">&quot;trials[n]&quot;</span></span>
<span id="cb595-7"><a href="glm.html#cb595-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb595-8"><a href="glm.html#cb595-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb595-9"><a href="glm.html#cb595-9" aria-hidden="true" tabindex="-1"></a><span class="co"># define custom stan functions</span></span>
<span id="cb595-10"><a href="glm.html#cb595-10" aria-hidden="true" tabindex="-1"></a>bb_stan_funs <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb595-11"><a href="glm.html#cb595-11" aria-hidden="true" tabindex="-1"></a><span class="st">  real beta_binomial2_lpmf(int y, real mu, real phi, int N) {</span></span>
<span id="cb595-12"><a href="glm.html#cb595-12" aria-hidden="true" tabindex="-1"></a><span class="st">    return beta_binomial_lpmf(y | N, mu * phi, (1 - mu) * phi);</span></span>
<span id="cb595-13"><a href="glm.html#cb595-13" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb595-14"><a href="glm.html#cb595-14" aria-hidden="true" tabindex="-1"></a><span class="st">  int beta_binomial2_rng(real mu, real phi, int N) {</span></span>
<span id="cb595-15"><a href="glm.html#cb595-15" aria-hidden="true" tabindex="-1"></a><span class="st">    return beta_binomial_rng(N, mu * phi, (1 - mu) * phi);</span></span>
<span id="cb595-16"><a href="glm.html#cb595-16" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb595-17"><a href="glm.html#cb595-17" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;</span></span></code></pre></div>
<p><em>Note</em> that Beta-binomial distribution are usually parametrized with two shape paramneter <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, which have a rather convoluted relationship with mean and variance. For a GLM a parametrization is required that has a mean parameter (for <span class="math inline">\(\mu_i\)</span>). Note, how the author of this code created a <code>beta_binomial2</code> distribution family, which takes <span class="math inline">\(\mu\)</span> and a scale parameter <span class="math inline">\(\phi\)</span>.</p>
<p>Defining the two functions is sufficient to estimate beta-binomial models with Brms. In the following I simulate two outcomes from nine trials, <code>y</code> is sampled from a beta-binomial distribution, whereas <code>ybin</code> is from a Binomial distribution. Both have the same mean of <span class="math inline">\(.1\)</span> (10% correct). Subsequently, a beta-binomial and a binomial grand mean models are estimated. Note that the sampling function is taken from package VGAM, as this has the same parametrization.</p>
<div class="sourceCode" id="cb596"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb596-1"><a href="glm.html#cb596-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb596-2"><a href="glm.html#cb596-2" aria-hidden="true" tabindex="-1"></a>D_betabin <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb596-3"><a href="glm.html#cb596-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> VGAM<span class="sc">::</span><span class="fu">rbetabinom</span>(<span class="dv">1000</span>, <span class="dv">9</span>, <span class="at">prob =</span> .<span class="dv">1</span>, <span class="at">rho =</span> .<span class="dv">3</span>),</span>
<span id="cb596-4"><a href="glm.html#cb596-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">9</span></span>
<span id="cb596-5"><a href="glm.html#cb596-5" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb596-6"><a href="glm.html#cb596-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tbl_obs</span>()</span>
<span id="cb596-7"><a href="glm.html#cb596-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb596-8"><a href="glm.html#cb596-8" aria-hidden="true" tabindex="-1"></a>M_betabin <span class="ot">&lt;-</span></span>
<span id="cb596-9"><a href="glm.html#cb596-9" aria-hidden="true" tabindex="-1"></a>  D_betabin <span class="sc">%&gt;%</span></span>
<span id="cb596-10"><a href="glm.html#cb596-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(y <span class="sc">|</span> <span class="fu">trials</span>(n) <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb596-11"><a href="glm.html#cb596-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> beta_binomial2,</span>
<span id="cb596-12"><a href="glm.html#cb596-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">stan_funs =</span> bb_stan_funs, <span class="at">data =</span> .</span>
<span id="cb596-13"><a href="glm.html#cb596-13" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb596-14"><a href="glm.html#cb596-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb596-15"><a href="glm.html#cb596-15" aria-hidden="true" tabindex="-1"></a>M_bin <span class="ot">&lt;-</span> <span class="fu">brm</span>(y <span class="sc">|</span> <span class="fu">trials</span>(n) <span class="sc">~</span> <span class="dv">1</span>, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="at">data =</span> D_betabin)</span></code></pre></div>
<p>The following CLU table collects the estimates from both models, the true beta-binomial and the binomial, which does not account for over-dispersion in the data.</p>
<div class="sourceCode" id="cb597"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb597-1"><a href="glm.html#cb597-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb597-2"><a href="glm.html#cb597-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_bin),</span>
<span id="cb597-3"><a href="glm.html#cb597-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_betabin)</span>
<span id="cb597-4"><a href="glm.html#cb597-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb597-5"><a href="glm.html#cb597-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">clu</span>(<span class="at">mean.func =</span> inv_logit)</span></code></pre></div>
<table>
<caption><span id="tab:betabin-3">Table 7.17: </span>Parameter estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">M_betabin</td>
<td align="left">b_Intercept</td>
<td align="left">Intercept</td>
<td align="right">0.104</td>
<td align="right">0.093</td>
<td align="right">0.116</td>
</tr>
<tr class="even">
<td align="left">M_betabin</td>
<td align="left">phi</td>
<td align="left"></td>
<td align="right">0.927</td>
<td align="right">0.893</td>
<td align="right">0.955</td>
</tr>
<tr class="odd">
<td align="left">M_bin</td>
<td align="left">b_Intercept</td>
<td align="left">Intercept</td>
<td align="right">0.103</td>
<td align="right">0.097</td>
<td align="right">0.110</td>
</tr>
</tbody>
</table>
<p>When comparing the two intercept estimates, we notice that the center estimate is not affected by over-dispersion. But, just like with Poisson models, the binomial model is too optimistic about the level of certainty.</p>
<p>To summarize: one-parameter distributions usually cannot be used to model count data due to extra variance. One solution to the problem is to switch to a family with a second parameter. These exist for the most common situations. When we turn to modeling durations, we will use the Gamma family to extend the Exponential distribution <a href="glm.html#exp-gam-reg">7.3.1</a>. Gamma distributions have another problem: while extra variance can be accounted by a scale parameter, we will see that another property of distribution families can be to rigid, the skew. The solution will be to switch to a three-parameter distribution family to gain more flexibility.</p>
<p>Another technique to model over-dispersion does not require to find (or define) a two-parametric distribution. Instead, <em>observation-level random effects</em> borrow concepts from multi-level modeling and allow to keep the one-parameter distributions.</p>
</div>
<div id="olre" class="section level4" number="7.2.3.3">
<h4><span class="header-section-number">7.2.3.3</span> Using observation-level random effects</h4>
<p>As we have seen in chapter <a href="mlm.html#mlm">6</a>, random effects are often interpreted towards variance in a population, with a Gaussian distribution. On several occasions we used multi-level models to separate sources of variance, such as between teams and participants in CUE8 (@ref()). Observation-level random effect (OLRE) use the same approach by just calling the set of observation a population.</p>
<p>Using random effects with GLMs is straight-forward, because random effects (or their dummy variable representation, to be precise), are part of the linear term, and undergo the log or logit linearization just like any other coefficient in the model.</p>
<!-- Recall how we regard variation between members of a population as normally distributed deviations from the population mean, by the example of a Poisson grand mean model with a participant-level ($p$) random effect: -->
<!-- $$ -->
<!-- \theta_{pi} = \beta_0 + x_p\beta_{0p} \\ -->
<!-- \mu_{pi} = \exp(\theta_{pi})\\ -->
<!-- \beta_{0p} \sim N(\mu_{p}, \sigma_p)\\ -->
<!-- y_{p} \sim \textrm{Pois}(\mu_{ij}) -->
<!-- $$ -->
<!-- The OLRE is normally distributed but does not cause any bounded range, as it is added on the level of the linear predictor before applying the exponential transformation. Observation-level random effects are completely analogous, except that every observation becomes its own group, in a Poisson grand mean model with added variation: -->
<!-- $$ -->
<!-- \theta_{i} = \beta_0 + \beta_{i} \\ -->
<!-- \mu_i = \exp(\theta_i)\\ -->
<!-- y_{ij} \sim \textrm{Pois}(\mu_{ij}) -->
<!-- $$ -->
<!-- See, how $\beta_i$ is a unique deviation per observation $i$, and how a variance parameter $\sigma$ appears in an otherwise purely Poisson model. Observation-level random effects are on the linear predictor level, and therefore additive. Compare this to the negative binomial distribution where variance is scaled up, which is multiplication. We find a resemblance with how sums on the linear predictor become multiplications on the fitted responses scale. -->
<p>For demonstration of the concept, we simulate from an overdispersed Poisson grand mean model with participant-level variation and observation-level variation.</p>
<div class="sourceCode" id="cb598"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb598-1"><a href="glm.html#cb598-1" aria-hidden="true" tabindex="-1"></a>sim_ovdsp <span class="ot">&lt;-</span> <span class="cf">function</span>(</span>
<span id="cb598-2"><a href="glm.html#cb598-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">beta_0 =</span> <span class="dv">2</span>, <span class="co"># mu = 8</span></span>
<span id="cb598-3"><a href="glm.html#cb598-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">sd_Obs =</span> .<span class="dv">3</span>,</span>
<span id="cb598-4"><a href="glm.html#cb598-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">sd_Part =</span> .<span class="dv">5</span>,</span>
<span id="cb598-5"><a href="glm.html#cb598-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">N_Part =</span> <span class="dv">30</span>,</span>
<span id="cb598-6"><a href="glm.html#cb598-6" aria-hidden="true" tabindex="-1"></a>                      <span class="at">N_Rep =</span> <span class="dv">20</span>,</span>
<span id="cb598-7"><a href="glm.html#cb598-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">N_Obs =</span> N_Part <span class="sc">*</span> N_Rep,</span>
<span id="cb598-8"><a href="glm.html#cb598-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">seed =</span> <span class="dv">1</span>) {</span>
<span id="cb598-9"><a href="glm.html#cb598-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(seed)</span>
<span id="cb598-10"><a href="glm.html#cb598-10" aria-hidden="true" tabindex="-1"></a>  Part <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb598-11"><a href="glm.html#cb598-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">Part =</span> <span class="dv">1</span><span class="sc">:</span>N_Part,</span>
<span id="cb598-12"><a href="glm.html#cb598-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">beta_0p =</span> <span class="fu">rnorm</span>(N_Part, <span class="dv">0</span>, sd_Part)</span>
<span id="cb598-13"><a href="glm.html#cb598-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="do">## participant-level RE</span></span>
<span id="cb598-14"><a href="glm.html#cb598-14" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb598-15"><a href="glm.html#cb598-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">Obs =</span> <span class="dv">1</span><span class="sc">:</span>N_Obs,</span>
<span id="cb598-16"><a href="glm.html#cb598-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">Part =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>N_Part, N_Rep),</span>
<span id="cb598-17"><a href="glm.html#cb598-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">beta_0i =</span> <span class="fu">rnorm</span>(N_Obs, <span class="dv">0</span>, sd_Obs), <span class="do">## observeration-level RE</span></span>
<span id="cb598-18"><a href="glm.html#cb598-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">beta_0 =</span> beta_0</span>
<span id="cb598-19"><a href="glm.html#cb598-19" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb598-20"><a href="glm.html#cb598-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">left_join</span>(Part) <span class="sc">%&gt;%</span></span>
<span id="cb598-21"><a href="glm.html#cb598-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(</span>
<span id="cb598-22"><a href="glm.html#cb598-22" aria-hidden="true" tabindex="-1"></a>      <span class="at">theta_i =</span> beta_0 <span class="sc">+</span> beta_0p <span class="sc">+</span> beta_0i,</span>
<span id="cb598-23"><a href="glm.html#cb598-23" aria-hidden="true" tabindex="-1"></a>      <span class="at">mu_i =</span> <span class="fu">exp</span>(theta_i), <span class="do">## inverse link function</span></span>
<span id="cb598-24"><a href="glm.html#cb598-24" aria-hidden="true" tabindex="-1"></a>      <span class="at">y_i =</span> <span class="fu">rpois</span>(N_Obs, mu_i)</span>
<span id="cb598-25"><a href="glm.html#cb598-25" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb598-26"><a href="glm.html#cb598-26" aria-hidden="true" tabindex="-1"></a>  D <span class="sc">%&gt;%</span> <span class="fu">as_tbl_obs</span>()</span>
<span id="cb598-27"><a href="glm.html#cb598-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb598-28"><a href="glm.html#cb598-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb598-29"><a href="glm.html#cb598-29" aria-hidden="true" tabindex="-1"></a>D_ovdsp <span class="ot">&lt;-</span> <span class="fu">sim_ovdsp</span>()</span></code></pre></div>
<p>The above code is instructive to how OLREs work:</p>
<ol style="list-style-type: decimal">
<li>A participant-level random effect is created as <code>beta_0p</code>. This random effect can be recovered, because we have repeated measures. This variation will not contaminate Poisson variance.</li>
<li>An observation-level random effect is created in much the same way.</li>
<li>Both random effects are on the linearized scale. The linear predictor <code>theta_i</code> is just the sum of random effects (and Intercept). It could take negative values, but …</li>
<li>… applying the inverse link function (<code>exp(theta_i)</code>) ensures that all responses are positive.</li>
</ol>
<p>The extra variation comes from two sources: participant-level and observation-level. While participant levels, observations do not. How does come that we can estimate random effects on single measures? Recall that one-parameter distributions, such as Poissons, have their variance fully tied to the mean. At any position does the distribution “know” how much variance it is suppose to have and therefore the OLREs can be recovered. The following model contains an participant-level random effects and an OLRE. Table <a href="glm.html#tab:olre-2">7.18</a> recovers the two standard deviations.</p>
<div class="sourceCode" id="cb599"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb599-1"><a href="glm.html#cb599-1" aria-hidden="true" tabindex="-1"></a>M_ovdsp <span class="ot">&lt;-</span></span>
<span id="cb599-2"><a href="glm.html#cb599-2" aria-hidden="true" tabindex="-1"></a>  D_ovdsp <span class="sc">%&gt;%</span></span>
<span id="cb599-3"><a href="glm.html#cb599-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glmer</span>(y_i <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Part) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Obs),</span>
<span id="cb599-4"><a href="glm.html#cb599-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .,</span>
<span id="cb599-5"><a href="glm.html#cb599-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> poisson</span>
<span id="cb599-6"><a href="glm.html#cb599-6" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb600-1"><a href="glm.html#cb600-1" aria-hidden="true" tabindex="-1"></a><span class="fu">grpef</span>(M_ovdsp)</span></code></pre></div>
<table>
<caption><span id="tab:olre-2">Table 7.18: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">re_factor</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Obs</td>
<td align="right">0.330</td>
<td align="right">0.289</td>
<td align="right">0.373</td>
</tr>
<tr class="even">
<td align="left">Part</td>
<td align="right">0.503</td>
<td align="right">0.387</td>
<td align="right">0.683</td>
</tr>
</tbody>
</table>
<p>Let’s first take a look at the two random effect standard errors above. It seems that we got a fair recovery on the center estimates (for standard deviation). For the OLRE certainty is also good, even better than for the participant-level, which is simply due to the fact that there are more levels. Random effect variation is accurately recovered from the simulated data, but can we also recover the full vector of factor levels? In the following I am extracting observation-level random effects and plot them against the simulated (linearized) coefficients.</p>
<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb601-1"><a href="glm.html#cb601-1" aria-hidden="true" tabindex="-1"></a>OLRE <span class="ot">&lt;-</span></span>
<span id="cb601-2"><a href="glm.html#cb601-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_ovdsp) <span class="sc">%&gt;%</span></span>
<span id="cb601-3"><a href="glm.html#cb601-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(type <span class="sc">==</span> <span class="st">&quot;ranef&quot;</span>, re_factor <span class="sc">==</span> <span class="st">&quot;Obs&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb601-4"><a href="glm.html#cb601-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">clu</span>()</span>
<span id="cb601-5"><a href="glm.html#cb601-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb601-6"><a href="glm.html#cb601-6" aria-hidden="true" tabindex="-1"></a>D_ovdsp <span class="sc">%&gt;%</span></span>
<span id="cb601-7"><a href="glm.html#cb601-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(OLRE) <span class="sc">%&gt;%</span></span>
<span id="cb601-8"><a href="glm.html#cb601-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="at">true_score =</span> beta_0i, <span class="at">olre =</span> center) <span class="sc">%&gt;%</span></span>
<span id="cb601-9"><a href="glm.html#cb601-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> true_score, <span class="at">y =</span> olre)) <span class="sc">+</span></span>
<span id="cb601-10"><a href="glm.html#cb601-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb601-11"><a href="glm.html#cb601-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> F)</span></code></pre></div>
<div class="figure"><span id="fig:olre-4"></span>
<img src="Generalized_Linear_Models_files/figure-html/olre-4-1.png" alt="Recovery of observation-level random effects" width="90%" />
<p class="caption">
Figure 7.15: Recovery of observation-level random effects
</p>
</div>
<p>Figure <a href="glm.html#fig:olre-4">7.15</a> shows that the observation-level deviations can be recovered not perfectly, but to some extent. An OLRE can be seen as generalized residual, or <em>linearized residuals</em>.</p>
<p>Linearized residuals can be used for different purposes, such as outlier detection or to compare sources of variation. Frequently, I reminded the reader to interpret parameters quantitatively by translating their magnitude to statements of practical relevance. For random effects variance this is not always straight forward, especially when we are on a linearized scale. One way is to make comparative statements on the sources of variance, like “the variance due to individual differences exceeds the measurement error.” OLREs are on the same scale as all other random effects in the model, which makes it a suitable reference source of variation.</p>
<!-- A non-default comparison of sources of variance is the one of Dennis Egan, that people cause more variance than designs do. With <!-- #111 GLMMs, the marriage of LMM and GLM this claim is testable. -->
<!-- Up to this point, this would only make sense for Gaussian models, as only those have this parameter. And only with Gaussian models does it make sense to talk about residuals in the sense of `y_i = \mu_i + \epsilon_i`. Residuals are summands on the response scale, whereas Poisson models are multiplicative on the response scale. OLREs operate  -->
<!-- From LMM we borrow a surprisingly simple and general solution, observation-level random effects. So, most of the time we will not need one of those twisted two parameter random distributions to account for overdispersion. With OLRE models we get an estimate that is very similar to *residuals*, which has proven very useful in model criticism. -->
<pre><code>## [1] &quot;sim_ovdsp&quot; &quot;D_ovdsp&quot;</code></pre>
</div>
</div>
</div>
<div id="duration-measures" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Duration measures</h2>
<p>Time is a highly accessible measure, as clocks are all around us: on your wrist, in transport stations, in your computers and a very big (and accurate) one is hosted at the Physikalisch-Technischen Bundesanstalt in Braunschweig (Physical-technological federal institute in Braunschweig, Germany). Duration measures often carry useful information; especially, <em>Reaction time (RT)</em> measures are prime in experimental cognitive studies and have revealed fascinating phenomena of the human mind, such as the Stroop effect, memory priming, motor learning and the structure of attention.</p>
<p>In design research, reaction times are also sometimes used in experiments, but more common is <em>time-on-task (ToT)</em> as a measure of task efficiency. Formally, both outcome types measure a period of time. I am deliberately making a distinction between the two, because the data generating process of reacting to a simple task (like naming a color) may be different to a complex task, like finding information on a website. Also, RT and ToT usually are on different scales, with RT being typically in the fraction-of-seconds range and ToT in the minutes range.</p>
<p>Temporal variables are practically continuous (as long as one measures with sufficient precision), but always have lower bounds. First, I will introduce two families of zero-bounded model classes that use exponentially or Gamma distributed error terms. Modern Bayesian estimation engines offer an increasing variety of more exotic response distributions. Among those are Exgaussian response distributions, which works well when the lower bound is positive. Normally, this is more realistic for durations.</p>
<div id="exp-gam-reg" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Exponential and Gamma regression</h3>
<p>Exponential distributions arise from basic random processes under some very idealized conditions. First, the lower boundary must be zero and second, the rate at which events happen is assumed to be constant rate, just like Poisson distributions assumes a constant <span class="math inline">\(\lambda\)</span>.</p>
<!-- This leads to the property called *being memoryless*, which can be mind-boggling as . In many situations we have the intuition that the longer we wait, the shorter our expected remaining time-to-wait. Examples of such memoryful processes are plenty, such as: -->
<!-- + the longer a little girl waits, the closer is her birthday -->
<!-- + the longer you wait in a queue, the more likely you'll be called in, soon. -->
<!-- + earthquakes are essentially relaxations in the earth crust. Once it happened, it is less likely to reoccur in near future. -->
<p>Reconsider the subway smurfer example <a href="ebs.html#distributions">3.5.2</a>, where players collect items in a jump-and-run game. We have already seen how collection counts can be modeled using Poisson or binomial regression. Another way to look at it is the time between two events of item collection.
For demonstration only, we assume such idealized conditions in the subway smurfer example and generate a data set. Exponential distributions are determined by one parameter, the <em>rate</em> parameter <span class="math inline">\(\lambda\)</span>, which is strictly positive. The mean and variance of exponential distributions are as follows, and they are strictly tied to each other:</p>
<p><span class="math display">\[ 
\begin{aligned}
Y &amp;\sim \textrm{Exp} (\lambda) \\
\textrm {Mean}(Y) &amp;= 1/\lambda \\
\textrm {Var}(Y) &amp;= 1/\lambda^2\\ 
&amp;=  1/\lambda \times \textrm{Mean}(Y)\\
\end{aligned}
\]</span></p>
<p>In the following, data is simulated from an exponential distribution (Figure <a href="glm.html#fig:expreg-1">7.16</a>). Subsequently, the only parameter is recovered using an exponential GMM. Like for Poisson models, a log link function is reversed by exponentiation (Table <a href="glm.html#tab:expreg-2">7.19</a>).</p>
<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb603-1"><a href="glm.html#cb603-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20</span>)</span>
<span id="cb603-2"><a href="glm.html#cb603-2" aria-hidden="true" tabindex="-1"></a>D_exp <span class="ot">&lt;-</span></span>
<span id="cb603-3"><a href="glm.html#cb603-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb603-4"><a href="glm.html#cb603-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Obs =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,</span>
<span id="cb603-5"><a href="glm.html#cb603-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">time =</span> <span class="fu">rexp</span>(<span class="dv">100</span>, <span class="at">rate =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="dv">3</span>)</span>
<span id="cb603-6"><a href="glm.html#cb603-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb603-7"><a href="glm.html#cb603-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb603-8"><a href="glm.html#cb603-8" aria-hidden="true" tabindex="-1"></a>D_exp <span class="sc">%&gt;%</span></span>
<span id="cb603-9"><a href="glm.html#cb603-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> time)) <span class="sc">+</span></span>
<span id="cb603-10"><a href="glm.html#cb603-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">20</span>)</span></code></pre></div>
<div class="figure"><span id="fig:expreg-1"></span>
<img src="Generalized_Linear_Models_files/figure-html/expreg-1-1.png" alt="Data sampled from an Exponential distribution." width="90%" />
<p class="caption">
Figure 7.16: Data sampled from an Exponential distribution.
</p>
</div>
<div class="sourceCode" id="cb604"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb604-1"><a href="glm.html#cb604-1" aria-hidden="true" tabindex="-1"></a>M_exp <span class="ot">&lt;-</span> <span class="fu">brm</span>(time <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb604-2"><a href="glm.html#cb604-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="st">&quot;exponential&quot;</span>,</span>
<span id="cb604-3"><a href="glm.html#cb604-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_exp</span>
<span id="cb604-4"><a href="glm.html#cb604-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb605-1"><a href="glm.html#cb605-1" aria-hidden="true" tabindex="-1"></a><span class="fu">clu</span>(M_exp, <span class="at">mean.func =</span> exp)</span></code></pre></div>
<table>
<caption><span id="tab:expreg-2">Table 7.19: </span>Parameter estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">b_Intercept</td>
<td align="left">Intercept</td>
<td align="right">1.19</td>
<td align="right">1</td>
<td align="right">1.38</td>
</tr>
<tr class="even">
<td align="left">lp__</td>
<td align="left"></td>
<td align="right">-221.24</td>
<td align="right">-223</td>
<td align="right">-221.02</td>
</tr>
</tbody>
</table>
<p>Exponential distribution are rarely used in practice for two shortcomings: first, the strict mean-variance relation makes it prone to over-dispersion. This can be resolved by using observation-level random effects <a href="glm.html#olre">7.2.3.3</a> or using Gamma distributions, which accounts for extra variance by a second parameter. The second problem is the lower boundary of Zero, which will later be resolved by using Exgaussian error distributions (<a href="glm.html#exgauss-reg">7.3.2</a>).</p>
<p>Exponential regression has a single parameter and therefore has the same problem as seen with Poisson and binomial regression before. Only if all events have the same rate to occur, will an exponential distribution arise, which means for behavioral research: never. A general solution to the problem is introducing an observation-level random effect <a href="glm.html#olre">7.2.3.3</a>. Here, I will we will tackle the problem by using continuous, zero-bounded mixture distributions with two parameters, the Gamma family of distributions.</p>
<p>The canonical form of Gamma distribution uses two parameters rate and shape, which do not directly translate into location and dispersion. Still, the second parameter provides the extra degree of freedom to adjust variance. Albeit, the variance parameter is not set entirely loose, as with Gaussian distributions. Variance still rises with the mean, but as we have argued in <a href="glm.html#mean-var-rel">7.1.3</a>, this is rather a desired feature than a problem. In the following, we simulate Gamma distributed observations (Figure <a href="glm.html#fig:expreg-3">7.17</a>).</p>
<div class="sourceCode" id="cb606"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb606-1"><a href="glm.html#cb606-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20</span>)</span>
<span id="cb606-2"><a href="glm.html#cb606-2" aria-hidden="true" tabindex="-1"></a>D_gam <span class="ot">&lt;-</span></span>
<span id="cb606-3"><a href="glm.html#cb606-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb606-4"><a href="glm.html#cb606-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Obs =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,</span>
<span id="cb606-5"><a href="glm.html#cb606-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">time =</span> <span class="fu">rgamma</span>(<span class="dv">100</span>, <span class="at">rate =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="dv">3</span>, <span class="at">shape =</span> <span class="dv">2</span>)</span>
<span id="cb606-6"><a href="glm.html#cb606-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb606-7"><a href="glm.html#cb606-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb606-8"><a href="glm.html#cb606-8" aria-hidden="true" tabindex="-1"></a>D_gam <span class="sc">%&gt;%</span></span>
<span id="cb606-9"><a href="glm.html#cb606-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> time)) <span class="sc">+</span></span>
<span id="cb606-10"><a href="glm.html#cb606-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">20</span>)</span></code></pre></div>
<div class="figure"><span id="fig:expreg-3"></span>
<img src="Generalized_Linear_Models_files/figure-html/expreg-3-1.png" alt="Data sampled from a Gamma distribution." width="90%" />
<p class="caption">
Figure 7.17: Data sampled from a Gamma distribution.
</p>
</div>
<p>In comparison to the exponential distribution (Figure <a href="glm.html#fig:expreg-1">7.16</a>), a significant difference is that the mode of the gamma distribution (its peak) is not fixed at zero, but can move along the x-axis. That makes it appear a much more realistic choice for temporal data in behavioral research. We estimate a simple gamma GMM on the simulated data. For historical reasons, <code>brm</code> uses the inverse link function (<span class="math inline">\(\theta = 1/\mu\)</span>) for Gamma regression per default, but that does not actually serve the purpose of link functions to stretch <span class="math inline">\(\mu\)</span> into the range of real numbers. Instead, we explicitly demand a log link, which creates a multiplicative model (Table <a href="glm.html#tab:expreg-4">7.20</a>).</p>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb607-1"><a href="glm.html#cb607-1" aria-hidden="true" tabindex="-1"></a>M_gam <span class="ot">&lt;-</span> <span class="fu">brm</span>(time <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb607-2"><a href="glm.html#cb607-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">Gamma</span>(<span class="at">link =</span> log),</span>
<span id="cb607-3"><a href="glm.html#cb607-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_gam</span>
<span id="cb607-4"><a href="glm.html#cb607-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb608"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb608-1"><a href="glm.html#cb608-1" aria-hidden="true" tabindex="-1"></a><span class="fu">clu</span>(M_gam, <span class="at">mean.func =</span> exp)</span></code></pre></div>
<table>
<caption><span id="tab:expreg-4">Table 7.20: </span>Parameter estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">b_Intercept</td>
<td align="left">Intercept</td>
<td align="right">2.03</td>
<td align="right">1.90</td>
<td align="right">2.17</td>
</tr>
<tr class="even">
<td align="left">shape</td>
<td align="left"></td>
<td align="right">2.25</td>
<td align="right">1.73</td>
<td align="right">2.85</td>
</tr>
<tr class="odd">
<td align="left">lp__</td>
<td align="left"></td>
<td align="right">-294.69</td>
<td align="right">-297.65</td>
<td align="right">-294.03</td>
</tr>
</tbody>
</table>
<p>Both, Exponential and Gamma distributions support the range of real numbers including zero. The weak point of both models is that they have zero as their natural starting point. As we will see in the following section, this assumption is usually violated with RT and ToT data. So, what are they good for, after all? These two models are routinely used for the <em>time intervals (TI)</em> between events that are triggered independently. In nuclear physics the individual triggers are atoms, each one <em>deciding on their own</em> when to decay. If you measure the interval between two decays the time interval is exponentially distributed. (And if you count the neutrons per time interval, the result is a Poisson distribution).</p>
<p>Analog situations can be found in service design and logistics. Take the example of customer support systems. Customers are like atoms in that their decision to file a request is usually independent from each other. Just by chance it can truly happen that two customers call the service center practically in the same moment, so that the lower bound of Zero can actually be reached by some observations. Overwhelmed hotline queues do not make people happy. When planning a support system, the risk of angry customers has to be weighed against the costs of over-staffing. A good design would hit a certain sweet spot and in the ideal case there would be a predictive model of inflow rate of customers.</p>
</div>
<div id="exgauss-reg" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> ExGaussian regression</h3>
<p>The problem with RT and ToT data is that Zero is not a possible outcome, as any task uses up a minimum time to complete. For example, Table <a href="glm.html#tab:exgreg-0">7.21</a> shows the minimum ToT for finding the academic calendar on ten university websites (case Egan). This varies a lot between designs, but is never even close to zero. The last column puts the minimum observed ToT in relation to the observed range. On two of the websites, the offset was even larger than the observed range itself, hence the problem of positive lower boundaries is real in user studies.</p>
<div class="sourceCode" id="cb609"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb609-1"><a href="glm.html#cb609-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Egan)</span></code></pre></div>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb610-1"><a href="glm.html#cb610-1" aria-hidden="true" tabindex="-1"></a>D_egan <span class="sc">%&gt;%</span></span>
<span id="cb610-2"><a href="glm.html#cb610-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(</span>
<span id="cb610-3"><a href="glm.html#cb610-3" aria-hidden="true" tabindex="-1"></a>    success,</span>
<span id="cb610-4"><a href="glm.html#cb610-4" aria-hidden="true" tabindex="-1"></a>    Task <span class="sc">==</span> <span class="st">&quot;academic calendar&quot;</span></span>
<span id="cb610-5"><a href="glm.html#cb610-5" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb610-6"><a href="glm.html#cb610-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Task, Design) <span class="sc">%&gt;%</span></span>
<span id="cb610-7"><a href="glm.html#cb610-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb610-8"><a href="glm.html#cb610-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">min_time =</span> <span class="fu">min</span>(ToT),</span>
<span id="cb610-9"><a href="glm.html#cb610-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">range =</span> <span class="fu">max</span>(ToT) <span class="sc">-</span> min_time,</span>
<span id="cb610-10"><a href="glm.html#cb610-10" aria-hidden="true" tabindex="-1"></a>    min_time <span class="sc">/</span> range</span>
<span id="cb610-11"><a href="glm.html#cb610-11" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb610-12"><a href="glm.html#cb610-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(min_time) </span></code></pre></div>
<table>
<caption><span id="tab:exgreg-0">Table 7.21: </span>The lower boundaries of ToT are not Zero</caption>
<thead>
<tr class="header">
<th align="left">Task</th>
<th align="left">Design</th>
<th align="right">min_time</th>
<th align="right">range</th>
<th align="right">min_time/range</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">academic calendar</td>
<td align="left">University of Antwerp</td>
<td align="right">3</td>
<td align="right">7</td>
<td align="right">0.429</td>
</tr>
<tr class="even">
<td align="left">academic calendar</td>
<td align="left">UGent</td>
<td align="right">8</td>
<td align="right">70</td>
<td align="right">0.114</td>
</tr>
<tr class="odd">
<td align="left">academic calendar</td>
<td align="left">VU Brussel</td>
<td align="right">21</td>
<td align="right">130</td>
<td align="right">0.162</td>
</tr>
<tr class="even">
<td align="left">academic calendar</td>
<td align="left">UHasselt</td>
<td align="right">21</td>
<td align="right">48</td>
<td align="right">0.438</td>
</tr>
<tr class="odd">
<td align="left">academic calendar</td>
<td align="left">University Tilburg</td>
<td align="right">24</td>
<td align="right">39</td>
<td align="right">0.615</td>
</tr>
<tr class="even">
<td align="left">academic calendar</td>
<td align="left">KU Leuven</td>
<td align="right">52</td>
<td align="right">40</td>
<td align="right">1.300</td>
</tr>
<tr class="odd">
<td align="left">academic calendar</td>
<td align="left">RUG</td>
<td align="right">119</td>
<td align="right">132</td>
<td align="right">0.902</td>
</tr>
<tr class="even">
<td align="left">academic calendar</td>
<td align="left">Leiden University</td>
<td align="right">130</td>
<td align="right">181</td>
<td align="right">0.718</td>
</tr>
<tr class="odd">
<td align="left">academic calendar</td>
<td align="left">VU Amsterdam</td>
<td align="right">207</td>
<td align="right">188</td>
<td align="right">1.101</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb611"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>On the first glance, that does not seem to pose a major problem for Gamma distributions, as the left tail vanishes the more a Gamma distribution is shifted to the right, the impossible regions get smaller. However, Gamma distributions inevitably become more symmetric when moving away from the lower boundary. A Gamma distribution far to the right has almost symmetric tails and we may eventually use a Gaussian distribution for approximation. As there is no separate parameter controlling the skew of the curve it may happen that the random component captures the amount of variance, but overdoes the left tail, which introduces a bias on the coefficients. Figure <a href="glm.html#fig:exgreg-1">7.18</a> illustrates the mean-variance-skew relationship on three Gamma distributions that move from left to right (<code>M</code>), keeping the variance constant (<code>V</code>):</p>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb612-1"><a href="glm.html#cb612-1" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">400</span>)</span>
<span id="cb612-2"><a href="glm.html#cb612-2" aria-hidden="true" tabindex="-1"></a>V <span class="ot">&lt;-</span> <span class="dv">8000</span></span>
<span id="cb612-3"><a href="glm.html#cb612-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb612-4"><a href="glm.html#cb612-4" aria-hidden="true" tabindex="-1"></a><span class="do">## gamma</span></span>
<span id="cb612-5"><a href="glm.html#cb612-5" aria-hidden="true" tabindex="-1"></a>rate <span class="ot">&lt;-</span> M <span class="sc">/</span> V</span>
<span id="cb612-6"><a href="glm.html#cb612-6" aria-hidden="true" tabindex="-1"></a>shape <span class="ot">&lt;-</span> rate<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> V</span>
<span id="cb612-7"><a href="glm.html#cb612-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb612-8"><a href="glm.html#cb612-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3000</span>)), <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb612-9"><a href="glm.html#cb612-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb612-10"><a href="glm.html#cb612-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dgamma,</span>
<span id="cb612-11"><a href="glm.html#cb612-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">rate =</span> rate[<span class="dv">1</span>], <span class="at">shape =</span> shape[<span class="dv">1</span>])</span>
<span id="cb612-12"><a href="glm.html#cb612-12" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb612-13"><a href="glm.html#cb612-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb612-14"><a href="glm.html#cb612-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dgamma,</span>
<span id="cb612-15"><a href="glm.html#cb612-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">rate =</span> rate[<span class="dv">1</span>], <span class="at">shape =</span> shape[<span class="dv">2</span>])</span>
<span id="cb612-16"><a href="glm.html#cb612-16" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb612-17"><a href="glm.html#cb612-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb612-18"><a href="glm.html#cb612-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dgamma,</span>
<span id="cb612-19"><a href="glm.html#cb612-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">rate =</span> rate[<span class="dv">1</span>], <span class="at">shape =</span> shape[<span class="dv">3</span>])</span>
<span id="cb612-20"><a href="glm.html#cb612-20" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb612-21"><a href="glm.html#cb612-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;ToT&quot;</span>, <span class="at">y =</span> <span class="st">&quot;density&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:exgreg-1"></span>
<img src="Generalized_Linear_Models_files/figure-html/exgreg-1-1.png" alt="Gamma distributions loose their left-skewness, when moving away from the boundary." width="90%" />
<p class="caption">
Figure 7.18: Gamma distributions loose their left-skewness, when moving away from the boundary.
</p>
</div>
<p>So far in this chapter, we have seen that distributions with one parameter (Poisson, binomial, exponential) have a fixed relationship between mean and variance. In order to vary location and dispersion independently, a second parameter is needed (neg-binomial, beta-binomial, Gamma, Gaussian). However, only a three-parameter distributions can do the trick of setting skew separately. So called <em>exponentially modified Gaussian</em> (Exgaussian) distributions are convolutions of a Gaussian distribution and exponential distribution and have three parameters, <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span> (as usual) and rate <span class="math inline">\(\beta\)</span>. Very roughly, the Gaussian component controls location and dispersion whereas the exponential part adjusts the skew. When <span class="math inline">\(\beta\)</span> is large in comparison to <span class="math inline">\(\mu\)</span>, the distribution is more left skewed. With this additional degree of freedom we can simulate (and estimate) distributions that are far to the right, have strong dispersion <em>and</em> strong skew. Figure <a href="glm.html#fig:exgreg-2">7.19</a> shows Gamma, Gaussian and Exgaussian distributions with exact same mean and variance.</p>
<div class="sourceCode" id="cb613"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb613-1"><a href="glm.html#cb613-1" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">400</span></span>
<span id="cb613-2"><a href="glm.html#cb613-2" aria-hidden="true" tabindex="-1"></a>V <span class="ot">&lt;-</span> <span class="dv">8000</span></span>
<span id="cb613-3"><a href="glm.html#cb613-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb613-4"><a href="glm.html#cb613-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Exgaussian</span></span>
<span id="cb613-5"><a href="glm.html#cb613-5" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> M</span>
<span id="cb613-6"><a href="glm.html#cb613-6" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">80</span></span>
<span id="cb613-7"><a href="glm.html#cb613-7" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(V <span class="sc">-</span> beta<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb613-8"><a href="glm.html#cb613-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb613-9"><a href="glm.html#cb613-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Gamma</span></span>
<span id="cb613-10"><a href="glm.html#cb613-10" aria-hidden="true" tabindex="-1"></a>rate <span class="ot">&lt;-</span> M <span class="sc">/</span> V</span>
<span id="cb613-11"><a href="glm.html#cb613-11" aria-hidden="true" tabindex="-1"></a>shape <span class="ot">&lt;-</span> rate<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> V</span>
<span id="cb613-12"><a href="glm.html#cb613-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb613-13"><a href="glm.html#cb613-13" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">800</span>)), <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb613-14"><a href="glm.html#cb613-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb613-15"><a href="glm.html#cb613-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dgamma,</span>
<span id="cb613-16"><a href="glm.html#cb613-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">rate =</span> rate, <span class="at">shape =</span> shape),</span>
<span id="cb613-17"><a href="glm.html#cb613-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">colour =</span> <span class="st">&quot;Gamma&quot;</span>)</span>
<span id="cb613-18"><a href="glm.html#cb613-18" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb613-19"><a href="glm.html#cb613-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb613-20"><a href="glm.html#cb613-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dnorm,</span>
<span id="cb613-21"><a href="glm.html#cb613-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> M, <span class="at">sd =</span> <span class="fu">sqrt</span>(V)),</span>
<span id="cb613-22"><a href="glm.html#cb613-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">colour =</span> <span class="st">&quot;Gaussian&quot;</span>)</span>
<span id="cb613-23"><a href="glm.html#cb613-23" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb613-24"><a href="glm.html#cb613-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb613-25"><a href="glm.html#cb613-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> brms<span class="sc">::</span>dexgaussian,</span>
<span id="cb613-26"><a href="glm.html#cb613-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(</span>
<span id="cb613-27"><a href="glm.html#cb613-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">mu =</span> M,</span>
<span id="cb613-28"><a href="glm.html#cb613-28" aria-hidden="true" tabindex="-1"></a>      <span class="at">sigma =</span> sigma,</span>
<span id="cb613-29"><a href="glm.html#cb613-29" aria-hidden="true" tabindex="-1"></a>      <span class="at">beta =</span> beta</span>
<span id="cb613-30"><a href="glm.html#cb613-30" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb613-31"><a href="glm.html#cb613-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">colour =</span> <span class="st">&quot;Exgaussian&quot;</span>)</span>
<span id="cb613-32"><a href="glm.html#cb613-32" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb613-33"><a href="glm.html#cb613-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">colour =</span> <span class="st">&quot;Distribution&quot;</span>, <span class="at">x =</span> <span class="st">&quot;ToT&quot;</span>, <span class="at">y =</span> <span class="st">&quot;density&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:exgreg-2"></span>
<img src="Generalized_Linear_Models_files/figure-html/exgreg-2-1.png" alt="Exgaussian distributions can be far from zero and still be left skewed" width="90%" />
<p class="caption">
Figure 7.19: Exgaussian distributions can be far from zero and still be left skewed
</p>
</div>
<p>The Gamma distribution in this example starts approaching a the perfect bell curve of the Gaussian distribution. In contrast, the Exgaussian distribution takes a steep left climb followed by a long right tail, which is caused by its pronounced exponential component. We do the usual exercise to simulate a grand mean model (Figure <a href="glm.html#fig:exgreg-4">7.20</a>) and recover the parameters with the help of the <code>brm</code> engine (Table <a href="glm.html#tab:exgreg-5">7.22</a>))</p>
<div class="sourceCode" id="cb614"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb614-1"><a href="glm.html#cb614-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Chapter_GLM)</span></code></pre></div>
<div class="sourceCode" id="cb615"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb615-1"><a href="glm.html#cb615-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">126</span>)</span>
<span id="cb615-2"><a href="glm.html#cb615-2" aria-hidden="true" tabindex="-1"></a>D_exg <span class="ot">&lt;-</span></span>
<span id="cb615-3"><a href="glm.html#cb615-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">Y =</span> <span class="fu">rexgaussian</span>(<span class="dv">100</span>, <span class="at">mu =</span> <span class="dv">100</span>, <span class="at">sigma =</span> <span class="dv">20</span>, <span class="at">beta =</span> <span class="dv">30</span>))</span>
<span id="cb615-4"><a href="glm.html#cb615-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb615-5"><a href="glm.html#cb615-5" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(D_exg<span class="sc">$</span>Y) <span class="sc">+</span> <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">300</span>)</span></code></pre></div>
<div class="figure"><span id="fig:exgreg-4"></span>
<img src="Generalized_Linear_Models_files/figure-html/exgreg-4-1.png" alt="Sampling from an Exgaussian distribution" width="90%" />
<p class="caption">
Figure 7.20: Sampling from an Exgaussian distribution
</p>
</div>
<div class="sourceCode" id="cb616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb616-1"><a href="glm.html#cb616-1" aria-hidden="true" tabindex="-1"></a>M_exg <span class="ot">&lt;-</span> <span class="fu">brm</span>(Y <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb616-2"><a href="glm.html#cb616-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> exgaussian,</span>
<span id="cb616-3"><a href="glm.html#cb616-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_exg</span>
<span id="cb616-4"><a href="glm.html#cb616-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb617"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb617-1"><a href="glm.html#cb617-1" aria-hidden="true" tabindex="-1"></a><span class="fu">clu</span>(M_exg)</span></code></pre></div>
<table>
<caption><span id="tab:exgreg-5">Table 7.22: </span>Parameter estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">b_Intercept</td>
<td align="left">Intercept</td>
<td align="right">99.4</td>
<td align="right">92.9</td>
<td align="right">106.6</td>
</tr>
<tr class="even">
<td align="left">sigma</td>
<td align="left"></td>
<td align="right">17.7</td>
<td align="right">12.3</td>
<td align="right">24.0</td>
</tr>
<tr class="odd">
<td align="left">beta</td>
<td align="left"></td>
<td align="right">31.6</td>
<td align="right">23.5</td>
<td align="right">41.6</td>
</tr>
<tr class="even">
<td align="left">lp__</td>
<td align="left"></td>
<td align="right">-500.1</td>
<td align="right">-503.6</td>
<td align="right">-499.0</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>Noteworthy, for Exgaussian models the brm engine uses the identity link function by default. While this is rather convenient for interpretation, it could theoretically lead to impossible predictions. As we will see later, the Exgaussian family is not immune, but <em>robust to impossible predictions</em> because of its tiny left tail.</p>
<p>Most GLM family members introduced so far are more or less established in the literature. The Exgaussian is a newcomer and it does not come with a clear cut data generation process and may therefore very well be considered just a hack. The following two sections examine Exgaussian models more closely and out them in competition against Gamma and Gaussian models. We will be using primarily graphical methods here, but will come back these cases in chapter <a href="wwm.html#choose-dist">8.2.5</a> with a more formal approach.</p>
<!-- In chapter \@ref(crossover) we have tested Egan's assumption that user differences produce more variation in performance than designs do. In order to "normalize" the ToT outcome, log transformation was applied. Using an exgaussian regression as part of the GLM framework, we are now set to do without this humble trick. -->
<div id="rt" class="section level4" number="7.3.2.1">
<h4><span class="header-section-number">7.3.2.1</span> Reaction times</h4>
<p>In experimental studies, the <em>inertia of the nervous system</em> sets a limit larger than zero for reaction times. This is partly due to to some hard electrochemical and biomechanical limits of the peripheral systems. (Perhaps, octopuses with their long arms have decentralized nervous system for a reason!) Nerve cells and muscle fibers are slow working horses, adding to the time our minds need to process complex information. Even in the most simple priming experiments, there always is a minimum time necessary to collect an idea from the memories and activate the surrounding nodes. Therefore, experimental reaction times have a positive minimum of at least 200ms.</p>
<p>Let’s see an example: In the Hugme case, we tried to pin down the hypothetical Geek personality and used Need-for-cognition as a predictor for reaction times in a semantic Stroop task. Like in the original Stroop task, participants must name the color of words, but these are non-color words from two categories (geek/non-geek). Furthermore, these words are preceded by Geek/non-geek pictures. The theory was that a real geek, when seeing an open computer case followed by the word “explore” will briefly reminisce and be distracted from the primary task, naming the color. It did not work this way at all and we only saw minuscule effects. That is good news for the analysis here. The only effect was that Geek primes caused a minimal delay. Because there are no other effects, we can use a rather simple multi-level CGM to compare how Gaussian, Gamma and Exgaussian fit reaction times. Let’s take a first look some parts of the data:</p>
<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb619-1"><a href="glm.html#cb619-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Hugme)</span></code></pre></div>
<p>Most of the participant-level frequency distributions of RT have a clear cut-off at around .25 seconds. The steepness of the left climb varies between participants, but some at least are rather sharp, with a right tail that is leveling off slowly. When compared to the illustrations above, it seems that an Exgaussian model could accommodate this data well (Figure <a href="glm.html#fig:rt-1">7.21</a>).</p>
<div class="sourceCode" id="cb620"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb620-1"><a href="glm.html#cb620-1" aria-hidden="true" tabindex="-1"></a>D_hugme <span class="sc">%&gt;%</span></span>
<span id="cb620-2"><a href="glm.html#cb620-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(Part <span class="sc">&lt;=</span> <span class="dv">6</span>) <span class="sc">%&gt;%</span></span>
<span id="cb620-3"><a href="glm.html#cb620-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> RT, <span class="at">x =</span> PrimeGeek)) <span class="sc">+</span></span>
<span id="cb620-4"><a href="glm.html#cb620-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>Part, <span class="at">nrow =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb620-5"><a href="glm.html#cb620-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_violin</span>()</span></code></pre></div>
<div class="figure"><span id="fig:rt-1"></span>
<img src="Generalized_Linear_Models_files/figure-html/rt-1-1.png" alt="Participant-level RT distributions" width="90%" />
<p class="caption">
Figure 7.21: Participant-level RT distributions
</p>
</div>
<p>Even the effect of geek primes is barely visible, but we clearly observe a left skew in most of the participants. In the following, we run three CGM models with Exgaussian, Gamma or Gaussian response distributions. For the subsequent analysis, multi-model posterior distributions and posterior predictive distributions are extracted and merged into one multi-model posterior object <code>P_1</code>.</p>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb621-1"><a href="glm.html#cb621-1" aria-hidden="true" tabindex="-1"></a><span class="fu">memory.limit</span>(<span class="dv">16000</span>)</span>
<span id="cb621-2"><a href="glm.html#cb621-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb621-3"><a href="glm.html#cb621-3" aria-hidden="true" tabindex="-1"></a>F_1 <span class="ot">&lt;-</span> <span class="fu">formula</span>(RT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> PrimeGeek <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">+</span> PrimeGeek <span class="sc">|</span> Part))</span>
<span id="cb621-4"><a href="glm.html#cb621-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb621-5"><a href="glm.html#cb621-5" aria-hidden="true" tabindex="-1"></a>M_1_gau <span class="ot">&lt;-</span> D_hugme <span class="sc">%&gt;%</span></span>
<span id="cb621-6"><a href="glm.html#cb621-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(F_1,</span>
<span id="cb621-7"><a href="glm.html#cb621-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> gaussian,</span>
<span id="cb621-8"><a href="glm.html#cb621-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb621-9"><a href="glm.html#cb621-9" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb621-10"><a href="glm.html#cb621-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb621-11"><a href="glm.html#cb621-11" aria-hidden="true" tabindex="-1"></a>M_1_gam <span class="ot">&lt;-</span> D_hugme <span class="sc">%&gt;%</span></span>
<span id="cb621-12"><a href="glm.html#cb621-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(F_1,</span>
<span id="cb621-13"><a href="glm.html#cb621-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">Gamma</span>(<span class="at">link =</span> identity),</span>
<span id="cb621-14"><a href="glm.html#cb621-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb621-15"><a href="glm.html#cb621-15" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb621-16"><a href="glm.html#cb621-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb621-17"><a href="glm.html#cb621-17" aria-hidden="true" tabindex="-1"></a>M_1_exg <span class="ot">&lt;-</span> D_hugme <span class="sc">%&gt;%</span></span>
<span id="cb621-18"><a href="glm.html#cb621-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(F_1,</span>
<span id="cb621-19"><a href="glm.html#cb621-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> exgaussian,</span>
<span id="cb621-20"><a href="glm.html#cb621-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb621-21"><a href="glm.html#cb621-21" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb621-22"><a href="glm.html#cb621-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb621-23"><a href="glm.html#cb621-23" aria-hidden="true" tabindex="-1"></a>P_1 <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb621-24"><a href="glm.html#cb621-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_1_gau),</span>
<span id="cb621-25"><a href="glm.html#cb621-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_1_gam),</span>
<span id="cb621-26"><a href="glm.html#cb621-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_1_exg)</span>
<span id="cb621-27"><a href="glm.html#cb621-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb621-28"><a href="glm.html#cb621-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb621-29"><a href="glm.html#cb621-29" aria-hidden="true" tabindex="-1"></a>T_1_predict <span class="ot">&lt;-</span></span>
<span id="cb621-30"><a href="glm.html#cb621-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(</span>
<span id="cb621-31"><a href="glm.html#cb621-31" aria-hidden="true" tabindex="-1"></a>    <span class="fu">post_pred</span>(M_1_gau, <span class="at">thin =</span> <span class="dv">5</span>),</span>
<span id="cb621-32"><a href="glm.html#cb621-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">post_pred</span>(M_1_gam, <span class="at">thin =</span> <span class="dv">5</span>),</span>
<span id="cb621-33"><a href="glm.html#cb621-33" aria-hidden="true" tabindex="-1"></a>    <span class="fu">post_pred</span>(M_1_exg, <span class="at">thin =</span> <span class="dv">5</span>)</span>
<span id="cb621-34"><a href="glm.html#cb621-34" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb621-35"><a href="glm.html#cb621-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>()</span></code></pre></div>
<p><em>Note</em> that the predictive posterior distributions runs over thousands of observation, which creates very large objects in your computer’s RAM. To prevent running into a memory limit, you can crank up the memory limit (<code>memory.limit</code>) or thin out the number of posterior predictive samples by a factor, or do both.</p>
<p>The below plot shows the population-level effects for the three models. The center estimates are very close, which means that neither of the models has a significant bias. However, the Exgaussian model produces much tighter credibility intervals. We have seen such an effect before, when on over-dispersed data, a Poisson model produced tighter intervals than the Negbinomial model. Here it is the other way round: the model with more parameters produces better levels of certainty (Figure <a href="glm.html#fig:rt-2">7.22</a>).</p>
<div class="sourceCode" id="cb622"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb622-1"><a href="glm.html#cb622-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(P_1) <span class="sc">%&gt;%</span></span>
<span id="cb622-2"><a href="glm.html#cb622-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(</span>
<span id="cb622-3"><a href="glm.html#cb622-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> center, <span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper,</span>
<span id="cb622-4"><a href="glm.html#cb622-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> model</span>
<span id="cb622-5"><a href="glm.html#cb622-5" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb622-6"><a href="glm.html#cb622-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>fixef, <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb622-7"><a href="glm.html#cb622-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_crossbar</span>(<span class="at">width =</span> .<span class="dv">2</span>, <span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:rt-2"></span>
<img src="Generalized_Linear_Models_files/figure-html/rt-2-1.png" alt="Comparing CLU estimates of Exgaussian, Gamma and Gaussian models" width="90%" />
<p class="caption">
Figure 7.22: Comparing CLU estimates of Exgaussian, Gamma and Gaussian models
</p>
</div>
<!-- The following residual plot give us a hint why this might be so: Apparently, the Exgaussian model takes a much steeper left climb, whereas  -->
<p>If the Exgaussian model has a better fit, we should primarily see that in how the residuals are shaped. The Exgaussian distribution has one more degree of freedom, which can be used to set an arbitrary skew. The following reveals that the extra flexibility of the Exgaussian has been employed. Both, Gaussian and Gamma are almost symmetric, whereas the Exgaussian takes a steeper left climb. The three distributions have almost the same right tail, but the left tail of the Exgaussian is consistently shorter.</p>
<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb623-1"><a href="glm.html#cb623-1" aria-hidden="true" tabindex="-1"></a>D_hugme <span class="ot">&lt;-</span> D_hugme <span class="sc">%&gt;%</span></span>
<span id="cb623-2"><a href="glm.html#cb623-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(T_1_predict) <span class="sc">%&gt;%</span></span>
<span id="cb623-3"><a href="glm.html#cb623-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">resid =</span> RT <span class="sc">-</span> center)</span>
<span id="cb623-4"><a href="glm.html#cb623-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb623-5"><a href="glm.html#cb623-5" aria-hidden="true" tabindex="-1"></a>D_hugme <span class="sc">%&gt;%</span></span>
<span id="cb623-6"><a href="glm.html#cb623-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> resid, <span class="at">x =</span> PrimeGeek)) <span class="sc">+</span></span>
<span id="cb623-7"><a href="glm.html#cb623-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(. <span class="sc">~</span> model) <span class="sc">+</span></span>
<span id="cb623-8"><a href="glm.html#cb623-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_violin</span>()</span></code></pre></div>
<div class="figure"><span id="fig:rt-3"></span>
<img src="Generalized_Linear_Models_files/figure-html/rt-3-1.png" alt="Comparing residuals of Exgaussian, Gamma and Gaussian models" width="90%" />
<p class="caption">
Figure 7.23: Comparing residuals of Exgaussian, Gamma and Gaussian models
</p>
</div>
<p>We can carefully conclude that the Exgaussian may be very useful for analyzing psychological experiments as it seems to better accommodate reaction times. Given the novelty of Exgaussian models, it is recommended that researchers carry out a careful multi-model analysis. In <a href="wwm.html#choose-dist">8.2.5</a> we will come back to this case with a more formal approach and confirm that from the three response distributions, the Exgaussian has the best predictive accuracy.</p>
<div class="sourceCode" id="cb624"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="tot" class="section level4" number="7.3.2.2">
<h4><span class="header-section-number">7.3.2.2</span> Time-on-task</h4>
<p>Experimental psychologists call the Stroop task a complex one. But, essentially it is a decision between three colors and minimal processing time is rather short. Compared to tasks in usability studies, such as finding information on websites, this is almost trivial. Also, the dynamics of complex tasks can be rather different. For example, a single user error at the beginning of a task sequence can have dramatic consequences, such as getting totally lost. While ToT data also has a strictly positive lower boundary (the fastest way of achieving the goal), it often has a much wider spread than RT data. In the following we will repeat the informal model comparison from the last section for ToT data.</p>
<p>We compare the three patterns of randomness on the CUE8 data set, which contains ToT measures on five tasks on a car rental website. In this study 14 professional teams took part with two conditions: remote and moderated sessions. As data from the remote condition is contaminated with cheaters, we only use the moderated sessions. In order to compare the impact of the chosen distribution on the coefficient estimates, we examine the estimation of the five tasks (as an AGM).</p>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb625-1"><a href="glm.html#cb625-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(CUE8)</span></code></pre></div>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb626-1"><a href="glm.html#cb626-1" aria-hidden="true" tabindex="-1"></a>D_cue8_mod <span class="ot">&lt;-</span></span>
<span id="cb626-2"><a href="glm.html#cb626-2" aria-hidden="true" tabindex="-1"></a>  D_cue8 <span class="sc">%&gt;%</span></span>
<span id="cb626-3"><a href="glm.html#cb626-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(Condition <span class="sc">==</span> <span class="st">&quot;moderated&quot;</span>, <span class="sc">!</span><span class="fu">is.na</span>(ToT)) <span class="sc">%&gt;%</span></span>
<span id="cb626-4"><a href="glm.html#cb626-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tbl_obs</span>()</span></code></pre></div>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb627-1"><a href="glm.html#cb627-1" aria-hidden="true" tabindex="-1"></a>F_4 <span class="ot">&lt;-</span> <span class="fu">formula</span>(ToT <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> Task <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Part))</span>
<span id="cb627-2"><a href="glm.html#cb627-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb627-3"><a href="glm.html#cb627-3" aria-hidden="true" tabindex="-1"></a>M_4_gau <span class="ot">&lt;-</span> D_cue8_mod <span class="sc">%&gt;%</span></span>
<span id="cb627-4"><a href="glm.html#cb627-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(F_4,</span>
<span id="cb627-5"><a href="glm.html#cb627-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">gaussian</span>(<span class="at">link =</span> log),</span>
<span id="cb627-6"><a href="glm.html#cb627-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ., <span class="at">iter =</span> <span class="dv">2000</span></span>
<span id="cb627-7"><a href="glm.html#cb627-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb627-8"><a href="glm.html#cb627-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb627-9"><a href="glm.html#cb627-9" aria-hidden="true" tabindex="-1"></a>M_4_exg <span class="ot">&lt;-</span> D_cue8_mod <span class="sc">%&gt;%</span></span>
<span id="cb627-10"><a href="glm.html#cb627-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(F_4,</span>
<span id="cb627-11"><a href="glm.html#cb627-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">exgaussian</span>(<span class="at">link =</span> log),</span>
<span id="cb627-12"><a href="glm.html#cb627-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ., <span class="at">iter =</span> <span class="dv">2000</span></span>
<span id="cb627-13"><a href="glm.html#cb627-13" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb627-14"><a href="glm.html#cb627-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb627-15"><a href="glm.html#cb627-15" aria-hidden="true" tabindex="-1"></a>M_4_gam <span class="ot">&lt;-</span> D_cue8_mod <span class="sc">%&gt;%</span></span>
<span id="cb627-16"><a href="glm.html#cb627-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(F_4,</span>
<span id="cb627-17"><a href="glm.html#cb627-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">Gamma</span>(<span class="at">link =</span> log),</span>
<span id="cb627-18"><a href="glm.html#cb627-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ., <span class="at">iter =</span> <span class="dv">2000</span></span>
<span id="cb627-19"><a href="glm.html#cb627-19" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb627-20"><a href="glm.html#cb627-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb627-21"><a href="glm.html#cb627-21" aria-hidden="true" tabindex="-1"></a>P_4 <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb627-22"><a href="glm.html#cb627-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_4_gau),</span>
<span id="cb627-23"><a href="glm.html#cb627-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_4_gam) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">value =</span> <span class="fu">if_else</span>(value <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;fixef&quot;</span>, <span class="st">&quot;ranef&quot;</span>), <span class="fu">exp</span>(value), value)),</span>
<span id="cb627-24"><a href="glm.html#cb627-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">posterior</span>(M_4_exg)</span>
<span id="cb627-25"><a href="glm.html#cb627-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb627-26"><a href="glm.html#cb627-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb627-27"><a href="glm.html#cb627-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb627-28"><a href="glm.html#cb627-28" aria-hidden="true" tabindex="-1"></a>T_4_predict <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb627-29"><a href="glm.html#cb627-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">post_pred</span>(M_4_gau, <span class="at">thin =</span> <span class="dv">5</span>),</span>
<span id="cb627-30"><a href="glm.html#cb627-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">post_pred</span>(M_4_gam, <span class="at">thin =</span> <span class="dv">5</span>),</span>
<span id="cb627-31"><a href="glm.html#cb627-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">post_pred</span>(M_4_exg, <span class="at">thin =</span> <span class="dv">5</span>)</span>
<span id="cb627-32"><a href="glm.html#cb627-32" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb627-33"><a href="glm.html#cb627-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>()</span></code></pre></div>
<p><em>Note</em> that the Gamma model caused trouble when estimated with an identity link. For this reason, all three models were estimated on a log-linearized scale. This makes the back-transformed coefficients multiplicative, which actually makes more sense, as we have seen in <a href="glm.html#speaking-multipliers">7.2.1.1</a>.</p>
<div class="sourceCode" id="cb628"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb628-1"><a href="glm.html#cb628-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(P_4, <span class="at">mean.func =</span> exp) <span class="sc">%&gt;%</span></span>
<span id="cb628-2"><a href="glm.html#cb628-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(</span>
<span id="cb628-3"><a href="glm.html#cb628-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> center, <span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper,</span>
<span id="cb628-4"><a href="glm.html#cb628-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> model</span>
<span id="cb628-5"><a href="glm.html#cb628-5" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb628-6"><a href="glm.html#cb628-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>fixef) <span class="sc">+</span></span>
<span id="cb628-7"><a href="glm.html#cb628-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_crossbar</span>(<span class="at">width =</span> .<span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb628-8"><a href="glm.html#cb628-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">45</span>, <span class="at">hjust =</span> <span class="dv">1</span>))</span></code></pre></div>
<div class="figure"><span id="fig:tot-2"></span>
<img src="Generalized_Linear_Models_files/figure-html/tot-2-1.png" alt="Comparing CLU estimates of Exgaussian, Gamma and Gaussian models" width="90%" />
<p class="caption">
Figure 7.24: Comparing CLU estimates of Exgaussian, Gamma and Gaussian models
</p>
</div>
<p>In the previous section <a href="glm.html#rt">7.3.2.1</a>, we have seen for RT data, that the three models agreed on the center estimates. As shown in Figure <a href="glm.html#fig:tot-2">7.24</a>, for ToT data the three families produce rather different coefficients. It seems that the models disagree on all but Task 1. It is not more than an observation, but the Exgaussian model produces the smallest variance between tasks. A recurring observation is that Exgaussian regression also produced the tightest credibility intervals.</p>
<p>Inspecting the residual distributions yields a similar pattern as with RT data: the Exgaussian model predicts a much sharper left raise than the other two.</p>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="glm.html#cb629-1" aria-hidden="true" tabindex="-1"></a><span class="fu">left_join</span>(T_4_predict, D_cue8_mod, <span class="at">by =</span> <span class="st">&quot;Obs&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb629-2"><a href="glm.html#cb629-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">resid =</span> ToT <span class="sc">-</span> center) <span class="sc">%&gt;%</span></span>
<span id="cb629-3"><a href="glm.html#cb629-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> model, <span class="at">y =</span> resid)) <span class="sc">+</span></span>
<span id="cb629-4"><a href="glm.html#cb629-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>Task, <span class="at">nrow =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb629-5"><a href="glm.html#cb629-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_violin</span>() <span class="sc">+</span></span>
<span id="cb629-6"><a href="glm.html#cb629-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">45</span>, <span class="at">hjust =</span> <span class="dv">1</span>))</span></code></pre></div>
<div class="figure"><span id="fig:tot-3"></span>
<img src="Generalized_Linear_Models_files/figure-html/tot-3-1.png" alt="Comparing residuals of Exgaussian, Gamma and Gaussian models" width="90%" />
<p class="caption">
Figure 7.25: Comparing residuals of Exgaussian, Gamma and Gaussian models
</p>
</div>
<div class="sourceCode" id="cb630"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>In general, it seems that Exgaussian models for RT and ToT accommodates left skew better and produces estimates that are sharp and conservative. But, These are just informal comparisons. In chapter <a href="wwm.html#wwm">8</a>, we will apply formal criteria for selecting between distributions. As will turn out, Gamma distribution is the preferred distribution for ToT in CUE8.</p>
<!-- Could it be true, that Gaussian and Gamma models overestimate group mean differences for left skewed RT and ToT responses? We examine this possibility by simulating a well-known experiment using an exgaussian distribution. -->
<!-- #### MAKE Stroop -->
<!-- ```{r} -->
<!-- sim_Stroop <- function(beta_0 = 500, # congruent -->
<!--                        beta_1 = 50,  # neutral -->
<!--                        beta_2 = 120, # incongr -->
<!--                        sigma = 20,   -->
<!--                        beta = 70, -->
<!--                        N = 400,      # obs per condition -->
<!--                        seed = 42){ -->
<!--   set.seed(seed) -->
<!--   tibble(Condition = rep(c("con", "neu", "inc"), N), -->
<!--              mu = beta_0 + beta_1 * (Condition == "neu") + beta_2 * (Condition == "inc"), -->
<!--              RT = rexgaussian(N * 3, mu, sigma, beta)) %>%  -->
<!--     as_tbl_obs() -->
<!--   } -->
<!-- D_stroop <- sim_Stroop() -->
<!-- D_stroop %>%  -->
<!--   group_by(Condition) %>%  -->
<!--   summarize(mean(RT)) %>%  -->
<!--   kable() -->
<!-- D_stroop %>%  -->
<!--   ggplot(aes(x = RT, col = Condition)) + -->
<!--   geom_density() -->
<!-- ``` -->
<!-- In the same way as above, we recover the effects by three models that have the same likelihood, but differ in their response distribution. -->
<!-- ```{r opts.label = "mcmc"} -->
<!-- F_stroop <- formula(RT ~ Condition) -->
<!-- M_stroop_gau <- D_stroop %>%  -->
<!--   brm(F_stroop, -->
<!--       family = gaussian, -->
<!--       data = .) -->
<!-- M_stroop_exg <- D_stroop %>%  -->
<!--   brm(F_stroop, -->
<!--       family = exgaussian, -->
<!--       data = .) -->
<!-- M_stroop_gam <- D_stroop %>%  -->
<!--   brm(F_stroop, -->
<!--       family = Gamma(link = identity), -->
<!--       data = .) -->
<!-- P <- bind_rows(  -->
<!--   posterior(M_stroop_gau), -->
<!--   posterior(M_stroop_gam), -->
<!--   posterior(M_stroop_exg) -->
<!-- ) -->
<!-- T_stroop_predict <- bind_rows(  -->
<!--   post_pred(M_stroop_gau, thin = 4), -->
<!--   post_pred(M_stroop_gam, thin = 4), -->
<!--   post_pred(M_stroop_exg, thin = 4) -->
<!-- ) %>%  -->
<!--   predict() -->
<!-- ``` -->
<!-- ```{r opts.label = "mcsync"} -->
<!-- sync_CE(Chapter_GLM, M_stroop_gau, M_stroop_gam, M_stroop_exg, P_stroop, T_stroop_predict, D_stroop) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- fixef(P_stroop) %>%  -->
<!--   ggplot(aes(y = fixef, x = center, xmin = lower, xmax = upper, color = model)) + -->
<!--   facet_wrap(~fixef, ncol = 1, scales = "free") + -->
<!--   geom_point(size = 2) + -->
<!--   geom_errorbarh() -->
<!-- ``` -->
<!-- What is confirmed by the simulation is that the exgaussian model, when it is the true one, produces more certain estimates.  -->
<!-- ```{r} -->
<!-- D_stroop  %>%    -->
<!--   left_join(T_stroop_predict) %>%  -->
<!--   mutate(resid = RT - center) %>%  -->
<!--   ggplot(aes(x = resid, color = model)) + -->
<!--   facet_wrap(~Condition) + -->
<!--   geom_density(adjust = 2) + -->
<!--   xlim(-200, 350) -->
<!-- ``` -->
<!-- Different to what has been observed above, the shapes of the residual distributions do not differ much, except for a shift to the right. What could be accountable for that is that the simulation only contained three homogenous groups, rather than the many groups in the previous multi-level data sets (random factors). It remains to be clarified what precisely the biases and drags are caused by, ill-specified response distribution for RT or ToT in complex research designs. Despite these question marks, it has been confirmed that the superior certainty of estimates is not just an artifact of the exgaussian model, but is real and likely to make quantitative inference from RT and ToT data more efficient. -->
<p>One last issue remains to get clarified: using the identity link for Exgaussian models is very convenient and is probably much safer as compared to Gaussian models with their fatter left tails. Still, impossible predictions can arise. But, how much of a risk is there? We can check this on the posterior predictive distributions of both studies, CUE8 and Hugme. Table <a href="glm.html#tab:tot-5">7.23</a> shows the proportion of observations, that get a negative 2.5% credibility limit assigned.</p>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="glm.html#cb631-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(Hugme<span class="sc">$</span>T_1_predict, CUE8<span class="sc">$</span>T_4_predict) <span class="sc">%&gt;%</span></span>
<span id="cb631-2"><a href="glm.html#cb631-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(model) <span class="sc">%&gt;%</span></span>
<span id="cb631-3"><a href="glm.html#cb631-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="fu">mean</span>(lower <span class="sc">&lt;</span> <span class="dv">0</span>)) </span></code></pre></div>
<table>
<caption><span id="tab:tot-5">Table 7.23: </span></caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">mean(lower &lt; 0)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">M_1_exg</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">M_1_gam</td>
<td align="right">0.000</td>
</tr>
<tr class="odd">
<td align="left">M_1_gau</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">M_4_exg</td>
<td align="right">0.014</td>
</tr>
<tr class="odd">
<td align="left">M_4_gam</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="left">M_4_gau</td>
<td align="right">0.750</td>
</tr>
</tbody>
</table>
<p>For the RT data (M_1), impossible predictions are not an issue with any of the models, as all 2.5% quantiles are positive. That is different for ToT (M_4): while the Gamma model is inherently immune to negative predictions, the Exgaussian model produced a few impossible lower 2.5% limits (around 3%). The Gaussian model is extremely off: more than 70% of all predictions have impossible lower 2.5% limits.</p>
<p>In the scientific literature, using the Gaussian model for RT and ToT is still the default. Probably, that is due to the lack of user-friendly engines supporting the more exotic GLM family members, Gamma or Exgaussian regression. The Brms engine covers a much broader set of distributions than any other implementation before and researchers have the choice. This chapter attempted to provide theoretical arguments as well as empirical indications that the Exgaussian regression can be a better choice than Gaussian and gamma. First of all, it accommodates the strong left skew of RT and ToT much better than the Gamma, which takes a too symmetric form when far from the left boundary. Second, it is reasonably robust to impossible predictions, even when using the convenient identity link function. Third, and that is almost too good to be true, it strongly improves certainty in predictors. It seems that Exgaussian models are more efficient for carving out delicate effects in experimental studies.</p>
<p>However, as the discussion has just started, to declare it settled would be premature. In contrast, the aim of this section was to illustrate a semi-formal approach that researchers can follow to choose among the candidate models for their specific RT and ToT data. Data from other RT paradigms might take different shapes. For example, when measuring RT by events in EEG signals (rather than actual key presses), motor time plays a much smaller role, pushing RTs closer to the left boundary and the Gamma model would be in favor.</p>
<p>That being said, the brms engine offers even more opportunities. First, it supports two more distributions with an offset component: the shifted log-normal and the Wiener distribution. Interestingly, the latter grounds on one of the few formally specified cognitive process models, the diffusion model for simple choice tasks. All four parameters of the Wiener distribution are directly linked to individual elements of the cognitive process. This brings us to another extension of Brms, which I will briefly cover in the last section of this chapter. <em>Distributional models</em> put linearized predictor terms not just on the distribution mean, but also on other parameters of a distribution family. It is thrilling to imagine what such models can do for experimental cognitive research.</p>
<!-- The vast majority of statistical analysis capitalizes on the location parameters. We ask whether an increase in a continuous predictor causes an increase in the outcome or if one group has a higher average than the other.  -->
<!-- Only in the analysis of random effects have we drawn conclusions from dispersion parameters, such as to test Egans claim. In design research, the variance in performance is a crucial issue. To give another example: reportedly, several areas of cognitive functioning deteriorate with age, on average, but variance typically increases. It was the very idea of Dennis Egan that designs should not just improve performance on average, but also keep variance at a minimum. Hence, linking variance to predictors, such as design and age, can be a fruitful endeavour under the paradigm of robust designs. For RT the beforementioned Wiener distribution matches RTs in simple choice tasks and every parameter corresponds with an element in the so called  diffusion model. In design research, such ideas are almost unexplored. Perhaps, one day a researcher finds that the gaussian and the exponential component are influenced by different design features. -->
<!-- ### Literature -->
<!--The theoretical thoughts at the beginning of the chapter spoke of two components of tasks that have different properties: the motor part is slow, but higly predictable, whereas the cognitive part is very fast, but performance varies a lot. Possibly, motor time could be mapped to the gaussian component and the cognitive time be assigned to the exponential component. In a distributional model, all parameters of the distribution can get their own set of predictors. For example, in a Stroop experiment, the exgaussian $\beta$ could  be linked to the experimental conditions, whereas $\mu$ gets a random effect to accomodate individual differences in motor speed.-->
<!--
#### Dealing with upper boundaries

As we have seen, the exgaussian linear model gracefully deals with lower boundaries larger than zero. However, so far it maintains the assumption that there is no upper boundary for durations. This assumption is uncritical when the empirical setup gives all participants enough headroom to finish their tasks. When most durations are rather short, fractions of a second to just a few minutes, one does not mind waiting for a few that take longer. However, in user research tasks are often rather complex and purportedly routine tasks, like renting a car online can be surprisingly long-running. In order to reduce the risk of user frustration, user studies are often imposing a limit for task completion, say five minutes, and give participants the freedom to give up whenever they want. As a consequence, there will be a number of *censorded* observations. These cannot be taken at face value, such as "user $i$ took five minutes  to complete task $j$", but as a minimum for an otherwise unknown duration: "it would have taken user $i$ *at least* five minutes to complete the task".

Had the researchers in the CUE8 study limited the time per task to ten minutes, the distribution would take the following shape:
-->
<!--
For tasks 1 and 4, the smooth running right tail of the distribution is replaced by a sudden peak. So, how to deal with such observations? A brute-force approach would be to regard them as outliers and completely remove them from the data. However, that would induce a serious bias as those observations are real and they are even the most crucial to pin-point problems in the design. In consequence, one would get a highly optimistic estimate for ToT. The optimistic bias would be reduced, but not removed, if one would take the censored observations at face value. In addition to the location bias, all dispersion estimates (residuals, random effects variance) will be underestimated, too.

The `brm` engine allows to specify censored #114 -->
<!--observations by another formula extension, *additional response information*. For censoring, a Boolean indicator variable needs to be added to the data frame  (`cens`, see above) and the formula specified as follows: 

-->
<!--
We compare the results of the (artificially) censored data with the original. At least for this data set, I had to seriously crank up the number of iterations to obtain adequatly effective samples.-->
<!--
In the above example, we have used a generic censoring rule for all observations, but in fact, all individual observations may have their own limit, which is very convenient as in user studies participants typically have the freedom to give up at any time they want and you never know in advance when that is. Furthermore, the censoring function can also deal with lower bound censoring and even interval censoring, but it is difficult to come up with convincing examples from design research.

When observations are censored, the regression engine interpolates the missing parts from the random distribution. Review the comparison of residuals from the previous section. It is the real observations on the long right tail, that drag the notoriously symmetric Gaussian distribution into some skew. If the far right observations were not available due to censoring, a Gaussian model would snap back to a more symmetric form. Therefore, when data is censored an adequate distributional choice is more important than ever.

-->
<!-- ### Exercises: -->
<!-- 1. Review the literature on reaction times. What is the shortest time length you can find? <!-- #113 ->  -->
<!-- 1. The user can only do things one after the other and therefore, RT and ToT will never come close to zero. Conceive examples of independently triggered events that *happen* to users and effect user satisfaction.  -->
</div>
</div>
</div>
<div id="rating-scales" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Rating scales</h2>
<p>In classic design research of the last millennium, die-hard Human Factors researchers have mainly been asking objectively sounding questions, like:</p>
<ul>
<li>Can a user achieve accurate results with the system?</li>
<li>Can they do so in less time?</li>
<li>Is the number of errors reasonably confined?</li>
</ul>
<p>For professional systems, these are still highly valid questions. For example, the purpose of a medical infusion pump is to improve the health status of a patient by accurately and reliably delivering medication into the bloodstream and the extent to which a nurse achieves this by correctly programming the device can be measured directly.</p>
<p>However, starting with the 1990s, wave after wave of novel electronic entertainment systems and digital gadgets rolled over the consumer market and user feelings got more into the focus. The purpose of a video recorder or a smartphone is to deliver joy. With the new millennium, design researchers began to recognize what consumer psychologists had discovered two decades earlier: users are not rational decision makers in a utilitarian sense, but are lead by their feelings. When people decide to adopt (or buy) a new system, this is only partly driven by their expectation of productivity. Such feelings come as a variety of different constructs:</p>
<ul>
<li>feeling of joy</li>
<li>expression of self</li>
<li>social connectedness</li>
<li>aesthetic perception</li>
<li>personal growth</li>
</ul>
<p>Whether or not these concepts are well-defined from a psychological point-of-view is beyond the scope of this book. What matters is that feelings are so elusive, that the most sincere researchers have not yet found objective criteria to measure them. Instead, almost everyone resorts to use of <em>self-report rating scales</em>, like this one:</p>
<p>How beautiful do you perceive the user interface to be?</p>
<p>unattractive 1 – 2 – 3 – 4 – 5 a piece of art</p>
<p>If you use a 5 point rating scale like this one to measure perceived beauty, it is the participants who convert their gut feeling into a number. This is a hidden and uncontrolled process, but it presumably involves the following three subprocesses:</p>
<ol style="list-style-type: decimal">
<li>anchoring</li>
<li>introspection</li>
<li>binning</li>
</ol>
<p>By <em>anchoring</em> participants establish an idea of how ugly or beautiful something has be to get an extreme rating of 1 or 5. These imaginary endpoints define the <em>absolute range</em> of the rating scale. The researcher might early on give explicit or implicit cues to let the participant guess the class of to-be-rated objects. If an experiment is overtly about web design, then probably “very ugly” means the least attractive commercial website the participant can think of. However, participants old enough to remember web design in its infancy (say the early attempts of disney.com), may end up with a lower anchor than today’s kids. If not enough cues are given upfront, participants will make up their own minds (in an uncontrolled way) and at best adjust their anchors to the stimuli they see throughout the experiment. Probably, it will make a difference for what 1 or 5 mean, when the set of stimuli contain just websites, or websites <em>and</em> impressionist paintings <em>and</em> horrifying screenshots from splatter movies or brutal computer games.</p>
<p>By <em>introspection</em> participants intuitively assess the intensity of their <em>real feelings</em> as compared to the anchors. Reportedly, such feelings are influenced by:</p>
<ol style="list-style-type: decimal">
<li>visual simplicity</li>
<li>prototypicality</li>
<li>second exposure</li>
<li>Gestalt principles</li>
<li>fluency of processing</li>
<li>attribute substitution heuristics</li>
<li>color aesthetics</li>
<li>fashion</li>
<li>previous stimuli</li>
<li>current mood</li>
<li>a person’s history</li>
<li>and cultural background <!-- #104 --></li>
</ol>
<p>Finally, by <em>binning</em> the participant mentally divides the absolute range into five categories that are either fuzzy or defined by stereotypes, like “It must look at least as elegant as the Mapple website to get a 4.”</p>
<p>As the outcome of anchoring, introspection and binning are not under the control of the researcher, the response patterns can vary between participants. Let’s consider a few possible patterns of participants (and their dramatic stories, see Figure <a href="glm.html#fig:rating-1">7.26</a>):</p>
<ol style="list-style-type: decimal">
<li>A is indecisive and stays in the center region</li>
<li>B has a crush on the experimenter and always responds slightly more positive</li>
<li>C is Swedish and avoids extremes</li>
<li>D is a civil rights activist and habitually treats all bins equally.</li>
<li>E is annoyed by the experiment and falls into an almost dichotomous pattern</li>
</ol>
<div class="figure"><span id="fig:rating-1"></span>
<img src="Generalized_Linear_Models_files/figure-html/rating-1-1.png" alt="Five participants with different response styles" width="90%" />
<p class="caption">
Figure 7.26: Five participants with different response styles
</p>
</div>
<p>Many unknowns are in the game. Only one special case we can alleviate with the multilevel modeling tools in our hands. Anchoring can (but not necessarily does) result in a constant <em>shift</em> between participants. Compare participants A and B: A collects almost all stimuli in categories 2, 3 and 4, whereas B uses 3, 4 and 5. This is not much else than a participant-level intercept random effect. Unfortunately, the situation can be more difficult than that. When participants differ in how extreme they set their endpoints, like C and D, their responses will differ in variance. The maximum variance, however, will be found in participant D.</p>
<p>To speak of a real case: In the IPump study, a single-item rating scale was used to measure mental workload. The results suggest that all participants used the lower range of the scale, but differed vastly in where they set their upper point. Figure <a href="glm.html#fig:rating-2">7.27</a> orders participants by the maximum value they used. This is obviously related to variance, but seemingly not so much with location. It does not suffice to use a response distribution with mean-variance relationship, as we used to. All these issues make rating scales peculiar and we should not pretend they have the same neat arithmetic properties as objective measures.</p>
<div class="sourceCode" id="cb632"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb632-1"><a href="glm.html#cb632-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(IPump)</span>
<span id="cb632-2"><a href="glm.html#cb632-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb632-3"><a href="glm.html#cb632-3" aria-hidden="true" tabindex="-1"></a>D_pumps <span class="sc">%&gt;%</span></span>
<span id="cb632-4"><a href="glm.html#cb632-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Part) <span class="sc">%&gt;%</span></span>
<span id="cb632-5"><a href="glm.html#cb632-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb632-6"><a href="glm.html#cb632-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">min =</span> <span class="fu">min</span>(workload),</span>
<span id="cb632-7"><a href="glm.html#cb632-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">max =</span> <span class="fu">max</span>(workload),</span>
<span id="cb632-8"><a href="glm.html#cb632-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">median =</span> <span class="fu">median</span>(workload)</span>
<span id="cb632-9"><a href="glm.html#cb632-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb632-10"><a href="glm.html#cb632-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Part_ord =</span> <span class="fu">rank</span>(max, <span class="at">ties.method =</span> <span class="st">&quot;first&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb632-11"><a href="glm.html#cb632-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Part_ord, <span class="at">ymax =</span> max, <span class="at">ymin =</span> min, <span class="at">y =</span> median)) <span class="sc">+</span></span>
<span id="cb632-12"><a href="glm.html#cb632-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_crossbar</span>() <span class="sc">+</span></span>
<span id="cb632-13"><a href="glm.html#cb632-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Part(ordered)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;workload&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:rating-2"></span>
<img src="Generalized_Linear_Models_files/figure-html/rating-2-1.png" alt="In the IPump study participants vary greatly in how they exploit the range of a mental workload scale." width="90%" />
<p class="caption">
Figure 7.27: In the IPump study participants vary greatly in how they exploit the range of a mental workload scale.
</p>
</div>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>Setting the idiosyncratic rating scale responses aside, how does a common rating scale appear in our framework of link functions and patterns of randomness? Rating scales are bounded on two sides and we already know what that means: a suitable model for rating scales will likely contain a logit link function and a distribution of randomness that is bounded on two sides.</p>
<p>A real problem with rating scales is that they often are discrete. Most instruments force participants to give their answer as a choice between five or seven ordered levels. When the response variable has just a few levels, <em>ordinal regression</em> is a good choice, which is an extension of logistic regression.</p>
<p>However, most properly designed rating scales are composed of several items, because only that can sufficiently reduce measurement error and allow for in-depth psychometric assessments (<a href="mlm.html#psychometrics">6.8</a>).
While from a psychometric perspective, single-item scales are susceptible, there can be situations where a researcher may use a validated single item scale for pragmatic reasons. Especially, when measures happen in-situ, such as during a usability test or even a real operation, being brief and unobtrusive might be more important than perfecting the measures.</p>
<!-- 40 write a separate subsection on multilevel models with multi-item scales -->
<p>With multi-item rating scales, one also has the possibility to build a psychometric multi-level model, where items are considered a sample of a population of possible items. That is actually a very good idea, as the item-level random effects control for differences in item location. For example, the following item is likely to produce generally lower beauty ratings than the one shown earlier, because the anchors have been moved downwards:</p>
<p>How beautiful do you perceive the user interface?</p>
<p>like a screenshot from a splatter movie 1 – 2 – 3 – 4 – 5 quite attractive</p>
<p>Unless one builds such a psychometric multi-level model, ordinal regression is not very suitable for multi-item scales and here is why: The sum (or mean) score is still binned, but more finely grained. A sum score over three seven-binned items already has 21 bins, which would result in an inflation of number of parameters in ordinal regression.</p>
<p>As a rescue, one might well regard a measure with 21 bins as continuous <!-- #115 -->. Furthermore, there actually is no strong reason to use binned rating scales at all. So called <em>visual analog scales</em> let participants make continuous choices by either drawing a cross on a line or move a slider control. For sum scores and visual analogue scales, the problem of choice reduces to a logit link function (they still have two boundaries) and a continuous distribution bounded on both sides. That is precisely what is behind <em>beta regression</em> and, as we shall see, this distribution is flexible enough to smooth over several of the rating scale problems that were just discussed.</p>
<!--Moreover, they are not directly linked 
to the purpose. When testing a novel design of a commercial website against the current version
-->
<div id="ord-logist-reg" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Ordered logistic regression</h3>
<p>When the ordinal response has a low number of response categories (between 4 and 7), ordinal regression applies. Recall logistic regression: the response falls into one of two categories, which are coded as 0 and 1. Although not in a strict sense, the two categories can often be thought of as in an order: success is better than failure, presence more than absence and a return better than staying away. Instead of two categories, we can also conceive the situation as a <em>threshold</em> between the categories, that needs force to jump over it. Any positive impact factor <span class="math inline">\(x_i\)</span> can then be thought of as such a force that pushes a response probability to the higher category, by the amount <span class="math inline">\(\beta_i\)</span> (on logit scale). At the same time, the intercept <span class="math inline">\(\beta_0\)</span> represents the basic log-odds of falling into category 1 in a default state, that is <span class="math inline">\(x_i = 0\)</span>.</p>
<p>In ordinal regression, the idea of thresholds is extended more than two ordered response categories. The only arising complication is that with two categories, we have one threshold to overcome, whereas with three categories there are two thresholds and generally, with <span class="math inline">\(c\)</span> categories, there are <span class="math inline">\(c - 1\)</span> thresholds. Ordinal regression deals with the problem by estimating <span class="math inline">\(c - 1\)</span> intercept estimates <span class="math inline">\(\beta_{0[k]}\)</span>. Each threshold intercept <span class="math inline">\(\beta_{0[k]}\)</span> represents the probability (on logit scale) that the response falls into category <span class="math inline">\(k\)</span> <em>or lower</em>, or formally:</p>
<p><span class="math display">\[
\text{logit}(P(y_i \leq k)) = \beta_{0[k]}
\]</span></p>
<p>Let’s see this at the example of the BrowsingAB case, first. User ratings have been simulated with seven levels (Figure <a href="glm.html#fig:rating-4">7.28</a>):</p>
<div class="sourceCode" id="cb634"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb634-1"><a href="glm.html#cb634-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(BrowsingAB)</span></code></pre></div>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb635-1"><a href="glm.html#cb635-1" aria-hidden="true" tabindex="-1"></a>BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb635-2"><a href="glm.html#cb635-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> rating)) <span class="sc">+</span></span>
<span id="cb635-3"><a href="glm.html#cb635-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(Design <span class="sc">~</span> .) <span class="sc">+</span></span>
<span id="cb635-4"><a href="glm.html#cb635-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb635-5"><a href="glm.html#cb635-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">1</span>, <span class="dv">7</span>)</span></code></pre></div>
<div class="figure"><span id="fig:rating-4"></span>
<img src="Generalized_Linear_Models_files/figure-html/rating-4-1.png" alt="Designs A and B rated on a sevel-point scale" width="90%" />
<p class="caption">
Figure 7.28: Designs A and B rated on a sevel-point scale
</p>
</div>
<p>The brms regression engine implements ordinal regression by the family <code>cratio</code> (cumulative odds ratio <!-- #116 -->) with a default logit link function.</p>
<div class="sourceCode" id="cb636"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb636-1"><a href="glm.html#cb636-1" aria-hidden="true" tabindex="-1"></a>M_ord_1 <span class="ot">&lt;-</span></span>
<span id="cb636-2"><a href="glm.html#cb636-2" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb636-3"><a href="glm.html#cb636-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(rating <span class="sc">~</span> Design,</span>
<span id="cb636-4"><a href="glm.html#cb636-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;cratio&quot;</span>,</span>
<span id="cb636-5"><a href="glm.html#cb636-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb636-6"><a href="glm.html#cb636-6" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<!--We have to keep in mind, though, that the constant shift of thresholds is an assumption [EXPLAIN ASSUMPTION FIRST EXPLICITLY], which may fail. For example, when participants choose different anchor points for their judgements, such as in the IPump study, this is no longer the case. [WHY NOT?] -->
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb637-1"><a href="glm.html#cb637-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(M_ord_1)</span></code></pre></div>
<table>
<caption><span id="tab:rating-5">Table 7.24: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="right">-2.072</td>
<td align="right">-2.557</td>
<td align="right">-1.607</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">-0.276</td>
<td align="right">-0.633</td>
<td align="right">0.075</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right">0.646</td>
<td align="right">0.168</td>
<td align="right">1.162</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">1.205</td>
<td align="right">0.320</td>
<td align="right">2.274</td>
</tr>
<tr class="odd">
<td align="left">DesignB</td>
<td align="right">-0.543</td>
<td align="right">-0.955</td>
<td align="right">-0.122</td>
</tr>
</tbody>
</table>
<p>The six intercepts correspond with the thresholds between the seven levels. It is no coincidence that the intercept estimates increase by order, as they are cumulative (the “c” in cratio). The first intercept estimate represents (the logit of) the proportion of responses <span class="math inline">\(y_i \leq 1\)</span>, the second <span class="math inline">\(y_i \leq 2\)</span> etc.
The Design effect has the usual interpretation as compared to logistic regression, a change in odds. The only difference is that it now acts on all six reference points. The expected proportion of responses equal to or smaller than 2 for design A is:</p>
<p><span class="math display">\[
\begin{aligned}
\pi(y_i \leq 2|A) &amp;= 
\text{logit}^{-1}(\beta_{0[2]}) \\ 
&amp;=\text{logit}^{-1}(-0.3) = 
0.426
\end{aligned}
\]</span></p>
<p>The expected proportion of responses equal to or smaller than 2 for design B we get by the usual linear combination:</p>
<p><span class="math display">\[
\begin{aligned}
\pi(y_i \leq 2|B) &amp;= \text{logit}^{-1}(\beta_{0[2]} + \beta_1) \\
&amp;= 
\text{logit}^{-1}(NA) = 
NA
\end{aligned}
\]</span></p>
<p>All coefficients are shifting all thresholds by the same amount (on the linear predictor scale). You can picture this as a single puppeteer controlling multiple puppets by just one stick, making them dance synchronously. As long as the ordinal scale has only a low number of bins, that keeps the number of parameters at a reasonable level. Just imagine, you were estimating an ordinal multilevel model and all participant-level effects were five or seven-folded, too. However, the equidistancy of effects on bin thresholds is an assumption by itself, and in the presence of response styles on rating scales, it cannot be taken for granted <!-- #117 -->.</p>
<p>Besides that, the ordinal model appears very snug to the structure of the data. It does not wipe over the fact that the response is discrete and the thresholds represent the order. Conveniently, effects are represented by a single estimate, which one can use to communicate direction and certainty of effects. On the downside, communicating absolute performance (that is, including the intercept) is more complicated. When presenting predictions from an ordinal model one actually has to present all thresholds, rather than a single mean. In practice that probably is less relevant than one might think at first, because predictions on self-report scales is less useful than metric performance data. Ordinal data also does not lend itself so much to further calculations. For example, you can use ToT measures on infusion pumps in calculating the required staffing of an intensive care unit, because seconds are metric and can be summed and divided. In contrast, it does not make sense to calculate the cognitive workload of a team of nurses by summing their self-report scores. The only possibility is to compare the strengths of predictors, but that does not require predictions.</p>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="beta-reg" class="section level3" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Beta regression</h3>
<p>In my opinion, one of the most futile discussions in methodology research whether one should use a four, five or seven binned Likert scale. From a pure measurement point of view, more bins give better resolution, the ultimate consequence being not to bin at all, that is using continuous rating scales. At the same time, many rating responses come from multiple item scales, which multiplies the number of bins. Speaking of ordinal regression, it seems reasonable to have seven intercepts for a single item scale, but who would want 14 or 21 for a two or three-item scale? And most scales have even more items.</p>
<p>Psychometric research in the process of developing rating scales routinely uses a method called <em>confirmatory factor analysis</em>, which derives from the Gaussian linear model and inherits its assumptions. As always, Gaussian models are sometimes a reasonable approximation, but given the framework of GLM, it is unnecessary (to put it mildly) to go along with the Gaussian, violating the assumptions of normality and linearity. While the link function for a double bounded response variable is simply the logit, the only missing ingredient is a double bounded error distribution. Enter Beta distribution!</p>
<p>We demonstrate beta regression on rating scales at the example of the CUE8 study. This study aimed at assessing whether remote usability testing arrives at the same ToT measures as in moderated sessions. As we have seen in <a href="mlm.html#nested-re">6.6</a>, the difference is marginal for ToT.</p>
<p>As rating scales are susceptible to all kinds of cognitive and social biases, a golden rule for user test moderators is to constantly remind participants to not blame themselves for errors. Reportedly, test moderators also do help participants (after counting to 10) in order to minimize frustration (and maximize information flow). What could the presence or absence of a moderator do to satisfaction ratings? Perhaps, remote participants feel the lack of assurance and support as higher levels of frustration. Furthermore, it is not unlikely that satisfaction ratings are sensitive to idiosyncrasies in the process of the user test, such that we could even expect differences between teams.</p>
<!--




MOVE TO MODEL SELECTION







-->
<p>Before we build the model, there are two issues to regard: First, the boundaries of Beta distributions are 0 and 1, which requires a rescaling of responses into this interval. The SUS <!-- #119 -->scores are on a scale from 0 to 100 and a divisor of 100 would produce the desired interval. Second, the responses must actually lie <em>strictly between 0 and 1</em>, excluding the boundaries. On (quasi)continuous scales, it seems not very likely to have 0 or 1 as response, but it can happen. Indeed, participants in the CUE8 study have responded with a satisfaction rating of 100 quite often.</p>
<p>A practical solution is to scale the responses in such a way as to avoid the two boundaries, which is what the following hack does.</p>
<ol style="list-style-type: decimal">
<li>add a tiny value to all responses</li>
<li>create a divisor by adding twice that value to the maximum value the responses can take</li>
<li>divide all responses by that divisor</li>
</ol>
<p>You may find it inappropriate to mangle a response variable in such an arbitrary way. However, keep in mind that the levels of ordinal responses are highly arbitrary. In terms of measurement theory, all transformations that maintain the order are permitted for ordinal scales. For the following analysis, the data set was further reduced by averaging the scores across tasks and excluding probable cheaters with a ToT &lt; 30s.</p>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb639-1"><a href="glm.html#cb639-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(CUE8)</span></code></pre></div>
<div class="sourceCode" id="cb640"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb640-1"><a href="glm.html#cb640-1" aria-hidden="true" tabindex="-1"></a>D_cue8_SUS <span class="ot">&lt;-</span></span>
<span id="cb640-2"><a href="glm.html#cb640-2" aria-hidden="true" tabindex="-1"></a>  D_cue8 <span class="sc">%&gt;%</span></span>
<span id="cb640-3"><a href="glm.html#cb640-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(SUS)) <span class="sc">%&gt;%</span></span>
<span id="cb640-4"><a href="glm.html#cb640-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Part, Team, Condition) <span class="sc">%&gt;%</span></span>
<span id="cb640-5"><a href="glm.html#cb640-5" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarize</span>(</span>
<span id="cb640-6"><a href="glm.html#cb640-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">ToT =</span> <span class="fu">sum</span>(ToT),</span>
<span id="cb640-7"><a href="glm.html#cb640-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">SUS =</span> <span class="fu">mean</span>(SUS)</span>
<span id="cb640-8"><a href="glm.html#cb640-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb640-9"><a href="glm.html#cb640-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb640-10"><a href="glm.html#cb640-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(ToT <span class="sc">&gt;</span> <span class="dv">30</span>) <span class="sc">%&gt;%</span></span>
<span id="cb640-11"><a href="glm.html#cb640-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">SUS =</span> mascutils<span class="sc">::</span><span class="fu">rescale_unit</span>(SUS)) <span class="sc">%&gt;%</span></span>
<span id="cb640-12"><a href="glm.html#cb640-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tbl_obs</span>()</span>
<span id="cb640-13"><a href="glm.html#cb640-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb640-14"><a href="glm.html#cb640-14" aria-hidden="true" tabindex="-1"></a>D_cue8_SUS <span class="sc">%&gt;%</span></span>
<span id="cb640-15"><a href="glm.html#cb640-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Team, <span class="at">y =</span> SUS)) <span class="sc">+</span></span>
<span id="cb640-16"><a href="glm.html#cb640-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span>Condition) <span class="sc">+</span></span>
<span id="cb640-17"><a href="glm.html#cb640-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_violin</span>() <span class="sc">+</span></span>
<span id="cb640-18"><a href="glm.html#cb640-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_count</span>()</span></code></pre></div>
<div class="figure"><span id="fig:rating-6"></span>
<img src="Generalized_Linear_Models_files/figure-html/rating-6-1.png" alt="Response distribution of SUS ratings across teams and conditions" width="90%" />
<p class="caption">
Figure 7.29: Response distribution of SUS ratings across teams and conditions
</p>
</div>
<div class="sourceCode" id="cb641"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb641-1"><a href="glm.html#cb641-1" aria-hidden="true" tabindex="-1"></a>M_5_bet <span class="ot">&lt;-</span></span>
<span id="cb641-2"><a href="glm.html#cb641-2" aria-hidden="true" tabindex="-1"></a>  D_cue8_SUS <span class="sc">%&gt;%</span></span>
<span id="cb641-3"><a href="glm.html#cb641-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(SUS <span class="sc">~</span> Condition <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Team),</span>
<span id="cb641-4"><a href="glm.html#cb641-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">Beta</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>),</span>
<span id="cb641-5"><a href="glm.html#cb641-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb641-6"><a href="glm.html#cb641-6" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb642-1"><a href="glm.html#cb642-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef_ml</span>(M_5_bet)</span></code></pre></div>
<table>
<caption><span id="tab:rating-7">Table 7.25: </span>Population-level coefficients with random effects standard deviations</caption>
<thead>
<tr class="header">
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
<th align="right">SD_Team</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">0.771</td>
<td align="right">-0.131</td>
<td align="right">1.645</td>
<td align="right">0.678</td>
</tr>
<tr class="even">
<td align="left">Conditionmoderated</td>
<td align="right">-0.300</td>
<td align="right">-1.572</td>
<td align="right">0.958</td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>Do participants in remote sessions feel less satisfied? There seems to be a slight disadvantage, but we cannot confirm this with sufficient certainty. In contrast, the variation between teams is substantial, which indicates that SUS ratings are not independent of the particular setting. That is rather concerning for a widely used and allegedly validated rating scale.</p>
<!-- Is that a realistic model for the SUS responses? Another glance at the violin plot suggests another pathology: the teams seem to differ in variation. In the closing section I will briefly demonstrate how to deal with differences in variance (rather than mean) by a *distributional model*. -->
</div>
</div>
<div id="distributional-models" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Beyond mean: distributional models</h2>
<p>The framework of GLM, as flexible as it has proven to be up to this point, has one major limitation: it only renders the relationship between predictors and the mean of the distribution. We only think in terms of impact factors that improve (or damage) average response times, error rates, satisfaction ratings etc. As we have seen multiple times in chapter <a href="mlm.html#mlm">6</a>, variance matters, too. With multi-level models we can estimate variance within a sample and even compare variance across samples, like in the CUE8 case, where more variance is due to teams rather than due to participants.</p>
<p>What cannot be done with plain multi-level models is estimate effects on variance, like: “Do teams in CUE8 differ in the variation of responses?” That brings me to the final feature of modern regression modeling in the scope of this book. With <em>distributional models</em>, we can put predictors on any distribution parameter that we want, not just location <span class="math inline">\(\mu\)</span>.</p>
<!-- That brings me to the final feature of modern regression modelling in the scope of this book. GLMs greatly enhanced the scope of modelling by giving us the choice of response distributions and linearizing functions. Still, all models introduced so far establish an association between predictors and location ($\mu$), only.  -->
<p>All but the one-parameter distributions come with additional parameters that allow to accommodate the dispersion of the error distribution, or even its skew in the case of Exgaussian models. In the following I will present two application scenarios for distributional models. We start with a designometric problem when using bi-polar rating scales (see <a href="glm.html#rating-scales">7.4</a>.) In the next section I will illustrate the problem on simulated data from two items with different anchoring. Subsequently, we will see on a real data set, that participants differ in how they exploit the range of a rating scale. by a simple simulation will show what effect item anchoring can have and how a distributional model can deal with such a situation.</p>
<div id="item-anchoring" class="section level3" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Item-level anchoring in rating scales</h3>
<p>As a first illustration, imagine two versions of a continuous rating scale for visual beauty that differ in how their anchors are labeled. The first question is moderately labeled, the second is extreme.</p>
<ol style="list-style-type: decimal">
<li>like the ugliest website I have ever seen: 0 ——– 1 like the most beautiful website</li>
<li>distgusting as a screenshot from a splatter movie: 0 ——– 1 like the most beautiful impressionist painting</li>
</ol>
<!-- The second scale has the lower and upper anchors moved to more extremes and two questions arise: -->
<!-- 1. Is the overall location of the scale untouched, that is, have both anchors been moved by the same distance outwards? -->
<!-- 1. What range do websites cover as compared to all thinkable visual impressions? -->
<p>For the sake of simplicity (not for a strong research design), let us assume that one participant has rated a sample of 400 websites in two conditions: moderate anchoring and extreme anchoring. The following simulates data such that both anchorings have the same location (<code>mu</code>), but the moderate anchoring condition produces a much more exploitation of the range. Note that Beta distribution’s parameter <code>phi</code> does not increase variance, but just the opposite: the range of the scale is expanded, which lets the variance shrink. It is comparable to the number of trials in a binomial process. The more trials there are, the relatively tighter the distribution becomes (Figure <a href="glm.html#fig:distmod-1">7.30</a>)</p>
<div class="sourceCode" id="cb643"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb643-1"><a href="glm.html#cb643-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb643-2"><a href="glm.html#cb643-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb643-3"><a href="glm.html#cb643-3" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">400</span></span>
<span id="cb643-4"><a href="glm.html#cb643-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb643-5"><a href="glm.html#cb643-5" aria-hidden="true" tabindex="-1"></a>Conditions <span class="ot">&lt;-</span></span>
<span id="cb643-6"><a href="glm.html#cb643-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tribble</span>(</span>
<span id="cb643-7"><a href="glm.html#cb643-7" aria-hidden="true" tabindex="-1"></a>    <span class="sc">~</span>Anchoring, <span class="sc">~</span>mu, <span class="sc">~</span>phi,</span>
<span id="cb643-8"><a href="glm.html#cb643-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;extreme&quot;</span>, .<span class="dv">3</span>, <span class="dv">18</span>,</span>
<span id="cb643-9"><a href="glm.html#cb643-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;moderate&quot;</span>, .<span class="dv">3</span>, <span class="dv">6</span></span>
<span id="cb643-10"><a href="glm.html#cb643-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb643-11"><a href="glm.html#cb643-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb643-12"><a href="glm.html#cb643-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">a =</span> mu <span class="sc">*</span> phi,</span>
<span id="cb643-13"><a href="glm.html#cb643-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">b =</span> phi <span class="sc">-</span> mu <span class="sc">*</span> phi</span>
<span id="cb643-14"><a href="glm.html#cb643-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb643-15"><a href="glm.html#cb643-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb643-16"><a href="glm.html#cb643-16" aria-hidden="true" tabindex="-1"></a>D_Anchor <span class="ot">&lt;-</span></span>
<span id="cb643-17"><a href="glm.html#cb643-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb643-18"><a href="glm.html#cb643-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">Obs =</span> <span class="dv">1</span><span class="sc">:</span>N,</span>
<span id="cb643-19"><a href="glm.html#cb643-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">Anchoring =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;extreme&quot;</span>, <span class="st">&quot;moderate&quot;</span>), N <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb643-20"><a href="glm.html#cb643-20" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb643-21"><a href="glm.html#cb643-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(Conditions) <span class="sc">%&gt;%</span></span>
<span id="cb643-22"><a href="glm.html#cb643-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">rating =</span> <span class="fu">rbeta</span>(N, a, b))</span>
<span id="cb643-23"><a href="glm.html#cb643-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb643-24"><a href="glm.html#cb643-24" aria-hidden="true" tabindex="-1"></a>D_Anchor <span class="sc">%&gt;%</span></span>
<span id="cb643-25"><a href="glm.html#cb643-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Anchoring, <span class="at">y =</span> rating)) <span class="sc">+</span></span>
<span id="cb643-26"><a href="glm.html#cb643-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_violin</span>() <span class="sc">+</span></span>
<span id="cb643-27"><a href="glm.html#cb643-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span id="fig:distmod-1"></span>
<img src="Generalized_Linear_Models_files/figure-html/distmod-1-1.png" alt="Extreme and moderate anchoring produce differently varied response distributions" width="90%" />
<p class="caption">
Figure 7.30: Extreme and moderate anchoring produce differently varied response distributions
</p>
</div>
<p><em>Note</em> that the <code>rbeta</code> command uses a different parametrization of beta distribution, with parameters <code>a</code> and <code>b</code>, which have are linked to the distribution mean and variance in rather convoluted ways.</p>
<p>The two response distributions have the same location, but the more narrow anchoring produces a wider dispersion of responses. How would we confirm this statistically? The Brms engine can link predictors to <em>any</em> other parameter of the response distribution, which the author of the package calls <em>distributional models</em>. They have an immense potential as they relax another assumption of GLM, namely that all variance parameters must strictly follow the mean-variance relationship demanded by a distribution family. As we have seen, one can easily create a case where this assumption is violated.</p>
<!-- ```{r} -->
<!-- plot_rbinom <- function(N, size, p) { -->
<!--   tibble(succ = rbinom(N, size, p)) %>%  -->
<!--   ggplot(aes(x = succ)) + -->
<!--   geom_histogram(bins = size + 1) + -->
<!--   xlim(0, size)   -->
<!-- } -->
<!-- plot_rbinom(1000, 10, .5) -->
<!-- plot_rbinom(1000, 50, .5) -->
<!-- ``` -->
<p>For Beta distributions, a large <span class="math inline">\(\phi\)</span> indicates less dispersion. Accordingly, when used in a distributional model, a positive effect decreases variance.</p>
<p>When estimating dispersion or scale parameters, we have to regard that these are positive, strictly. The Brms engine simply extends the principle of link functions to parameters other than the <span class="math inline">\(\mu\)</span> and sets a default log link for <span class="math inline">\(\phi\)</span>. In order to estimate the changes in <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\phi\)</span> simultaneously, the brms engine receives two regression formulas. Having multiple formulas for a regresson model is a notable extension of the R model specification language, which is why Brms brings its own command <code>bf()</code> to collect these. We run a distributional Beta regression on the simulated rating scale responses:</p>
<div class="sourceCode" id="cb644"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb644-1"><a href="glm.html#cb644-1" aria-hidden="true" tabindex="-1"></a>M_beta <span class="ot">&lt;-</span> <span class="fu">brm</span>(<span class="fu">bf</span>(</span>
<span id="cb644-2"><a href="glm.html#cb644-2" aria-hidden="true" tabindex="-1"></a>  rating <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> Anchoring,</span>
<span id="cb644-3"><a href="glm.html#cb644-3" aria-hidden="true" tabindex="-1"></a>  phi <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> Anchoring</span>
<span id="cb644-4"><a href="glm.html#cb644-4" aria-hidden="true" tabindex="-1"></a>),</span>
<span id="cb644-5"><a href="glm.html#cb644-5" aria-hidden="true" tabindex="-1"></a><span class="at">family =</span> <span class="fu">Beta</span>(),</span>
<span id="cb644-6"><a href="glm.html#cb644-6" aria-hidden="true" tabindex="-1"></a><span class="at">data =</span> D_Anchor</span>
<span id="cb644-7"><a href="glm.html#cb644-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>The parameter table below contains the two regular coefficients on location and, as expected, there is little difference. The intercept on scale parameter <span class="math inline">\(\phi\)</span> is the scale in the narrow condition (with wider variance). The treatment effect on <span class="math inline">\(\phi\)</span> is positive on the log scale, which means it deflates variance, just as expected (Table <a href="glm.html#tab:distmod-2">7.26</a>).</p>
<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb645-1"><a href="glm.html#cb645-1" aria-hidden="true" tabindex="-1"></a><span class="fu">posterior</span>(M_beta) <span class="sc">%&gt;%</span></span>
<span id="cb645-2"><a href="glm.html#cb645-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">value =</span> <span class="fu">if_else</span>(<span class="fu">str_detect</span>(parameter, <span class="st">&quot;phi&quot;</span>),</span>
<span id="cb645-3"><a href="glm.html#cb645-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">exp</span>(value),</span>
<span id="cb645-4"><a href="glm.html#cb645-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">inv_logit</span>(value)</span>
<span id="cb645-5"><a href="glm.html#cb645-5" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">%&gt;%</span></span>
<span id="cb645-6"><a href="glm.html#cb645-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">clu</span>()</span></code></pre></div>
<table>
<caption><span id="tab:distmod-2">Table 7.26: </span>Parameter estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">b_Anchoringextreme</td>
<td align="left">Anchoringextreme</td>
<td align="right">0.294</td>
<td align="right">0.281</td>
<td align="right">0.307</td>
</tr>
<tr class="even">
<td align="left">b_Anchoringmoderate</td>
<td align="left">Anchoringmoderate</td>
<td align="right">0.301</td>
<td align="right">0.276</td>
<td align="right">0.325</td>
</tr>
<tr class="odd">
<td align="left">b_phi_Anchoringextreme</td>
<td align="left"></td>
<td align="right">20.833</td>
<td align="right">17.199</td>
<td align="right">25.171</td>
</tr>
<tr class="even">
<td align="left">b_phi_Anchoringmoderate</td>
<td align="left"></td>
<td align="right">5.717</td>
<td align="right">4.701</td>
<td align="right">6.842</td>
</tr>
</tbody>
</table>
<p>With such a distributional model we can discover differences in anchoring. And, even better, we can account for it. It intuitively makes sense to mix items with extreme and modest anchoring. By merging a distributional model with a designometric multi-level model <a href="mlm.html#designometrix">6.8.4</a>, we can evaluate and use such heterogeneous scales. The general concept is shown in the next section, where we account for participant-level employment of scale.</p>
</div>
<div id="part-employment" class="section level3" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> Participant-level employment of scale</h3>
<p>In the previous section, we have seen how to discover differences in anchoring of items. What is seen frequently in designometric studies is different response patterns in participants, in particular how much they tend to use the extremes. We can account for that by letting the variance parameter vary across participants. The following model comprises:</p>
<ol style="list-style-type: decimal">
<li>a structural part for the mean with a fixed-effects polynomial term and a participant-level random effect for the response curve.</li>
<li>a structural part for scale parameter <code>phi</code> with a participant-level random effect on the scale parameter <code>phi</code></li>
<li>a Beta shape of randomness</li>
</ol>
<div class="sourceCode" id="cb646"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb646-1"><a href="glm.html#cb646-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Uncanny)</span></code></pre></div>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb647-1"><a href="glm.html#cb647-1" aria-hidden="true" tabindex="-1"></a>RK_1 <span class="ot">&lt;-</span></span>
<span id="cb647-2"><a href="glm.html#cb647-2" aria-hidden="true" tabindex="-1"></a>  RK_1 <span class="sc">%&gt;%</span></span>
<span id="cb647-3"><a href="glm.html#cb647-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">response_unit =</span> mascutils<span class="sc">::</span><span class="fu">rescale_centered</span>(response, <span class="at">scale =</span> .<span class="dv">99</span>) <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb647-4"><a href="glm.html#cb647-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb647-5"><a href="glm.html#cb647-5" aria-hidden="true" tabindex="-1"></a>M_poly_3_beta <span class="ot">&lt;-</span></span>
<span id="cb647-6"><a href="glm.html#cb647-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(</span>
<span id="cb647-7"><a href="glm.html#cb647-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">formula =</span> response_unit <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> huMech1 <span class="sc">+</span> huMech2 <span class="sc">+</span> huMech3 <span class="sc">+</span></span>
<span id="cb647-8"><a href="glm.html#cb647-8" aria-hidden="true" tabindex="-1"></a>      (<span class="dv">1</span> <span class="sc">+</span> huMech1 <span class="sc">+</span> huMech2 <span class="sc">+</span> huMech3 <span class="sc">|</span> Part),</span>
<span id="cb647-9"><a href="glm.html#cb647-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">Beta</span>(),</span>
<span id="cb647-10"><a href="glm.html#cb647-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> RK_1,</span>
<span id="cb647-11"><a href="glm.html#cb647-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">inits =</span> <span class="dv">0</span></span>
<span id="cb647-12"><a href="glm.html#cb647-12" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb647-13"><a href="glm.html#cb647-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb647-14"><a href="glm.html#cb647-14" aria-hidden="true" tabindex="-1"></a>M_poly_3_beta_dist <span class="ot">&lt;-</span></span>
<span id="cb647-15"><a href="glm.html#cb647-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(</span>
<span id="cb647-16"><a href="glm.html#cb647-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">formula =</span> <span class="fu">bf</span>(</span>
<span id="cb647-17"><a href="glm.html#cb647-17" aria-hidden="true" tabindex="-1"></a>      response_unit <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> huMech1 <span class="sc">+</span> huMech2 <span class="sc">+</span> huMech3 <span class="sc">+</span></span>
<span id="cb647-18"><a href="glm.html#cb647-18" aria-hidden="true" tabindex="-1"></a>        (<span class="dv">1</span> <span class="sc">+</span> huMech1 <span class="sc">+</span> huMech2 <span class="sc">+</span> huMech3 <span class="sc">|</span> Part),</span>
<span id="cb647-19"><a href="glm.html#cb647-19" aria-hidden="true" tabindex="-1"></a>      phi <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Part)</span>
<span id="cb647-20"><a href="glm.html#cb647-20" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb647-21"><a href="glm.html#cb647-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">Beta</span>(),</span>
<span id="cb647-22"><a href="glm.html#cb647-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> RK_1,</span>
<span id="cb647-23"><a href="glm.html#cb647-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">inits =</span> <span class="dv">0</span></span>
<span id="cb647-24"><a href="glm.html#cb647-24" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><em>Note</em> that on such more exotic and complex models, the regression engine sometimes has difficulties in finding valid starting values for the MCMC walk. Like here, it often helps to fix all starting values to Zero. If participants show different employment of scale, we should see that on the participant-level standard deviation of <code>phi</code>.</p>
<!-- We return to this case with a formal and a theoretical conclusion in \@ref(choos_resp). -->
<div class="sourceCode" id="cb648"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb648-1"><a href="glm.html#cb648-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ranef</span>(M_poly_3_beta_dist) <span class="sc">%&gt;%</span></span>
<span id="cb648-2"><a href="glm.html#cb648-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(nonlin <span class="sc">==</span> <span class="st">&quot;phi&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb648-3"><a href="glm.html#cb648-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Part_ord =</span> <span class="fu">dense_rank</span>(center)) <span class="sc">%&gt;%</span></span>
<span id="cb648-4"><a href="glm.html#cb648-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Part_ord, <span class="at">y =</span> center, <span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper)) <span class="sc">+</span></span>
<span id="cb648-5"><a href="glm.html#cb648-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_crossbar</span>()</span></code></pre></div>
<div class="figure"><span id="fig:distmod-4"></span>
<img src="Generalized_Linear_Models_files/figure-html/distmod-4-1.png" alt="Participant-level random effects for scale parameter phi" width="90%" />
<p class="caption">
Figure 7.31: Participant-level random effects for scale parameter phi
</p>
</div>
<p>As Figure <a href="glm.html#fig:distmod-4">7.31</a> shows (on the linearized scale), that participants vary considerably around the population-level value of <code>phi</code> (<span class="math inline">\(\approx 1.4\)</span>). While a regular Beta model adjusts the variance according to the usual mean-variance relationship, the distributional model also accounts for overall broader and narrower distributions. We would probably see some subtle adjustments in predictive plots, but this model is too complex for visual analysis (as every participant get their own <code>phi</code>). Therefore, we defer a deeper inspection to @ref(choos_resp), where we demonstrate the superiority of the distributional model using information criteria.</p>
<!-- , for example on mental *workload*. Imagine you are evaluating a simulator-based driver training, designed with the  idea that movement tasks are better learned if you begin fast, not accurate. This training uses *speed episodes*, where the trainee is asked to do 20% faster, at the expense of accuracy. In such an experiment, *workload* can play a moderating factor, because if you work very hard, you can be fast and accurate at the same time. In such a case, we can  -->
<div class="sourceCode" id="cb649"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="part-skew" class="section level3" number="7.5.3">
<h3><span class="header-section-number">7.5.3</span> Participant-level skew in reaction times</h3>
<p>In <a href="glm.html#exgauss-reg">7.3.2</a>, the Exgaussian model seemed to sit well with reaction times, accommodating their left skew better than Gaussian or Gamma distributions. But if we review the participant-level plots carefully, we see that the shape of randomness differ between participants. There is visible differences in variance, as well as in skew.</p>
<p>The following distributional model estimates the same location effects (PrimeGeek), but granting all participants their own variance and skew. The maximum distributional model would estimate fully separate distributions per every participant and condition. But, since the two response distributions (Geek/nogeek) appear similar in the plots, a reduced predictor term is used, assuming that variance and shape are not affected by the experimental condition, but only the person doing the task. That is further justified by the fact, that the effect of priming categories were meager, at best.</p>
<div class="sourceCode" id="cb650"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb650-1"><a href="glm.html#cb650-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Hugme)</span></code></pre></div>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb651-1"><a href="glm.html#cb651-1" aria-hidden="true" tabindex="-1"></a>M_1_exg_dist <span class="ot">&lt;-</span></span>
<span id="cb651-2"><a href="glm.html#cb651-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span>(</span>
<span id="cb651-3"><a href="glm.html#cb651-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">formula =</span> <span class="fu">bf</span>(</span>
<span id="cb651-4"><a href="glm.html#cb651-4" aria-hidden="true" tabindex="-1"></a>      RT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> PrimeGeek <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">+</span> PrimeGeek <span class="sc">|</span> Part),</span>
<span id="cb651-5"><a href="glm.html#cb651-5" aria-hidden="true" tabindex="-1"></a>      sigma <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Part),</span>
<span id="cb651-6"><a href="glm.html#cb651-6" aria-hidden="true" tabindex="-1"></a>      beta <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Part)</span>
<span id="cb651-7"><a href="glm.html#cb651-7" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb651-8"><a href="glm.html#cb651-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">exgaussian</span>(),</span>
<span id="cb651-9"><a href="glm.html#cb651-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> D_hugme,</span>
<span id="cb651-10"><a href="glm.html#cb651-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">inits =</span> <span class="dv">0</span></span>
<span id="cb651-11"><a href="glm.html#cb651-11" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>First, we extract the fixed effects, as well as the group-level standard deviation from the posterior. If participants show different patterns of randomness, we should see that in participant-level variation of <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<div class="sourceCode" id="cb652"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb652-1"><a href="glm.html#cb652-1" aria-hidden="true" tabindex="-1"></a>P_1_exg_dist <span class="ot">&lt;-</span> <span class="fu">posterior</span>(M_1_exg_dist)</span>
<span id="cb652-2"><a href="glm.html#cb652-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb652-3"><a href="glm.html#cb652-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ranef</span>(P_1_exg_dist) <span class="sc">%&gt;%</span></span>
<span id="cb652-4"><a href="glm.html#cb652-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(nonlin <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;beta&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb652-5"><a href="glm.html#cb652-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(nonlin) <span class="sc">%&gt;%</span></span>
<span id="cb652-6"><a href="glm.html#cb652-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Part_ord =</span> <span class="fu">dense_rank</span>(center)) <span class="sc">%&gt;%</span></span>
<span id="cb652-7"><a href="glm.html#cb652-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb652-8"><a href="glm.html#cb652-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Part_ord, <span class="at">y =</span> center, <span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper)) <span class="sc">+</span></span>
<span id="cb652-9"><a href="glm.html#cb652-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span>nonlin) <span class="sc">+</span></span>
<span id="cb652-10"><a href="glm.html#cb652-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_crossbar</span>()</span></code></pre></div>
<div class="figure"><span id="fig:distmod-5"></span>
<img src="Generalized_Linear_Models_files/figure-html/distmod-5-1.png" alt="Participant-level random effects for sigma in beta" width="90%" />
<p class="caption">
Figure 7.32: Participant-level random effects for sigma in beta
</p>
</div>
<p>Figure <a href="glm.html#fig:distmod-5">7.32</a> confirms, that participants vary significantly in how there RTs are dispersed (<code>sigma</code>), as well as how skewed they are (<code>beta</code>). Finally, we can also examine whether there are any associations between location, variance and skew. Recall that parameter <code>beta</code> gives the model the flexibility to add extra skew for when the location is far from the left boundary. If that were the case, we would see a correlation between participant-level random effects between location and skew.</p>
<div class="sourceCode" id="cb653"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb653-1"><a href="glm.html#cb653-1" aria-hidden="true" tabindex="-1"></a>P_1_exg_dist <span class="sc">%&gt;%</span></span>
<span id="cb653-2"><a href="glm.html#cb653-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(type <span class="sc">==</span> <span class="st">&quot;ranef&quot;</span> <span class="sc">&amp;</span> fixef <span class="sc">==</span> <span class="st">&quot;Intercept&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb653-3"><a href="glm.html#cb653-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb653-4"><a href="glm.html#cb653-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">dist_par =</span> <span class="fu">str_match</span>(parameter, <span class="st">&quot;beta|sigma&quot;</span>),</span>
<span id="cb653-5"><a href="glm.html#cb653-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">dist_par =</span> <span class="fu">if_else</span>(<span class="fu">is.na</span>(dist_par), <span class="st">&quot;mu&quot;</span>, dist_par)</span>
<span id="cb653-6"><a href="glm.html#cb653-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb653-7"><a href="glm.html#cb653-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(dist_par, re_entity) <span class="sc">%&gt;%</span></span>
<span id="cb653-8"><a href="glm.html#cb653-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">center =</span> <span class="fu">median</span>(value)) <span class="sc">%&gt;%</span></span>
<span id="cb653-9"><a href="glm.html#cb653-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb653-10"><a href="glm.html#cb653-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spread</span>(<span class="at">key =</span> dist_par, <span class="at">value =</span> center) <span class="sc">%&gt;%</span></span>
<span id="cb653-11"><a href="glm.html#cb653-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>re_entity) <span class="sc">%&gt;%</span></span>
<span id="cb653-12"><a href="glm.html#cb653-12" aria-hidden="true" tabindex="-1"></a>  corrr<span class="sc">::</span><span class="fu">correlate</span>() </span></code></pre></div>
<table>
<caption><span id="tab:distmod-6">Table 7.27: </span>Correlations between individual location (mu), variance (sigma) and skewness (beta)</caption>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">beta</th>
<th align="right">mu</th>
<th align="right">sigma</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">beta</td>
<td align="right"></td>
<td align="right">0.800</td>
<td align="right">0.272</td>
</tr>
<tr class="even">
<td align="left">mu</td>
<td align="right">0.800</td>
<td align="right"></td>
<td align="right">0.463</td>
</tr>
<tr class="odd">
<td align="left">sigma</td>
<td align="right">0.272</td>
<td align="right">0.463</td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>One could get excited about the strong correlation between <code>mu</code> and <code>beta</code> in Table <a href="glm.html#tab:distmod-6">7.27</a>, but recall that the amount of skew introduced by <code>beta</code> is proportional to the location. At the same time we see a mild correlation between <code>mu</code> and <code>sigma</code>, which is just the usual mean-variance relationship, which most other distribution fanilies have already factored in. In chapter <a href="wwm.html#wwm">8</a>, we will learn how to compare models by predictive accuracy. As it will turn out in <a href="wwm.html#choose-dist">8.2.5</a>, the distributional Beta model in the Hugme case has a better predictive accuracy than the location-only model.</p>
<p>Distributional models can do more than just better accommodate data. Applied researchers and experimentalists traditionally embrace on-average effects, but have been mostly blind to effects on variance. In safety relevant systems, strong variance can be a real problem, because this implies that extremely poor performance is becoming more likely, increasing the risk of hazard. For such systems, variance is a crucial parameter and the dispersion effects of design changes should be tracked, carefully. For the readers interested in exercising this technique, several data sets from this book are candidates for such an analysis:</p>
<ul>
<li>CUE8: Do teams differ in the variance of ToT variance?</li>
<li>Egan: What is the most robust web design (the one that reduces variance)?</li>
<li>IPump: Does training (or the Novel design) reduce variance?</li>
</ul>
<p>Experimentalists should scratch their heads, too, and take a fresh look at their theories. Systematic variation in variance can give further clues about what is going on in the minds of their participants.</p>
<!-- For example, it has been suggested that the Uncanny Valley effect is due to category confusion, which suggests that the longest RTs are to be found in the trough, which [%M&R] have shown to be the case. In [%Keeris], we have fleshed out this theory a little more, basically saying that at every trial the confusion only occurs if the mind picks up enough detail to reject the initial category (Human Face). So, if we reduce the presentation time, we would see more responses where the initial category is kept and the emotional response is positive, rather than extrenmely negative. This hypothesis implies that variance should be largest in the range of the valley, and that with shorter presentation times variance goes up, as responses are a mix of category confusion and category retainers. If we go even shorter, retaining the category will become prominent and variance decreases again. -->
<!-- Strong variance can be a real problem, because this implies  that extremely poor performance is possible, increasing the risk of hazards. In hazardous environments, variance  is a crucial parameter should be track carefully. Especially,  a design improvement *on average* is not necessarily accompanied by less variance (other than the mean-variance relationship prescribed by the error distribution). -->
<!-- Generalized Linear Models gave us control over the average shape of randomness, average variance and average skew. When we use these models, we gain control over the data generating process on an individual level. Distributional models have enormous potential for research and applications. *Rating scales* are vulnerable to different response patterns when the anchoring is ambivalent. But that is not the only possible response pattern. For example, in our own studies on the Uncanny Valley effect, we used analog rating scales and found some participants to reduce the task to a binary answer (left or right end). A Beta distribution can handle such a pattern well, but only if it is consistent. -->
<!--  Another example is mental *workload*. Imagine you are evaluating a simulator-based driver training, designed with the  idea that movement tasks are better learned if you begin fast, not accurate. This training uses *speed episodes*, where the trainee is asked to do 20% faster, at the expense of accuracy. In such an experiment, *workload* can play a moderating factor, because if you work very hard, you can be fast and accurate at the same time. And a few things you only learn, when you try really hard. Try you must!
-->
<div class="sourceCode" id="cb654"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<hr />
<p>In the last three chapters we have met the family of models called <em>Generalized Multi-level Linear Models</em> (GLMM). Linear linear models mainly rest on the linear term, a simple mechanism that unfolds into a combinatoric explosion of possibilities for the predictor side. Groudning on the idea of factors, this is conceptually further expanded by random effects and multi-level models. Multi-level models redeem variance as a first-class citizen among model parameters, and we got used to think in terms of distributions.</p>
<p>Generalized Linear Models expand on that perspective and deal exclusively with the response side of a model. We saw that for all design research measures the Gaussian model is wrong and an (almost) right distribution can just be identified by just following a few rules. Is it just me, or does this feel like the sudden relief, when you untie a pair of bad fitting shoes after a full day hike. And almost literally, some real pain goes away, like ordered factors for a learning curve and saturation effects.</p>
<p>In the recent past have seen many researchers beginning to see the beauty of the New Statistics approach. Suddenly, people start thinking of models, rather than arcane procedures, and start tinkering. Then their favorite regression engine gets a new feature, say distributional models, and it is always worth a try to improve a model.</p>
<p>Ironically, the abundance of possibilities leaves open the question of when to stop. How do we decide whether introducing, say distributional random effects, really improves the model? As we will see in the final chapter, there are two opposite forces at work, model complexity, which is to be avoided, and model fit. Let’s tie our shoes one more time and get a good hold!</p>
</div>
</div>
<div id="further-readings-3" class="section level2" number="7.6">
<h2><span class="header-section-number">7.6</span> Further readings</h2>
<ol style="list-style-type: decimal">
<li>In <span class="citation">(Bürkner and Vuorre 2019)</span> more variations of ordinal models are demonstrated using the Brms regression engine</li>
<li>The extended formula interface of package Brms is explained in <span class="citation">(Bürkner 2018)</span>, including distributional and even non-linear models.</li>
<li>The Rasch model is famous in psychometrics. It can be estimated as a logistic regression model with crossed random effects for persons and items <span class="citation">(Doran et al. 2007)</span>.</li>
</ol>
<!-- Both random effects variance parameters are certaintly positive  -->
<!-- The IPump case we compared two infusion pump interfaces in three sessions and also collected measures on workload, using a self-report ratings. The results suggest that participants responded with quite different ranges. The following Beta regression  -->
<!-- ```{r} -->
<!-- detach(IPump) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- D_pumps %>%  -->
<!--   group_by(Part) %>%  -->
<!--   mutate(Part_mean = mean(workload, na.rm = T)) %>%  -->
<!--   ungroup() %>%  -->
<!--   mutate(Part_ordered = rank(Part_mean)) %>%  -->
<!--   ggplot(aes(x = as.factor(Part_ordered), y = workload)) + -->
<!--   geom_point() -->
<!-- ``` -->
<!-- ```{r} -->
<!-- M_distr <-  -->
<!--   brms(workload ~ ) -->
<!-- ``` -->
<!-- The following distributional Beta regression  -->
<!-- Back to the CUE8 study. Do teams differ in how much variance arises, as the violin plot suggests? Before we test this by a distributional model, recall once again the principle of the mean-variance relation. Beta distributions have variance tied to the mean in much the same way as binomial distributions. When moving along the range between 0 and 1, variance is largest in the center and decreases towards the boundaries. That does not seem to be the case in CUE8. Compare the distributions of Teams B and K to H and L. They all are close to the upper boundary, but show inflated variance. It could be the case, that somehow some teams trigger more extreme responses, inflating variance. The following model tests the effect of testing condition and teams on the location and dispersion of responses simultaneously. -->
<!-- ```{r opts.label = "mcmc"} -->
<!-- F_unequal_var <- bf(SUS ~ 1 + Condition + (1 | Team), -->
<!--                     phi ~ 1 + Condition + (1 | Team)) -->
<!-- M_6_bet <- -->
<!--   D_cue8_mod %>%  -->
<!--   brm(F_unequal_var, -->
<!--       family = Beta(), iter = 1, chains = 1, -->
<!--       data = .) -->
<!-- M_6_bet <- -->
<!--   D_cue8_SUS %>%  -->
<!--   brm(fit = M_6_bet, data = ., -->
<!--       chains = 6, -->
<!--       iter = 4000, -->
<!--       warmup = 2000) -->
<!-- sync_CE(M_6_bet, Env = CUE8) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- T_fixef <-  -->
<!--   brms::fixef(M_6_bet)[,c(1,3,4)] %>%  -->
<!--   as_tibble(rownames = "fixef") %>%  -->
<!--   dplyr::rename(center = Estimate) -->
<!-- T_fixef -->
<!-- ``` -->
<!-- The default behaviour of brm is that the scale parameter $\phi$ is on a log scale. As usual, lifting it to original scale by exponantiation makes it a multiplier. By $\exp(0.774) = 2.167$, we see that there is just a small difference between the two conditions. But, are there other differences between teams? The following table shows the scale multipliers for individual teams. All multipliers are closely arranged around 1, which means there is little differences. The SUS scale can safely be adminstsred in remote and moderated usability testing. -->
<!-- ```{r} -->
<!-- T_ranef <- -->
<!--   ranef(M_6_bet) %>% -->
<!--   filter(nonlin == "phi") %>%  -->
<!--   mutate_if(is.numeric, exp) %>%  -->
<!--   discard_redundant() -->
<!-- T_ranef -->
<!-- ``` -->
<!-- ```{r} -->
<!-- detach(CUE8) -->
<!-- ``` -->
<!-- ### Exercises -->
<!-- 1. The IPump study took repeated measure of workload on a one-item quasi-continuous rating scale. Examine the effect of training and Design on workload. Then take a closer look at how participants used the scale. Build a distributional model. -->
<!-- ```{r opts.label = "future"} -->
<!-- attach(IPump) -->
<!-- max(IPump$D_pumps$workload) -->
<!-- D_pumps <- -->
<!--   D_pumps %>%  -->
<!--   mutate(workload = (workload + 1)/(150 + 2)) %>%  -->
<!--   group_by(Part) %>%  -->
<!--   mutate(mean_workload = mean(workload)) %>%  -->
<!--   ungroup() %>%  -->
<!--   mutate(Part_ord = as.factor(dense_rank(mean_workload))) -->
<!-- D_pumps %>%  -->
<!--   ggplot(aes(x = Part_ord, y = workload)) + -->
<!--   geom_boxplot() + -->
<!--   geom_point() -->
<!-- ``` -->
<!-- ```{r opts.label = "future"} -->
<!-- M_bet <- -->
<!--   D_pumps %>%  -->
<!--   brm(bf(workload ~ 1 + session * Design + (session * Design|Part_ord), -->
<!--          phi ~ 1|Part), -->
<!--       family = Beta(), -->
<!--       data = ., iter = 0, chains = 0) -->
<!-- M_bet <- -->
<!--   D_pumps %>%  -->
<!--   brm(fit = M_bet, -->
<!--       data = ., iter = 3000, chains = 6, warmup = 2000) -->
<!-- T_ranef <- -->
<!--   ranef(M_bet) %>%  -->
<!--   filter(nonlin == "phi") -->
<!-- T_ranef -->
<!-- ``` -->
<!-- ## Gaussian regression revisited

The classic linear model has mercilessly been debunked at the beginning of the chapter. Time and count measures are truly not Gaussian distributed. In \@ref(), the Gaussian situation was illustrated as a bunch of SMURFS that bump into each other.

However, , is a member of the GLM framework, with some rather unusual properties. It assumes normally distributed randomness, which implies a constant variance. In contrast to all other members so far, there is no relation between mean and variance of randomness. The link function simply is *identity* and is consequential for the two problems of Gaussian regression: linearity and bounded ranges. The general take is that, whenever there is a theoretically justified and reasonably pragmatic choice, one better leaves the Gaussian alone. Here it makes its re-entrance for a type of outcome measures that are so problematic that a pragmatic choice is indicated.

### Outlier removal

Outliers are observations that were hit by one strong force, that is of no further interest. Outliers can be upper or lower. One often sees the advice to remove outliers that exceed x times the standard deviation of the outcome variable. That is clearly mistaken as we have pointed out in \@ref(residual_analysis). Outliers must be identified by their residuals, first of all. Another problem arises because the removal intervals are symmetric. Boxplots pull up the Tukey fence at $1.35 \sigma_\epsilon$ beyond or above the first and third interquartile range. No matter, whether you can easily imagine that rule, it assumes symmetry. A boxplot of the two conditions X and Y in the CUE8 study looks like this:
-->
<!--
Boxplots routinely fail at non-symmetric pattens of randomness. Violin plots render a more accurate shape of the residual distribution. The distribution in the moderated condition almost resemble a ginko leaf, full of grace and substantial all the way up. The remote condition looks like a moth that fell prey to a spider (or an obese sword-billed hummingbird). The upper part is long but of thin substance. Seeing the condition on its own, one would perhaps pull up the outlier fence at 800 seconds. However, the tail of the moderated condition is substantial and there is no firm cue there being outliers at all. Since both conditions used comparable instructions and equal tasks, the maximum value of the moderated condition is a conservative upper fence.
-->
<!--

Again, the moderated condition is smooth as a leaf. There is no reason to believe that even the shortest observation is an outlier  at all and we pull this fence up in the remote condition. Here we observe a characteristic pattern, the lower socket of the distribution supported by a clear disruption in the cloud. One possible story for this irregularity is that some remote-testing teams have not checked on task completion and the data set contains observations of cheaters (only if they were payed). Cheating is something that is untypical for the real use of the car rental website. When people enter such a site, they usually want to rent a car. Cheaters in remote studies tell us nothing about performance of real users. They are interuptive, but irrelevant. 

Now, consider the famous *Stroop task*: participants are shown a word that is written to the screen in one of three ink colours (red, blue, green). They are asked to name the ink color as quickly as possible, usually by pressing one of four assigned keys (e.g., Y = red, X = blue, N = green). The task has a twist: the presented words are themselves colour words. It can happen that the word "RED" appears in red ink, which we call the *congruent condition*, but it can also happen that the word "BLUE" appears in red letters, which is *incongruent*. Hundreds of experiments have shown, that in the incongruent condition people respond noticably slower. This observation is called the *Stroop effect* and dozens of theories have spawned from this observation. Discussing them is beyond the scope, but to not let you completely dissatisfied: people seem to always read the words, although they were not asked to do so and we can conclude:

1. Reading is overlearned. It is easier to catch a thrown stone than to resist a word.
1. When people do not follow instructions, this is often a sign of *bottom-up processing*.

What strong forces of no interest can disturb a trial in the Stroop task? Here are some examples:

1. a door could slam
2. participants motivation dropped
3. participants got really exhausted
4. participants went into a zen state of mind, absorbed in deep self observation

What about a possible number 5 on the list: some participant saw the word "gun" in red and the following association raced through their minds:

gun -> huntsman -> man who shot the wolf in little red riding hood




What would be a legitimate way to catch outliers? The Stroop task has been replicated dozens and dozens of times. For rough approximation, one takes a sample of ten papers reporting on variants of the Stroop task and derives a broadly typical range from it. As a rule of thumb, for an upper threshold adding 3 standard deviations to the highest 
, say, between 500ms and 1500ms. 


-->
<!-- ## Robust Models -->
<!-- ```{r} -->
<!-- attach(Egan) -->
<!-- M_1 -->
<!-- insight::find_formula(M_1) -->
<!-- M_1_t <-  -->
<!--   D_egan %>%  -->
<!--   mutate(logToT = log(ToT)) %>%  -->
<!--   brm(formula(M_1), family = "student", data = .) -->
<!-- ls(Egan) -->
<!-- ``` -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mlm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="wwm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsubsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
