<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Basic Linear models | New statistics for design researchers</title>
  <meta name="description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works to make this world an easier place." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Basic Linear models | New statistics for design researchers" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works to make this world an easier place." />
  <meta name="github-repo" content="schmettow/New_Stats" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Basic Linear models | New statistics for design researchers" />
  
  <meta name="twitter:description" content="A statistics book for designers, human factors specialists, UX researchers, applied psychologists and everyone else who works to make this world an easier place." />
  

<meta name="author" content="Martin Schmettow" />


<meta name="date" content="2021-12-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ebs.html"/>
<link rel="next" href="mpm.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="NewStats.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="part"><span><b>I Preparations</b></span></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#whom-for"><i class="fa fa-check"></i><b>1.1</b> Whom this book is for</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#quant-design-research"><i class="fa fa-check"></i><b>1.2</b> Quantitative design research</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#what-new-stats"><i class="fa fa-check"></i><b>1.3</b> What is New Statistics?</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#how-to-use"><i class="fa fa-check"></i><b>1.4</b> How to use this book</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#routes"><i class="fa fa-check"></i><b>1.4.1</b> Routes</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#classroom"><i class="fa fa-check"></i><b>1.4.2</b> In the classroom</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#rosetta"><i class="fa fa-check"></i><b>1.4.3</b> The stone of Rosetta</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#thank-you-and-supplementary-readings"><i class="fa fa-check"></i><b>1.5</b> Thank you and supplementary readings</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="gsr.html"><a href="gsr.html"><i class="fa fa-check"></i><b>2</b> Getting started with R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="gsr.html"><a href="gsr.html#setting-up-r"><i class="fa fa-check"></i><b>2.1</b> Setting up the R environment</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="gsr.html"><a href="gsr.html#install-cran"><i class="fa fa-check"></i><b>2.1.1</b> Installing CRAN packages</a></li>
<li class="chapter" data-level="2.1.2" data-path="gsr.html"><a href="gsr.html#install-github"><i class="fa fa-check"></i><b>2.1.2</b> Installing packages from Github</a></li>
<li class="chapter" data-level="2.1.3" data-path="gsr.html"><a href="gsr.html#first-program"><i class="fa fa-check"></i><b>2.1.3</b> A first statistical program</a></li>
<li class="chapter" data-level="2.1.4" data-path="gsr.html"><a href="gsr.html#bibliographic-notes"><i class="fa fa-check"></i><b>2.1.4</b> Bibliographic notes</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="gsr.html"><a href="gsr.html#r-primer"><i class="fa fa-check"></i><b>2.2</b> Learning R: a primer</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="gsr.html"><a href="gsr.html#objects"><i class="fa fa-check"></i><b>2.2.1</b> Assigning and calling Objects</a></li>
<li class="chapter" data-level="2.2.2" data-path="gsr.html"><a href="gsr.html#vectors"><i class="fa fa-check"></i><b>2.2.2</b> Vectors</a></li>
<li class="chapter" data-level="2.2.3" data-path="gsr.html"><a href="gsr.html#object-types"><i class="fa fa-check"></i><b>2.2.3</b> Basic object types</a></li>
<li class="chapter" data-level="2.2.4" data-path="gsr.html"><a href="gsr.html#operators-functions"><i class="fa fa-check"></i><b>2.2.4</b> Operators and functions</a></li>
<li class="chapter" data-level="2.2.5" data-path="gsr.html"><a href="gsr.html#data-frames"><i class="fa fa-check"></i><b>2.2.5</b> Storing data in data frames</a></li>
<li class="chapter" data-level="2.2.6" data-path="gsr.html"><a href="gsr.html#import-export"><i class="fa fa-check"></i><b>2.2.6</b> Import, export and archiving</a></li>
<li class="chapter" data-level="2.2.7" data-path="gsr.html"><a href="gsr.html#case-env"><i class="fa fa-check"></i><b>2.2.7</b> Case environments</a></li>
<li class="chapter" data-level="2.2.8" data-path="gsr.html"><a href="gsr.html#structuring-data"><i class="fa fa-check"></i><b>2.2.8</b> Structuring data</a></li>
<li class="chapter" data-level="2.2.9" data-path="gsr.html"><a href="gsr.html#data-transformation"><i class="fa fa-check"></i><b>2.2.9</b> Data transformation</a></li>
<li class="chapter" data-level="2.2.10" data-path="gsr.html"><a href="gsr.html#plotting"><i class="fa fa-check"></i><b>2.2.10</b> Plotting data</a></li>
<li class="chapter" data-level="2.2.11" data-path="gsr.html"><a href="gsr.html#fitting"><i class="fa fa-check"></i><b>2.2.11</b> Fitting regression models</a></li>
<li class="chapter" data-level="2.2.12" data-path="gsr.html"><a href="gsr.html#knitting"><i class="fa fa-check"></i><b>2.2.12</b> Knitting statistical reports</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="gsr.html"><a href="gsr.html#lib_gsr"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ebs.html"><a href="ebs.html"><i class="fa fa-check"></i><b>3</b> Elements of Bayesian statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ebs.html"><a href="ebs.html#decision-making"><i class="fa fa-check"></i><b>3.1</b> Rational decision making in design research</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ebs.html"><a href="ebs.html#measuring-uncertainty"><i class="fa fa-check"></i><b>3.1.1</b> Measuring uncertainty</a></li>
<li class="chapter" data-level="3.1.2" data-path="ebs.html"><a href="ebs.html#benchmarking-designs"><i class="fa fa-check"></i><b>3.1.2</b> Benchmarking designs</a></li>
<li class="chapter" data-level="3.1.3" data-path="ebs.html"><a href="ebs.html#comparing-designs"><i class="fa fa-check"></i><b>3.1.3</b> Comparison of designs</a></li>
<li class="chapter" data-level="3.1.4" data-path="ebs.html"><a href="ebs.html#prior-knowledge"><i class="fa fa-check"></i><b>3.1.4</b> Prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ebs.html"><a href="ebs.html#observations-measures"><i class="fa fa-check"></i><b>3.2</b> Observations and measures</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ebs.html"><a href="ebs.html#interaction-seq"><i class="fa fa-check"></i><b>3.2.1</b> Interaction sequences</a></li>
<li class="chapter" data-level="3.2.2" data-path="ebs.html"><a href="ebs.html#perf-measures"><i class="fa fa-check"></i><b>3.2.2</b> Performance measures</a></li>
<li class="chapter" data-level="3.2.3" data-path="ebs.html"><a href="ebs.html#satisfaction-and-other-feelings"><i class="fa fa-check"></i><b>3.2.3</b> Satisfaction and other feelings</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ebs.html"><a href="ebs.html#descriptive-stats"><i class="fa fa-check"></i><b>3.3</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ebs.html"><a href="ebs.html#frequencies"><i class="fa fa-check"></i><b>3.3.1</b> Frequencies</a></li>
<li class="chapter" data-level="3.3.2" data-path="ebs.html"><a href="ebs.html#central-tendency"><i class="fa fa-check"></i><b>3.3.2</b> Central tendency</a></li>
<li class="chapter" data-level="3.3.3" data-path="ebs.html"><a href="ebs.html#dispersion"><i class="fa fa-check"></i><b>3.3.3</b> Dispersion</a></li>
<li class="chapter" data-level="3.3.4" data-path="ebs.html"><a href="ebs.html#associations"><i class="fa fa-check"></i><b>3.3.4</b> Associations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ebs.html"><a href="ebs.html#bayes-prob-theory"><i class="fa fa-check"></i><b>3.4</b> Bayesian probability theory</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ebs.html"><a href="ebs.html#set-theory"><i class="fa fa-check"></i><b>3.4.1</b> Some set theory</a></li>
<li class="chapter" data-level="3.4.2" data-path="ebs.html"><a href="ebs.html#probability"><i class="fa fa-check"></i><b>3.4.2</b> Probability</a></li>
<li class="chapter" data-level="3.4.3" data-path="ebs.html"><a href="ebs.html#likelihood"><i class="fa fa-check"></i><b>3.4.3</b> Likelihood</a></li>
<li class="chapter" data-level="3.4.4" data-path="ebs.html"><a href="ebs.html#bayes-freq-prob"><i class="fa fa-check"></i><b>3.4.4</b> Bayesian and frequentist probability</a></li>
<li class="chapter" data-level="3.4.5" data-path="ebs.html"><a href="ebs.html#bayes-theorem"><i class="fa fa-check"></i><b>3.4.5</b> Bayes theorem</a></li>
<li class="chapter" data-level="3.4.6" data-path="ebs.html"><a href="ebs.html#dynamics-belief"><i class="fa fa-check"></i><b>3.4.6</b> Bayesian dynamics of belief</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ebs.html"><a href="ebs.html#statmod"><i class="fa fa-check"></i><b>3.5</b> Statistical models</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="ebs.html"><a href="ebs.html#structural-part"><i class="fa fa-check"></i><b>3.5.1</b> The structural part</a></li>
<li class="chapter" data-level="3.5.2" data-path="ebs.html"><a href="ebs.html#distributions"><i class="fa fa-check"></i><b>3.5.2</b> Distributions: shapes of randomness</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="ebs.html"><a href="ebs.html#bayes-estimation"><i class="fa fa-check"></i><b>3.6</b> Towards Bayesian estimation</a></li>
<li class="chapter" data-level="3.7" data-path="ebs.html"><a href="ebs.html#priors-defaults"><i class="fa fa-check"></i><b>3.7</b> On priors and defaults</a></li>
<li class="chapter" data-level="3.8" data-path="ebs.html"><a href="ebs.html#further-readings"><i class="fa fa-check"></i><b>3.8</b> Further readings</a></li>
</ul></li>
<li class="part"><span><b>II Models</b></span></li>
<li class="chapter" data-level="4" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>4</b> Basic Linear models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lm.html"><a href="lm.html#gmm"><i class="fa fa-check"></i><b>4.1</b> Quantification at work: grand mean models</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="lm.html"><a href="lm.html#random-walk"><i class="fa fa-check"></i><b>4.1.1</b> Do the random walk: Markov Chain Monte Carlo sampling</a></li>
<li class="chapter" data-level="4.1.2" data-path="lm.html"><a href="lm.html#likelihood-random-term"><i class="fa fa-check"></i><b>4.1.2</b> Likelihood and random term</a></li>
<li class="chapter" data-level="4.1.3" data-path="lm.html"><a href="lm.html#posterior-dist"><i class="fa fa-check"></i><b>4.1.3</b> Working with the posterior distribution</a></li>
<li class="chapter" data-level="4.1.4" data-path="lm.html"><a href="lm.html#clu"><i class="fa fa-check"></i><b>4.1.4</b> Center and interval estimates</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lm.html"><a href="lm.html#lrm"><i class="fa fa-check"></i><b>4.2</b> Walk the line: linear regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="lm.html"><a href="lm.html#transform-measures"><i class="fa fa-check"></i><b>4.2.1</b> Transforming measures</a></li>
<li class="chapter" data-level="4.2.2" data-path="lm.html"><a href="lm.html#correlations"><i class="fa fa-check"></i><b>4.2.2</b> Correlations</a></li>
<li class="chapter" data-level="4.2.3" data-path="lm.html"><a href="lm.html#endless-linear"><i class="fa fa-check"></i><b>4.2.3</b> Endlessly linear</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lm.html"><a href="lm.html#factorial-models"><i class="fa fa-check"></i><b>4.3</b> Factorial Models</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="lm.html"><a href="lm.html#cgm"><i class="fa fa-check"></i><b>4.3.1</b> A versus B: Comparison of groups</a></li>
<li class="chapter" data-level="4.3.2" data-path="lm.html"><a href="lm.html#dummy"><i class="fa fa-check"></i><b>4.3.2</b> Not stupid: dummy variables</a></li>
<li class="chapter" data-level="4.3.3" data-path="lm.html"><a href="lm.html#treatment-contrasts"><i class="fa fa-check"></i><b>4.3.3</b> Treatment contrast coding</a></li>
<li class="chapter" data-level="4.3.4" data-path="lm.html"><a href="lm.html#amm"><i class="fa fa-check"></i><b>4.3.4</b> Absolute Means Model</a></li>
<li class="chapter" data-level="4.3.5" data-path="lm.html"><a href="lm.html#ofm"><i class="fa fa-check"></i><b>4.3.5</b> Ordered Factorial Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mpm.html"><a href="mpm.html"><i class="fa fa-check"></i><b>5</b> Multi-predictor models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mpm.html"><a href="mpm.html#mrm"><i class="fa fa-check"></i><b>5.1</b> On surface: multiple regression models</a></li>
<li class="chapter" data-level="5.2" data-path="mpm.html"><a href="mpm.html#mfm"><i class="fa fa-check"></i><b>5.2</b> Crossover: multifactorial models</a></li>
<li class="chapter" data-level="5.3" data-path="mpm.html"><a href="mpm.html#grm"><i class="fa fa-check"></i><b>5.3</b> Line-by-line: grouped regression models</a></li>
<li class="chapter" data-level="5.4" data-path="mpm.html"><a href="mpm.html#cfxm"><i class="fa fa-check"></i><b>5.4</b> Conditional effects models</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="mpm.html"><a href="mpm.html#cmrm"><i class="fa fa-check"></i><b>5.4.1</b> Conditional multiple regression</a></li>
<li class="chapter" data-level="5.4.2" data-path="mpm.html"><a href="mpm.html#cmfm"><i class="fa fa-check"></i><b>5.4.2</b> Conditional multifactorial models</a></li>
<li class="chapter" data-level="5.4.3" data-path="mpm.html"><a href="mpm.html#saturation"><i class="fa fa-check"></i><b>5.4.3</b> Saturation: hitting the boundaries</a></li>
<li class="chapter" data-level="5.4.4" data-path="mpm.html"><a href="mpm.html#amplification"><i class="fa fa-check"></i><b>5.4.4</b> Amplification: more than the sum</a></li>
<li class="chapter" data-level="5.4.5" data-path="mpm.html"><a href="mpm.html#cfx-theory"><i class="fa fa-check"></i><b>5.4.5</b> Conditional effects and design theory</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="mpm.html"><a href="mpm.html#prm"><i class="fa fa-check"></i><b>5.5</b> Doing the rollercoaster: polynomial regression models</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="mpm.html"><a href="mpm.html#test-stat"><i class="fa fa-check"></i><b>5.5.1</b> Make yourself a test statistic</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="mpm.html"><a href="mpm.html#further-readings-1"><i class="fa fa-check"></i><b>5.6</b> Further readings</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mlm.html"><a href="mlm.html"><i class="fa fa-check"></i><b>6</b> Multilevel models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mlm.html"><a href="mlm.html#intercept-re"><i class="fa fa-check"></i><b>6.1</b> The Human Factor: Intercept random effects</a></li>
<li class="chapter" data-level="6.2" data-path="mlm.html"><a href="mlm.html#slope-re"><i class="fa fa-check"></i><b>6.2</b> Multi-level linear regression: variance in change</a></li>
<li class="chapter" data-level="6.3" data-path="mlm.html"><a href="mlm.html#thinking-multi-level"><i class="fa fa-check"></i><b>6.3</b> Thinking multi-level</a></li>
<li class="chapter" data-level="6.4" data-path="mlm.html"><a href="mlm.html#universality"><i class="fa fa-check"></i><b>6.4</b> Testing universality of theories</a></li>
<li class="chapter" data-level="6.5" data-path="mlm.html"><a href="mlm.html#non-human-populations"><i class="fa fa-check"></i><b>6.5</b> Non-human populations and cross-overs</a></li>
<li class="chapter" data-level="6.6" data-path="mlm.html"><a href="mlm.html#nested-re"><i class="fa fa-check"></i><b>6.6</b> Nested random effects</a></li>
<li class="chapter" data-level="6.7" data-path="mlm.html"><a href="mlm.html#pool-shrink"><i class="fa fa-check"></i><b>6.7</b> What are random effects? On pooling and shrinkage</a></li>
<li class="chapter" data-level="6.8" data-path="mlm.html"><a href="mlm.html#psychometrics"><i class="fa fa-check"></i><b>6.8</b> Psychometrics and design-o-metric models</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="mlm.html"><a href="mlm.html#coverage"><i class="fa fa-check"></i><b>6.8.1</b> Coverage</a></li>
<li class="chapter" data-level="6.8.2" data-path="mlm.html"><a href="mlm.html#reliability"><i class="fa fa-check"></i><b>6.8.2</b> Reliability</a></li>
<li class="chapter" data-level="6.8.3" data-path="mlm.html"><a href="mlm.html#validity"><i class="fa fa-check"></i><b>6.8.3</b> Validity</a></li>
<li class="chapter" data-level="6.8.4" data-path="mlm.html"><a href="mlm.html#designometrix"><i class="fa fa-check"></i><b>6.8.4</b> Towards Design-o-metrix</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="mlm.html"><a href="mlm.html#further-readings-2"><i class="fa fa-check"></i><b>6.9</b> Further readings</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="glm.html"><a href="glm.html#elements-glm"><i class="fa fa-check"></i><b>7.1</b> Elements of Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="glm.html"><a href="glm.html#relinking-linearity"><i class="fa fa-check"></i><b>7.1.1</b> Re-linking linearity</a></li>
<li class="chapter" data-level="7.1.2" data-path="glm.html"><a href="glm.html#choosing-randomness"><i class="fa fa-check"></i><b>7.1.2</b> Choosing patterns of randomness</a></li>
<li class="chapter" data-level="7.1.3" data-path="glm.html"><a href="glm.html#mean-var-rel"><i class="fa fa-check"></i><b>7.1.3</b> Mean-variance relationship</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="glm.html"><a href="glm.html#count-data"><i class="fa fa-check"></i><b>7.2</b> Count data</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="glm.html"><a href="glm.html#pois-reg"><i class="fa fa-check"></i><b>7.2.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="glm.html"><a href="glm.html#logistic-reg"><i class="fa fa-check"></i><b>7.2.2</b> Logistic (aka Binomial) regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="glm.html"><a href="glm.html#overdispersion"><i class="fa fa-check"></i><b>7.2.3</b> Modelling overdispersion</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="glm.html"><a href="glm.html#duration-measures"><i class="fa fa-check"></i><b>7.3</b> Duration measures</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="glm.html"><a href="glm.html#exp-gam-reg"><i class="fa fa-check"></i><b>7.3.1</b> Exponential and Gamma regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="glm.html"><a href="glm.html#exgauss-reg"><i class="fa fa-check"></i><b>7.3.2</b> ExGaussian regression</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="glm.html"><a href="glm.html#rating-scales"><i class="fa fa-check"></i><b>7.4</b> Rating scales</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="glm.html"><a href="glm.html#ord-logist-reg"><i class="fa fa-check"></i><b>7.4.1</b> Ordered logistic regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="glm.html"><a href="glm.html#beta-reg"><i class="fa fa-check"></i><b>7.4.2</b> Beta regression</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="glm.html"><a href="glm.html#distributional-models"><i class="fa fa-check"></i><b>7.5</b> Beyond mean: distributional models</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="glm.html"><a href="glm.html#item-anchoring"><i class="fa fa-check"></i><b>7.5.1</b> Item-level anchoring in rating scales</a></li>
<li class="chapter" data-level="7.5.2" data-path="glm.html"><a href="glm.html#part-employment"><i class="fa fa-check"></i><b>7.5.2</b> Participant-level employment of scale</a></li>
<li class="chapter" data-level="7.5.3" data-path="glm.html"><a href="glm.html#part-skew"><i class="fa fa-check"></i><b>7.5.3</b> Participant-level skew in reaction times</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="glm.html"><a href="glm.html#further-readings-3"><i class="fa fa-check"></i><b>7.6</b> Further readings</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="wwm.html"><a href="wwm.html"><i class="fa fa-check"></i><b>8</b> Working with models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="wwm.html"><a href="wwm.html#model-criticism"><i class="fa fa-check"></i><b>8.1</b> Model criticism</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="wwm.html"><a href="wwm.html#residual-analysis"><i class="fa fa-check"></i><b>8.1.1</b> Residual analysis</a></li>
<li class="chapter" data-level="8.1.2" data-path="wwm.html"><a href="wwm.html#fitted-responses"><i class="fa fa-check"></i><b>8.1.2</b> Fitted responses analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="wwm.html"><a href="wwm.html#model-comp"><i class="fa fa-check"></i><b>8.2</b> Model comparison</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="wwm.html"><a href="wwm.html#overfitting"><i class="fa fa-check"></i><b>8.2.1</b> The problem of over-fitting</a></li>
<li class="chapter" data-level="8.2.2" data-path="wwm.html"><a href="wwm.html#cross-validation"><i class="fa fa-check"></i><b>8.2.2</b> Cross validation and LOO</a></li>
<li class="chapter" data-level="8.2.3" data-path="wwm.html"><a href="wwm.html#ic"><i class="fa fa-check"></i><b>8.2.3</b> Information Criteria</a></li>
<li class="chapter" data-level="8.2.4" data-path="wwm.html"><a href="wwm.html#model-selection"><i class="fa fa-check"></i><b>8.2.4</b> Model selection</a></li>
<li class="chapter" data-level="8.2.5" data-path="wwm.html"><a href="wwm.html#choose-dist"><i class="fa fa-check"></i><b>8.2.5</b> Comparing response distributions</a></li>
<li class="chapter" data-level="8.2.6" data-path="wwm.html"><a href="wwm.html#testing-theories"><i class="fa fa-check"></i><b>8.2.6</b> Testing hypotheses</a></li>
<li class="chapter" data-level="8.2.7" data-path="wwm.html"><a href="wwm.html#bayes-factor"><i class="fa fa-check"></i><b>8.2.7</b> A note on Bayes Factor</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="wwm.html"><a href="wwm.html#further-readings-4"><i class="fa fa-check"></i><b>8.3</b> Further readings</a></li>
</ul></li>
<li class="part"><span><b>III Preview Chapters</b></span></li>
<li class="chapter" data-level="9" data-path="LCM.html"><a href="LCM.html"><i class="fa fa-check"></i><b>9</b> Learning curve models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="LCM.html"><a href="LCM.html#the-tweak-finder-model-of-building-skills"><i class="fa fa-check"></i><b>9.1</b> The tweak-finder model of building skills</a></li>
<li class="chapter" data-level="9.2" data-path="LCM.html"><a href="LCM.html#the-exponential-law-of-finding-tweaks"><i class="fa fa-check"></i><b>9.2</b> The Exponential law of finding tweaks</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="LCM.html"><a href="LCM.html#linearization-of-parameters"><i class="fa fa-check"></i><b>9.2.1</b> Linearization of parameters</a></li>
<li class="chapter" data-level="9.2.2" data-path="LCM.html"><a href="LCM.html#priors"><i class="fa fa-check"></i><b>9.2.2</b> Priors</a></li>
<li class="chapter" data-level="9.2.3" data-path="LCM.html"><a href="LCM.html#building-the-model"><i class="fa fa-check"></i><b>9.2.3</b> Building the model</a></li>
<li class="chapter" data-level="9.2.4" data-path="LCM.html"><a href="LCM.html#analyzing-the-results"><i class="fa fa-check"></i><b>9.2.4</b> Analyzing the results</a></li>
<li class="chapter" data-level="9.2.5" data-path="LCM.html"><a href="LCM.html#a-bunch-of-girls-and-their-lacies"><i class="fa fa-check"></i><b>9.2.5</b> A bunch of girls and their Lacies</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="LCM.html"><a href="LCM.html#how-to-deal-with-flat-lines"><i class="fa fa-check"></i><b>9.3</b> How to deal with flat lines</a></li>
<li class="chapter" data-level="9.4" data-path="LCM.html"><a href="LCM.html#conditional-learning-curve-models"><i class="fa fa-check"></i><b>9.4</b> Conditional Learning Curve Models</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="LCM.html"><a href="LCM.html#differences-in-experience"><i class="fa fa-check"></i><b>9.4.1</b> Differences in experience</a></li>
<li class="chapter" data-level="9.4.2" data-path="LCM.html"><a href="LCM.html#conditional-rate-and-amplitude"><i class="fa fa-check"></i><b>9.4.2</b> Conditional Rate and Amplitude</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="LCM.html"><a href="LCM.html#compound-learning-curves-experimental"><i class="fa fa-check"></i><b>9.5</b> Compound learning curves (EXPERIMENTAL)</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="LCM.html"><a href="LCM.html#adjusting-for-fatigue-and-finding-a-separation"><i class="fa fa-check"></i><b>9.5.1</b> Adjusting for fatigue and finding a separation</a></li>
<li class="chapter" data-level="9.5.2" data-path="LCM.html"><a href="LCM.html#lace-acy"><i class="fa fa-check"></i><b>9.5.2</b> Lace ACY!</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="cases.html"><a href="cases.html"><i class="fa fa-check"></i><b>A</b> Cases</a>
<ul>
<li class="chapter" data-level="A.1" data-path="cases.html"><a href="cases.html#real-cases"><i class="fa fa-check"></i><b>A.1</b> Real cases</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="cases.html"><a href="cases.html#hugme"><i class="fa fa-check"></i><b>A.1.1</b> Hugme</a></li>
<li class="chapter" data-level="A.1.2" data-path="cases.html"><a href="cases.html#cue8"><i class="fa fa-check"></i><b>A.1.2</b> CUE8</a></li>
<li class="chapter" data-level="A.1.3" data-path="cases.html"><a href="cases.html#uncanny"><i class="fa fa-check"></i><b>A.1.3</b> Uncanny Valley</a></li>
<li class="chapter" data-level="A.1.4" data-path="cases.html"><a href="cases.html#ipump"><i class="fa fa-check"></i><b>A.1.4</b> IPump</a></li>
<li class="chapter" data-level="A.1.5" data-path="cases.html"><a href="cases.html#sleepstudy"><i class="fa fa-check"></i><b>A.1.5</b> Case Sleepstudy</a></li>
<li class="chapter" data-level="A.1.6" data-path="cases.html"><a href="cases.html#egan"><i class="fa fa-check"></i><b>A.1.6</b> Egan</a></li>
<li class="chapter" data-level="A.1.7" data-path="cases.html"><a href="cases.html#mmn"><i class="fa fa-check"></i><b>A.1.7</b> Case: Millers Magic Number</a></li>
<li class="chapter" data-level="A.1.8" data-path="cases.html"><a href="cases.html#aup"><i class="fa fa-check"></i><b>A.1.8</b> AUP</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="cases.html"><a href="cases.html#synthetic-data"><i class="fa fa-check"></i><b>A.2</b> Synthetic data sets</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="cases.html"><a href="cases.html#rainfall"><i class="fa fa-check"></i><b>A.2.1</b> Rainfall</a></li>
<li class="chapter" data-level="A.2.2" data-path="cases.html"><a href="cases.html#sec99"><i class="fa fa-check"></i><b>A.2.2</b> 99 seconds</a></li>
<li class="chapter" data-level="A.2.3" data-path="cases.html"><a href="cases.html#rational"><i class="fa fa-check"></i><b>A.2.3</b> Rational</a></li>
<li class="chapter" data-level="A.2.4" data-path="cases.html"><a href="cases.html#browsingab"><i class="fa fa-check"></i><b>A.2.4</b> BrowsingAB</a></li>
<li class="chapter" data-level="A.2.5" data-path="cases.html"><a href="cases.html#headache"><i class="fa fa-check"></i><b>A.2.5</b> Headache</a></li>
<li class="chapter" data-level="A.2.6" data-path="cases.html"><a href="cases.html#reading"><i class="fa fa-check"></i><b>A.2.6</b> Reading time</a></li>
<li class="chapter" data-level="A.2.7" data-path="cases.html"><a href="cases.html#argame"><i class="fa fa-check"></i><b>A.2.7</b> AR_game</a></li>
<li class="chapter" data-level="A.2.8" data-path="cases.html"><a href="cases.html#sleep"><i class="fa fa-check"></i><b>A.2.8</b> Sleep</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">New statistics for design researchers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lm" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Basic Linear models</h1>
<p>Linear models answer the question of how one quantitative outcome, say ToT, decreases or increases, when a condition changes.</p>
<p>First, I will introduce the most basic LM.
The grand mean model (GMM does not have a single predictors and produces just a single estimate: the grand mean in the population. That can be useful, when there exists an external standard to which the design must adhere to. In R, the GMM has a formula, like this: <code>ToT ~ 1</code>. At the example of the GMM, I describe some basic concepts of Bayesian linear models. On the practical side of things, CLU tables are introduced, which is one major work horse to report our results.</p>
<p>The most obvious application of LM comes next: in linear regression models (LRM), a metric predictor (e.g. age) is linked to the outcome by a linear function, such as: <span class="math inline">\(f(x) = \beta_0 + \beta_1x\)</span>.
In R, this is: <code>ToT ~ 1 + age</code>.
Underneath is a very practical section that explains how simple transformations can make the results of an estimation more clear.
Correlations work in similar situations as LRMs, and it is good to know what the differences and how correlations are linked to linear slope parameters.
I end this section with a warning: the assumption of linearity is limited to a straight line and this is violated not by some, but all possible data.</p>
<p>A very common type of research question is how an outcome changes under different conditions.
In design research this is always the case, when designs are compared.
Expressed as a formula, a <em>factorial model</em> looks just like an LRM: <code>ToT ~ 1 + Design</code>.
The rest of the section is dedicated to the techniques that make it possible to put a qualitative variable into a linear equation.
Understanding these techniques opens the door to making your own variants that exactly fit your purpose, such as when a factor is ordered and you think of it of as a stairway, rather than treatments.
Finally, we will see how ordered factor models can resolve non-linear relationships, as they appear with learning curves.</p>
<div id="gmm" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Quantification at work: grand mean models</h2>
<p>Reconsider Jane from <a href="ebs.html#decision-making">3.1</a>.
She was faced with the problem that potential competitors could challenge the claim “rent a car in 99 seconds” and in consequence drag them to court.
More precisely, the question was: “will users on average be able …,” which is nothing but the <em>population mean</em>.
A statistical model estimating just that, we call a <em>grand mean model</em> (GMM).
The GMM is the most simple of all models, so in a way, we can also think of it as the “grandmother of all models.”
Although its is the simplest of all, it is of useful application in design research.
For many high risk situations, there often exist minimum standards for performance to which one can compare the population mean, here are a few examples:</p>
<ul>
<li>with medical infusion pump the frequency of decimal input error (giving the tenfold or the tenth of the prescribed dose) must be below a bearable level</li>
<li>the checkout process of an e-commerce website must have a a cancel rate not higher than …</li>
<li>the timing of a traffic light must be designed to give drivers enough time to hit the brakes.</li>
</ul>
<!-- #15 -->
<p>A GMM predicts the <em>average</em> expected level of performance in the population (<span class="math inline">\(\beta_0\)</span>).
Let’s atsrt with a toy example: When you want to predict the IQ score of a totally random and anonymous individual (from this population), the population average (which is standardized to be 100) is your best guess.
However, this best guess is imperfect due to the individual differences.
and chances are rather low that 100 is the perfect guess.</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="lm.html#cb284-1" aria-hidden="true" tabindex="-1"></a><span class="co"># random IQ sample, rounded to whole numbers</span></span>
<span id="cb284-2"><a href="lm.html#cb284-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb284-3"><a href="lm.html#cb284-3" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb284-4"><a href="lm.html#cb284-4" aria-hidden="true" tabindex="-1"></a>D_IQ <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb284-5"><a href="lm.html#cb284-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">score =</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>),</span>
<span id="cb284-6"><a href="lm.html#cb284-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">IQ =</span> <span class="fu">round</span>(score, <span class="dv">0</span>)</span>
<span id="cb284-7"><a href="lm.html#cb284-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb284-8"><a href="lm.html#cb284-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb284-9"><a href="lm.html#cb284-9" aria-hidden="true" tabindex="-1"></a><span class="co"># proportion of correct guesses</span></span>
<span id="cb284-10"><a href="lm.html#cb284-10" aria-hidden="true" tabindex="-1"></a>pi_100 <span class="ot">&lt;-</span> <span class="fu">sum</span>(D_IQ<span class="sc">$</span>IQ <span class="sc">==</span> <span class="dv">100</span>) <span class="sc">/</span> N</span>
<span id="cb284-11"><a href="lm.html#cb284-11" aria-hidden="true" tabindex="-1"></a><span class="fu">str_c</span>(<span class="st">&quot;Proportion of correct guesses (IQ = 100): &quot;</span>, pi_100)</span></code></pre></div>
<pre><code>## [1] &quot;Proportion of correct guesses (IQ = 100): 0.031&quot;</code></pre>
<p>This best guess is imperfect, for a variety reasons:</p>
<ol style="list-style-type: decimal">
<li>People differ a lot in intelligence.</li>
<li>The IQ measure itself is uncertain. A person could have had a bad day, when doing the test, whereas another person just had more experience with being tested.</li>
<li>If test items are sampled from a larger set, tests may still differ a tiny bit.</li>
<li>The person is like the Slum Dog Millionaire, who by pure coincidence encountered precisely those questions, he could answer.</li>
</ol>
<p>In a later chapters we will investigate on the sources of randomness <a href="mlm.html#mlm">6</a>.
But, like all other models in this chapter, the GMM is a <em>single level linear model</em>.
This single level is the <em>population-level</em> and all unexplained effects that make variation are collected in <span class="math inline">\(\epsilon_i\)</span>, the <em>residuals</em> or <em>errors</em>, which are assumed to follow a Gaussian distribution with center zero and <em>standard error</em> <span class="math inline">\(\sigma_\epsilon\)</span>.</p>
<p>Formally, a GMM is written as follows, where <span class="math inline">\(\mu_i\)</span> is the <em>predicted value</em> of person <span class="math inline">\(i\)</span> and <span class="math inline">\(\beta_0\)</span> is the population mean <span class="math inline">\(\beta_0\)</span>, which is referred to as <em>Intercept</em> (see <a href="lm.html#cgm">4.3.1</a>).</p>
<p><span class="math display">\[
\begin{aligned}
\mu_i &amp;= \beta_0\\
y_i &amp;= \mu_i, + \epsilon_i\\
\epsilon_i &amp;\sim \textrm{Gaus}(0, \sigma_\epsilon)
\end{aligned}
\]</span></p>
<p>This way of writing a linear model only works for Gaussian linear models, as only here, the residuals are symmetric and are adding up to Zero.
In chapter <a href="glm.html#glm">7</a>, we will introduce linear models with different error distributions.
For that reason, I will use a slightly different notation throughout:</p>
<p><span class="math display">\[
\begin{aligned}
\mu_i &amp;= \beta_0\\
y_i &amp;\sim \textrm{Gaus}(\mu_i, \sigma_\epsilon)
\end{aligned}
\]</span></p>
<p>The notable difference between the two notations is that in the first we have just one error distribution.
In the second model, every observation actually is taken from its own distribution, located at <span class="math inline">\(\mu_i\)</span>, albeit with a <em>constant variance</em>.</p>
<p>Enough about mathematic formulas for now.
In R regression models are specified by a dedicated formula language, which I will develop step-by-step in chapter.
This formula language is not very complex, at the same time provides a surprisingly high flexibility for specification of models.
The only really odd feature of this formula language is that it represents the intercept <span class="math inline">\(\beta_0\)</span> with <code>1</code>.
To add to the confusion, the intercept means something different, depending on what type of model is estimated.
In GMMs, it is the grand mean, whereas in group-mean comparisons, it is the mean of one reference group <a href="lm.html#cgm">4.3.1</a> and in linear regression, it is has the usual meaning as in linear equations.</p>
<!-- Below we estimate a GMM on (simulated) IQ scores using the `stan_glm` regression engine. -->
<!-- The `clu()` command extracts all the parameters of a models and reports them with 95% certainty limits (Table \@ref(tab:iq-2)). -->
<!-- ```{r opts.label = "invisible"} -->
<!-- attach(Chapter_LM) -->
<!-- ``` -->
<!-- ```{r opts.label = "mcmc"} -->
<!-- M_IQ <- stan_glm(IQ ~ 1, -->
<!--                  data = D_IQ) -->
<!-- ``` -->
<!-- ```{r iq-2} -->
<!-- bayr::clu(M_IQ) -->
<!-- ``` -->
<!-- ```{r opts.label = "mcsync"} -->
<!-- sync_CE(Chapter_LM, D_IQ, M_IQ) -->
<!-- ``` -->
<!-- ```{r opts.label = "invisible"} -->
<!-- detach(Chapter_LM) -->
<!-- ``` -->
<!-- So, when estimating the grand mean model, we estimate the intercept $\beta_0$ and the standard error $\sigma$. -->
<p>Only estimating the population mean may appear futile to many, because interesting research questions seem to involve associations between variable. However, sometimes it is as simply as comparing against an external criterion to see whether there may be a problem. Such a case was construed in Section (<a href="ebs.html#decision-making">3.1</a>): Is it legally safe to claim a task can be completed in 99 seconds? In R, the analysis unfolds as follows: completion times (ToT) are stored in a data frame, with one observation per row.</p>
<p>This data frame is send to the R command <code>stan_glm</code> for estimation, using <code>data = D_1</code>.
<!-- #16 --> The formula of the grand mean model is <code>ToT ~ 1</code>.
Left of the <code>~</code> (<em>tilde</em>) operator is the outcome variable.
In design research, this often is a performance measure, such as time-on-task, number-of-errors or self-reported cognitive workload.
The right hand side specifies the <em>deterministic part</em>, containing all variables that are used to predict performance.</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="lm.html#cb286-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Sec99)</span></code></pre></div>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="lm.html#cb287-1" aria-hidden="true" tabindex="-1"></a>M_1 <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> D_1)</span></code></pre></div>
<p>The result is a complex model object; the <code>summary</code> command produces a detailed overview on how the model was specified, the estimated parameters, as well as some diagnostics.</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="lm.html#cb288-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(M_1)</span></code></pre></div>
<pre><code>## 
## Model Info:
##  function:     stan_glm
##  family:       gaussian [identity]
##  formula:      ToT ~ 1
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help(&#39;prior_summary&#39;)
##  observations: 100
##  predictors:   1
## 
## Estimates:
##               mean   sd    10%   50%
## (Intercept) 106.0    3.2 101.9 106.0
## sigma        31.5    2.2  28.7  31.4
##               90%
## (Intercept) 110.0
## sigma        34.4
## 
## Fit Diagnostics:
##            mean   sd    10%   50%   90%
## mean_PPD 105.9    4.5 100.2 105.9 111.8
## 
## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)).
## 
## MCMC diagnostics
##               mcse Rhat n_eff
## (Intercept)   0.1  1.0  2191 
## sigma         0.0  1.0  2591 
## mean_PPD      0.1  1.0  2902 
## log-posterior 0.0  1.0  1666 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).</code></pre>
<p>For the researcher, the essential part is the parameter estimates. Such a table must contain information about the location of the effect (left or right, large or small) and the uncertainty. A more common form than the summary above is to present a central tendency and lower and upper credibility limits for the degree of uncertainty. The <em>CLU</em> tables used in this book report the median for the center and the 2.5% and 97.5% lower and upper quantiles, which is called a 95% credibility interval. The less certain an estimate is, the wider is the interval. Due to the Bayesian interpretation, it is legit so say that the true value of the parameter is within these limits with a certainty of 95%, which as an odds is 19:1. The <code>clu</code> command from the Bayr package produces such a parameter table from the model object (Table <a href="lm.html#tab:sec99-21">4.1</a>).</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="lm.html#cb290-1" aria-hidden="true" tabindex="-1"></a><span class="fu">clu</span>(M_1)</span></code></pre></div>
<table>
<caption><span id="tab:sec99-21">Table 4.1: </span>Parameter estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">106.0</td>
<td align="right">99.7</td>
<td align="right">112.2</td>
</tr>
<tr class="even">
<td align="left">sigma_resid</td>
<td align="left"></td>
<td align="right">31.4</td>
<td align="right">27.5</td>
<td align="right">36.1</td>
</tr>
</tbody>
</table>
<p>The <code>clu</code> command being used in this book is from the accompanying R package <code>bayr</code> and produces tables of estimates, showing <em>all parameters</em> in a model, that covers the effects, i.e. coefficients the dispersion or shape of the error distribution, here the standard error.
Often, the distribution parameters are of lesser interest and <code>clu</code> comes with sibling commands to only show the coefficients:</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="lm.html#cb291-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_1)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-4">Table 4.2: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">type</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">fixef</td>
<td align="left">Intercept</td>
<td align="right">106</td>
<td align="right">99.7</td>
<td align="right">112</td>
</tr>
</tbody>
</table>
<p>Note that the Rstanarm regression engines brings its own <code>coef</code> command to extract estimates, but this often report the center estimates, only.</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="lm.html#cb292-1" aria-hidden="true" tabindex="-1"></a>rstanarm<span class="sc">:::</span><span class="fu">coef.stanreg</span>(M_1)</span></code></pre></div>
<pre><code>## (Intercept) 
##         106</code></pre>
<p>In order to always use the convenient commands from package bayr, it is necessary to load Bayr after package Rstanarm.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="lm.html#cb294-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rstanarm)</span>
<span id="cb294-2"><a href="lm.html#cb294-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bayr)</span></code></pre></div>
<p>Then, Bayr overwrites the commands for reporting to produce consistent coefficient tables (and others), which can go into a report, as they are.</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<!-- #49 -->
<p>A GMM is the simplest linear model and as such makes absolute minimal use of knowledge when doing its predictions.
The only thing one knows is that test persons come from one and the same population (humans, users, psychology students).
Accordingly, individual predictions are very inaccurate.
From the GMM we will depart in two directions.
First, in the remainder of this chapter, we will add predictors to the model, for example age of participants or a experimental conditions.
These models will improve our predictive accuracy by using additional knowledge about participants and conditions of testing.</p>
<p>Reporting a model estimate together with its level of certainty is what makes a statistic <em>inferential</em> (rather than merely descriptive).
In Bayesian statistics, the posterior distribution is estimated (usually by means of MCMC sampling) and this distribution carries the full information on certainty.
If the posterior is widely spread, an estimate is rather uncertain.
You may still bet on values close to the center estimate, but you should keep your bid low.
Some authors (or regression engines) express the level of certainty by means of the standard error.
However, the standard deviation is a single value and has the disadvantage that a single value does not represent non-symmetric distributions well.
A better way is to express certainty as limits, a lower and an upper.
The most simple method resembles that of the median by using quantiles.</p>
<!-- #19 -->
<p>It is common practice to explain and interpret coefficient tables for the audience.
My suggestion of how to <em>report regression results</em> is to simply walk through the table row-by-row and for every parameter make <em>three statements</em>:</p>
<ol style="list-style-type: decimal">
<li>What the parameter says</li>
<li>a quantitative statement based on the central tendency</li>
<li>an uncertainty statement based on the CIs</li>
</ol>
<p>In the present case Sec99 that would be:</p>
<p>The <em>intercept</em> (or <span class="math inline">\(\beta_0\)</span>) is the population average and is in the region of 106 seconds, which is pretty far from the target of 99 seconds.
The certainty is pretty good.
At least we can say that the chance of the true mean being 99 seconds or smaller is pretty marginal, as it is not even contained in the 95% CI.</p>
<p>And for <span class="math inline">\(\sigma\)</span>:</p>
<p>The population mean is rather not representative for the observations as the standard error is almost one third of it.
There is much deviation from the population mean in the measures.</p>
<p>From here on, we will build up a whole family of models that go beyond the population mean, but have effects.
A <em>linear regression model</em> can tell us what effect <em>metric predictors</em>, like age or experience have on user performance.
<a href="lm.html#lrm">4.2</a> <em>Factorial models</em> we can use for experimental conditions, or when comparing designs.</p>
<!-- #123 -->
<div id="random-walk" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Do the random walk: Markov Chain Monte Carlo sampling</h3>
<p>So far, we have seen how linear models are specified and how parameters are interpreted from standard coefficient tables.
While it is convenient to have a standard procedure it may be useful to understand how these estimates came into being.
In Bayesian estimation, an approximation of the <em>posterior distribution (PD)</em> is the result of running the engine and is the central point of departure for creating output, such as coefficient tables.
PD assigns a degree of certainty for every possible combination of parameter values.
In the current case, you can ask the PD, where and how certain the population mean and the residual standard error are, but you can also ask: How certain are we that the population mean is smaller than 99 seconds and <span class="math inline">\(\sigma\)</span> is smaller than 10?</p>
<p>In a perfect world, we would know the analytic formula of the posterior and derive statements from it.
In most non-trivial models, though, there is no such formula one can work with.
Instead, what the regression engine does is to approximate the PD by a random-walk algorithm called Markov-Chain Monte Carlo sampling (MCMC).</p>
<p>The <code>stan_glm</code> command returns a large object that stores, among others, the full random walk.
This random walk represents the posterior distribution almost directly.
The following code extracts the posterior distribution from the regression object and prints it.
When calling the new object (class: tbl_post) directly, it provides a compact summary of all parameters in the model, in this case the intercept and the residual standard error.</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="lm.html#cb296-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Sec99)</span></code></pre></div>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="lm.html#cb297-1" aria-hidden="true" tabindex="-1"></a>P_1 <span class="ot">&lt;-</span> <span class="fu">posterior</span>(M_1)</span>
<span id="cb297-2"><a href="lm.html#cb297-2" aria-hidden="true" tabindex="-1"></a>P_1</span></code></pre></div>
<table>
<caption><span id="tab:sec99-22">Table 4.3: </span>MCMC posterior with 4000 samples of 2 parameters in 1 model(s)</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">parameter</th>
<th align="left">type</th>
<th align="left">fixef</th>
<th align="right">count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">M_1</td>
<td align="left">Intercept</td>
<td align="left">fixef</td>
<td align="left">Intercept</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">M_1</td>
<td align="left">sigma_resid</td>
<td align="left">disp</td>
<td align="left"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>The 99 second GMM has two parameters and therefore the posterior distribution has three dimensions: the parameter dimensions <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\sigma\)</span> and the probability density.
Three dimensional plots are difficult to put on a surface, but for somewhat regular patterns, a density plot with dots does a sufficient job (Figure <a href="lm.html#fig:sec99-23">4.1</a>, left).</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="lm.html#cb298-1" aria-hidden="true" tabindex="-1"></a>P_1 <span class="sc">%&gt;%</span></span>
<span id="cb298-2"><a href="lm.html#cb298-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(chain, iter, parameter, value) <span class="sc">%&gt;%</span></span>
<span id="cb298-3"><a href="lm.html#cb298-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spread</span>(parameter, value) <span class="sc">%&gt;%</span></span>
<span id="cb298-4"><a href="lm.html#cb298-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Intercept, <span class="at">y =</span> sigma_resid)) <span class="sc">+</span></span>
<span id="cb298-5"><a href="lm.html#cb298-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_density_2d</span>(<span class="at">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="fu">aes</span>(<span class="at">size =</span> <span class="fu">after_stat</span>(density)), <span class="at">n =</span> <span class="dv">20</span>, <span class="at">contour =</span> F) <span class="sc">+</span></span>
<span id="cb298-6"><a href="lm.html#cb298-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">95</span>, <span class="dv">115</span>) <span class="sc">+</span></span>
<span id="cb298-7"><a href="lm.html#cb298-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">25</span>, <span class="dv">40</span>)</span>
<span id="cb298-8"><a href="lm.html#cb298-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb298-9"><a href="lm.html#cb298-9" aria-hidden="true" tabindex="-1"></a>P_1 <span class="sc">%&gt;%</span></span>
<span id="cb298-10"><a href="lm.html#cb298-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(iter <span class="sc">&lt;=</span> <span class="dv">50</span>) <span class="sc">%&gt;%</span></span>
<span id="cb298-11"><a href="lm.html#cb298-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(iter, parameter, value) <span class="sc">%&gt;%</span></span>
<span id="cb298-12"><a href="lm.html#cb298-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spread</span>(parameter, value) <span class="sc">%&gt;%</span></span>
<span id="cb298-13"><a href="lm.html#cb298-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Intercept, <span class="at">y =</span> sigma_resid, <span class="at">label =</span> iter)) <span class="sc">+</span></span>
<span id="cb298-14"><a href="lm.html#cb298-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>() <span class="sc">+</span></span>
<span id="cb298-15"><a href="lm.html#cb298-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_path</span>(<span class="at">alpha =</span> .<span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb298-16"><a href="lm.html#cb298-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">95</span>, <span class="dv">115</span>) <span class="sc">+</span></span>
<span id="cb298-17"><a href="lm.html#cb298-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">25</span>, <span class="dv">40</span>)</span></code></pre></div>
<div class="figure"><span id="fig:sec99-23"></span>
<img src="Classic_linear_models_files/figure-html/sec99-23-1.png" alt="Left: The sampled posterior distribution of a GMM. Right: 50 iterations of the MCMC random walk." width="50%" /><img src="Classic_linear_models_files/figure-html/sec99-23-2.png" alt="Left: The sampled posterior distribution of a GMM. Right: 50 iterations of the MCMC random walk." width="50%" />
<p class="caption">
Figure 4.1: Left: The sampled posterior distribution of a GMM. Right: 50 iterations of the MCMC random walk.
</p>
</div>
<p>Let’s see how this landscape actually emerged from the random walk.
In the current case, the <em>parameter space</em> is two-dimensional, as we have <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.
The MCMC procedure starts at a deliberate point in parameter space.
At every iteration, the MCMC algorithm attempts a probabilistic jump to another location in parameter space and stores the coordinates.
This jump is called probabilistic for two reasons: first, the new coordinates are selected by a random number generator and second, it is either carried out, or not, and that is probabilistic, too.
If the new target is in a highly likely region, it is carried out with a higher chance.
This sounds circular, but it provenly works.
More specifically, the MCMC sampling approach rests on a general proof, that the emerging frequency distribution converges towards the true posterior distribution.
This property is called <em>ergodicity</em> and it means we can take the <em>relative frequencies</em> of jumps into a certain area of parameter space as an approximation for our degree of belief that the true parameter value is within this region.</p>
<p>The regression object stores the MCMC results as a long series of positions in parameter space.
For any range of interest, it is the relative frequency of visits that represents its certainty.
The first 50 jumps of the MCMC random walk are shown in Figure <a href="lm.html#fig:sec99-23">4.1</a>(right).
Apparently, the random walk is not fully random, as the point cloud is more dense in the center area.
This is where the more probable parameter values lie.
One can clearly see how the MCMC algorithm jumps to more likely areas more frequently.
These areas become more dense and, finally, the cloud of visits will approach the contour density plot above.</p>
<p>The more complex regression models grow, the more dimensions the PD gets.
The linear regression model in the next chapter has three parameter dimensions, which is difficult to visualize.
Multi-level models <a href="mlm.html#mlm">6</a> have hundreds of parameters, which is impossible to intellectually grasp at once.
Therefore, it is common to use the <em>marginal posterior distributions</em> (MPD), which give the density of one coefficient at time.
My preferred geometry for plotting many MPDs is the violin plot, which packs a bunch of densities and therefore can be used when models of many more dimensions.</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="lm.html#cb299-1" aria-hidden="true" tabindex="-1"></a>P_1 <span class="sc">%&gt;%</span></span>
<span id="cb299-2"><a href="lm.html#cb299-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> parameter, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb299-3"><a href="lm.html#cb299-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_violin</span>() <span class="sc">+</span></span>
<span id="cb299-4"><a href="lm.html#cb299-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="cn">NA</span>)</span></code></pre></div>
<div class="figure"><span id="fig:sec99-25"></span>
<img src="Classic_linear_models_files/figure-html/sec99-25-1.png" alt="Violin plots for (marginal) posterior density" width="90%" />
<p class="caption">
Figure 4.2: Violin plots for (marginal) posterior density
</p>
</div>
<p>In Figure <a href="lm.html#fig:sec99-25">4.2</a> we can spot that the most likely value for average time-on-task is <span class="math inline">\(106.14\)</span>.
Both distributions have a certain spread.
With a wider PD, far-off values have been visited by the MCMC chain more frequently.
The probability mass is more evenly distributed and there is less certainty for the parameter to fall in the central region.
In the current case, a risk averse decision maker would maybe take the credibility interval as “reasonably certain.”</p>
<p>Andrew and Jane expect some scepticism from the marketing people, and some lack in statistical skills, too.
What would be the most comprehensible single number to report?
As critical decisions are involved, it seems plausible to report the risk to err: how certain are they that the true value is more than 99 seconds.
We inspect the histograms.
The MPD of the intercept indicates that the average time-on-task is rather unlikely in the range of 99 seconds or better.
But what is the precise probability to err for the 99 seconds statement?
The above summary with <code>coef()</code> does not accurately answer the question.
The CI gives lower and upper limits for a range of 95% certainty in total.
What is needed is the certainty of <span class="math inline">\(\mu \geq 99\)</span>.
Specific questions deserve precise answers.
And once we have understood the MCMC chain as a frequency distribution, the answer is easy: we simply count how many visited values are larger than 99 or 111 (Table <a href="lm.html#tab:sec99-26">4.4</a>).</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="lm.html#cb300-1" aria-hidden="true" tabindex="-1"></a>P_1 <span class="sc">%&gt;%</span></span>
<span id="cb300-2"><a href="lm.html#cb300-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(parameter <span class="sc">==</span> <span class="st">&quot;Intercept&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb300-3"><a href="lm.html#cb300-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb300-4"><a href="lm.html#cb300-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">p_99 =</span> <span class="fu">mean</span>(value <span class="sc">&gt;=</span> <span class="dv">99</span>),</span>
<span id="cb300-5"><a href="lm.html#cb300-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">p_111 =</span> <span class="fu">mean</span>(value <span class="sc">&gt;=</span> <span class="dv">111</span>)</span>
<span id="cb300-6"><a href="lm.html#cb300-6" aria-hidden="true" tabindex="-1"></a>  ) </span></code></pre></div>
<table>
<caption><span id="tab:sec99-26">Table 4.4: </span>Estimating the certainty for average ToT being larger than 99 (111) seconds</caption>
<thead>
<tr class="header">
<th align="right">p_99</th>
<th align="right">p_111</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.985</td>
<td align="right">0.059</td>
</tr>
</tbody>
</table>
<p>It turns out that the certainty for average time-on-task above the 99 is an overwhelming 0.985.
The alternative claim, that average completion time is better than 111 seconds, has a rather moderate risk to err (0.059).</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="likelihood-random-term" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Likelihood and random term</h3>
<p>In formal language, regression models are usually specified by <em>likelihood functions</em> and one or more <em>random terms</em> (exactly one in linear models).
The likelihood represents the common, predictable pattern in the data.
Formally, the likelihood establishes a link between <em>predicted values</em> <span class="math inline">\(\mu_i\)</span> and predictors.
It is common to call predictors with the Greek letter <span class="math inline">\(\beta\)</span> (beta).
If there is more than one predictor, these are marked with subscripts, starting at zero.
The “best guess” is called the <em>expected value</em> and is denoted with <span class="math inline">\(\mu_i\)</span> (“mju i”).
If you just know that the average ToT is 106 seconds and you are asked to guess the performance of the next user arriving in the lab, the reasonable guess is just that, 106 seconds.</p>
<p><span class="math display">\[\mu_i = \beta_0\]</span></p>
<p>Of course, we would never expect this person to use 106 second, exactly.
All observed and imagined observations are more or less clumped around the expected value.
The <em>random term</em> specifies our assumptions on the pattern of randomness.
It is given as distributions (note the plural), denoted by the <span class="math inline">\(\sim\)</span> (tilde) operator, which reads as: “is distributed.”
In the case of linear models, the assumed distribution is always the Normal or <em>Gaussian distribution</em>.
Gaussian distributions have a characteristic bell curve and depend on two parameters: the mean <span class="math inline">\(\mu\)</span> as the central measure and the standard deviation <span class="math inline">\(\sigma\)</span> giving the spread.</p>
<p><span class="math display">\[y_i \sim \textrm{Gaus}(\mu_i, \sigma_{\epsilon})\]</span></p>
<p>The random term specifies how all unknown sources of variation take effect on the measures, and these are manifold.
Randomness can arise due to all kinds of individual differences, situational conditions, and, last but not least, measurement errors.
The Gaussian distribution sometimes is a good approximation for randomness and linear models are routinely used in research.
In several classic statistics books, the following formula is used to describe the GMM (and likewise more complex linear models):</p>
<p><span class="math display">\[
\begin{aligned}
y_i &amp;= \mu_i + \epsilon_i\\
\mu_i &amp;= \beta_0\\
\epsilon_i &amp;\sim \textrm{Gaus}(0, \sigma_\epsilon)
\end{aligned}
\]</span></p>
<p>First, it is to say, that these two formulas are mathematically equivalent.
The primary difference to our formula is that the <em>residuals</em> <span class="math inline">\(\epsilon_i\)</span>, are given separately.
The pattern of residuals is then specified as a single Gaussian distribution.
Residual distributions are a highly useful concept in modelling, as they can be used to check a given model.
Then the the classic formula is more intuitive.
The reason for separating the model into likelihood and random term is that it works in more cases.
When turning to Generalized Linear Models (GLM) in chapter <a href="glm.html#glm">7</a>, we will use other patterns of randomness, that are no longer additive, like in <span class="math inline">\(\mu_i + \epsilon_i\)</span>.
As I consider the use of GLMs an element of professional statistical practice, I use the general formula throughout.</p>
</div>
<div id="posterior-dist" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Working with the posterior distribution</h3>
<p>Coefficient tables are the standard way to report regression models.
They contain all effects (or a selection of interest) in rows.
For every parameter, the central tendency (center, magnitude, location) is given, and a statement of uncertainty, by convention 95% credibility intervals (CI).</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="lm.html#cb302-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Sec99)</span></code></pre></div>
<p>The object <code>M_1</code> is the model object created by <code>stan_glm</code>.
When you call <code>summary</code> you get complex listings that represent different aspects of the estimated model.
These aspects and more are saved inside the object in a hierarchy of lists.
The central result of the estimation is the <em>posterior distribution (HPD)</em>.
With package Rstanarm, the posterior distribution is extracted as follows (Table <a href="lm.html#tab:sec99-27">4.5</a>):</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="lm.html#cb303-1" aria-hidden="true" tabindex="-1"></a>P_1_wide <span class="ot">&lt;-</span></span>
<span id="cb303-2"><a href="lm.html#cb303-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>(M_1) <span class="sc">%&gt;%</span></span>
<span id="cb303-3"><a href="lm.html#cb303-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">Intercept =</span> <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>) <span class="sc">%&gt;%</span></span>
<span id="cb303-4"><a href="lm.html#cb303-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Iter =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb303-5"><a href="lm.html#cb303-5" aria-hidden="true" tabindex="-1"></a>  mascutils<span class="sc">::</span><span class="fu">go_first</span>(Iter)</span>
<span id="cb303-6"><a href="lm.html#cb303-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-7"><a href="lm.html#cb303-7" aria-hidden="true" tabindex="-1"></a>P_1_wide <span class="sc">%&gt;%</span></span>
<span id="cb303-8"><a href="lm.html#cb303-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">8</span>) </span></code></pre></div>
<table>
<caption><span id="tab:sec99-27">Table 4.5: </span>Rstanarm reports posterior samples in a wide format, with one row per iteration (eight shown</caption>
<thead>
<tr class="header">
<th align="right">Iter</th>
<th align="right">Intercept</th>
<th align="right">sigma</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">3439</td>
<td align="right">109.3</td>
<td align="right">32.3</td>
</tr>
<tr class="even">
<td align="right">66</td>
<td align="right">105.9</td>
<td align="right">28.8</td>
</tr>
<tr class="odd">
<td align="right">813</td>
<td align="right">106.8</td>
<td align="right">30.4</td>
</tr>
<tr class="even">
<td align="right">923</td>
<td align="right">108.0</td>
<td align="right">32.9</td>
</tr>
<tr class="odd">
<td align="right">1420</td>
<td align="right">107.8</td>
<td align="right">31.7</td>
</tr>
<tr class="even">
<td align="right">3076</td>
<td align="right">99.2</td>
<td align="right">32.7</td>
</tr>
<tr class="odd">
<td align="right">1436</td>
<td align="right">107.5</td>
<td align="right">30.2</td>
</tr>
<tr class="even">
<td align="right">1991</td>
<td align="right">104.8</td>
<td align="right">31.7</td>
</tr>
</tbody>
</table>
<p>The resulting data frame is actually a matrix, where each of the 4000 rows is one coordinate the MCMC walk has visited in a two-dimensional parameter space <a href="lm.html#random-walk">4.1.1</a>.
For the purpose of reporting parameter estimates, we could create a CLU table as follows (Table <a href="lm.html#tab:sec99-28">4.6</a>):</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="lm.html#cb304-1" aria-hidden="true" tabindex="-1"></a>P_1_wide <span class="sc">%&gt;%</span></span>
<span id="cb304-2"><a href="lm.html#cb304-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb304-3"><a href="lm.html#cb304-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">c_Intercept =</span> <span class="fu">median</span>(Intercept),</span>
<span id="cb304-4"><a href="lm.html#cb304-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">l_Intercept =</span> <span class="fu">quantile</span>(Intercept, .<span class="dv">025</span>),</span>
<span id="cb304-5"><a href="lm.html#cb304-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">u_Intercept =</span> <span class="fu">quantile</span>(Intercept, .<span class="dv">975</span>),</span>
<span id="cb304-6"><a href="lm.html#cb304-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">c_sigma =</span> <span class="fu">median</span>(sigma),</span>
<span id="cb304-7"><a href="lm.html#cb304-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">l_sigma =</span> <span class="fu">quantile</span>(sigma, .<span class="dv">025</span>),</span>
<span id="cb304-8"><a href="lm.html#cb304-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">u_sigma =</span> <span class="fu">quantile</span>(sigma, .<span class="dv">975</span>)</span>
<span id="cb304-9"><a href="lm.html#cb304-9" aria-hidden="true" tabindex="-1"></a>  ) </span></code></pre></div>
<table>
<caption><span id="tab:sec99-28">Table 4.6: </span>A wide CLU table extracted from a wide posterior object</caption>
<thead>
<tr class="header">
<th align="right">c_Intercept</th>
<th align="right">l_Intercept</th>
<th align="right">u_Intercept</th>
<th align="right">c_sigma</th>
<th align="right">l_sigma</th>
<th align="right">u_sigma</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">106</td>
<td align="right">99.7</td>
<td align="right">112</td>
<td align="right">31.4</td>
<td align="right">27.5</td>
<td align="right">36.1</td>
</tr>
</tbody>
</table>
<p>As can be seen, creating coefficient tables from wide posterior objects is awful and repetitive, even when there are just two parameters (some models contain hundreds of parameters).
Additional effort would be needed to get a well structured table.
The package Bayr extracts posterior distributions into a <em>long format</em>.
This works approximately like can be seen in the following code, which employs <code>tidyr::pivot_longer</code> to make the wide Rstanarm posterior long (Table <a href="lm.html#tab:sec99-29">4.7</a>).</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="lm.html#cb305-1" aria-hidden="true" tabindex="-1"></a>P_1_long <span class="ot">&lt;-</span></span>
<span id="cb305-2"><a href="lm.html#cb305-2" aria-hidden="true" tabindex="-1"></a>  P_1_wide <span class="sc">%&gt;%</span></span>
<span id="cb305-3"><a href="lm.html#cb305-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">!</span>Iter, <span class="at">names_to =</span> <span class="st">&quot;parameter&quot;</span>)</span>
<span id="cb305-4"><a href="lm.html#cb305-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb305-5"><a href="lm.html#cb305-5" aria-hidden="true" tabindex="-1"></a>P_1_long <span class="sc">%&gt;%</span></span>
<span id="cb305-6"><a href="lm.html#cb305-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">8</span>) </span></code></pre></div>
<table>
<caption><span id="tab:sec99-29">Table 4.7: </span>A long table for posterior samples stores one value per row (rather than one iteration)</caption>
<thead>
<tr class="header">
<th align="right">Iter</th>
<th align="left">parameter</th>
<th align="right">value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1136</td>
<td align="left">Intercept</td>
<td align="right">103.5</td>
</tr>
<tr class="even">
<td align="right">3726</td>
<td align="left">sigma</td>
<td align="right">29.3</td>
</tr>
<tr class="odd">
<td align="right">823</td>
<td align="left">Intercept</td>
<td align="right">104.2</td>
</tr>
<tr class="even">
<td align="right">2151</td>
<td align="left">Intercept</td>
<td align="right">110.2</td>
</tr>
<tr class="odd">
<td align="right">1581</td>
<td align="left">sigma</td>
<td align="right">28.2</td>
</tr>
<tr class="even">
<td align="right">56</td>
<td align="left">sigma</td>
<td align="right">29.8</td>
</tr>
<tr class="odd">
<td align="right">1669</td>
<td align="left">sigma</td>
<td align="right">32.8</td>
</tr>
<tr class="even">
<td align="right">2401</td>
<td align="left">Intercept</td>
<td align="right">103.9</td>
</tr>
</tbody>
</table>
<p>With long posterior objects, summarizing over the parameters is more straight-forward and produces a long CLU table, such as <a href="lm.html#tab:sec99-30">4.8</a>. In other words: Starting from a long posterior makes for a tidy workflow.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="lm.html#cb306-1" aria-hidden="true" tabindex="-1"></a>P_1_long <span class="sc">%&gt;%</span></span>
<span id="cb306-2"><a href="lm.html#cb306-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(parameter) <span class="sc">%&gt;%</span></span>
<span id="cb306-3"><a href="lm.html#cb306-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb306-4"><a href="lm.html#cb306-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">center =</span> <span class="fu">median</span>(value),</span>
<span id="cb306-5"><a href="lm.html#cb306-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">lower =</span> <span class="fu">quantile</span>(value, .<span class="dv">025</span>),</span>
<span id="cb306-6"><a href="lm.html#cb306-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">upper =</span> <span class="fu">quantile</span>(value, .<span class="dv">975</span>)</span>
<span id="cb306-7"><a href="lm.html#cb306-7" aria-hidden="true" tabindex="-1"></a>  ) </span></code></pre></div>
<table>
<caption><span id="tab:sec99-30">Table 4.8: </span>A long CLU table extracted from a long posterior table.</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">106.0</td>
<td align="right">99.7</td>
<td align="right">112.2</td>
</tr>
<tr class="even">
<td align="left">sigma</td>
<td align="right">31.4</td>
<td align="right">27.5</td>
<td align="right">36.1</td>
</tr>
</tbody>
</table>
<p>With the Bayr package, the <code>posterior</code> command produces such a long posterior object. When called, a Bayr posterior object (class <em>Tbl_post</em>) identifies itself by telling the number of MCMC samples, and the estimates contained in the model, grouped by <em>type of parameter</em> (Table <a href="lm.html#tab:sec99-30a">4.9</a>).</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="lm.html#cb307-1" aria-hidden="true" tabindex="-1"></a>P_1 <span class="ot">&lt;-</span> bayr<span class="sc">::</span><span class="fu">posterior</span>(M_1)</span>
<span id="cb307-2"><a href="lm.html#cb307-2" aria-hidden="true" tabindex="-1"></a>P_1</span></code></pre></div>
<table>
<caption><span id="tab:sec99-30a">Table 4.9: </span>MCMC posterior with 4000 samples of 2 parameters in 1 model(s)</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">parameter</th>
<th align="left">type</th>
<th align="left">fixef</th>
<th align="right">count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">M_1</td>
<td align="left">Intercept</td>
<td align="left">fixef</td>
<td align="left">Intercept</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">M_1</td>
<td align="left">sigma_resid</td>
<td align="left">disp</td>
<td align="left"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>The most important benefit of posterior extraction with Bayr is that parameters are classified. Note how the two parameters <code>Intercept</code> and <code>sigma</code> are assigned different parameter types: fixed effect (which is a population-level coefficient) and dispersion. This classification allows us to filter by type of parameter and produce CLU tables, such as Table <a href="lm.html#tab:sec99-31">4.10</a>.</p>
<!-- #120 -->
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="lm.html#cb308-1" aria-hidden="true" tabindex="-1"></a>P_1 <span class="sc">%&gt;%</span></span>
<span id="cb308-2"><a href="lm.html#cb308-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(type <span class="sc">==</span> <span class="st">&quot;fixef&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb308-3"><a href="lm.html#cb308-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">clu</span>()</span></code></pre></div>
<table>
<caption><span id="tab:sec99-31">Table 4.10: </span>Parameter estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">106</td>
<td align="right">99.7</td>
<td align="right">112</td>
</tr>
</tbody>
</table>
<p>Bayr also provides shortcut commands for extracting parameters of a certain type. The above code is very similar to how the <code>bayr::fixef</code> command is implemented.
Note that <code>coef</code> and <code>fixef</code> can be called on the Rstanarm model object, directly, making it unnecessary to first create a posterior table object.</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="lm.html#cb309-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_1)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-12">Table 4.11: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">type</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">fixef</td>
<td align="left">Intercept</td>
<td align="right">106</td>
<td align="right">99.7</td>
<td align="right">112</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="clu" class="section level3" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> Center and interval estimates</h3>
<p>The authors of Bayesian books and the various regression engines have different opinions on what to use as center statistic in a coefficient table. The best known option are the mean, the median and the mode. The following code produces these statistics and the results are shown in Table <a href="lm.html#tab:clu-1">4.12</a></p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="lm.html#cb311-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Sec99)</span></code></pre></div>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="lm.html#cb312-1" aria-hidden="true" tabindex="-1"></a>T_1 <span class="ot">&lt;-</span></span>
<span id="cb312-2"><a href="lm.html#cb312-2" aria-hidden="true" tabindex="-1"></a>  P_1 <span class="sc">%&gt;%</span></span>
<span id="cb312-3"><a href="lm.html#cb312-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(parameter) <span class="sc">%&gt;%</span></span>
<span id="cb312-4"><a href="lm.html#cb312-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb312-5"><a href="lm.html#cb312-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean =</span> <span class="fu">mean</span>(value),</span>
<span id="cb312-6"><a href="lm.html#cb312-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">median =</span> <span class="fu">median</span>(value),</span>
<span id="cb312-7"><a href="lm.html#cb312-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">mode =</span> mascutils<span class="sc">::</span><span class="fu">mode</span>(value),</span>
<span id="cb312-8"><a href="lm.html#cb312-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">q_025 =</span> <span class="fu">quantile</span>(value, .<span class="dv">025</span>),</span>
<span id="cb312-9"><a href="lm.html#cb312-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">q_975 =</span> <span class="fu">quantile</span>(value, .<span class="dv">975</span>)</span>
<span id="cb312-10"><a href="lm.html#cb312-10" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb312-11"><a href="lm.html#cb312-11" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(T_1, <span class="at">caption =</span> <span class="st">&quot;Various center statistics and 95 percent quantiles&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:clu-1">Table 4.12: </span>Various center statistics and 95 percent quantiles</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="right">mean</th>
<th align="right">median</th>
<th align="right">mode</th>
<th align="right">q_025</th>
<th align="right">q_975</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">106.0</td>
<td align="right">106.0</td>
<td align="right">106</td>
<td align="right">99.7</td>
<td align="right">112.2</td>
</tr>
<tr class="even">
<td align="left">sigma_resid</td>
<td align="right">31.5</td>
<td align="right">31.4</td>
<td align="right">31</td>
<td align="right">27.5</td>
<td align="right">36.1</td>
</tr>
</tbody>
</table>
<p>We observe that for the Intercept it barely matters which center statistic we use, but there are minor differences for the standard error. We investigate this further by producing a plot with the marginal posterior distributions of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> with mean, median and mode (Figure <a href="lm.html#fig:clu-2">4.3</a>).</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="lm.html#cb313-1" aria-hidden="true" tabindex="-1"></a>T_1_long <span class="ot">&lt;-</span></span>
<span id="cb313-2"><a href="lm.html#cb313-2" aria-hidden="true" tabindex="-1"></a>  T_1 <span class="sc">%&gt;%</span></span>
<span id="cb313-3"><a href="lm.html#cb313-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="at">key =</span> center, <span class="at">value =</span> value, <span class="sc">-</span>parameter)</span>
<span id="cb313-4"><a href="lm.html#cb313-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb313-5"><a href="lm.html#cb313-5" aria-hidden="true" tabindex="-1"></a>P_1 <span class="sc">%&gt;%</span></span>
<span id="cb313-6"><a href="lm.html#cb313-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value)) <span class="sc">+</span></span>
<span id="cb313-7"><a href="lm.html#cb313-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>parameter, <span class="at">scales =</span> <span class="st">&quot;free_x&quot;</span>) <span class="sc">+</span></span>
<span id="cb313-8"><a href="lm.html#cb313-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb313-9"><a href="lm.html#cb313-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(</span>
<span id="cb313-10"><a href="lm.html#cb313-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">xintercept =</span> value,</span>
<span id="cb313-11"><a href="lm.html#cb313-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">col =</span> center</span>
<span id="cb313-12"><a href="lm.html#cb313-12" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb313-13"><a href="lm.html#cb313-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> T_1_long</span>
<span id="cb313-14"><a href="lm.html#cb313-14" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="figure"><span id="fig:clu-2"></span>
<img src="Classic_linear_models_files/figure-html/clu-2-1.png" alt="Comparing mean, median and mode of marginal posterior distributions" width="90%" />
<p class="caption">
Figure 4.3: Comparing mean, median and mode of marginal posterior distributions
</p>
</div>
<p>This example demonstrates how the long format posterior works together with the GGplot graphics engine.
A density plot very accurately renders how certainty is distributed over the range of a parameter.
In order to produce vertical lines for point estimate and limits, we first make the summary table long, with one value per row.
This is not how we would usually like to read it, but it is very efficient for adding to the plot.</p>
<p>When inspecting the two distributions, it appears that the distribution of Intercept is completely symmetric.
For the standard error, in contrast, we note a slight left skewness.
This is rather typical for dispersion parameters, as these have a lower boundary.
The closer the distribution sits to the boundary, the steeper becomes the left tail.</p>
<p>A disadvantage of the <em>mean</em> is that it may change under monotonic transformations.
A monotonic transformations is a recoding of a variable <span class="math inline">\(x_1\)</span> into a new variable <span class="math inline">\(x_2\)</span> by a transformation function <span class="math inline">\(\phi\)</span> (<span class="math inline">\(phi\)</span>) such that the order of values stays untouched.
Examples of monotonic functions are the logarithm (<span class="math inline">\(x_2 = \log(x_1)\)</span>), the exponential function (<span class="math inline">\(x_2 = \exp(x_1)\)</span>), or simply <span class="math inline">\(x_2 = x_1 + 1\)</span>.
A counter example is the quadratic function <span class="math inline">\(x_2 = x_1^2\)</span>.
In data analysis monotonous transformations are used a lot.
Especially Generalized Linear Models make use of monotonous link functions to establish linearity <a href="glm.html#relinking-linearity">7.1.1</a>.
Furthermore, the mean can also be highly influenced by outliers.</p>
<p>The <em>mode</em> of a distribution is its point of highest density.
It is invariant under monotonic transformations.
It also has a rather intuitive meaning as the most likely value for the true parameter.
Next to that, the mode is compatible with classic maximum likelihood estimation.
When a Bayesian takes a pass on any prior information, the posterior mode should precisely match the results of a classic regression engine (e.g. <code>glm</code>).
The main disadvantage of the mode is that it has to be estimated by one of several heuristic algorithms.
These add some computing time and may fail when the posterior distribution is bi-modal.
However, when that happens, you probably have a more deeply rooted problem, than just deciding on a suitable summary statistic.</p>
<p>The <em>median</em> of a distribution marks the point where half the values are below and the other half are equal or above.
Technically, the median is just the 50% quantile of the distribution.
The median is extremely easy and reliable to compute, and it shares the invariance of monotonous transformations.
This is easy to conceive: The median is computed by ordering all values in a row and then picking the value that is exactly in the middle.
Obviously, this value only changes if the order changes, i.e. a non-monotonous function was applied.
For these advantages, I prefer using the median as center estimates.
Researchers who desire a different center estimate can easily write their own <code>clu</code>.
<!-- #81 --></p>
<p>In this book, <em>2.5% and 97.5% certainty quantiles</em> are routinely used to form <em>95% credibility intervals (CI)</em>.
There is nothing special about these intervals, they are just conventions, Again, another method exists to obtain CIs.
Some authors prefer to report the <em>highest posterior density interval (HPD)</em>, which is the narrowest interval that contains 95% of the probability mass.
While this is intriguing to some extent, HPDs are not invariant to monotonic transformations, either.</p>
<!-- #49 -->
<p>So, the parameter extraction commands used here give the median and the 2.5% and 97.5% limits. The three parameters have in common that they are quantiles, which are handled by Rs <code>quantile</code> command.
To demystify the <code>clu</code>, here is how you can make a basic coefficient table yourself Table <a href="lm.html#tab:clu-3">4.13</a>:</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="lm.html#cb314-1" aria-hidden="true" tabindex="-1"></a>P_1 <span class="sc">%&gt;%</span></span>
<span id="cb314-2"><a href="lm.html#cb314-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(parameter) <span class="sc">%&gt;%</span></span>
<span id="cb314-3"><a href="lm.html#cb314-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb314-4"><a href="lm.html#cb314-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">center =</span> <span class="fu">quantile</span>(value, <span class="fl">0.5</span>),</span>
<span id="cb314-5"><a href="lm.html#cb314-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">lower =</span> <span class="fu">quantile</span>(value, <span class="fl">0.025</span>),</span>
<span id="cb314-6"><a href="lm.html#cb314-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">upper =</span> <span class="fu">quantile</span>(value, <span class="fl">0.975</span>)</span>
<span id="cb314-7"><a href="lm.html#cb314-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb314-8"><a href="lm.html#cb314-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() </span></code></pre></div>
<table>
<caption><span id="tab:clu-3">Table 4.13: </span>Posterior medians and 95 percent credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="right">106.0</td>
<td align="right">99.7</td>
<td align="right">112.2</td>
</tr>
<tr class="even">
<td align="left">sigma_resid</td>
<td align="right">31.4</td>
<td align="right">27.5</td>
<td align="right">36.1</td>
</tr>
</tbody>
</table>
<p>Note that the posterior contains samples of the dispersion parameter <span class="math inline">\(\sigma\)</span>, too, which means we can get CIs for it. Classic regression engines don’t yield any measures of certainty on dispersion parameters. In classic analyses <span class="math inline">\(\sigma\)</span> is often denounced as a nuisance parameter and would not be used for inference. I believe that measuring and understanding sources of variation is crucial for design research and several of the examples that follow try to build this case, especially Sections <a href="mlm.html#non-human-populations">6.5</a> and <a href="glm.html#distributional-models">7.5</a>. Therefore, the capability of reporting uncertainty on all parameters, not just coefficients, is a sur-plus of Bayesian estimation.</p>
</div>
</div>
<div id="lrm" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Walk the line: linear regression</h2>
<p>In the previous section we have introduced the most basic of all regression models: the grand mean model.
It assigns rather coarse predictions, without any real predictors.
Routinely, design researchers desire to predict performance based on <em>metric variables</em>, such as:</p>
<ul>
<li>previous experience</li>
<li>age</li>
<li>font size</li>
<li>intelligence level and other innate abilities</li>
<li>level of self efficiacy, neuroticism or other traits</li>
<li>number of social media contacts</li>
</ul>
<p>To carry out such a research question, the variable of interest needs to be measured next to the outcome variable.
And, the variable must vary.
You cannot examine the effects of age or font size on reading performance, when all participants are of same age and you test only one size.
Then, for specifying the model, the researcher has to come up with an expectation of how the two are related.
Theoretically, that can be any mathematical function, but practically, a <em>linear function</em> is often presumed.
Figure <a href="lm.html#fig:walk-1">4.4</a> shows a variety of linear relations between two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="lm.html#cb315-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb315-2"><a href="lm.html#cb315-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">intercept =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb315-3"><a href="lm.html#cb315-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">slope =</span> <span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">5</span>, <span class="dv">0</span>, <span class="fl">1.5</span>),</span>
<span id="cb315-4"><a href="lm.html#cb315-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="sc">-</span><span class="dv">3</span><span class="sc">:</span><span class="dv">3</span></span>
<span id="cb315-5"><a href="lm.html#cb315-5" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb315-6"><a href="lm.html#cb315-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(x) <span class="sc">%&gt;%</span></span>
<span id="cb315-7"><a href="lm.html#cb315-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb315-8"><a href="lm.html#cb315-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> intercept <span class="sc">+</span> x <span class="sc">*</span> slope,</span>
<span id="cb315-9"><a href="lm.html#cb315-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">slope =</span> <span class="fu">as.factor</span>(slope),</span>
<span id="cb315-10"><a href="lm.html#cb315-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">intercept =</span> <span class="fu">as.factor</span>(intercept)</span>
<span id="cb315-11"><a href="lm.html#cb315-11" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb315-12"><a href="lm.html#cb315-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> slope)) <span class="sc">+</span></span>
<span id="cb315-13"><a href="lm.html#cb315-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb315-14"><a href="lm.html#cb315-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span>intercept)</span></code></pre></div>
<div class="figure"><span id="fig:walk-1"></span>
<img src="Classic_linear_models_files/figure-html/walk-1-1.png" alt="Linear terms differing by intercepts and slopes" width="90%" />
<p class="caption">
Figure 4.4: Linear terms differing by intercepts and slopes
</p>
</div>
<p>A linear function is a straight line, which is specified by two parameters: <em>intercept</em> <span class="math inline">\(\beta_0\)</span> and <em>slope</em> <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[
f(x_1) = \beta_0 + \beta_1x_{1i}
\]</span></p>
<p>The intercept is <em>“the point where a function graph crosses the x-axis”</em>, or more formally:</p>
<p><span class="math display">\[
f(x_1 = 0) = \beta_0
\]</span></p>
<p>The second parameter, <span class="math inline">\(\beta_1\)</span> is called the <em>slope</em>.
The slope determines the steepness of the line.
When the slope is <span class="math inline">\(.5\)</span>, the line will rise up by .5 on Y, when moving one step to the right on X.</p>
<p><span class="math display">\[
f(x_1 + 1) = \beta_0 + \beta_1x_{1i} + \beta_1
\]</span></p>
<p>There is also the possibility that the slope is zero.
In such a case, the predictor has no effect and can be left out.
Setting <span class="math inline">\(\beta_1 = 0\)</span> produces a horizontal line, with <span class="math inline">\(y_i\)</span> being constant over the whole range.
This shows that the GMM is a special case of LRMs, where the slope is fixed to zero, hence <span class="math inline">\(\mu_i = \beta_0\)</span>.</p>
<p>Linear regression gives us the opportunity to discover how ToT can be predicted by age (<span class="math inline">\(x_1\)</span>) in the BrowsingAB case.
In this hypothetical experiment, two designs A and B are compared, but we ignore this for now.
Instead we ask: are older people slower when using the internet?
Or: is there a linear relationship between age and ToT?
The structural term is: <!-- #49 --></p>
<p><span class="math display">\[
\mu_i = \beta_0 + \beta_1\textrm{age}_{i}
\]</span></p>
<p>This literally means: with every year of age, ToT increases by <span class="math inline">\(\beta_1\)</span> seconds.
Before we run a linear regression with <code>stan_glm</code>, we visually explore the association between age and ToT using a scatter plot.
The blue line in the graph is a so called a <em>smoother</em>, more specifically a LOESS.
A smoother is an estimated line, just as linear function.
But, it is way more flexible.
Where the linear function is a straight stick fixed at a pivotal point, LOESS is more like a pipe cleaner.
here, LOESS shows a more detailed picture of the relation between age and ToT.
There is a rise between 20 and 40, followed by a stable plateau, and another rise starting at 60.
Actually, that does not look like a straight line, but at least there is steady upwards trend (Figure <a href="lm.html#fig:bab-1">4.5</a>).</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="lm.html#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(BrowsingAB)</span></code></pre></div>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="lm.html#cb317-1" aria-hidden="true" tabindex="-1"></a>BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb317-2"><a href="lm.html#cb317-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> age, <span class="at">y =</span> ToT)) <span class="sc">+</span></span>
<span id="cb317-3"><a href="lm.html#cb317-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb317-4"><a href="lm.html#cb317-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">se =</span> F, <span class="at">fullrange =</span> F)</span></code></pre></div>
<div class="figure"><span id="fig:bab-1"></span>
<img src="Classic_linear_models_files/figure-html/bab-1-1.png" alt="Using a scatterplot and smoother to check for linear trends" width="90%" />
<p class="caption">
Figure 4.5: Using a scatterplot and smoother to check for linear trends
</p>
</div>
<p>In fact, the BrowsingAB simulation contains what one could call a psychological model.
The effect of age is partly due to farsightedness of participants (making them slower at reading), which more or less suddenly kicks in at a certain range of age.
Still, we make do with a rough linear approximation.
To estimate the model, we use the <code>stan_glm</code> command in much the same way as before, but add the predictor age.
The command will internally check the data type of your variable, which is metric in this case.
Therefore, it is treated as a <em>metric predictor</em> (sometimes also called covariate) <!-- #51-->.</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="lm.html#cb318-1" aria-hidden="true" tabindex="-1"></a>M_age <span class="ot">&lt;-</span></span>
<span id="cb318-2"><a href="lm.html#cb318-2" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb318-3"><a href="lm.html#cb318-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> age,</span>
<span id="cb318-4"><a href="lm.html#cb318-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb318-5"><a href="lm.html#cb318-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="lm.html#cb319-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(T_age)</span></code></pre></div>
<table>
<caption><span id="tab:BAB-age">Table 4.14: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">57.17</td>
<td align="right">34.053</td>
<td align="right">79.53</td>
</tr>
<tr class="even">
<td align="left">age</td>
<td align="left">age</td>
<td align="right">0.82</td>
<td align="right">0.401</td>
<td align="right">1.24</td>
</tr>
</tbody>
</table>
<p>Is age associated with ToT?
Table <a href="lm.html#tab:BAB-age">4.14</a> tells us that with every year of age, users get <span class="math inline">\(0.82\)</span> seconds slower, which is considerable.
It also tells us that the predicted performance at age = 0 is <span class="math inline">\(57.17\)</span>.</p>
<!-- #123 -->
<div id="transform-measures" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Transforming measures</h3>
<p>In the above model, the intercept represents the predicted ToT at <code>age == 0</code>, of a newborn.
We would never seriously put that forward in a stakeholder presentation, trying to prove that babies benefit from the redesign of a public website, would we?
The prediction is bizarre because we intuitively understand that there is a discontinuity up the road, which is the moment where a teenager starts using public websites.
We also realize that over the whole life span of a typical web user, say 12 years to 90 years, age actually is a proxy variable for two distinct processes: the rapid build-up of intellectual skills from childhood to young adulthood and the slow decline of cognitive performance, which starts approximately, when the first of us get age-related far-sightedness.
Generally, with linear models, one should avoid making statements about a range that has not been observed.
Linearity, as we will see in <a href="glm.html#relinking-linearity">7.1.1</a>, always is just an approximation for a process that truly is non-linear.</p>
<p>Placing the intercept where there is no data has another consequence: the estimate is rather uncertain, with a wide 95% CI, <span class="math inline">\(57.17 [34.05, 79.54]_{CI95}\)</span>.
As a metaphor, think of the data as a hand that holds the a stick, the regression line and tries to push a light switch.
The longer the stick, the more difficult is becomes to hit the target.</p>
<div id="shift-center" class="section level4" number="4.2.1.1">
<h4><span class="header-section-number">4.2.1.1</span> Shifting an centering</h4>
<p><em>Shifting the predictor</em> is a pragmatic solution to the problem: “Shifting” means that the age predictor is moved to the right or the left, such that point zero is in a region populated with observations.
In this case, two options seem to make sense: either, the intercept is in the region of youngest participants, or it is the sample average, which is then called <em>centering</em>.
To shift a variable, just subtract the amount of units (years) where you want the intercept to be.
The following code produces a shift of -20 and a centering on the original variable age:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="lm.html#cb320-1" aria-hidden="true" tabindex="-1"></a>BAB1 <span class="ot">&lt;-</span></span>
<span id="cb320-2"><a href="lm.html#cb320-2" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb320-3"><a href="lm.html#cb320-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb320-4"><a href="lm.html#cb320-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">age_shft =</span> age <span class="sc">-</span> <span class="dv">20</span>,</span>
<span id="cb320-5"><a href="lm.html#cb320-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">age_cntr =</span> age <span class="sc">-</span> <span class="fu">mean</span>(age)</span>
<span id="cb320-6"><a href="lm.html#cb320-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb320-7"><a href="lm.html#cb320-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-8"><a href="lm.html#cb320-8" aria-hidden="true" tabindex="-1"></a>BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb320-9"><a href="lm.html#cb320-9" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">gather</span>(<span class="st">&quot;predictor&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="fu">starts_with</span>(<span class="st">&quot;age&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb320-10"><a href="lm.html#cb320-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> age, <span class="at">y =</span> ToT)) <span class="sc">+</span></span>
<span id="cb320-11"><a href="lm.html#cb320-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(predictor <span class="sc">~</span> .) <span class="sc">+</span></span>
<span id="cb320-12"><a href="lm.html#cb320-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb320-13"><a href="lm.html#cb320-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">se =</span> F, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">fullrange =</span> T)</span></code></pre></div>
<div class="figure"><span id="fig:shift-1"></span>
<img src="Classic_linear_models_files/figure-html/shift-1-1.png" alt="Shifting and centering of variable Age" width="90%" />
<p class="caption">
Figure 4.6: Shifting and centering of variable Age
</p>
</div>
<p>By shifting the age variable, the whole data cloud is moved to the left (Figure <a href="lm.html#fig:shift-1">4.6</a>).
To see what happens on the inferential level, we repeat the LRM estimation with the two shifted variables:</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="lm.html#cb321-1" aria-hidden="true" tabindex="-1"></a>M_age_shft <span class="ot">&lt;-</span></span>
<span id="cb321-2"><a href="lm.html#cb321-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> age_shft, <span class="at">data =</span> BAB1)</span>
<span id="cb321-3"><a href="lm.html#cb321-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-4"><a href="lm.html#cb321-4" aria-hidden="true" tabindex="-1"></a>M_age_cntr <span class="ot">&lt;-</span></span>
<span id="cb321-5"><a href="lm.html#cb321-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> age_cntr, <span class="at">data =</span> BAB1)</span></code></pre></div>
<p>We combine the posterior distributions into one multi-model posterior and read the <em>multi-model coefficient table</em>:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="lm.html#cb322-1" aria-hidden="true" tabindex="-1"></a>P_age <span class="ot">&lt;-</span></span>
<span id="cb322-2"><a href="lm.html#cb322-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(</span>
<span id="cb322-3"><a href="lm.html#cb322-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">posterior</span>(M_age),</span>
<span id="cb322-4"><a href="lm.html#cb322-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">posterior</span>(M_age_shft),</span>
<span id="cb322-5"><a href="lm.html#cb322-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">posterior</span>(M_age_cntr)</span>
<span id="cb322-6"><a href="lm.html#cb322-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb322-7"><a href="lm.html#cb322-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb322-8"><a href="lm.html#cb322-8" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(P_age)</span></code></pre></div>
<table>
<caption><span id="tab:shift-2">Table 4.15: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">M_age</td>
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">57.17</td>
<td align="right">34.053</td>
<td align="right">79.53</td>
</tr>
<tr class="even">
<td align="left">M_age</td>
<td align="left">age</td>
<td align="left">age</td>
<td align="right">0.82</td>
<td align="right">0.401</td>
<td align="right">1.24</td>
</tr>
<tr class="odd">
<td align="left">M_age_cntr</td>
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">98.04</td>
<td align="right">91.521</td>
<td align="right">104.21</td>
</tr>
<tr class="even">
<td align="left">M_age_cntr</td>
<td align="left">age_cntr</td>
<td align="left">age_cntr</td>
<td align="right">0.82</td>
<td align="right">0.395</td>
<td align="right">1.24</td>
</tr>
<tr class="odd">
<td align="left">M_age_shft</td>
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">73.20</td>
<td align="right">59.117</td>
<td align="right">87.75</td>
</tr>
<tr class="even">
<td align="left">M_age_shft</td>
<td align="left">age_shft</td>
<td align="left">age_shft</td>
<td align="right">0.82</td>
<td align="right">0.395</td>
<td align="right">1.23</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<pre><code>## [1] &quot;BAB1&quot;</code></pre>
<p>When comparing the regression results the shifted intercepts have moved to higher values, as expected.
Surprisingly, the simple shift is not exactly 20 years.
This is due to the high uncertainty of the first model, as well as the relation not being exactly linear (see Figure XY).
The shifted age predictor has a slightly better uncertainty, but not by much.
This is, because the region around the lowest age is only scarcely populated with data.
Centering, on the other hand, results in a highly certain estimate, due to the dence data.
The slope parameter, however, practically does not change, neither in magnitude nor in certainty.</p>
<p>Shift (and centering) move the scale of measurement and make sure that the intercept falls close (or within) the cluster of observations.
Shifting does not change the unit size, which is still in years.
For truly metric predictors, changing the unit is not desirable, as the unit of measurement is natural and intuitive.</p>
</div>
<div id="rescale" class="section level4" number="4.2.1.2">
<h4><span class="header-section-number">4.2.1.2</span> Rescaling</h4>
<p>Most rating scales are not natural units of measure.
Most of the time it is not meaningful to say: “the user experience rating improved by one.”
The problem has two roots, as I will illustrate by the following four rating scale items:</p>
<p>This product is …</p>
<ol style="list-style-type: decimal">
<li><code>easy to use |1 ... X ... 3 ... 4 ... 5 ... 6 ... 7| difficult to use</code></li>
<li><code>heavenly    |-----X-------------------------------| hellish</code></li>
<li><code>neutral     |1    ...    2    ...    3    ...    4| uncanny</code></li>
</ol>
<p>If you would employ these three scales to assess one and the same product. Using a simulator for rating scale data from package Mascutils (Table <a href="lm.html#tab:rescale-1">4.16</a>), the data acquired these three rating scales could look like (Figure <a href="lm.html#fig:rescale-3">4.7</a>)</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="lm.html#cb325-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mascutils)</span>
<span id="cb325-2"><a href="lm.html#cb325-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb325-3"><a href="lm.html#cb325-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb325-4"><a href="lm.html#cb325-4" aria-hidden="true" tabindex="-1"></a>Raw_ratings <span class="ot">&lt;-</span></span>
<span id="cb325-5"><a href="lm.html#cb325-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb325-6"><a href="lm.html#cb325-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">Part =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,</span>
<span id="cb325-7"><a href="lm.html#cb325-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">easy_difficult =</span> <span class="fu">rrating_scale</span>(<span class="dv">100</span>, <span class="dv">0</span>, .<span class="dv">5</span>,</span>
<span id="cb325-8"><a href="lm.html#cb325-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">ends =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">7</span>)</span>
<span id="cb325-9"><a href="lm.html#cb325-9" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb325-10"><a href="lm.html#cb325-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">heavenly_hellish =</span> <span class="fu">rrating_scale</span>(<span class="dv">100</span>, <span class="dv">0</span>, .<span class="dv">2</span>,</span>
<span id="cb325-11"><a href="lm.html#cb325-11" aria-hidden="true" tabindex="-1"></a>      <span class="at">ends =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb325-12"><a href="lm.html#cb325-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">bin =</span> F</span>
<span id="cb325-13"><a href="lm.html#cb325-13" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb325-14"><a href="lm.html#cb325-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">neutral_uncanny =</span> <span class="fu">rrating_scale</span>(<span class="dv">100</span>, <span class="sc">-</span>.<span class="dv">5</span>, .<span class="dv">5</span>,</span>
<span id="cb325-15"><a href="lm.html#cb325-15" aria-hidden="true" tabindex="-1"></a>      <span class="at">ends =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>)</span>
<span id="cb325-16"><a href="lm.html#cb325-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb325-17"><a href="lm.html#cb325-17" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb325-18"><a href="lm.html#cb325-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tbl_obs</span>()</span>
<span id="cb325-19"><a href="lm.html#cb325-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb325-20"><a href="lm.html#cb325-20" aria-hidden="true" tabindex="-1"></a>Raw_ratings</span></code></pre></div>
<table>
<caption><span id="tab:rescale-1">Table 4.16: </span>Data set with 5 variables, showing 8 of 100 observations.</caption>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="right">Part</th>
<th align="right">easy_difficult</th>
<th align="right">heavenly_hellish</th>
<th align="right">neutral_uncanny</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">8</td>
<td align="right">8</td>
<td align="right">4</td>
<td align="right">4.94</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">24</td>
<td align="right">24</td>
<td align="right">5</td>
<td align="right">4.50</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">25</td>
<td align="right">25</td>
<td align="right">6</td>
<td align="right">5.00</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="right">70</td>
<td align="right">70</td>
<td align="right">5</td>
<td align="right">5.45</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">74</td>
<td align="right">74</td>
<td align="right">3</td>
<td align="right">5.84</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="right">80</td>
<td align="right">80</td>
<td align="right">3</td>
<td align="right">5.00</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">84</td>
<td align="right">84</td>
<td align="right">4</td>
<td align="right">5.15</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="right">91</td>
<td align="right">91</td>
<td align="right">5</td>
<td align="right">5.10</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p>In the following, we are comparing the results of these three items.
However, data came in the wide format, as you would use to create a correlation table.
For a tidy analysis, we first make the data set long.
Ratings are now classified by the item they came from (Table <a href="lm.html#tab:rescale-2">4.17</a>).
From this we can produce a grid histogram (Figure <a href="lm.html#fig:rescale-3">4.7</a>).</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="lm.html#cb326-1" aria-hidden="true" tabindex="-1"></a>D_ratings <span class="ot">&lt;-</span></span>
<span id="cb326-2"><a href="lm.html#cb326-2" aria-hidden="true" tabindex="-1"></a>  Raw_ratings <span class="sc">%&gt;%</span></span>
<span id="cb326-3"><a href="lm.html#cb326-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>Obs) <span class="sc">%&gt;%</span></span>
<span id="cb326-4"><a href="lm.html#cb326-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">!</span>Part, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">names_to =</span> <span class="st">&quot;Item&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;rating&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb326-5"><a href="lm.html#cb326-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tbl_obs</span>()</span>
<span id="cb326-6"><a href="lm.html#cb326-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb326-7"><a href="lm.html#cb326-7" aria-hidden="true" tabindex="-1"></a>D_ratings</span></code></pre></div>
<table>
<caption><span id="tab:rescale-2">Table 4.17: </span>Data set with 4 variables, showing 8 of 300 observations.</caption>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="right">Part</th>
<th align="left">Item</th>
<th align="right">rating</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="left">neutral_uncanny</td>
<td align="right">1.00</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">4</td>
<td align="left">easy_difficult</td>
<td align="right">5.00</td>
</tr>
<tr class="odd">
<td align="right">16</td>
<td align="right">6</td>
<td align="left">easy_difficult</td>
<td align="right">4.00</td>
</tr>
<tr class="even">
<td align="right">149</td>
<td align="right">50</td>
<td align="left">heavenly_hellish</td>
<td align="right">4.47</td>
</tr>
<tr class="odd">
<td align="right">195</td>
<td align="right">65</td>
<td align="left">neutral_uncanny</td>
<td align="right">2.00</td>
</tr>
<tr class="even">
<td align="right">224</td>
<td align="right">75</td>
<td align="left">heavenly_hellish</td>
<td align="right">5.43</td>
</tr>
<tr class="odd">
<td align="right">231</td>
<td align="right">77</td>
<td align="left">neutral_uncanny</td>
<td align="right">3.00</td>
</tr>
<tr class="even">
<td align="right">243</td>
<td align="right">81</td>
<td align="left">neutral_uncanny</td>
<td align="right">3.00</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="lm.html#cb327-1" aria-hidden="true" tabindex="-1"></a>D_ratings <span class="sc">%&gt;%</span></span>
<span id="cb327-2"><a href="lm.html#cb327-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> rating)) <span class="sc">+</span></span>
<span id="cb327-3"><a href="lm.html#cb327-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(Item <span class="sc">~</span> .) <span class="sc">+</span></span>
<span id="cb327-4"><a href="lm.html#cb327-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb327-5"><a href="lm.html#cb327-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">10</span>)</span></code></pre></div>
<div class="figure"><span id="fig:rescale-3"></span>
<img src="Classic_linear_models_files/figure-html/rescale-3-1.png" alt="Distribution of three rating scale items" width="90%" />
<p class="caption">
Figure 4.7: Distribution of three rating scale items
</p>
</div>
<p>The first problem is that rating scales have been designed with different end points.
The first step when using different rating scales is shifting the left-end point to zero and dividing by the range of the measure (<code>upper - lower</code> boundary).
That brings all items down to the range between zero and one.
Note how the following tidy code joins <code>D_ratings</code> with a table <code>D_Items</code>. That adds the lower and upper boundaries for every observation, from which we can standardize the range (Figure <a href="lm.html#fig:rescale-4">4.8</a>).</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="lm.html#cb328-1" aria-hidden="true" tabindex="-1"></a>D_Items <span class="ot">&lt;-</span> <span class="fu">tribble</span>(</span>
<span id="cb328-2"><a href="lm.html#cb328-2" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>Item, <span class="sc">~</span>lower, <span class="sc">~</span>upper,</span>
<span id="cb328-3"><a href="lm.html#cb328-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;easy_difficult&quot;</span>, <span class="dv">1</span>, <span class="dv">7</span>,</span>
<span id="cb328-4"><a href="lm.html#cb328-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;heavenly_hellish&quot;</span>, <span class="dv">0</span>, <span class="dv">10</span>,</span>
<span id="cb328-5"><a href="lm.html#cb328-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;neutral_uncanny&quot;</span>, <span class="dv">1</span>, <span class="dv">5</span></span>
<span id="cb328-6"><a href="lm.html#cb328-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb328-7"><a href="lm.html#cb328-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb328-8"><a href="lm.html#cb328-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb328-9"><a href="lm.html#cb328-9" aria-hidden="true" tabindex="-1"></a>D_standard <span class="ot">&lt;-</span></span>
<span id="cb328-10"><a href="lm.html#cb328-10" aria-hidden="true" tabindex="-1"></a>  D_ratings <span class="sc">%&gt;%</span></span>
<span id="cb328-11"><a href="lm.html#cb328-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(D_Items, <span class="at">by =</span> <span class="st">&quot;Item&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb328-12"><a href="lm.html#cb328-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">scaled =</span> (rating <span class="sc">-</span> lower) <span class="sc">/</span> (upper <span class="sc">-</span> lower))</span>
<span id="cb328-13"><a href="lm.html#cb328-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb328-14"><a href="lm.html#cb328-14" aria-hidden="true" tabindex="-1"></a>D_standard <span class="sc">%&gt;%</span></span>
<span id="cb328-15"><a href="lm.html#cb328-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> scaled)) <span class="sc">+</span></span>
<span id="cb328-16"><a href="lm.html#cb328-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(Item <span class="sc">~</span> .) <span class="sc">+</span></span>
<span id="cb328-17"><a href="lm.html#cb328-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">20</span>) <span class="sc">+</span></span>
<span id="cb328-18"><a href="lm.html#cb328-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span id="fig:rescale-4"></span>
<img src="Classic_linear_models_files/figure-html/rescale-4-1.png" alt="Distribution of three rating scale items with standardized boundaries" width="90%" />
<p class="caption">
Figure 4.8: Distribution of three rating scale items with standardized boundaries
</p>
</div>
<p>This partly corrects the horizontal shift between scales.
However, the ratings on the third item still are shifted relative to the other two.
The reason is that the first two items have the neutral zone right in the center, whereas the third item is neutral at its left-end point. That is called bipolar and monopolar items.
The second inconsistency is that the second item uses rather extreme anchors (end point labels, which produces a tight accumulation in the center of the range. You could say that, on a cosmic scale people agree. The three scales have been rescaled by their <em>nominal range</em>, but they differ in their observed variance.</p>
<p>By <em>z-transformation</em> a measure is shifted, not by its nominal boundaries, but <em>observed standard deviation</em>.
A set of measures is z-transformed by centering it and scaling it by its own standard deviation.</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="lm.html#cb329-1" aria-hidden="true" tabindex="-1"></a>D_ratings <span class="sc">%&gt;%</span></span>
<span id="cb329-2"><a href="lm.html#cb329-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Item) <span class="sc">%&gt;%</span></span>
<span id="cb329-3"><a href="lm.html#cb329-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">zrating =</span> (rating <span class="sc">-</span> <span class="fu">mean</span>(rating)) <span class="sc">/</span> <span class="fu">sd</span>(rating)) <span class="sc">%&gt;%</span></span>
<span id="cb329-4"><a href="lm.html#cb329-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb329-5"><a href="lm.html#cb329-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> zrating)) <span class="sc">+</span></span>
<span id="cb329-6"><a href="lm.html#cb329-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(Item <span class="sc">~</span> .) <span class="sc">+</span></span>
<span id="cb329-7"><a href="lm.html#cb329-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">10</span>)</span></code></pre></div>
<div class="figure"><span id="fig:rescale-5"></span>
<img src="Classic_linear_models_files/figure-html/rescale-5-1.png" alt="Z-transformation removes differences in location and dispersion" width="90%" />
<p class="caption">
Figure 4.9: Z-transformation removes differences in location and dispersion
</p>
</div>
<p>By z-transformation, the three scales now exhibit the same mean location and the same dispersion Table <a href="lm.html#fig:rescale-5">4.9</a>.
This could be used to combine them into one general score.
Note however, that information is lost by this process, namely the differences in location or dispersion.
If the research question is highly detailed, such as “Is the design consistently rated low on uncanniness?” this can no longer be answered from the z-transformed variable.</p>
<p>Finally, sometimes researchers use <em>logarithmic transformation</em> of outcome measures to reduce what they perceive as pathologies of tha data.
In particular, many outcome variables do not follow a Normal distribution, as the random term of linear models assumes, but are left-skewed.
Log-transformation often mitigates such problems.
However, as we will see in chapter <a href="glm.html#glm">7</a>, linear models can be estimated gracefully with a random component that precisely matches the data as it comes.
The following time-on-task data is from the IPump study, where nurses have tested two infusion pump interfaces. The original ToT data is strongly left-skewed, which can be mitigated by log-transformation (Figure <a href="lm.html#fig:rescale-6">4.10</a>).</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="lm.html#cb330-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(IPump)</span></code></pre></div>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="lm.html#cb331-1" aria-hidden="true" tabindex="-1"></a>D_pumps <span class="sc">%&gt;%</span></span>
<span id="cb331-2"><a href="lm.html#cb331-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">log_ToT =</span> <span class="fu">log</span>(ToT)) <span class="sc">%&gt;%</span></span>
<span id="cb331-3"><a href="lm.html#cb331-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Design, ToT, log_ToT) <span class="sc">%&gt;%</span></span>
<span id="cb331-4"><a href="lm.html#cb331-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="at">key =</span> Measure, <span class="at">value =</span> value, <span class="sc">-</span>Design) <span class="sc">%&gt;%</span></span>
<span id="cb331-5"><a href="lm.html#cb331-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">color =</span> Design)) <span class="sc">+</span></span>
<span id="cb331-6"><a href="lm.html#cb331-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(Measure <span class="sc">~</span> ., <span class="at">scale =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb331-7"><a href="lm.html#cb331-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>()</span></code></pre></div>
<div class="figure"><span id="fig:rescale-6"></span>
<img src="Classic_linear_models_files/figure-html/rescale-6-1.png" alt="Log transformation cam be used to bend highly left skewed distributions into a more symmetric shape" width="90%" />
<p class="caption">
Figure 4.10: Log transformation cam be used to bend highly left skewed distributions into a more symmetric shape
</p>
</div>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>Frequently, it is count measures and temporal measures to exhibit non-symmetric error distributions.
By log transformation one often arrives at a reasonably Gaussian distributed error.
However, the natural unit of te measure (seconds) gets lost by the transformation, making it very difficult to report the results in a quantitative manner.</p>
<!-- #53 -->
<!-- #122 -->
</div>
</div>
<div id="correlations" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Correlations</h3>
<p>LRM render the quantitative relationship between two metric variables.
Another commonly known statistic that seems to do something similar is Pearson’s correlation statistic <span class="math inline">\(r\)</span> (<a href="ebs.html#associations">3.3.4</a>).
In the following, we will see that a tight connection between correlation and linear coefficients exists, albeit both having their own advantages.
For a demonstration, we reproduce the steps on a simulated data set where X and Y are linearly linked (Figure @ref(fig:corr_1))</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="lm.html#cb333-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb333-2"><a href="lm.html#cb333-2" aria-hidden="true" tabindex="-1"></a>D_cor <span class="ot">&lt;-</span></span>
<span id="cb333-3"><a href="lm.html#cb333-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb333-4"><a href="lm.html#cb333-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">50</span>, <span class="dv">0</span>, <span class="dv">50</span>),</span>
<span id="cb333-5"><a href="lm.html#cb333-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">rnorm</span>(<span class="dv">50</span>, x <span class="sc">*</span> .<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb333-6"><a href="lm.html#cb333-6" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="lm.html#cb334-1" aria-hidden="true" tabindex="-1"></a>D_cor <span class="sc">%&gt;%</span></span>
<span id="cb334-2"><a href="lm.html#cb334-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb334-3"><a href="lm.html#cb334-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb334-4"><a href="lm.html#cb334-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> F)</span></code></pre></div>
<div class="figure"><span id="fig:corr-1"></span>
<img src="Classic_linear_models_files/figure-html/corr-1-1.png" alt="A linear association between X and Y" width="90%" />
<p class="caption">
Figure 4.11: A linear association between X and Y
</p>
</div>
<p>Recall, that <span class="math inline">\(r\)</span> is covariance standardized for dispersion, not unsimilar to z-transformation <a href="lm.html#transform-measures">4.2.1</a> and that a covariance is the mean squared deviance from the population mean.
This is how the correlation is decontaminated from the idiosyncracies of the involved measures, their location and dispersion.
Similarly, the slope parameter in a LRM is a measure of association, too.
It is agnostic of the overall location of measures since this is captured by the intercept.
However, dispersion remains intact.
This ensures that the slope and the intercept together retain information about location, dispersion and association of data, and we can ultimately make predictions.
Still, there is a tight relationship between Pearson’s <span class="math inline">\(r\)</span> and a slope coefficient <span class="math inline">\(\beta_1\)</span>, namely:</p>
<p><span class="math display">\[
r = \beta_1 \frac{\sigma_X}{\sigma_Y}
\]</span></p>
<p>For the sole purpose of demonstration, we here resort to the built-in non-Bayesian command <code>lm</code> for doing the regression.</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="lm.html#cb335-1" aria-hidden="true" tabindex="-1"></a>M_cor <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> D_cor)</span></code></pre></div>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="lm.html#cb336-1" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> <span class="fu">coef</span>(M_cor)<span class="sc">$</span>center[<span class="dv">2</span>]</span>
<span id="cb336-2"><a href="lm.html#cb336-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb336-3"><a href="lm.html#cb336-3" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span> beta_1 <span class="sc">*</span> <span class="fu">sd</span>(D_cor<span class="sc">$</span>x) <span class="sc">/</span> <span class="fu">sd</span>(D_cor<span class="sc">$</span>y)</span>
<span id="cb336-4"><a href="lm.html#cb336-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;the correlation is: &quot;</span>, r)</span></code></pre></div>
<pre><code>## the correlation is:  0.715</code></pre>
<p>The clue with Pearson’s <span class="math inline">\(r\)</span> is that it normalized the slope coefficient by the variation found in the sample.
This resembles z-transformation as was introduced in <a href="lm.html#transform-measures">4.2.1</a>.
In fact, when both, predictor and outcome, are z-transformed before estimation, the coefficient equals Pearson’s <span class="math inline">\(r\)</span> almost precisely. The minor deviation stems from the relatively short MCMC chains.</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="lm.html#cb338-1" aria-hidden="true" tabindex="-1"></a>M_z <span class="ot">&lt;-</span></span>
<span id="cb338-2"><a href="lm.html#cb338-2" aria-hidden="true" tabindex="-1"></a>  D_cor <span class="sc">%&gt;%</span></span>
<span id="cb338-3"><a href="lm.html#cb338-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb338-4"><a href="lm.html#cb338-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">x_z =</span> (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">/</span> <span class="fu">sd</span>(x),</span>
<span id="cb338-5"><a href="lm.html#cb338-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">y_z =</span> (y <span class="sc">-</span> <span class="fu">mean</span>(y)) <span class="sc">/</span> <span class="fu">sd</span>(y)</span>
<span id="cb338-6"><a href="lm.html#cb338-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb338-7"><a href="lm.html#cb338-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(y_z <span class="sc">~</span> x_z,</span>
<span id="cb338-8"><a href="lm.html#cb338-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb338-9"><a href="lm.html#cb338-9" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="lm.html#cb339-1" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> <span class="fu">coef</span>(M_z)<span class="sc">$</span>center[<span class="dv">2</span>]</span>
<span id="cb339-2"><a href="lm.html#cb339-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb339-3"><a href="lm.html#cb339-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;On z standardized outcomes the coefficient is&quot;</span>, beta_1)</span></code></pre></div>
<pre><code>## On z standardized outcomes the coefficient is 0.717</code></pre>
<p>Pearson’s <span class="math inline">\(r\)</span> spawns from a different school of thinking than Bayesian parameter estimation: analysis of variance (ANOVA). Roughly, this family of methods draws on the idea of dividing the <em>total variance</em> of the outcome variable into two components: <em>explained variance</em> and <em>residual variance</em>. The very formula of the variance parameter reveals its connection to covariance (it is even allowed to say, that variance is the covariance of a variable with itself):</p>
<p><span class="math display">\[
\textrm{Var}_{X} = \frac{1}{n} \sum_{i=1}^n (x_i - E(X))^2
\]</span></p>
<p>In ANOVA models, when explained variance is large, as compared to residual variance, the F-statistic goes up and stars twinkle behind the p-value. While I am far from promoting any legacy approaches, here, a scale-less measure of association strength bears some intuition in situations, where at least one of the involved variables has no well-defined scale. That is in particular the case with rating scales. Measurement theory tells that we may actually transform rating scales fully to our liking, if just the order is preserved (ordinal scales). That is a pretty weak criterion and, strictly speaking, forbids the application of linear models (and ANOVA) altogether, where at least sums must be well defined (interval scales).</p>
<p>Not by coincidence, a measure of explained variance, the <em>coefficient of determination r^2</em> can be derived as from Pearson’s <span class="math inline">\(r\)</span>, by simply squaring it. <span class="math inline">\(r^2\)</span> is in the range <span class="math inline">\([0;1]\)</span> and represents the proportion of variability that is explained by the predictor:</p>
<p>To sum it up, Pearson <span class="math inline">\(r\)</span> and <span class="math inline">\(r^2\)</span> are useful statistics to express the strength of an association, when the scale of measurement does not matter or when one desires to compare across scales.
Furthermore, correlations can be part of advanced multilevel models which we will treat in</p>
<p>In regression modelling the use of <!-- #54 --> coefficients allows for predictions made in the original units of measurement.
Correlations, in contrast, are unit-less.
Still, correlation coefficients play an important role in exploratory data analysis (but also in multilevel models, see <a href="mlm.html#reliability">6.8.2</a> for the following reasons:</p>
<ol style="list-style-type: decimal">
<li>Correlations between predictors and responses are a quick and dirty assessment of the expected associations.</li>
<li>Correlations between multiple response modalities (e.g., ToT and number of errors) indicate to what extent these responses can be considered exchangeable.</li>
<li>Correlations between predictors should be checked upfront to avoid problems arising from so-called collinearity.</li>
</ol>
<p>The following table shows the correlations between measures in the MMN study, where we tested the association between verbal and spatial working memory capacity (Ospan and Corsi tests and performance in a search task on a website (clicks and time).</p>
<p>These correlations give an approximate picture of associations in the data:</p>
<ol style="list-style-type: decimal">
<li>Working memory capacity is barely related to performance.</li>
<li>There is a strong correlations between the performance measures.</li>
<li>There is a strong correlation between the two predictors Ospan.A and Ospan.B.</li>
</ol>
<!--30-->
<p>Linear coefficients and correlations both represent associations between measures.
Coefficients preserve units of measuremnent, allowing us to make meaningful quantitative statements.
Correlations are re-scaled by the observed dispersion of measures in the sample, making them unit-less.
The advantage is that larger sets of associations can be screened at once and compared easily.</p>
<!-- #30 
Linear models link multiple predictors to an outcome variable. In the MMN case we observed that, by adding another predictor to a model, the strength of the original predictor only changes marginally. That is *not* the general rule as we will see now!

Why is it better to have three legs under a table than four? Because three legs are always stable, no beer coasters needed. Recall basic geometry: the straight connection between two points is called a line (like in regression line). Three points make a triangle. An astonishing property of triangles is that in a three-dimension space they are always flat. There is never bending needed to connect three points.
-->
<!--
Elaborations on Euclidean space, how double linear regression creates flat surfaces and how collinearity reduces this to a line that is unstable.-->
</div>
<div id="endless-linear" class="section level3" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Endlessly linear</h3>
<p>On a deeper level the bizarre age = 0 prediction is an example of a principle <!-- #55 -->, that will re-occur several times throughout this book.</p>
<p><strong>In our endless universe everything is finite.</strong></p>
<p>A well understood fact about LRM is that they allow us to fit a straight line to data.
A lesser regarded consequence from the mathematical underpinnings of such models is that this line extends infinitely in both directions.
To fulfill this assumption, the outcome variable needs to have an infinite range, too, <span class="math inline">\(y_i \in [-\infty; \infty]\)</span> (unless the slope is zero).
Every scientifically trained person and many lay people know, that even elementary magnitudes in physics are finite: all speeds are limited to <span class="math inline">\(\approx 300.000 km/s\)</span>, the speed of light, and temperature has a lower limit of <span class="math inline">\(-276°C\)</span> (or <span class="math inline">\(0°K\)</span>).
If there can neither be endless acceleration nor cold, it would be daring to assume any psychological effect to be infinite in both directions.</p>
<p>The endlessly linear assumption (ELA) is a central piece of all LRMs, that is always violated in a universe like ours.
So, should we never ever use a linear model and move on to non-linear models right away?
Pragmatically, the LRM often is a reasonably effective approximation.
At the beginning of section <a href="lm.html#lrm">4.2</a>, we have seen that the increase of time-on-task by age is not strictly linear, but follows a more complex curved pattern.
This pattern might be of interest to someone studying the psychological causes of the decline in performance.
For the applied design researcher it probably suffices to see that the increase is monotonous and model it approximately by one slope coefficient.
In <a href="mpm.html#mrm">5.1</a> we will estimate the age effects for designs A and B separately, which lets us compare fairness towards older people.</p>
<p>As has been said, theorists may desire a more detailed picture and see a disruption of linearity as indicators for interesting psychological processes.
A literally uncanny example of such theoretical work will be given when introducing polynomial regression <a href="mpm.html#prm">5.5</a>.
For now, linear regression is a pragmatic choice, as long as:</p>
<ol style="list-style-type: decimal">
<li>the pattern is monotonically increasing</li>
<li>any predictions stay in the observed range and avoid the boundary regions, or beyond.</li>
</ol>
<!-- ### Exercises -->
<!-- 1. Examine the linear parameters of model `M_age_rtrn` and derive some impossible predictions, as was done in the previous section. -->
<!-- 1. The BAB1 data set contains another outcome variable where the number of clicks was measured until the participant found the desired information. Specify a LRM with age and examine the residual distribution. Is the Normality assumption defendable? What is the difference to home returns, despite both variables being counts? -->
<!-- 1. Review the two figures in the first example of [GSR]. The observations are bi-modally distributed, nowhere near Gaussian. After (graphically) applying the model they are well-shaped. What does that tell you about checking residual assumptions before running the model? <!-- #82 -> -->
</div>
</div>
<div id="factorial-models" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Factorial Models</h2>
<p>In the previous section we have seen how linear models are fitting the association between a metric predictor X and an outcome variable Y to a straight line with a slope and a point of intercept.
Such a model creates a prediction of Y, given you know the value of measure X.</p>
<p>However, in many research situations, the predictor variable carries not a measure, but a <em>group label</em>.
<em>Factor variables</em> assign observations to one of a set of predefined groups, such as the following variables do in the BrowsingAB case (Table <a href="lm.html#tab:bab-2">4.18</a>):</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="lm.html#cb341-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(BrowsingAB)</span>
<span id="cb341-2"><a href="lm.html#cb341-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb341-3"><a href="lm.html#cb341-3" aria-hidden="true" tabindex="-1"></a>BAB5 <span class="sc">%&gt;%</span></span>
<span id="cb341-4"><a href="lm.html#cb341-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Obs, Part, Task, Design, Gender, Education, Far_sighted)</span></code></pre></div>
<table>
<caption><span id="tab:bab-2">Table 4.18: </span>Data set with 7 variables, showing 8 of 300 observations.</caption>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="left">Part</th>
<th align="left">Task</th>
<th align="left">Design</th>
<th align="left">Gender</th>
<th align="left">Education</th>
<th align="left">Far_sighted</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">191</td>
<td align="left">11</td>
<td align="left">2</td>
<td align="left">B</td>
<td align="left">F</td>
<td align="left">Middle</td>
<td align="left">FALSE</td>
</tr>
<tr class="even">
<td align="right">251</td>
<td align="left">11</td>
<td align="left">4</td>
<td align="left">B</td>
<td align="left">F</td>
<td align="left">Middle</td>
<td align="left">FALSE</td>
</tr>
<tr class="odd">
<td align="right">42</td>
<td align="left">12</td>
<td align="left">2</td>
<td align="left">A</td>
<td align="left">M</td>
<td align="left">Middle</td>
<td align="left">FALSE</td>
</tr>
<tr class="even">
<td align="right">32</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">A</td>
<td align="left">F</td>
<td align="left">High</td>
<td align="left">TRUE</td>
</tr>
<tr class="odd">
<td align="right">25</td>
<td align="left">25</td>
<td align="left">1</td>
<td align="left">A</td>
<td align="left">F</td>
<td align="left">Middle</td>
<td align="left">FALSE</td>
</tr>
<tr class="even">
<td align="right">238</td>
<td align="left">28</td>
<td align="left">3</td>
<td align="left">B</td>
<td align="left">F</td>
<td align="left">Low</td>
<td align="left">TRUE</td>
</tr>
<tr class="odd">
<td align="right">157</td>
<td align="left">7</td>
<td align="left">1</td>
<td align="left">B</td>
<td align="left">M</td>
<td align="left">High</td>
<td align="left">FALSE</td>
</tr>
<tr class="even">
<td align="right">248</td>
<td align="left">8</td>
<td align="left">4</td>
<td align="left">B</td>
<td align="left">M</td>
<td align="left">High</td>
<td align="left">FALSE</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>Two of the variables, Gender and Education clearly carry a group membership of the participants.
That is a natural way to think of people groups, such as male of female, or which school type they went to.
But, models are inert to anthropocentrism and can divide everything into groups.
Most generally, it is always the observations, i.e. the rows in a (tidy) data table, which are divided into groups.
Half of the observations habe been made with design A, the rest with B.</p>
<p>The data also identifies the participant and the task for every observation.
Although do we see numbers on participants, these are factors, not metric variables.
If one had used initials of participants, that would not make the slightest difference of what this variable tells.
It also does not matter, whether the researcher has actually created the levels, for example by assigning participants to one of two deisgn conditions, or has just observed it, such as demographic variables.</p>
<p>Factorial models are frequently used in experiments, where the effect a certain condition on performance is measured.
In design research, that is the case when comparing two (or more) designs and the basic model for that, the comparison of group means model, will be introduced, first <a href="lm.html#cgm">4.3.1</a>, with more details on the inner workings and variations in the two sections that follow: dummy variables <a href="lm.html#dummy">4.3.2</a> and <a href="lm.html#treatment-contrasts">4.3.3</a>.
A CGM requires that one can think of one of the groups as some kind of default to which all the other conditions are compared to.
That is not always given.
When groups are truly equal among sisters, the absolute means model (AMM) <a href="lm.html#amm">4.3.4</a> does just estimates the absolute group mans, being like multi-facetted GMMs.</p>
<p>Factors are not metric, but sometimes they have a natural ordered, for example levels of education, or position in a sequence.
In section <a href="lm.html#ofm">4.3.5</a> we will apply an ordered factorial model to a learning sequence.</p>
<div id="cgm" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> A versus B: Comparison of groups</h3>
<p>The most common linear models on factors is the <em>comparison of groups</em> (CGM), which replaces the commonly known analysis of variance (ANOVA).
In design research group comparisons are all over the place, for example:</p>
<ul>
<li>comparing designs: as we have seen in the A/B testing scenario</li>
<li>comparing groups of people, based on e.g. gender or whether they have a high school degree</li>
<li>comparing situations, like whether an app was used on the go or standing still</li>
</ul>
<p>In order to perform a CGM, a variable is needed that establishes the groups.
This is commonly called a <em>factor</em>.
A factor is a variable that identifies members of groups, like “A” and “B” or “male” and “female.”
The groups are called <em>factor levels</em>.
In the BrowsingAB case, the most interesting factor is Design with its levels A and B.</p>
<p>Asking for differences between two (or more) designs is routine in design research.
For example, it could occur during an overhaul of a municipal website.
With the emerge of e-government, many municipal websites have grown wildly over a decade.
What once was a lean (but not pretty) 1990 website has grown into a jungle over time, to the disadvantage for users.
The BrowsingAB case could represent the prototype of a novel web design, which is developed and tested via A/B testing at 200 users.
Every user is given the same task, but sees only one of the two designs.
The design team is interested in: <em>Do the two web designs A and B differ in user performance?</em> Again, we first take a look at the raw data (Figure <a href="lm.html#fig:eda-anova">4.12</a>)),</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="lm.html#cb343-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(BrowsingAB)</span></code></pre></div>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="lm.html#cb344-1" aria-hidden="true" tabindex="-1"></a>BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb344-2"><a href="lm.html#cb344-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> ToT)) <span class="sc">+</span></span>
<span id="cb344-3"><a href="lm.html#cb344-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb344-4"><a href="lm.html#cb344-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(Design <span class="sc">~</span> .)</span></code></pre></div>
<div class="figure"><span id="fig:eda-anova"></span>
<img src="Classic_linear_models_files/figure-html/eda-anova-1.png" alt="Histogram showing ToT distributions in two groups" width="90%" />
<p class="caption">
Figure 4.12: Histogram showing ToT distributions in two groups
</p>
</div>
<p>The difference, if it exists, is not striking.
We might consider a slight advantage for design B, but the overlap is immense.
We perform the CGM.
Again, this is a two-step procedure:</p>
<ol style="list-style-type: decimal">
<li>The <code>stan_glm</code> command lets you specify a simple formula to express the dependency between one or more predictors (education) and an outcome variable (ToT). It performs the parameter estimation using the method of <em>Markov-Chain Monte-Carlo Sampling</em>. The results are stored in a new object <code>M_CGM</code>.</li>
<li>With the <code>coef</code> command the estimates are extracted and can be interpreted (Table <a href="lm.html#tab:M-CGM">4.19</a>).</li>
</ol>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="lm.html#cb345-1" aria-hidden="true" tabindex="-1"></a>M_CGM <span class="ot">&lt;-</span></span>
<span id="cb345-2"><a href="lm.html#cb345-2" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb345-3"><a href="lm.html#cb345-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Design,</span>
<span id="cb345-4"><a href="lm.html#cb345-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .</span>
<span id="cb345-5"><a href="lm.html#cb345-5" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="lm.html#cb346-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_CGM)</span></code></pre></div>
<table>
<caption><span id="tab:M-CGM">Table 4.19: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">106.6</td>
<td align="right">97.2</td>
<td align="right">116.4</td>
</tr>
<tr class="even">
<td align="left">DesignB</td>
<td align="left">DesignB</td>
<td align="right">-17.5</td>
<td align="right">-30.6</td>
<td align="right">-4.2</td>
</tr>
</tbody>
</table>
<p>The model contains two parameters, one Intercept and one slope.
Wait a second?
How can you have a slope and a “crossing point zero,” when there is no line, but just two groups?
This will be explained further in <a href="lm.html#dummy">4.3.2</a> and <a href="lm.html#treatment-contrasts">4.3.3</a>.
Fact is, in the model at hand, the Intercept is the <em>mean of a reference group</em>.
Per default, stan_glm chooses the alphabetically first group label as the reference group, in this case design A.
We can therefore say that design A has an average performance of <span class="math inline">\(106.65 [97.2, 116.41]_{CI95}\)</span>.</p>
<p>The second parameter is the effect of “moving to design B.”
It is given as the <em>difference to the reference group</em>.
With design B it took users <span class="math inline">\(17.49 [30.62, 4.2]_{CI95}\)</span> seconds less to complete the task.
However, this effect appears rather small and there is huge uncertainty about it.
It barely justifies the effort to replace design A with B.
If the BrowsingAB data set has some exciting stories to tell, the design difference is not it.</p>
<p>A frequent user problem with CGMs is that the regression engine selects the alphabetically first level as the reference level, which often is not correct.
Supposed, the two designs had been called Old (A) and New (B), then regression engine would pick New as the reference group.
Or think of non-discriminating language in statistical reports.
In BrowsingAB, gender is coded as f/m and female participants conquer the Intercept.
But, sometimes my students code gender as v/m or w/m.
Oh, my dear!
The best solution is, indeed, to think upfront and try to find level names that make sense.
If that is not possible, then the factor variable, which is often of type <code>character</code> must be made a factor, which is a data type in its own right in R.
When the regression engine sees a factor variable, it takes the first factor level as reference group.
That would be nice, but when a factor is created using the <code>as.factor</code>, it again takes an alphabethical order of levels.
This is over-run by giving a vector of levels in the desired order.
The tidy Foracts package provides further commands to set the order of a factor levels.</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="lm.html#cb347-1" aria-hidden="true" tabindex="-1"></a>Gender <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">&quot;v&quot;</span>, <span class="st">&quot;m&quot;</span>), <span class="dv">4</span>, <span class="at">replace =</span> T)</span>
<span id="cb347-2"><a href="lm.html#cb347-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb347-3"><a href="lm.html#cb347-3" aria-hidden="true" tabindex="-1"></a><span class="fu">factor</span>(Gender)</span></code></pre></div>
<pre><code>## [1] m v m m
## Levels: m v</code></pre>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="lm.html#cb349-1" aria-hidden="true" tabindex="-1"></a><span class="fu">factor</span>(Gender, <span class="fu">c</span>(<span class="st">&quot;v&quot;</span>, <span class="st">&quot;m&quot;</span>))</span></code></pre></div>
<pre><code>## [1] m v m m
## Levels: v m</code></pre>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="dummy" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Not stupid: dummy variables</h3>
<p>Are we missing anything so far?
Indeed, I avoided to show any mathematics on factorial models.
The CGM really is a linear model, although it may not appear so, at first.
So, how can a variable enter a linear model equation, that is not a number?
Linear model terms are a sum of products <span class="math inline">\(\beta_ix_i\)</span>, but factors cannot just enter such a term.
What would be the result of <span class="math inline">\(\mathrm{DesignB} \times\beta_1\)</span>?</p>
<p><em>Factors</em> basically answer the question: <em>What group does the observation belong to?</em>.
This is a label, not a number, and cannot enter the regression formula.
<em>Dummy variables</em> solve the dilemma by converting factor levels to numbers.
This is done by giving <em>every level</em> <span class="math inline">\(l\)</span> of factor <span class="math inline">\(K\)</span> its own dummy variable <em>K_l</em>.
Now every dummy represents the simple question: <em>Does this observation belong to group DesignB?</em>.
The answer is coded as <span class="math inline">\(0\)</span> for “Yes” and <span class="math inline">\(1\)</span> for “No.”</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="lm.html#cb352-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(BrowsingAB)</span></code></pre></div>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="lm.html#cb353-1" aria-hidden="true" tabindex="-1"></a>BAB1_dum <span class="ot">&lt;-</span></span>
<span id="cb353-2"><a href="lm.html#cb353-2" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb353-3"><a href="lm.html#cb353-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb353-4"><a href="lm.html#cb353-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Design_A =</span> <span class="fu">if_else</span>(Design <span class="sc">==</span> <span class="st">&quot;A&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb353-5"><a href="lm.html#cb353-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">Design_B =</span> <span class="fu">if_else</span>(Design <span class="sc">==</span> <span class="st">&quot;B&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb353-6"><a href="lm.html#cb353-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb353-7"><a href="lm.html#cb353-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Obs, Design, Design_A, Design_B, ToT)</span>
<span id="cb353-8"><a href="lm.html#cb353-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb353-9"><a href="lm.html#cb353-9" aria-hidden="true" tabindex="-1"></a>BAB1_dum</span></code></pre></div>
<table>
<caption><span id="tab:dummy-1">Table 4.20: </span>Data set with 5 variables, showing 8 of 200 observations.</caption>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="left">Design</th>
<th align="right">Design_A</th>
<th align="right">Design_B</th>
<th align="right">ToT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">31</td>
<td align="left">A</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">182.7</td>
</tr>
<tr class="even">
<td align="right">43</td>
<td align="left">A</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">186.9</td>
</tr>
<tr class="odd">
<td align="right">113</td>
<td align="left">B</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">57.7</td>
</tr>
<tr class="even">
<td align="right">162</td>
<td align="left">B</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">28.5</td>
</tr>
<tr class="odd">
<td align="right">165</td>
<td align="left">B</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">56.4</td>
</tr>
<tr class="even">
<td align="right">175</td>
<td align="left">B</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">64.4</td>
</tr>
<tr class="odd">
<td align="right">190</td>
<td align="left">B</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">111.3</td>
</tr>
<tr class="even">
<td align="right">196</td>
<td align="left">B</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">121.2</td>
</tr>
</tbody>
</table>
<!--Vice versa, numbers in a logical context are always interpreted in complete reverse <!-- #57 ->.

$$ 
v = 1 \mapsto \mathrm{TRUE}\\ v = 0 \mapsto \mathrm{FALSE} 
$$

Boolean variables can be used in all contexts, including numerical and categorical. It is unfortunate that we call them dummies in such a belittling manner, as they are truly bridges between the world of categories and arithmetic. However, if preferred, they can also be called indicator variable, Boolean indicator or binary variable. They identify groups in our data set and switch on or off the effect in the linear term. -->
<!-- All arithmetic commands in R do an implicit typecast when encountering a Boolean variable, e.g. for `sum(TRUE, FALSE, TRUE)` the result is 2. In contrast, regression engines interpret Boolean variables as categorical. Therefore, dummy variables have to be passed on as explicitly numeric to the regression engine. Only when a variable truly consists of zeroes and ones, it will be interpreted as desired. This has been done that with the explicit typecast `as.numeric` above. -->
<p>The new dummy variables are numerical and can very well enter a linear formula, every one getting its own coefficient.
For a factor K with levels A, B and C the linear formula can include the dummy variables <span class="math inline">\(K_{Ai}\)</span> and <span class="math inline">\(K_{Bi}\)</span>:</p>
<p><span class="math display">\[ 
\mu_i = K_{Ai} \beta_{A} + K_{Bi}  \beta_{B} 
\]</span></p>
<p>The zero/one coding acts like a switches.
When <span class="math inline">\(K_{Ai}=1\)</span>, the parameter <span class="math inline">\(\beta_A\)</span> is switched on and enters the sum, and <span class="math inline">\(\beta_B\)</span> is switched off.
An observation of group A gets the predicted value: <span class="math inline">\(\mu_i = \beta_A\)</span>, vice versa for members of group B.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="lm.html#cb354-1" aria-hidden="true" tabindex="-1"></a>M_dummy_1 <span class="ot">&lt;-</span></span>
<span id="cb354-2"><a href="lm.html#cb354-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> Design_A <span class="sc">+</span> Design_B,</span>
<span id="cb354-3"><a href="lm.html#cb354-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> BAB1_dum</span>
<span id="cb354-4"><a href="lm.html#cb354-4" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="lm.html#cb355-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_dummy_1)</span></code></pre></div>
<table>
<caption><span id="tab:dummy-3">Table 4.21: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Design_A</td>
<td align="left">Design_A</td>
<td align="right">106.6</td>
<td align="right">96.8</td>
<td align="right">116.0</td>
</tr>
<tr class="even">
<td align="left">Design_B</td>
<td align="left">Design_B</td>
<td align="right">89.4</td>
<td align="right">79.9</td>
<td align="right">98.9</td>
</tr>
</tbody>
</table>
<p>In its predictions, the model <code>M_dummy</code> is equivalent to the CGM model <code>M_CGM</code>, but the coefficients mean something different. In the CGM, we got an intercept and a group difference, here we get both group means (Table <a href="lm.html#tab:dummy-3">4.21</a>).
This model we call an <em>absolute means model (AMM)</em> and will discuss it in section <a href="lm.html#amm">4.3.4</a>.
First, we have to come back to the question, how the regression engine produces its dummy variables, such that the coefficients are differences towards one Intercept.
When encountering a factorial predictor, our regression engine uses not group means, but what is called <em>treatment contrast coding</em> (<a href="lm.html#treatment-contrasts">4.3.3</a>).</p>
</div>
<div id="treatment-contrasts" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Treatment contrast coding</h3>
<p>The default behaviour of regression engines, when encountering a factor, is to select the first level as reference group and estimate all other levels relative to that.
Coefficients express differences.
This fully makes sense if the effect of a treatment is what you are after, and is therefore called <em>treatment contrasts</em>.
Treatment contrasts do not have anything special or natural to them, but is a very particular way of thinking about levels of a factor, namely that <em>one level is special</em>.
In controlled experiments, this special level often is the <em>control condition</em>, whereas the coefficients are the effects of well-defined manipulations.
This most prominently is the case for clinical trials, where the <em>placebo group</em> is untreated.
This works well in all situations where a default situation exists and the other factor levels can be thought of manipulations of the default:</p>
<ul>
<li>A redesign as an improvement over the <em>current design</em>.</li>
<li>A quiet, comfortable environment is the <em>optimal situation</em> for cognitive performance.</li>
<li>There is a <em>minimum level</em> of education requird for most jobs</li>
</ul>
<p>We have seen how to create dummy variables ourselves by means of mututally exclusive on-off switches, which results in absolute means coefficients.
Regression engines quietly assume that treatment effects is what the user wants and expand dummy variables in a different way: For a factor with levels A and B, the dummy for B is an on-off switch, whereas the reference level A is set <em>always on</em>. This way of creating dummy variables is called <em>treatment contrast coding</em>:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="lm.html#cb356-1" aria-hidden="true" tabindex="-1"></a>BAB1_treat <span class="ot">&lt;-</span></span>
<span id="cb356-2"><a href="lm.html#cb356-2" aria-hidden="true" tabindex="-1"></a>  BAB1_dum <span class="sc">%&gt;%</span></span>
<span id="cb356-3"><a href="lm.html#cb356-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb356-4"><a href="lm.html#cb356-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Intercept =</span> <span class="dv">1</span>,</span>
<span id="cb356-5"><a href="lm.html#cb356-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">Design_B =</span> <span class="fu">if_else</span>(Design <span class="sc">==</span> <span class="st">&quot;B&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb356-6"><a href="lm.html#cb356-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb356-7"><a href="lm.html#cb356-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Obs, Design, Intercept, Design_B, ToT)</span>
<span id="cb356-8"><a href="lm.html#cb356-8" aria-hidden="true" tabindex="-1"></a>BAB1_treat</span></code></pre></div>
<table>
<caption><span id="tab:treatment-1">Table 4.22: </span>Data set with 5 variables, showing 8 of 200 observations.</caption>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="left">Design</th>
<th align="right">Intercept</th>
<th align="right">Design_B</th>
<th align="right">ToT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">6</td>
<td align="left">A</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">20.4</td>
</tr>
<tr class="even">
<td align="right">15</td>
<td align="left">A</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">185.7</td>
</tr>
<tr class="odd">
<td align="right">34</td>
<td align="left">A</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">168.8</td>
</tr>
<tr class="even">
<td align="right">38</td>
<td align="left">A</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">107.6</td>
</tr>
<tr class="odd">
<td align="right">84</td>
<td align="left">A</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">147.2</td>
</tr>
<tr class="even">
<td align="right">94</td>
<td align="left">A</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">120.4</td>
</tr>
<tr class="odd">
<td align="right">95</td>
<td align="left">A</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">117.1</td>
</tr>
<tr class="even">
<td align="right">199</td>
<td align="left">B</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">112.9</td>
</tr>
</tbody>
</table>
<!-- Another Instead oTo see that, we extract the dummy variables from the CGM on design with the standard command `model.matrix`. As you can see, for all observations the `(Intercept)` column takes the value `1`, whereas level `DesignB` is switched on and off. -->
<!--when formally specifying the likelihood <!-- #58 ->, dummy variables must be made explicit. When doing an AGM on a factor $x_1$ with $1,...,k$ levels, the likelihood function becomes:-->
<!-- $$\mu_i=d_1 \beta_{1[1]}+...+d_k \beta_{1[k]}$$ -->
<!-- In the remainder of the book, we are dealing with more complex models (e.g., multifactorial models in the next section), as well as factors with many levels (random effects in multi-level models <a href="mlm.html#mlm">6</a>). With expanded dummy variables, the likelihood can become unduly long. -->
<!-- Up to this point, I have introduced dummy variables at the example of AGMs, where at any moment only one factor level is switched on. A more common CGMs  would have a the following likelihood specification (with a factor of $k$ levels): -->
<!-- $$\mu_i = \beta_0 + d_1 \beta_{1[1]}+...+d_k \beta_{1[k-1]}$$ -->
<!-- ```{r dummy_4, opts.label = "rtut"} -->
<!-- BAB1 %>%  -->
<!--   select(Part, Design, ToT) %>%  -->
<!--   cbind(model.matrix(M_CGM)) %>% ## <--- -->
<!--   as_tibble() %>%  -->
<!--   sample_n(8) %>%  -->
<!--   kable() -->
<!-- ``` -->
<p>A frequent user problem with treatment coding is that the regression engine selects the alphabetically first level as the reference level.
Supposed, the two designs had been called Old (A) and New (B), then regression engine would pick New as the reference group. But recall the more convient ways that were outlined earlier (<a href="lm.html#cgm">4.3.1</a>).</p>
<p>The following chapters deal with more variations of factorial models.
Next, we will take a closer look at the absolute means model (<a href="lm.html#amm">4.3.4</a>), which is useful, when a reference group does not come natural.
In section <a href="lm.html#ofm">4.3.5</a>, we deal with factorial models, where levels are ordered and introduce a third way of dummy coding: <em>stairways coding</em> for factors with ordered levels.</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="amm" class="section level3" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Absolute Means Model</h3>
<p>Not all factor variables are experimental and identifying a default can be difficult or unnatural.
This often happens when the levels are just a set of conditions that you have <em>found as given</em>, such as the individuals in the human population, or all words in a language.
Such is the case in the IPump study, where every session was composed of a set of tasks, such as starting the device or entering a dose.
These tasks were taken from existing training material and including them as a factor could help identify areas for improvement.
Although the tasks form a sequence, they are equally important for the operation.
Not one can be singled out as default.
Treatment coding would force us to name one default task for the Intercept.</p>
<p>The <em>absolute means model</em> represents all levels by their absolute means.
If you put in a factorial predictor with eight levels, you will get eight coefficients, which are the mean outcomes of every level.
Of course, what you can no longer do is find differences between levels.</p>
<p>We have seen in <a href="lm.html#dummy">4.3.2</a> how to create a AMM dummy variables.
IN fact, the linear models formula language this can be done more directly by either of the two option below, (but just leaving out the <code>1 +</code> does not suffice):</p>
<ul>
<li><code>0 + Task</code></li>
<li><code>Task - 1</code></li>
</ul>
<p>In the following, we estimate an AMM using the formula method.
In the IPump study we had a sample of nurses do a sequence of eight tasks on a medical infusion pump with a novel interface design.
For the further development of such a design it may be interesting to see, which tasks would most benefit from design improvements.
A possible way to look at it is by saying that a longer task has more potential to be optimized.
Under such a perspective, tasks are an equal set and there is no natural reference task for a CGM.
Instead we estimate the absolute group means, and visualize the CLU table as center dots and 95% credibility bars.</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="lm.html#cb358-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(IPump)</span></code></pre></div>
<!-- HERE -->
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="lm.html#cb359-1" aria-hidden="true" tabindex="-1"></a>M_AMM_1 <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> Task,</span>
<span id="cb359-2"><a href="lm.html#cb359-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_Novel</span>
<span id="cb359-3"><a href="lm.html#cb359-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="lm.html#cb360-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_AMM_1) <span class="sc">%&gt;%</span></span>
<span id="cb360-2"><a href="lm.html#cb360-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">Task =</span> fixef) <span class="sc">%&gt;%</span></span>
<span id="cb360-3"><a href="lm.html#cb360-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(</span>
<span id="cb360-4"><a href="lm.html#cb360-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> Task,</span>
<span id="cb360-5"><a href="lm.html#cb360-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> center, <span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper</span>
<span id="cb360-6"><a href="lm.html#cb360-6" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb360-7"><a href="lm.html#cb360-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_crossbar</span>()</span></code></pre></div>
<div class="figure"><span id="fig:amm-1"></span>
<img src="Classic_linear_models_files/figure-html/amm-1-1.png" alt="ToT on five tasks, center estimates and 95 percent credibility interval" width="90%" />
<p class="caption">
Figure 4.13: ToT on five tasks, center estimates and 95 percent credibility interval
</p>
</div>
<p>In Figure <a href="lm.html#fig:amm-1">4.13</a> we can easily discover that Task 2 is by far the longest and that tasks differ a lot, overall.
None of these relations can easily be seen in the CGM plot.
Note that the AMM is not a different model than the treatment effects model.
It is just a <em>different parametrization</em>, which makes interpretation easier.
Both models produce the exact same predictions (except for minimal variations from the MCMC random walk).</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>The choice between CGM and AMM depends on whether a factor represents designed manipulations or whether it is more something that has been collected, in other words: a sample. For experimental conditions, with one default conditions, a CGM is the best choice. When the factor levels are a sample of equals, an AMM is more useful. It is for now, because in <a href="mlm.html#non-human-populations">6.5</a>, we will see how random effects apply for non-humen populations.</p>
<!-- Psychological experiments often have fully designed stimuli.  -->
<!-- In the Stroop experiment, stimuli show a color word (red) written in a color (Red). -->
<!-- Both properties are well-defined and can be manipulated freely by the experimenter. -->
<!-- That is what you can call a fully controlled stimulus design, where the researcher precisely knows, in which aspect stimuli differ. -->
<!-- One could argue, that colors and color words have been collected from our perceptual and cultural heredity and are therefore are not really manipulated. -->
<!-- This is a valid issue, but in the original experiment it does not matter so much, because the real manipulation is congruency, which is assumed to be independent on the precise color (word). -->
<!-- ```{r} -->
<!-- tribble(~Word, ~Color, -->
<!--         "red", "Red", -->
<!--         "blue", "Blue", -->
<!--         "green", "Green") %>% -->
<!--   tidyr::complete(Word, Color) %>% -->
<!--   mutate(Condition = if_else(Word == str_to_lower(Color), -->
<!--                         "congruent", -->
<!--                         "incongruent")) -->
<!-- ``` -->
<!-- In more common version of the Stroop task, a neutral condition is added, where the task stays the same, but the word is a non-color word. -->
<!-- Almost every word in a language is a non-color word. -->
<!-- And among all those words, color neutrality is not so clear cut. -->
<!-- All objects around us show colors, and color symbolism pervades all areas of our lives. -->
<!-- We can easily imagine that "tree" is a less neutral word (green leaves), but to what extent is "hood" (little red riding ...) or "sad" (blue) color-neutral.  -->
<!-- <!-- As long as we don't know this precisely for a word, it is not controlled by manipulation, and we can give no default or base line condition. -->
<p>–&gt;
<!-- In multi-level models apply best when factors represent a collection of natural objects \@ref(mlm). --></p>
<!-- ```{r opts.label = "future"} -->
<!-- attach(IPump) -->
<!-- D_Novel %>% -->
<!-- M_log_lrm <- stan_glm(ToT ~ 1 + session, family = gaussian(link = log),data = .) -->
<!-- D_Novel %>% -->
<!-- M_log_cgm <- stan_glm(ToT ~ 1 + Session, family = gaussian(link = log),data = D_Novel, chains = 2) -->
<!-- coef(M_log_cgm) %>%  -->
<!--   mutate(log = exp(center)) -->
<!-- ``` -->
</div>
<div id="ofm" class="section level3" number="4.3.5">
<h3><span class="header-section-number">4.3.5</span> Ordered Factorial Models</h3>
<p>Factors usually are not metric, which would require them to have <em>an order</em> and a unit (like years or number of errors).
Age, for example, has an order and a unit, which is years. With a LRM this makes it possible to say: “<em>per year of age</em>, participants slow down by ….”
The same cannot be said for levels of education.
We could assign these levels the numbers 0, 1 and 2 to express the order, but we cannot assume that going from Low to Middle is the same amount of effective education as going from Middle to High.</p>
<p>One should not use LRM on a non-metric variable, AMM and CGM can be used, but actually they are too weak, because the coefficients do not represent the order.</p>
<p>For level of education, we could use a CGM or AMM, but the graphics and regression models will just order factors alphabetically: High, Low, Middle. Note how I first change the order of levels to over-ride alphabetical ordering. In Figure <a href="lm.html#fig:ofm-1">4.14</a> that Low and Middle are almost on the same level of performance, whereas High education as an advantage of around 30 seconds.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="lm.html#cb362-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(BrowsingAB)</span>
<span id="cb362-2"><a href="lm.html#cb362-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-3"><a href="lm.html#cb362-3" aria-hidden="true" tabindex="-1"></a>BAB1<span class="sc">$</span>Education <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">as.character</span>(BAB1<span class="sc">$</span>Education),</span>
<span id="cb362-4"><a href="lm.html#cb362-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;Low&quot;</span>, <span class="st">&quot;Middle&quot;</span>, <span class="st">&quot;High&quot;</span>)</span>
<span id="cb362-5"><a href="lm.html#cb362-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb362-6"><a href="lm.html#cb362-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-7"><a href="lm.html#cb362-7" aria-hidden="true" tabindex="-1"></a>BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb362-8"><a href="lm.html#cb362-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Education, <span class="at">y =</span> ToT)) <span class="sc">+</span></span>
<span id="cb362-9"><a href="lm.html#cb362-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb362-10"><a href="lm.html#cb362-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">250</span>)</span>
<span id="cb362-11"><a href="lm.html#cb362-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-12"><a href="lm.html#cb362-12" aria-hidden="true" tabindex="-1"></a>BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb362-13"><a href="lm.html#cb362-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Education) <span class="sc">%&gt;%</span></span>
<span id="cb362-14"><a href="lm.html#cb362-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_ToT =</span> <span class="fu">mean</span>(ToT)) <span class="sc">%&gt;%</span></span>
<span id="cb362-15"><a href="lm.html#cb362-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">as.integer</span>(Education), <span class="at">y =</span> mean_ToT)) <span class="sc">+</span></span>
<span id="cb362-16"><a href="lm.html#cb362-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>() <span class="sc">+</span></span>
<span id="cb362-17"><a href="lm.html#cb362-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb362-18"><a href="lm.html#cb362-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">250</span>)</span></code></pre></div>
<div class="figure"><span id="fig:ofm-1"></span>
<img src="Classic_linear_models_files/figure-html/ofm-1-1.png" alt="A boxplot and a step chart showing differences in ToT by level of education" width="50%" /><img src="Classic_linear_models_files/figure-html/ofm-1-2.png" alt="A boxplot and a step chart showing differences in ToT by level of education" width="50%" />
<p class="caption">
Figure 4.14: A boxplot and a step chart showing differences in ToT by level of education
</p>
</div>
<p>Note that R also knows a separate variable type called ordered factors, which only seemingly is useful.
At least, if we run a linear model with an officially ordered factor as predictor, the estimated model will be so unintelligible that I will not attempt to explain it here. Instead, we start by estimating a regular CGM with treatment contrasts (Table <a href="lm.html#tab:ofm-2">4.23</a>).</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="lm.html#cb363-1" aria-hidden="true" tabindex="-1"></a>M_OFM_1 <span class="ot">&lt;-</span></span>
<span id="cb363-2"><a href="lm.html#cb363-2" aria-hidden="true" tabindex="-1"></a>  BAB1 <span class="sc">%&gt;%</span></span>
<span id="cb363-3"><a href="lm.html#cb363-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Education, <span class="at">data =</span> .)</span></code></pre></div>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="lm.html#cb364-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_OFM_1)</span></code></pre></div>
<table>
<caption><span id="tab:ofm-2">Table 4.23: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">105.989</td>
<td align="right">95.2</td>
<td align="right">117.0</td>
</tr>
<tr class="even">
<td align="left">EducationMiddle</td>
<td align="left">EducationMiddle</td>
<td align="right">-0.654</td>
<td align="right">-15.8</td>
<td align="right">14.6</td>
</tr>
<tr class="odd">
<td align="left">EducationHigh</td>
<td align="left">EducationHigh</td>
<td align="right">-34.953</td>
<td align="right">-52.5</td>
<td align="right">-16.8</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<p>A basic ordered factor model is just a CGM where the coefficients are shown in the desired order.
The second and third coefficient carry the respective difference towards level Low.
EducationHigh is <em>not</em> the difference towards EducationMiddle.
In this case it makes sense to understand Middle and High as a smaller step or a larger step up from Low.
It is not always like that.
Sometimes, the only way of moving from the reference group to some other level implies going through all the intermediates, just like walking up a stairway.
Then it makes more sense to use a model where coefficients are individual steps.
In the IPump study, we looked at the speed of learning of a novel interface design by letting the participants repeat a set of tasks in three successive sessions.
here the three sessions make a stairway: going from the fist to the third session always involves the second session.
Before we come to that, we first have to see, why Session must be an ordered factor and not a metric predictor.</p>
<p>The first idea that could come to mind is to take session as a metric predictor and estimate a LRM – there is an ordering and it is the same amount of training per step, which you could call a unit.
The thing with learning processes is that they are curved, more precisely, they gradually move towards an asymptote.
The following curve shows the effect of a hypothetical training over 12 sessions.
What we see is that the steps are getting smaller when training continues.
While the amount of training is the same, the effect on performance declines, which is also called a curve of diminishing returns.
The asymptote of this curve is the <em>maximum performance</em> the participant can reach, which theoretically is only reached in infinity.
The following code defines an exponential learning curve function and renders the example in Figure <a href="lm.html#fig:ofm-3">4.15</a>.</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="lm.html#cb366-1" aria-hidden="true" tabindex="-1"></a>learning_curve <span class="ot">&lt;-</span></span>
<span id="cb366-2"><a href="lm.html#cb366-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(session, amplitude, rate, asymptote) {</span>
<span id="cb366-3"><a href="lm.html#cb366-3" aria-hidden="true" tabindex="-1"></a>    amplitude <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span>rate <span class="sc">*</span> session) <span class="sc">+</span> asymptote</span>
<span id="cb366-4"><a href="lm.html#cb366-4" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb366-5"><a href="lm.html#cb366-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb366-6"><a href="lm.html#cb366-6" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">session =</span> <span class="fu">as.integer</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb366-7"><a href="lm.html#cb366-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">ToT =</span> <span class="fu">learning_curve</span>(session, <span class="dv">10</span>, .<span class="dv">3</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb366-8"><a href="lm.html#cb366-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> session, <span class="at">y =</span> ToT)) <span class="sc">+</span></span>
<span id="cb366-9"><a href="lm.html#cb366-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>() <span class="sc">+</span></span>
<span id="cb366-10"><a href="lm.html#cb366-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>)</span></code></pre></div>
<div class="figure"><span id="fig:ofm-3"></span>
<img src="Classic_linear_models_files/figure-html/ofm-3-1.png" alt="A learning curve" width="90%" />
<p class="caption">
Figure 4.15: A learning curve
</p>
</div>
<p>LRMs can only do straight lines, which means constant effects, whereas learning curves have diminishing effects.
For short learning sequences, we can use ordered factorial models, where every session becomes a level.
As these levels get their own coefficients, the steps no longer have to be constant.
When levels are ordered, the two endpoint levels (first session, last session) can serve as a natural reference group for the intercept.
However, how useful would it be to express the performance in session 3 as differences to reference level (session 1).
It is more natural to think of learning to take place incrementally, like <em>walking up stairways</em> (or down), where the previous step always is your reference (Figure <a href="lm.html#fig:ofm-4">4.16</a>).</p>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="lm.html#cb367-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(IPump)</span>
<span id="cb367-2"><a href="lm.html#cb367-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb367-3"><a href="lm.html#cb367-3" aria-hidden="true" tabindex="-1"></a>D_Novel <span class="sc">%&gt;%</span></span>
<span id="cb367-4"><a href="lm.html#cb367-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Session, session) <span class="sc">%&gt;%</span></span>
<span id="cb367-5"><a href="lm.html#cb367-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_ToT =</span> <span class="fu">mean</span>(ToT)) <span class="sc">%&gt;%</span></span>
<span id="cb367-6"><a href="lm.html#cb367-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">as.integer</span>(Session), <span class="at">y =</span> mean_ToT)) <span class="sc">+</span></span>
<span id="cb367-7"><a href="lm.html#cb367-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>() <span class="sc">+</span></span>
<span id="cb367-8"><a href="lm.html#cb367-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:ofm-4"></span>
<img src="Classic_linear_models_files/figure-html/ofm-4-1.png" alt="Learning to execute a task faster over three sessions." width="90%" />
<p class="caption">
Figure 4.16: Learning to execute a task faster over three sessions.
</p>
</div>
<p>This is what a factorial model with <em>stairways dummy coding</em> does.
The first coefficient <span class="math inline">\(\beta_0\)</span> is the starting point, for example the first session, and all other coefficients ( <span class="math inline">\(\beta_1, \beta_2\)</span>) are a sequence of step sizes.
The expected value <span class="math inline">\(\mu_i\)</span> for session <span class="math inline">\(K\)</span>, using stairways dummies <span class="math inline">\(K_0, K_1, K_2\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
\mu_i = &amp; K_{1i} \beta_{0} &amp;+ \\
&amp;K_{2i} (\beta_{0} + \beta_{1}) &amp;+\\
&amp;K_{2i} (\beta_{0} + \beta_{1} + \beta_{2})
\end{aligned}
\]</span></p>
<p>Thinking of these dummy variables as switches once again: Recall that treatment dummies have an always-on reference level and exclusive switches for the other levels <a href="lm.html#dummy">4.3.2</a>.
Stairways dummies are like a <em>incremental switches</em>: when switch <span class="math inline">\(K\)</span> is on, this implies all previous switches are on, too.
<em>Stairways-down</em> dummies are made as follows:</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="lm.html#cb368-1" aria-hidden="true" tabindex="-1"></a>D_Novel <span class="ot">&lt;-</span></span>
<span id="cb368-2"><a href="lm.html#cb368-2" aria-hidden="true" tabindex="-1"></a>  D_Novel <span class="sc">%&gt;%</span></span>
<span id="cb368-3"><a href="lm.html#cb368-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb368-4"><a href="lm.html#cb368-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Session_1 =</span> <span class="dv">1</span>,</span>
<span id="cb368-5"><a href="lm.html#cb368-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">Step_1 =</span> <span class="fu">as.integer</span>(session <span class="sc">&gt;=</span> <span class="dv">1</span>),</span>
<span id="cb368-6"><a href="lm.html#cb368-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">Step_2 =</span> <span class="fu">as.integer</span>(session <span class="sc">&gt;=</span> <span class="dv">2</span>)</span>
<span id="cb368-7"><a href="lm.html#cb368-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb368-8"><a href="lm.html#cb368-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb368-9"><a href="lm.html#cb368-9" aria-hidden="true" tabindex="-1"></a>D_Novel <span class="sc">%&gt;%</span></span>
<span id="cb368-10"><a href="lm.html#cb368-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">distinct</span>(session, Session_1, Step_1, Step_2) <span class="sc">%&gt;%</span></span>
<span id="cb368-11"><a href="lm.html#cb368-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(session) </span></code></pre></div>
<table>
<caption><span id="tab:ofm-5">Table 4.24: </span>Stairways dummy coding over three sessions</caption>
<thead>
<tr class="header">
<th align="right">session</th>
<th align="right">Session_1</th>
<th align="right">Step_1</th>
<th align="right">Step_2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<p>Now we can run a factorial model using these stairway-down dummies, where the intercept is the upper floor and we are loosing height at every step (Table <a href="lm.html#tab:ofm-6">4.25</a>).</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="lm.html#cb369-1" aria-hidden="true" tabindex="-1"></a>M_OFM_2 <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> Session_1 <span class="sc">+</span> Step_1 <span class="sc">+</span> Step_2,</span>
<span id="cb369-2"><a href="lm.html#cb369-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_Novel</span>
<span id="cb369-3"><a href="lm.html#cb369-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="lm.html#cb370-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_OFM_2)</span></code></pre></div>
<table>
<caption><span id="tab:ofm-6">Table 4.25: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">23.71</td>
<td align="right">21.55</td>
<td align="right">25.896</td>
</tr>
<tr class="even">
<td align="left">Step_1</td>
<td align="left">Step_1</td>
<td align="right">-10.36</td>
<td align="right">-13.31</td>
<td align="right">-7.227</td>
</tr>
<tr class="odd">
<td align="left">Step_2</td>
<td align="left">Step_2</td>
<td align="right">-2.38</td>
<td align="right">-5.39</td>
<td align="right">0.529</td>
</tr>
</tbody>
</table>
<p>The Intercept is the performance in the first level, which is <em>initial performance</em>.
The first step is huge, almost reducing ToT by one half.
The second step is much smaller than the first and tiny compared to initial performance.
We see that high performance can be reached after just a few training sessions.
Clearly, the device is easy to learn.</p>
<p>Another question that arises is what level of performance is reached in the end.
Is <em>maximum performance</em> good enough, actually?
Strictly, this would require a non-linear learning curve model, which would contain an estimate for maximum performance.
With an OFM, the best guess we have is <em>final performance</em>.
Because the second step was already small, we may believe that the asymptote is not so far any more.
And we know that final performance is a conservative estimate for maximum performance.
With a <em>stairway-up model</em> model, the Intercept is the basement an we walk up step-by-step (Table <a href="lm.html#tab:ofm-7">4.26</a>).</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="lm.html#cb371-1" aria-hidden="true" tabindex="-1"></a>D_Novel <span class="ot">&lt;-</span></span>
<span id="cb371-2"><a href="lm.html#cb371-2" aria-hidden="true" tabindex="-1"></a>  D_Novel <span class="sc">%&gt;%</span></span>
<span id="cb371-3"><a href="lm.html#cb371-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb371-4"><a href="lm.html#cb371-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Session_3 =</span> <span class="dv">1</span>,</span>
<span id="cb371-5"><a href="lm.html#cb371-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">Step_1 =</span> <span class="fu">as.integer</span>(session <span class="sc">&lt;=</span> <span class="dv">1</span>),</span>
<span id="cb371-6"><a href="lm.html#cb371-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">Step_2 =</span> <span class="fu">as.integer</span>(session <span class="sc">&lt;=</span> <span class="dv">0</span>)</span>
<span id="cb371-7"><a href="lm.html#cb371-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb371-8"><a href="lm.html#cb371-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb371-9"><a href="lm.html#cb371-9" aria-hidden="true" tabindex="-1"></a>D_Novel <span class="sc">%&gt;%</span></span>
<span id="cb371-10"><a href="lm.html#cb371-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">distinct</span>(session, Session_3, Step_1, Step_2) <span class="sc">%&gt;%</span></span>
<span id="cb371-11"><a href="lm.html#cb371-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(session)) <span class="sc">%&gt;%</span></span>
<span id="cb371-12"><a href="lm.html#cb371-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>()</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">session</th>
<th align="right">Step_1</th>
<th align="right">Step_2</th>
<th align="right">Session_3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="lm.html#cb372-1" aria-hidden="true" tabindex="-1"></a>M_OFM_3 <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(ToT <span class="sc">~</span> Session_3 <span class="sc">+</span> Step_1 <span class="sc">+</span> Step_2,</span>
<span id="cb372-2"><a href="lm.html#cb372-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> D_Novel</span>
<span id="cb372-3"><a href="lm.html#cb372-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="lm.html#cb373-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(M_OFM_3)</span></code></pre></div>
<table>
<caption><span id="tab:ofm-7">Table 4.26: </span>Coefficient estimates with 95% credibility limits</caption>
<thead>
<tr class="header">
<th align="left">parameter</th>
<th align="left">fixef</th>
<th align="right">center</th>
<th align="right">lower</th>
<th align="right">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept</td>
<td align="left">Intercept</td>
<td align="right">10.93</td>
<td align="right">8.879</td>
<td align="right">13.01</td>
</tr>
<tr class="even">
<td align="left">Step_1</td>
<td align="left">Step_1</td>
<td align="right">2.42</td>
<td align="right">-0.717</td>
<td align="right">5.44</td>
</tr>
<tr class="odd">
<td align="left">Step_2</td>
<td align="left">Step_2</td>
<td align="right">10.33</td>
<td align="right">7.254</td>
<td align="right">13.41</td>
</tr>
</tbody>
</table>
<p>The Intercept is an estimate of final performance and we can ask whether this level of efficiency is actually good enough.
From a methodological perspective the results of this study indicate that it might often be worth-while to let participants do multiple session and observe the learning process.
In particular, when users do their tasks routinely with a device, like the nurses, initial performance can be a very poor estimate for long-term performance.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
<hr />
<p>To wrap it up: Factorial models use dummy variables to put factor levels into the linear term of a model.
These dummy variables can be understood as arrays of switches, and we saw how they can be arranged in different patterns:</p>
<ol style="list-style-type: decimal">
<li>Exclusive on-off switches produce an AMM. An AMM is the least specified among factorial models, all levels are equal. Often this is the best choice when the levels were drawn from a population.</li>
<li>One always-on Intercept and exclusive on-off switches produces a CGM with treatment effects. When a default level can be identified, such as the placebo condition in clinical trials, treatment contrasts are a good choice.</li>
<li>Stairway dummies produce an OFM, taking one end-point as first coefficient and stepping up (or down). This is particularly useful for short learning curves.</li>
</ol>
<p>The disadvantage of an OFM is that it cannot predict how performance will improve in <em>future</em> sessions. In <a href="glm.html#learning-curves">7.2.1.2</a> we will see how a Poisson regression model can linearizes a learning curve and can make forecasts.</p>
<p>The AMM deals with a situation where a factor is under-specified to be used with treatment contrasts. It lacks the default level. Ordered factors at the opposite side, being <em>more specific</em> than treatment factors. Treatment factors identify one default level, but the rest are just a set. Quite often, factors also carry a natural order, such as level of education or position in a sequence of tasks.</p>
<!-- Recall that we could specify an AMM with dummy variables or by modifying the model formula (supress the Intercept). For forwards-difference-coding models there is no such way by modifying the model formula. Instead, one can set the factor *contrasts*.   -->
<!-- First of all, how are contrasts set?  Although this is a bit odd, they are neither set by modifications to the regression formula, nor by additional command arguments. Instead, *contrasts are set as attributes of the factor variable*. More specifically, this attribut carries the *contrast coding matrix*, which we can retrieve by the standard command `contrasts`. `contrasts(data$factor_var)` retrieves the current coding and `contrasts(data$factor_var) <- new_coding` sets a new contrast coding.  A set of commands in R and its packages create different contrast codings. Usually, these commands take one argument `n` which is the number of levels.  Difference contrasts can be set using the command *contr.diff* from package Ic.infer.  -->
<!-- ```{r} -->
<!-- fwd_diff  <- MASS::contr.sdif(n = 3) -->
<!-- fwd_diff -->
<!-- contrasts(D_Novel$Session) <- fwd_diff -->
<!-- ``` -->
<!-- This is like the dummy coding matrix we used before, without the Intercept column. The Intercept column can be left out because it always is 1 (on). Now, the factor Session carries the difference coding matrix, and the regression engine will pick it up, silently. The engine does the dummy variable business in the back office and produces meaningful coefficients:  -->
<!-- ```{r opts.label = "mcmc"} -->
<!-- M_OFM_3 <-  -->
<!--   D_Novel %>%  -->
<!--   stan_glm(ToT  ~  1 + Session, data = ., iter = 1000) -->
<!-- ``` -->
<!-- The second and third coefficients of `M_OMF_2` represent the two learning steps. The second step is much smaller, a reduction of more than a third, the small  effect of session 2 almost diminished in comparison. -->
<!-- When we compare the two models -->
<!-- ```{r} -->
<!-- posterior(M_OFM_2) %>%  -->
<!--   coef() -->
<!-- ``` -->
<!-- ```{r} -->
<!-- D_Novel %>%  -->
<!--   group_by(Session) %>%  -->
<!--   summarize(mean(ToT)) -->
<!-- ``` -->
<!-- Before we come to an understanding of what contrasts are, let me point out what they are not: In \@ref(dummy_variables) it was mentioned, that the AGM and CGM make exactly the same predictions. For contrasts, this holds in general. Contrasts are always re-parametrizations of the same model, setting contrasts never changes the predictions $\mu_i$. Contrasts are used to make model coefficients easier to interpret by changing what a coefficient represents. Recall that in a CGM $\beta_1$ has the meaning of "difference to the reference group", whereas in an AMM it is "the mean of the second group".  -->
<!-- If you believe to have seen something similar before, you are right. Contrast tables are related to dummy variables (\@ref(dummy_variables)). The column Low is omitted in the contrasts coding matrix, due to being the reference level with an always-on intercept. In the previous section I mentioned that you can choose the reference level freely by constructing the dummy variables accordingly. However, that would mean to always bypass the regression engines dummy handling. 




<!-- If we were running a CGM on `Education_rev`, the intercept would now represent level High. Again, this model would make the very same predictions and residuals. In that respect, it is the same model as before, only the meanings (and values) of the coefficients change, now being differences to the level High. -->
<!-- For these situations, contrast codings can be customized, such that coefficients can be interpreted more intuitively. Many of contrast codings are known, but in the following I will introduce only two, which I find most useful in design research:  -->
<!-- 1. *Deviation coding*, applies, when the factor levels are just a set and there really is no default. The grand mean becomes the intercept and factor levels are represented as *deviations from the grand mean*. This coding is also called simple coding or sum-to-zero coding. -->
<!-- 1. *Difference coding* applies, when factor levels have a natural order, such as in a sequence of training sessions. Then, it often is more interesting to look at the *step size*, rather than the difference to the a default level. -->
<!-- *Forward difference coding* does by putting the Intercept on the first level and from there on go step-wise. Of course, one could also make a case for *backward difference coding*, where the last session becomes the Intercept. The last session is our closest estimate of the asmptote, which a user approaches with continued training. While this may be interesting to ask, the backwards coding matrix is just the negative of forward. -->
<!-- The results attested, that users acquire the skill of using the novel infusion pump, rapidly. After just one session, they are more than twice as fast. The second learning step is almost negigible, in comparison, suggesting that the users are already closing in on their asymptotes. -->
<!-- In conclusion, one could say that deviation, treatment and difference coding can be ordered by how specific the factor levels are. The most unspecific is deviation coding, which does not even single out a level for the Intercept. Treatment coding requires at least one specific default level, whereas difference coding imposes an order on the levels. Deviation coding, as the least specific of the three will re-appear in chapter [random_effects], when factor levels will represent *members of a sample*.  -->
<!-- #### IPump: The infusion pump study -->
<!-- [TODO:  -->
<!-- + simulate IPump case, non-repeated  -->
<!-- + demonstrate succ diff contr  -->
<!-- + find example for deviation contrasts  -->
<!-- + Contrasts extend the idea of dummy variables.  -->
<!-- + Dummy variables become continuous, thereby more flexible in their effects -->
<!-- ] -->
<!-- #126 Example for deviation coding -->
<!-- ### Sharper on the fly: derived quantities [TBD] -->
<!-- Contrasts are classic in aligning regression estimates with research questions that explore differences. With two groups A and B the following hypotheses of difference can be formed: -->
<!--     A - 0 -->
<!--     B - 0 -->
<!--     A - B -->
<!-- ### Exercises -->
<!-- 1. The `simulate` function in case environment BrowsingAB lets you change the residual error (in standard deviations). Simulate three data sets with different residual variance, and estimate them by the same model. See how the uncertainties of effects behave. -->
<!-- 1. With BrowsingAB, simulate several data sets of very small sample size. Observe how strongly the composition of education and age varies. -->
<!-- 1. BAB1 contains the variable Education, which separates the participants by three education levels (Low, Middle, High). Construct the dummy variables and run an AGM. -->
<!-- 1. Specify the expanded CGM likelihood for education. Construct the dummy variables and run a regression. -->
<!-- 1. Consider you wanted to use education level High as reference group. Create dummy variables accordingly and run the regression. -->
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ebs.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mpm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsubsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
