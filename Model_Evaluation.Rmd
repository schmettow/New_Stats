```{r setup, echo = FALSE, warnings = FALSE, eval = TRUE, message=F}
purp.mcmc = F
purp.rtut = T

source("RMDR.R")

CE <- c("Uncanny", "BrowsingAB", "CUE8", "Hugme", "Stroop_comp", "IPump", "Sec99")
load_CE_(CE)


```


# Working with models

<!--
Linear models are a language. Different to natural language it is unambiguous in its mathematical confinement. Like human language, thought preceeds any meaningful statements. Research with upfront thought is called *theory-driven* research. 



BrowsinAB is partly theory-driven (age), partly *opportunistic* (A,B).

[...]
Theory-driven research is interested in prediction of behaviour based on assumptions made by reason. Reasons can be: [...]


So, prediction models are usually theory-driven. This is not to be confused by models that aim for *theory testing*. In terms of statistical modelling, theory testing always implies at least two models. Often these two differ in just one parameter, that is present in one model but absent in another.

-->


## Model building


### Including predictors
<!--
+ iterative
+ liberal theory-driven inclusion of predictors
+ model driven removal of predictors (pruning)
+ interaction effects
+ rules for random effects


Arabesque statements can be construed with human language and linear models alike. Formally, nothing keeps a researcher from creating a questionnaire on the influence of color schemes on perceived usability, collect 32.537 responses via crowdsourcing and enter all, let's say 10 demographic variables into a linear model, including all interaction effects `PU ~ p1 * p2 * p3 * p4 * p5`. Obviously, this is not theory-driven, as *only theories have predictive value that also deny*.

What the researcher probably had in mind is to develop a *prediction model* for example, to be able to predict the most beneficial color scheme for young adults in South Korea. That is a completely rightous research question, but it appears rather opportunistic, rather than theory-driven. Make no mistake! Not even in theory are prediction models free of theory. The aim of the research question may be opportunistic, or market-driven, but the research itself is far from being a complete shot into the blue. Expecting cultural  differences in color appreciation is quite an assumption. The only difference to might be that really is that theory-driven parameters have an expected direction of effect.

-->

### Choosing a response distribution



### Setting priors

The first chapter of this book linked Bayesian statistics to rational decision making. You recall there was much a do about incorporating prior knowledge. In Bayes' formula, prior knowledge is a central component and was expressed as a probability. If this is so awsome as it sounds, why, you may ask, wasn't there any mentioning of priors in the following chapters. I made a difficult choice, here.

All modelling commands used so far, understand specification of prior knowledge. At the same time, these commands provide useful default priors. The defaults are *non-informative priors*. They are formally specified, but have negligible impact on the posterior distribution. Consider the case BrowsingAB. In [CLM], we estimated the linear slope between age and ToT.

```{r prior_BAB_age}
attach(BrowsingAB)
fixef(M_2)
```

The default prior specification can be found in the manuals of rstanarm (or brms). Or, you use the following command on the model object `M_2`:

```{r}
M_2$prior.info
```

```{r}
detach(BrowsingAB)
```

+ extract default prior from M_2
+ extreme example: guessing a persons IQ, without a measure (100+-15)

People had been using the data to come to conclusions, but they also relied on their own convictions. This sounds subjective and idiosyncratic, even. Is that still research, strictly? In fact, some have labelled Bayesian statistics subjective, and therefore inadmissible, for the concept of priors. What has been overlooked by those is:

1. Frequentist statistics has priors, too. They are just always flat (non-informative).
1. Non-informative priors in Bayesian estimation are routinely used, too.
1. By putting prior knowledge into a formula, it is objective and becomes criticizable. Every reviewer, stakeholder or follow-up researcher can inspect the chosen prior and follow it, or not. When research is published in a reproducable way, people can even reproduce the estimation  using their own priors, when disagreeing. 





### Model convergence

Throughout this book I have fooled you into thinking that the regression engine is something where you put your data and model in and you get some tidy MCMC chains as output, like putting gras into a coow produces milk. MCMC chains are random walks and they are not at all as cows. MCMC chains are like a bulk of young kitten, everyone on their own route, bumping into things or getting stuck in corners. Stan is very good with kitten and safely guides their path through the unknowns of a multidimensional parameter space. Most of the time.

Pathologies of MCMC chains:

1. too few cycles
2. chains plateau for longer periods
3. chains do not arrive at similar distributions
4. chains wipe over tiny gaps in the distribution



+ MCMC convergence
+ eff sample size
+ Tuning the Stan engine


### Model criticism

+ residual analysis
+ linearity 
+ colinearity
+ posterior predictive checks

Gelman, a, Gelman, a, Meng, X.-L., Meng, X.-L., Stern, H., & Stern, H. (1996). Posterior predictive assessment of model fitness via realized discrepancies. Vol.6, No.4. Statistica Sinica, 6(4), 733â€“807. http://doi.org/10.1.1.142.9951







## Model comparison

There are a number of ways to evaluate a model by absolute standards, by butting its predictive ability to test against other data. Collecting more data is not what most people desire. In this chapter we introduce the opposite approach, which is fitting several models to the same data and see which one fits best. 

While model criticism deals with the one model that is being presented to the audience, most of the time, there actually are two, at least. AndOne such situation we have outlined in the beginning: during model building the researcher adds or removes predictors in an iterative way, this actually is a series of models.


+ F-values: fit and parsimony
+ Bayes Factor
+ Cross validation
+ k-fold leave-one-out
+ Information criteria


As we will see in the remainder of this chapter, comparing models relative to each other has useful applications beyond die-hard hypothesis testing. As it turns out, the predictive value of a model depends on two counteracting aspects: the *model fit*, that is how neatly the model aligns with the data and *model parsimony*. Generally spoken, adding a parameter to a model gives you an advantage in fit, but reduces parsimony. As it turns out, models that have the optimal balance between fit and parsimony are better in predicting future observations. The next section introduces a number of criteria to choose the models with better predictive accuracy. The following sections introduce in more depth the modelling strategies where these criteria apply, model pruning, model selection and model averaging.


## Forecasting performance with LOO



<!-- #47 -->


```{r}


do_loo <- 
  function(data,
           f_fit, 
           f_predict = function(fit, obs) predict(fit, newdata = obs)$center[1]
           ) 
    { 
    leave_out <- function(obs) data %>% slice(-obs) # Quosure
    left_out  <- function(obs) data %>% slice(obs)  # Quosure
    f_predict <-  
      
      out <- data_frame(obs     = 1:nrow(data),
                        Sample  = obs %>% map(leave_out), # meta function
                        Obs     = obs %>% map(left_out),
                        Fitted  = Sample %>% map(f_fit)) %>% 
      mutate(forecast = unlist(map2(Fitted, Obs, f_predict)))
    return(out)
  }


```





### Side note on R programming

The `do_loo` function is a remarkable piece of code, thanks to others. Despite its brevity, it is highly generic as it computes a leave-one-out score no matter what model you through at it. *Functional programming* is flexing its muscles.

1. `doo_loo` is a *meta function* as argument `f_fit` is not data, but functional.
1. The two functions that are defined inside `do_loo` are *quosures*. Notice how they make use of `data`, which has been defined one level up in the functional hierarchy. Only seemingly this is just passing on a parameter. These function definintions are invoked everytime the user uses `do_loo`. At that moment `data` has become real, as the user provided it and the function *gets it own copy*. It sounds like putting *inverted object-orientation*. Not the object brings the function, the quosure brings its own data. The advantage is mostly computational as it prevents data to be copied every time it runs.
1. `map` is a meta function from package purrr. It takes a list of objects and applies a function that the user has to provide as an argument.
1. `map2` takes two parallel input lists and applies a function. Here the forecast is created by matching observations with the model they had been excluded from.
1. The function output is created as a *data_frame*. This one differs in that it carries complex objects. It stores every single model that has been estimated, neatly aligned with its matching observation. 
1. LOO runs are long runs. Here, it is extremely to keep a copy of the results on your disk: `save(Loo_1, file = "Loo_1.Rda")`




```{r opts.label = "mcmc"}

Ver20_1 <- Sec99$Ver20 %>% 
  filter(!is.na(age)) %>% 
  select(age, ToT) %>% 
  sample_n(20)


fit_M_1  <-  function(sample) 
  stan_glm(ToT ~    1, family = gaussian(), 
           iter = 400, chain = 1, data = sample)

fit_M_2  <-  function(sample) 
  stan_glm(ToT ~    1 + age, family = gaussian(), 
           iter = 400, chain = 1, data = sample)


Loo_1 <- do_loo(Ver20_1, fit_M_1)
Loo_2 <- do_loo(Ver20_1, fit_M_2)



```

```{r opts.label = "mcsync"}
sync_CE(Sec99, Ver20_1, Loo_1, Loo_2)
```



```{r opts.label = "mcmc"}


Loo_1 %>%
  mutate(ToT = D$ToT,
         error = ToT - forecast) %>% 
  ggplot(aes(x = error)) +
  geom_histogram()

Loo_2 %>%
  mutate(ToT = D$ToT,
         error = ToT - forecast) %>% 
  ggplot(aes(x = error)) +
  geom_histogram()


mse <- function(obs, forecast)
  mean((obs - forecast)^2)

mse(D$ToT, Loo_1$forecast)
mse(D$ToT, Loo_2$forecast)



```





## Scoring functions

The imaginary researchers of the Sec99 could evaluate the predictive performance of their GMM by doing leave-one-ot cross validation. As a score function, a common choice is the mean square error, which is:





## 


### Cross validation and information criteria

Models are all about their predictive ability. Predictive ability seems to imply that the data must speak about the future, which sounds like gathering more data. In fact, it suffices to have just *out-of-samnple data*, not true forecasting.

A very common procedure to assess reliability of a scale is splitting the items into two parts and assess how the average person score on the first part predicts the second. These two parts are often called *training sample* and  *validation sample*. The procedure of *leave-one-out cross validation* generalizes this idea in that:

1. The validation sample is just one observation at a time
2. As many models as there are observations to leave out
3. A suitable cost function determines how well each model has predicted the left-out observation

In theory, every practitioner who does research for decision making on a specific case should construct a specific cost function and use it to evaluate models. This is an ideal and almost nobody ever does it, because it at least expensive, if not impossible. That is why most researchers evaluate their model by generic cost functions, also called *score functions*. 

One in





### Model parsimony

Occam's razor sounds bloody business. In words of Bertrand Russell it says:

>if one can explain a phenomenon without assuming this or that hypothetical entity, there is no ground for assuming it.

Why not? And mind how the speaker carefully avoids to condempt unnecessary hypotheses right away. Occam's razor is sometimes taken as what mathematicians call an axiom, which is an assumption that  cannot have another Why.

One good reason for *parsimony principle* is that people assume things for their own grounds a lot. For scientists, parsimony acts as a counter balance, preventing that things are getting too creative. Unfortunately, that also means less fun, because you weren't right with your hypothesis, or there was not enough data to say.

The other good reason for parsimony is that unnecessary hypotheses inflate the posterior distri 


If you are an experimental scientist Occam's razor applies to you in the following form: 

> ..., there is no ground for assuming it, even if it is your darling


Luckily a statistician barely has to worry about the philosphical issues, but can right away replace "hypothetical entity" with "parameter" and "explain a phenomenon" with "predictive power". That implies that we are comparing two models: the one with the assumption, and the one without. Let's see that on the Stroop data set. We build the model with the assumption that the Stroop effect exists in form of a delay in  the incongruent condition.

```{r opts.label = "mcmc"}
attach(Stroop_comp)

M_0 <-  brm(RT ~    1, family = "exgaussian", data = D_1) 
M_1 <-  brm(RT ~ Cond, family = "exgaussian", data = D_1) 

detach(Stroop_comp)
```




```{r opts.label = "mcsync"}
sync_CE(Stroop_comp, M_1, M_0)
```





### One-sample cross validation






<!--
In the context of design research, it is assumed that statistical modeling is for decision making. In an ideal world, one specifies a utility function for a particular performance indicator, gathers data to obtain more certainty on the indicators strength and ultimately combines posterior probability with utility to make a decision. This book featured many examples of data analysis, where gathered (or simulated) data contained multiple variables. The data was fed into a particular regression model that answers the primary research question, and perhaps also included a few more control variables of lesser interest. 

If one measures two predictors $x_1$, $x_2$ and one outcome variable $y$, formally there exist four potential classic linear models to choose from:

+ `y ~ 1` (grand mean)
+ `y ~ x_1` (main effect 1)
+ `y ~ x_2` (main effect 2)
+ `y ~ x_1 + x_2` (both main effects)
+ `y ~ x_1 * x_2` (both main effects and interaction)

For a data set with three predictors, the set of possible models is already 18. Some models can be discarded by more or less informal reasoning. For example, even though most studies routinely gather the variable gender,  there is often little reason to assume that it makes a difference. 

For a number of reasons, it can be appealing to have more formalized criteria for which model to prefer (and by how much):

1. wanting to prune the model to ease its computation
1. wanting to prune the model to ease its presentation (conciseness)
1. wanting to obtain the best predictions for future values
1. selecting one of two very similar predictors, e.g. two procedures to score results form the same test
1. wanting to test a theory

In fact, model selection can be as easy as computing a single criterion per model and rank models by this criterion. Unfortunately, there appears to be a zoo of model selection criteria, and the question arises which to choose. A closer look reveals, that it actually is a family of criteria. One can best think of it as a dynasty, where there is one omnipotent matriarch. All offsprings admire the matriarch and aim at imitating her, which is sometimes more succesful, sometimes less. However, the matriarch is a diva. Getting her to act in a fruitful way can be a tedious endeavour. While being omnipotent, she is not very willing to answer more than the most simple questions. As an advice seeker with a non-trivial problem, you either go with the potentially imperfect answer of an offspring, or go without any advice at all.

The matriarch/diva's name is *Bayes Factor (BF)*. When comparing two models, BF is the odds of betting on one model or the other. A BF of 4 between two models $M_1$ and $M_2$ means that after seeing the data. It deems four times more likely than $M_2$. As a rational gambler, if an opponent puts 1 EUR on $M_2$, you would go 4 EUR 4 against it. 

BF is defined as:

[tbc]

Computing the BF for common models of moderate complexity is wicked and requires advanced skills. For, complex or exotic models BF is practically impossible.
The divas more accessible offsprings are the so-called *information criteria (IC)*. Basically, information criteria are approximations of BF that are sufficiently accurate under additional assumptions, only. These assumptions typically include certain classes of models, and exclude others. On the upside, information critera are computed in more straight forward manner and routinely reported by MCMC fitting engines. While information criteria are single numbers, they don't have a natural interpretation. Practical use of information criteria is simple, though: when comparing two or more models, every model receives an individual score on the IC and the model with the lowest score wins. 

All model comparison underlies a  primary conflict: cling and parsimony. Cling is a models tendency to neatly align the observed data points. Recall the distinction of observed and predicted values:

$y_i = \hat\mu_i + \hat\epsilon_i$

While you know $y_i$, precisely, the separation of fitted value and residual is result of the fitting and therefore uncertain. The ideal situation from a predictive perspective is that $\epsilon_i = 0$: with no noise in the data, prediction is perfect. Naively, one could now try to find a model such that $\hat\epsilon_i = 0$. 

Good fit between model and data certainly is a desireable property. But, there is a twist: models with many parameters tend to be more flexible. Recall the Uncanny study, where a third-degree polynomial was used to fit the association between human likeness and likeability. The third degree polynomial was chosen because it can bend around two stationary points: the trough and the shoulder. Third degree polynomials *can* do that, but can also take any form of lower degree polynomials. Ultimately, the grand mean model can be written as a third degree polynomials, 

$$y_i(x_i) = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \beta_3x_i^3$$

if we just set \beta_1 = \beta_2 = \beta_3 = 0. As lower degree polynomials are special cases of higher degree polynomials, where some parameters is fixed. Fixing one parameter has as a consequence that the function is less flexible and can describe fewer patterns that occur in observations. A model that is more parsimonous on parameters will always be less clingy. Too much cling is harmfull, though, which is best understood  at an extreme example: 

[Extreme example where parameters match $N_Obs$, it truly is a linear relation with noise]

Using highly oversaturated models is catastrophic for prediction. When the saturated model is asked to predict future observations, it will recover the full noise that was in the data, which is $mu_i + \epsilon_i$. Residuals represents the stochastical component, which by its very nature does not reproduce on the next occasion. Any more complex model reduces deviation from the observed data, but has less predictive value. The AIC accounts for the reduced predictive accuracy by penalizing the number of parameters. 

The oldest of all IC is the *Akaike Information Criterion (AIC)*. Under a Bayesian perspective, it has limited value, but its formula will be instructive to point out how the trade-off between cling and parsimony is solved. The AIC carries one term that increases when the model moves away from the observed data points. Another term increases with the number of parameters. 

$$AIC = log(p(D|\hat\Theta)) + k$$

The AIC contains the well known likelihood, which combines the probability of all individual observations into one probability score. While Bayesians are used to think of many likelihoods, every MCMC run has their own, this likelihood is the one in our parameter space that does best, it is therefore called the *maximum likelihood estimate (MLE)* It is intuitively clear that this term decreases, when the observed values are close to the best prediction of a model.


$AIC$

By two simple counter-acting terms the AIC brings model fit and complexity into balance. As it purely draws principles of maximum likelihood estimation, it turns out both terms are limited to a certain class of models: the likelihood part by definition depends on the data, but prior knowledge does not occur. Under a Bayesian perspective one has to always use flat priors. The likelihood part also contains just the point estimates for parameters. Arguably, these are the maximum likelihood estimates, but this is still much less than the full posterior distribution. The likelihood term forbids to make full use of the Bayesian paradigm; the penality term limits the AIC to a certain class of models, namely those where all parameters are independent of each other. In particular, this is not the case with random effects. A random effect, we recall, is a factor, where the individual means $mu_i$ are assumed to come from a shared distribution $mu_i ~ N(\mu, \sigma)$. In a classic ANOVA, the group means of, say the ToT in two design conditions are allowed to vary  freely. For a random factor, the individual $\mu_i$ determine $\mu and \sigma$, but there is feedback: if most observations happen to be in a close range, hence small $\sigma$,  extreme observations gravitate towards the group mean. The smaller the dispersion, the stronger this pull and the less independent are these  parameters and the less meaningless they get. As this is a process that happens gradually to every parameter, it cannot be solved by removing individual parameters (by model selection).

The *Deviance Information Criterion (DIC)* is a generalization of the *AIC* that solves two of the above issues. With the DIC, priors can be specified [check if there are limitations, for example only normal priors], and the penalty term is adjusted for dependence between parameters. In conequence, the DIC can be used in a Bayesian analysis and covers the important class of (G)LMM. The only remaining issue is that it still draws upon point estimates, the posterior modes.    


-->




### Testing hypothesis

New statistics, as understood in this book, ground on quantification of effects with accompanying levels of uncertainty. 



The prevalent perspective in this book is the quantification of influence factors in the human-design encounter. In applied domains, where stakes and trade-offs are plenty, the most logical question to asked is on effect size, or: would it be worth to act on it? Applied research aims at practice by telling the impact. 

If practice-oriented research is the right hand in applied sciences, the left hand is the body of theories that *explain* phenomena 
in the human-design encounter. And this is what most classical statistics thinking circles around: the idea of evaluating theories by testing hypothesis. Hypothesis tests are not quantitative, they are categorial. Every effect undergoing an hypothesis test can either be accepted as "significant" or be rejected. We may speak of magnitudes and uncertainties to continuous degrees, but testing a hypothesis means to define a hurdle upfront (such as the infamous $\alpha < .05$). Then, either the data jumps over it, or it does not.

Formally, all such hypothesis tests are comparisons of two models, one contains the parameter in question (the alternative model), the other one does not (the null model). The test itself estimates both models and compares them. If the null model wins, the hypothesis is rejected, if the alternative model wins, the hypothesis is accepted and indirectly, its source, the theory, gains credibility.

Recall the uncanny valley phenomenon: over a long range the emotional response to artificial faces gets more positive when faces get more humanlike. But, when the appearance of faces are getting really close to real, it drops -- so the theory.

In chapter \@ref(polynomial_regression) the non-linear relationship between human likeness of robot faces and emotional response was modelled as a 3-deg polynomial. We had taken the uncanny valley at face value, but now it is time to test it. Just by visual inspection of the graph below, it seems equally possible, that the relationship follows a much simpler trajectory, such as a straight regression line ($M_0$). As we know, such a model would have only two parameters, slope and intercept. Formally, that is just a 1-deg polynomial and we can regard it as a special case of the 3-deg polynomial model ($M_1$), where the quadratic and the cubic term are fixed to zero. Fixing a parameter is nothing else but creating a more parsimonous model.

$$
M_1: \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 \\
M_0: \beta_0 + \beta_1 x\\
$$

Should we find, that the more parsimonous model wins the comparison, we had to take a critical stance on Mori's theory of the uncanny valley. We estimate the linear regression model $M_0$ and compare it to the polynomial model:

```{r}
attach(Uncanny)
```



```{r opts.label = "mcmc"}
M_0 <- stan_glm(avg_like ~ huMech, 
               data = UV_1)
```

```{r opts.label = "mcsync"}
sync_CE()
```


```{r}
T_pred <- 
  bind_rows(post_pred(M_0),
            post_pred(M_1)) %>% 
  group_by(Obs, model) %>% 
  summarize(predicted = median(value))


UV_1 %>% 
  ggplot(aes(x = huMech, y = avg_like)) +
  geom_point() +
  geom_line(aes(y = predicted, col = model),
            data = T_pred)

```


```{r}
brms::waic(M_0)
brms::waic(M_1)
```


```{r}
detach(Uncanny)
```





At the same time, a 5-deg polynomial would add considerable more flex to the curvature, possibly  aligning much neatlier to the data


<!--
Using the [XIC] for model comparison, we make a clean decision in favor of M1, justfying the Yerkes-Dodson law. However, the model is, in fact, under-specified. The 2-deg model would also win, when the extremes of arousal were both beneficial. Even die-hard theorists should consult the coefficient table, next to the model comparison. In our case, $\beta_2$ is strongly positive, just the right direction.
-->

<!--### Model pruning

Wanting too much to be true can lead to theory tautology, which is dangerous (Reportedly, tautological theories can be die hard and survive eons.) For statistical models the effect of including too many variables is not dangerous, actually, but it is working against you. 
A model that includes factors of little significance will luckily not give any biased predictions. 

demonstrate by adding fantasy irrelevant variables to the age-by-AB (humpty, deppert, greenwing, ...). Show in a series of effect plots that 

In industrial design research, being able to predict crucial properties of a design reduces economical risks.

*Theory testing* [...] *a prior*

What the researcher on color schemes had not in mind, hopefully, is to mine for effects with certainty of more than 95%, create a coefficient table with only these effects and then, *a posterior*, fabulate a matching theory. In psychology, and probably most social sciences, this has happened and had let to ... [crisis]. [Garden of forking paths]
Researcher who walk the garden of forking paths are producing nothing but *babble*, the lingual equivalent to random noise. 

So, what is the power of models? Two answers:

1. develop theories
1. test theories
1. predict the future


Irrespective of the research purpose, a basic question on the model at hand is:  does it capture all features that stand out from the data, or are their any quirks that let us doubt the model's fidelity. This process is called *model criticism*, and frequently centers around details of lesser theoretical implication, such as: convergence of MCMC estimation, which distribution a variable takes, or whether the chosen priors are realistic. 

Model quality can be approached in two ways: either one focusses just on the model at hand and evaluates its quality against a set of absolute  criteria, or one evaluates the quality of a *model in comparison* to another model, or even a set of models. -->




### Model pruning

<!--Testing hypothesis is the prevalent statistical paradigm in academic research. Typically, a researcher formulates (or re-iterates) on a more or less profound theory on human behaviour and derives predictions that are observable. Two contradicting hypotheses are formulated on these claims. Both hypothesis are formalized as two separate statistical models and these models are ultimately compared to select the better one.

The most basic case is that one hypothesis states a variable to have an influence on the outcome, the alternative hypothesis says, it has not. In order to make a case, we re-iterate on the XY case study. Recall that a classic linear model was used for estimation, and flat priors were applied. In other words: no prior knowledge was applied and the estimation should neatly match any maximum likelihood estimates. In such a simple case, the oldest of all information criteria may be applied, the *Akaike Information Criterion (AIC)*. -->

The uncanny valley observation suggests that participants likeablity judgements follow a third-degree polynomial, as this is the smallest polynomial to allow for the trough and the shoulder left of it. Previously, we have already confirmed that the 3-deg polynomial predicts more accurately than a straight linear regression model. However, it could still be the case that a 2-deg polynomial is sufficient, as it allows for the valley. The cubic term just adds some more flexibility in the curvature. But, is that really required?

```{r}
attach(Uncanny)
```

```{r}
M_2 <- 
  UV_1 %>% 
  mutate(huMech_0 = 1,
         huMech_1 = huMech,
         huMech_2 = huMech^2) %>% 
stan_glm(avg_like ~ huMech_1 + huMech_2, data = .) 
```

```{r}
loo::waic(M_1)
loo::waic(M_2)
```

```{r}
detach(Uncanny)
```



### Choosing response distributions

In hypothesis tests, two (typically nested) models are compared for the purpose of deciding between theories. Model pruning proceeds from a full model and removes predictors for the sake of better certainty in predictions. Both are special cases of model selection. But, with modern evaluation criteria, there is more we can do to arrive at an optimal model.

The new Bayesian model selection criteria LOO and WAIC surpass the classic ones (F-test, AIC and DIC) in that they also allow to compare models with different response distributions. Recall the rather informal comparison of ToT distributions, Gaussian, gamma and exgaussian from \@ref(exgaussian-regression). By visual exploration of residuals, we concluded that the exgaussian is most suited for ToT and RT outcomes. Can we confirm this by formal model selection criteria?

```{r}
attach(CUE8)
```


```{r}
brms::waic(M_4_gau)
brms::waic(M_4_gam)
brms::waic(M_4_exg)
```

```{r}
detach(CUE8)
```



```{r}
attach(Hugme)
```


```{r}
brms::waic(M_1_gau)
brms::waic(M_1_gam)
brms::waic(M_1_exg)
```

```{r}
detach(Hugme)
```



### Model selection

The broadest application of model comparison arises, when a researcher just seeks to identify the one model with optimal predictive accuracy. In contrast to hypothesis testing and model pruning, this can take a fully optimistic route, without being hampered by prior expectations. 

<!-- A carefully pruned model results in optimal prediction using as many variables as is needed. In some cases, though, the researcher wants to select just one exclusive measure from a set of possibilities. Using the same model fit criteria one again, this results in the comparison of two non-nested models.

This situation is routine in research on measurement methods. A measure is an way to quantify a certain property of interest, be it perceived user experience, the general processing speed of a user or how extrovert a person is. In theoretical design research, measures operationalize the quantitative elements of theories. In applied design research, measures are used to evaluate and compare designs and examine differential design effects. Some measures are trivial to assess, such as age. Others require time anf advanced material, such as validated questionnaires or usability tests. Some require expensive equipment. For example, automotive designers use dedicated interfaces to capture measures of driving performance in high fidelity car simulators.

[Case study]

Measurement methods are ordered by three criteria:

1. unbiasedness: how accurately is the property of interest measured?
1. reliability:  does the method possess a good signal-to-noise ratio, providing sufficiently certain estimates?
1. costs: can the method be used with little money and time? Costs can also involve more subtle limitations or concerns, for example, the anatural situation when placed in an fMRI scanner.
-->

### Exercises:

1. When testing the Unvcanny Valley hypothesis, we fixed two parameters at once, the quadratic and the cubic term. While the quadratic term is essential for creating the valley, the cubic term just adds flexibility to the curvature. Make an attempt of model pruning.

```{r opts.label = "invisible"}


```









## Literature

+ MacElreaths statistical distributions
+ XY chapter on prior specification
+ MacElreaths information criteria
+ W&L on Bayes factor and model selection
+ Stan Manual
+ Stefan Raab entertained with the German life television show: quizz boxing. The opponents encountered in several rounds of boxing, each followed by a quizz contest. YD in action!
