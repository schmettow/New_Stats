---
title: "New statistics for design researcher"
author: "Martin Schmettow"
date: "Jan 5, 2015"
output:
  html_document:
    fig_caption: yes
    keep_md: yes
    number_sections: yes
    toc: yes
  pdf_document:
    toc: yes
  word_document: default
---



```{r profile, echo = FALSE, warnings = FALSE, eval = TRUE, message=F}
## The following is for running the script through knitr
thisdir = getwd()
purp.mcmc = F
purp.mcmc = F
source("RMDR.R")
Pglm = list() # plots
Dglm = list() # data sets
Mglm = list() # models
Sglm = list() # simulation functions
Tglm = list() # tables
#load("Generalized_Linear_Models.Rda")
```



```{r mystuff, echo = FALSE, warnings = FALSE, eval = FALSE}
## The following is just for running the script conventionally (no knitr) on the authors computer
```



# Generalized Linear Models

## Gaussian data and linear (aka *Gaussian*) regression

First, we simulate data from the most basic sitation that we measure response time (RT) on sample of participants in just one condition (e.g., a particular design).
ANOVA situation, where one group of participants tested design A and  the other group design B. Whenever we have just one observation per participant, we can short-circuit the eight steps, in starting directly with the observation table.

```{r sim_gaussian_one_condition}
D1 = data.frame(Participant = as.factor(1:100)) %>% 
	mutate(RT = rnorm(100, 300, 50))

D1 %>% 
	ggplot(aes(x = RT)) + 
	geom_histogram()


```

Often, observations are a combination of two factors, for example, participants and conditions. The following produces an ANOVA situation, where every participant encounters two designs A and B. However, there is not (yet) a difference in the designs. Both conditions are simulated from the same distribution:

$$RT ~ Norm(\mu = 300, \sigma = 50$$

Then we try to recover the parameters by running a linear mixed-effects model (LMEM) with Design as predictor for RT, and a subject-level intercept random effect.

```{r sim_gaussian_2Designs}
n.subj = 100

D2 = expand.grid(Participant = as.factor(1:n.subj),
								 Design = c("A", "B")) %>% 
	mutate(RT = rnorm(n.subj * 2, 300, 50))

M102 = MCMCglmm(RT ~ Design, 
	 random = ~ Participant,
	 data = D2)

```

In the next simulation, 100 participants encounter three designs in a within-subject design. In the participant data frame (D3_P), the participant sample is given two properties: 

1. with a chance of $65%$, a participant is female
2. participants vary by some amount how quickly the respond, which is represented by the ramdom effect 

$$\theta ~ Norm(0, 10)$$

A second table (D3_D) represents the three designs. Designs have one property assigned: background color is either white or yellow. Yellow background adds 50s to the response time.  

The observation table D3 first creates an empty grid of all combinations of participant and designs (This is called a _complete_ design). Then the object tables are joined, to gather all predictors in one place. 

Then we compute the expected value $\mu$ by applying the linear combination of coefficients $\beta$ (designs) and  $\theta$ (participants). Finally, observed values are created as

$$RT = Norm(\mu, 50)$$

Finally, we recover $\beta$ with a (LMEM), using the same structure as before. The next logical step, is to run a model with background color as predictor. Note that you should never estimate both at a time, as they are highly correlated. The associated problem is called *multi-colinearity* and can result in weird estimates.

```{r sim_gaussian_3Designs}

Sglm[["DesignABC"]] <- 
  function(n.subj = 50){
    Participants = data.frame(Participant = as.factor(1:n.subj)) %>% 
      mutate(female = as.logical(rbinom(n.subj, 1, 0.65))) %>% 
      mutate(theta = rnorm(n.subj, 0, 10))
    
    Designs = data.frame(Design = c("A", "B", "C")) %>% 
      mutate(bgcolor = c("yellow", "white", "white")) %>% 
      mutate(beta = c(350, 300, 300))
    
    Experiment = expand.grid(Participant = as.factor(1:n.subj),
                             Design = c("A", "B", "C")) %>%
      join(Participants, by = "Participant") %>% 
      join(Designs, by = "Design") %>% 
      mutate(eta = beta + theta) %>% 
      mutate(mu_RT = identity(eta)) %>% 
      mutate(RT = rnorm(n.subj * 3, mu_RT, 50)) %>% 
      mutate(mu_Success = plogis(eta/200)) %>% # logistic 
      mutate(trials = 10) %>% 
      mutate(Success = rbinom(3 * n.subj, trials, mu_Success)) %>% 
      mutate(mu_Errors = exp(eta/200)) %>%  ## expected value mu
      mutate(Errors = rpois(3 * n.subj, mu_Errors)) ## observed variable
      Experiment
  }

Dglm[["DesignABC"]] <- Sglm[["DesignABC"]]()

D3 %>% 
	ggplot(aes(x = RT, fill = Design)) +
	facet_grid(Design ~.) +
	geom_density(alpha = .5)

M103 = MCMCglmm(RT ~ Design,
					random =~ Participant,
					data = D3)
summary(M103)

```

## Binomial data and logistic regression

Now we simulate a data set with the same research design, but the outcome variable is successes in ten tasks. For instance, the designs are three websites and participants are given ten information retrieval tasks.

Because the research design is the same, we can keep the two object data frames. The only issue is that $\beta$ and $\theta$ are on the scale of the linear predictor $\eta$, which is linked to expected values $\mu$ by the *logit link function*:

$$\eta = \log(\mu/(1-\mu))$$ 

Expected values $\mu$ are computed from $\eta$ through the so-called *mean function*, which is the inverse of the logit, the *logistic tranformation*:

$$\mu = \exp(\eta)/{1 + \exp(eta)}$$ 

Note that values of \mu from the Gaussian data set above, are in the range of 300 - 350, which would result in chances of success of virtually 100%. To get more realistic values, we first divide $\eta$ by 100. 

SNIPPET 

from the experiment on Visual EDA:

Logistic regression coefficients are on a logit scale. For the interpretation, we want to back-transform to the original scale, which is: probability the assumption to be rejected. In order to make statements on the probability scale, we backtransform using the inverse function of the logit, the logistic function:
mu_x = exp(eta_x)/(1 + exp(eta_x))
eta in the above formula is called the "linear predictor". The linear predictor is the value you get when you do operations on the coefficients. The most simple application is computing the overall probability for a rejection. This is represented by the Intercept parameter. In the table below, this parameter accounts to practically zero, on the logit scale. Back-transforming to the probability scale gives exactly a chance of 50%:
exp(0)/(1 + exp(0)) = 1/2
 So, an intercept of zero always means that chances are equal, which is the same as saying: the coefficient does not add any information. The same is true for parameters: the closer a coefficient is to zero, the less does it influence prediction of reject. Like with general linear models, parameters close to zero have "no effect" and can even be removed to simplify a model (we don't do that here).
Note, that the intercept in our model represents the situation that skew is 0 and N = 0. A more useful baseline rate for rejection rate would be at skew = 0 and the smallest sample size N = 10.
We can compute any prediction we want by combining the parameters with the predictors. But, the other coefficients in the model represent differences, not absolute values. Therefore, when predicting a certain event, let's say rejection at a skew = 0 and N = 50, we first have to compute the linear predictor eta, then transform to get predicted value mu. 
eta = 0.042 + Skew * 0 + Sampel size * 10
mu = logist(eta)
To give another example, the chance of rejection at skew = .5 and N = 50 is computed as:
eta = 0.042 + Skew * 0.5 + Sampel size * 50
mu = logist(eta)
Watch out! It is a severe mistake to transform coefficients individually, then do the linear combination.
eta = logist(Skew * 0.5) + logist(Sampel size * 50)
In any case, it is much preferred to report the estimates on the probability scale, as this is right-away interpretable. Credibility limits can be reported on the linear predictor scale. 


Group effects indicate how homogeneous the overall pattern (Table 1) is on individual units in the data set. Relevant units here are the participant and stimuli. For exanple, the relativels large coefficient Skew means that people differ a lot in how much their response is influenced by skew (which is desireable): The strong effect on stimuli means that (after taking skew and N into account) stimuli differ a lot in which response they provoke. It sounds a bit paradox, but that means that stimuli differ systematically: there exist properties other then Skew and N, that trigger a response.

SNIPPET

```{r binomial}
n.subj = 50
D4 = expand.grid(Participant = as.factor(1:n.subj),
								 Design = c("A", "B", "C")) %>%
	join(D3_P, by = "Participant") %>% 
	join(D3_D, by = "Design") %>%
	mutate(eta = (theta + beta)/100) %>%   ## linear predictor eta
	mutate(mu = plogis(eta)) %>%  ## expected value mu
	mutate(trials = 10) %>% 
	mutate(Successes10 = rbinom(3 * n.subj, trials, mu)) ## observed variable

## Distribution of linear predictor eta
D4 %>% 
	ggplot(aes(x = eta, fill = Design)) +
	geom_density(alpha = .5) +
	facet_grid(Design~.)

## Distribution of expected value mu (or p)
D4 %>% 
	ggplot(aes(x = mu, fill = Design)) +
	geom_density(alpha = .5) +
	facet_grid(Design~.) +
	xlim(0,1)

## Distribution of (simulated) observed values
D4 %>% 
	ggplot(aes(x = Successes10, fill = Design)) +
	geom_histogram(alpha = .5) +
	facet_grid(Design~.)

M104 = MCMCglmm(cbind(Successes10,trials) ~ Design,
					family = "multinomial2",
					random = ~ Participant,
					data = D4)

summary(M104)

data.frame(summary(M104)$solution) %>% 
	add_rownames() %>% 
	mutate(mu_hat = plogis(post.mean))
```

## Count data and Poisson regression

Count measures are bounded to the below. Unlike the normal distribution, the minimal count is 0. Theoretically, there is no upper bound for counts, but larger counts just get more and more unlikely.

The Poisson distribution describes count data. Different to the successes-on-trials, these counts have no natural upper limit. The distribution is left-skewed. The smaller the measured values are, the stronger the skew. Vice versa, when the average count gets larger, the distribution becomes more symmetric. For this reason, it may be practical to use the classic linear model, with normally distributed residuals (see [CLM]).

Consider a study that let's participants seek a piece of information on a governmental website. As a measure of efficiency, the number of steps are counted that takes users to find the information. Two websites are compared, website A with a very broad navigation structure (the home page presents 16 categories and the navigation structure has just two levels). Website B has a very deep information structure, presenting information in a five-level deep taxonomy.



A common misconception is that the RD of a measure approaches normal distribution with larger sample sizes. That is not true. Increasing the number of participants just results in a cleaner shape of the distribution, with less ruggedness. Examine yourself, how the shape of Poisson distributions changes by mean count, as well as sample size. The following code simulates a Poisson distribution with a given $\lambda$ and $N$. With every new run of the code, `rpois` generates a new random set of counts. By repeatedly running this code and watching the output, you can get a good idea of how varied the shape of the distribution can be.

```{r fig:exercise_poisson_dist}
data_frame(count = rpois(n = 20,
                         lambda = 2)) %>% 
  ggplot(aes(x = count)) + 
  geom_histogram()

```



```{r fig:poisson_dist}
expand.grid(lambda = c(2, 5, 10), N = 10, 50, 100) %>% 
  as_data_frame() %>% 
  mutate()
ddply()
```




Poisson regression, ...

Therefore, the link function has to transform

$$[0;\infty] \to [-\infty, \infty]$$

Accordingly, the *exponential* serves as link function, and the *logarithm* is the mean function. Note that in order to get realistic number of errors, $\eta$ has been set to a 20th of the Gaussian variable from D3. 


```{r poisson}
D5 = expand.grid(Participant = as.factor(1:n.subj),
								 Design = c("A", "B", "C")) %>%
	join(D3_P, by = "Participant") %>% 
	join(D3_D, by = "Design") %>%
	mutate(eta = (theta + beta)/20) %>%   ## linear predictor eta
	mutate(mu = exp(eta)) %>%  ## expected value mu
	mutate(trials = 10) %>% 
	mutate(Errors = rpois(3 * n.subj, mu)) ## observed variable

## Distribution of linear predictor eta
D5 %>% 
	ggplot(aes(x = eta, fill = Design)) +
	geom_density(alpha = .5) +
	facet_grid(Design~.)

## Distribution of expected value mu (or p)
D5 %>% 
	ggplot(aes(x = mu, fill = Design)) +
	geom_density(alpha = .5) +
	facet_grid(Design~.)

## Distribution of (simulated) observed values
D5 %>% 
	ggplot(aes(x = Errors, fill = Design)) +
	geom_histogram(alpha = .5) +
	facet_grid(Design~.)

M105 = MCMCglmm(Errors ~ Design,
					family = "poisson",
					random = ~ Participant,
					data = D5)

summary(M105)

data.frame(summary(M105)$solution) %>% 
	add_rownames() %>% 
	mutate(mu_hat = plogis(post.mean))
```



