---
title: "New statistics for the design researcher"
author: "Martin Schmettow"
date: "March 13, 2017"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    theme: journal
    toc: yes
  word_document:
    toc: yes
bibliography: Bayesian_Stats.bib
---

<style type="text/css">
.table {
     width: auto;

}
</style>



```{r setup, 	message = FALSE,	warning = FALSE,	include = FALSE}
## The following is for running the script through knitr
purp.mcmc = F
source("RMDR.R")
# library(printr)1
CE <- c("IPump")
load_CE(CE)

#formals(stan_glm)$iter <- 200
#formals(stan_glm)$chains <- 1

```


# Generalized Linear Models

Have you wondered about the abundance of simulated data sets up to this point? One answer is that after I had figured out a general approach to simulation (see [SIM]), i just could not stop doing it. The real answer is: the linear model makes assumptions that are *never* met by real data. For some data the linear model is a reasonable approximation, but this occurs less often than you might think. 

In this next section we will revise the linear model assumption and lead them ad absurdum. At this point the apt reader may ask another critical question: why does chapter [LM] not account for linear model assumptions, and how to test them. Simply put, checking assumptions on model that you know is inapproriate, I regard a futile exercise. This perspective holds at least, when better alternatives are available, and that is the whole point of this chapter. 

The GzLM framework rests on two extensions that bring us a huge step closer to the data.  The first one is a minor mathematical tweak, the *link function*. The second is the informed choice from a set of statistical distributions and this will open a whole new world for you. Follow up on this path and discover the many sources of randomness.

In the preceding chapters we got acquainted with the linear model as an extremely flexible tool to represent dependencies between a bunch of predictors and an outcome variable. We saw how factors and covariates gracefully work together and how complex research designs can be captured by multiple crossed random effects. It was all about specifying appropriate (and often sophisticated) right-hand sides formula, the predictors term. Little space has been dedicated to the outcome variables. That is now going to change, and we will start by examining the assumptions that are associated with the outcome variable.

In the following three sections I will explain the three core assumptions of linear models, recall the canonical model formulation:

$$μ_i=β_0+ β_1 x_1i+ …+β_k x_ki\\
y_i \sim Norm(μ_i,σ)$$

The first term, we called the likelihood. It represents the quantitative relations we expect to find in the data. When it is a sum of products, like above, we call it linear. *Linearity* is the first assumption that fails. The second term represents the source of randomness and it hosts two further assumptions: *normal distribution* of errors and *constant error variance*. 

Some classic textbooks tend to present these assumptions as preconditions for a successful ANOVA or linear regression. As in pre-condition, they need to be checked upfront using a zoo of tests. If one fails, let's say the Kolmogorov-Smirnoff test on normality, one better considers non-parametric tests or a down-tempering statement.

Here, I will not present any tests on assumption, other than exploratory plots. Instead, I will right-away debunk all three assumptions for measures that design researchers are routinely dealing with. The three debunks take the LM formula  literally by simulating data from it. The *simulations* will raise severe skepticism as they do not look like real data. An idea as equally simple and immensely useful as the liner model can expected to be a tough survivor. Less useful ideas have outlived their expectation for decades and centuries. Therefore, in this chapter, *real data* will speak for itself, the infusion pump case study (*Ipump*).  After all, the poor fidelity of linear models, aka the  *Gaussian GzLM* is the real reason why the previous chapters made regular use of simulated, but not real, data.

Every assumption that fails, rebounces from one of two building blocks that GzLM add to our regression framework:

1. *link functions* re-establish linearity
2. *random distributions* cover randomness of "real" outcome variables

After a full debunk, the *GzLM* framework will rise from the ashes of *LM*. It hosts a variety of models that leaves little reason for crude approximations of the linear model, aka the *Gaussian LM*. The *Poisson LM* is the logical choice for outcome variables that are counted with no limit, like number of errors.  *Binomial regression* covers the case of succesful task completion, where counts have an upper boundary. These two GzLM members have been around for more than half a century in statistics. The quest for a good model for time-on-task was more difficult as the seemingly good candidates disappointed at first. Luckily, the with recent developments in Bayesian regression engines the choice of random distributions has become much broader. For ToT, I here suggest the exponentially-modified Gaussian *(ExGauss) LM*. Also for analogue rating scales will I present a relative novelty, the *Beta LM*.

## Case: user testing infusion pumps

Medical infusion pumps are unsuspicious looking devices that  are en-mass installed in surgery and intensive care. Their only purpose is controlled injection of medication in the blood stream. Pumps are rather simple devices as infusion is a function of volumen and time, only. They are routinely used by trained stuff, anaesthesiologists and  nurses. We should have great faith in safe operation under such conditions. The truth is, medical infusion pumps have reportedly killed dozens of people, thousands were harmed and an unknown number of nurses lost their jobs. The past generation of pumps is cursed with a hilarious set of completely unnecessary design mistakes:

* tiny 3-row LCD displays
* flimpsy foil buttons without haptic marking or feedback
* modes
* information hidden in menus

For fixing these issues no additional research is needed, as the problems and there fixes are pretty obvious to experienced usability specialists. What needs to be done, though, is proper validation testing of existing and novel interfaces, for example:

* is the interface safe to use?
* is it efficient to learn?
* is a novel interface better than a legacy design? And by how much?

We conducted such a study. 

[DESCRIBE STUDY]

## Debunking the Gaussian linear model

### Assuming linearity

Recall the principle, "all-finite-in-the-endless": when two or more interventions improve the same process, e.g. visual recognition of letters, the sum is less than the summands. This results in a non-linearity when the boundary of performance is reached. With a small set of predictors this can gracefully be modelled as saturation interaction effects.

Consider a study that assesses the improvement of safe operation with continued practice. For simplicity, we regard just a single nurse whose number of errors were measured on a chain of 8 tasks. Errors in operation were measured as number of deviations from the shortest possible interaction sequence. 

We simulate a linear model, assuming there is an improvement of one error less with every repetition of the sequence, with expected 7 deviations in the first session. We assume that path deviations has a normally distributed random component with $\sigma = 1$. 

```{r}
data_frame(session = as.integer(1:5),
           mu = 9 - session * 2.5,
           deviations = rnorm(5, mu, sd = 1)) %>% 
  ggplot(aes(x = session, 
             y = deviations)) +
  geom_point() +
  geom_abline(aes(intercept = 9, slope = -2.5))


```

See what happens when the linear model is naivly applied to the deviation counts. In no time, negative values are drawn, which is impossible. Similar to saturation effects, we would expect some asymptotic behaviour, rather than a straight line, when the outcome variable approaches its natural lower boundary. 

Here is some real world data from the IPump study. We take a look at a small slice of it: the total number of deviations of one participant across the three sessions.



```{r}

attach(IPump)

D_pumps %>% 
  filter(Part == nurse, Design == "Novel") %>% 
  group_by(Part, session) %>% 
  summarize(deviations = sum(deviations)) %>% 
  ggplot(aes(x = session, y = deviations)) +
  geom_point() +
  geom_line() + 
  ylim(0,7)

detach(IPump)
```

What really happens, when a performance measure approaches its natural limit, is an asymptotic leaning-on. Neither will the line break through the limit, nor will it stop there abruptly.

Such a non-linearity happens to all outcome variables that have natural lower or upper boundaries, and that includes all outcome variables in the universe, except its very own spatial extension. All outcome variables in design research suffer from the problem of impossible predictions as a consequence of their boundedness:

* Errors and other countable incidences are bound at zero
* ToT is bounded at zero and probably also bound above when users loose their patience
* Rating scales are bound at the lower and upper extreme item
* Task completion has a lower bound of zero and upper bound is the number of tasks. Or the average task completion is in the range $[0,1]$






### Assumping normal distribution


The second term of a linear model, $y_i \sim Norm(\mu_i, \sigma)$ states that the observed values are drawn from normal distributions. Note that the plural  (distributions) is by intention. Two observed values $y_i$ and $y_j$ are only drawn from the same distribution, when they have the same expected value $\mu_i = \mu_j$. Normal distribution sounds like this were the regular case. In fact, the normal distribution is a reasonable approximation in a limited number of situations. 

Consider a study comparing a novel and a legacy interface design for medical infusion pumps. The researchers let trained nurses perform a single task on both devices and count the errors. Assuming, the average number of errors per tasks is $\mu_L = 3$ for the legacy device and $\mu_N = 1.2$ for the novel device, with standard deviation of $\sigma = .8$. We can simulate a basic data set as:

```{r opts.label = "rtut"}
N = 80
pumps_2 <- 
  data_frame(Design = rep(c("L", "N"), N/2),
             mu = if_else(Design == "L", 3, 1.2),
             errors = rnorm(N, mu, sd = 1))
```

We illustrate the data set using histograms:

```{r opts.label = "rtut"}
pumps_2 %>% 
  ggplot(aes(x = errors)) +
  facet_grid(~Design) +
  geom_histogram(bins = 20) +
  geom_vline(col = "red", xintercept = 0) +
  coord_flip()
```

When instantiated by simulation, the linear model looses its unsuspicious flare (just like medical infusion pumps themselves). A substantial number of simulated observations is *negative*, which strictly makes no sense for error counts. The pragmatic and impatient reader may suggest to adjust the standard deviation (or move the averages up) to make negative values less unlikely. That would be a poor solution for the following two reasons. First, the normal distribution supports the full range of real numbers. There is always a chance of negative simulations, as tiny as it may be. Repeatedly hitting keys until `pumps` contains exclusively positive numbers (and zero), obviously is poor practice. The second reason is that the simulations very purpose was to express and explore expectations from the linear model (CG). We can simply conclude that any model that assumes normally distributed errors must be wrong when the outcome is bounded below or above, which means: always.

Recall how linearity is gradually bended when a magnitude approaches its natural limit. A similar effect occurs for distributions. Distributions that respect a lower or upper limit tend to get squeezed like chewing gum into a corner. And that is true for the distribution of real data as well as the theoretical distributions that are truly edaquate to present the data. Only the normal distribution stoically retains its symmetry in all infinity. The figure below shows the distribution of 



### Assuming constant variance





## Concepts of Generalized Linear Models

GzLM is a *framework for modeling* that produces a *family of models*. Every member of this family uses a specific link functions and chooses a particular random distribution. Sometimes GzLM are confused as a way to relax assumptions of linear models, (or even called non-parametric).  They absolutely are not! Every member of its own makes precise assumptions  on the level of measurement and the shape of randomness (see Table A). One can even argue that Poisson, Binomial and exponential regression are stricter than Gaussian, as they use only one parameter, with the consequence of an even  tighter association between variance and expected value. A few members of GzLM are classic: Poisson, Binomial (aks logistic) and exponential regression have routinely been used before they were united  under the hood of GzLM. These and a few others are called *canonical* GzLM, as they possess some convenient mathematical properties, that made efficient estimation possible, back in the days of expensive computer time. 


### Re-linking linearity

The strength of the linear term (the likelihood) is its endless versatility in specifying relations between predictor variables and outcome. Unfortunately, it represents all associations as straight lines. These lines extend from $-\infty$ to $\infty$ and will cross the lower or upper boundaries of every known outcome variable. The model could predict what cannot happen.

Generalized linear models use a simple mathematical trick to keep the linear term, but confine the expected values. We cannot just cut it off, apparently. But it can be bended. In arithmetics an abundance of functions exists for every possible purpose. The  *exponential* function supports the whole range of real numbers and bends it up into the non-negative range. Its inverse, the *logarithm* does the opposite: It restretches strictly positive numbers to the real range.

Other variables, like success rates or analog rating scales, have two boundaries. A function that bends the real range into $[0;1]$ is the *logistic* function. The two-sided bending makes its characteristic sigmoid shape. Again there is an inverse function, the log_odds, or *logit*. 


It could translate a strictly positive $\mu$

```{r, opts.label = "fig.wide"}
data_frame(x = seq(-3, 3, .01)) %>% 
  ggplot(aes(x)) +
  stat_function(fun = exp, aes(color = "exponential"))

data_frame(x = seq(-3, 3, .01)) %>% 
  ggplot(aes(x)) +
  stat_function(fun = inv_logit, aes(color = "logistic"))
```


What is further required for such a function is 











Using the link function comes at a cost: the linear coefficients $\beta_i$ no longer  have a natural interpretation, like "movin one step on the predictor, let the outcome change by $\beta_i$". The linear link is now strictly with the linear predictor $\theta_i$. Still, we will see that other, only slightly less intuitive statements can 
be made for link-transformed linear models.

Who needs a well-defined link between observations and expected values? Applied design researchers do when predictions over time is their business. Reconsider Andrew & Jane in [WIS], who predicted the population mean of ToT from a sample. This is a statement on the status quo, and therefore it is not a prediction, in a strict sense. Comparing designs A and B is status quo, too, as both designs do exist. Generally, we  classify all research questions as *status quo* that stay confined in the range of $y_i$. In the IPump study it is compelling to ask: "how will the nurse perform in session 4?". "When will she reach error-free operation?". These are predictive questions, where the question reaches into the unknown.   

With Bayesian regression engines we can compute a best guess for observed ranges and factor levels, but we can also simulate in the void. We estimate a linear model on learning 

```{r stanglm_predict}
attach(IPump)
M_1 <- 
  

```


```{r predicting_the_impossibe}
detach(IPump)
```




Some researchers apply transformations to their variables, to make the meet the assumptions. 


### Fun with random distributions 



## Modelling counts

### Poisson regression

### Binomial regression

### Overdispersion

Poisson and binomial regression differ from the well-known Gaussian model in that they only have one parameter. Consider, location and variance are two completely independent choices for the Gaussian distribution. For Variables with Binomial or Poisson distribution, only one parameter is estimated and that determines the location. The subtle consequence is that the variance of any such distributions is strictly tied to the mean. In fact, for Poisson distributions the variance *is* the mean:

$x \sim Pois(\lambda) \implies \mu = \sigma^2 = \lambda$

The variance of binomial variables is:

$x \sim Bin(p, k) \implies \mu = k\mu(1-\mu) = p$, where k is the number of trials.

The strict variance assumptions of Poisson, binomial and exponential models are frequently violated by real data. The violation happens when real impact factors have not been included in the likelhood equation. Whenever there is one or more imoact factors on the outcome at question that the researcher has not regarded, overdispersion inevitably happens. That means it always happens in studies other than purified physics experiments. 

Using Poisson regression for overdispersed  count data results in inflated certainty of estimates. When strictly inferential statistics is done, this must be accounted for. A classic way to regard overdispersion is to choose yet another random distríbution that adds a scale parameter for randomness variation. For Poisson regression that is the negative binomial distribution (never mind its name!), for binomial the beta-binomial and for exponential it is the gamma distribution. These are respected members of the broader GzLM family and two of them, negative binomial and gamma, are even canonical. Negative binomial and gamma distributions introduce a scale parameter for variance. They keep the basic relation between $\mu$ and variance intact, but 

With the broad implementation of random effects models in Bayesian regression engines, there is a generic procedure to capture the extra variance without extending the family any further. The trick is to introduce an *observation-level random effect*. Recall how we regard variation between members of a population as normally distributed deviations from the population mean, by the example of a Poisson grand mean model with a participant-level ($p$) random effect:

$$
\theta_{ij} = \beta_0 + \beta_{0j} \\
\mu_ij = \exp(\theta_{ij})\\
\beta_{0j} \sim N(\mu_{p}, \sigma_p)\\
y_{ij} \sim \textrm{Pois}(\mu_{ij})$$

The random effect does not cause any trouble with predictions, as it is added on the level of the linear predictor, before the expected value is obtained by the exponential transformation. It is very likely, that the above model contains unexplained variance. Observation-level random effects are additive deviations $\beta_i$ on the linear predictor, in a Poisson grand mean model:

$$
\theta_{i} = \beta_0 + \beta_{i} \\
\mu_i = \exp(\theta_i)\\
y_{ij} \sim \textrm{Pois}(\mu_{ij})$$

See, how $\beta_i$ is a unique deviation per observation $i$, and how a variance parameter $\sigma$ appears in an otherwise purely Poisson model. Observation-level random effects are on the linear predictor level, and therefore additive! The negative binomial distribution adds a scale parameter, which is multiplicative. We find a resemblance with how sums on the linear predictor become multiplications on the expected values scale.

For demonstration, we simulate from an overdispersed Poisson grand mean model with participant-level random effects, and recover it via regression. 

```{r sim:overdisp, opts.label = "sim"}

Part <- data_frame(Part = 1:100,
                   beta_0p = rnorm(100, 0, .75)) ## participant-level RE

D <- data_frame(Obs  = 1:1000,
                Part = rep(1:100, 10),
                beta_0i = rnorm(1000, 0, .5),    ## observeration-level RE
                beta_0 = .8) %>%
  left_join(Part) %>% 
  mutate(theta_i = beta_0 + beta_0p + beta_0i,
         mu_i    = exp(theta_i),
         y_i     = rpois(1000, mu_i))
```

```{r}
M <- D %>% 
  stan_glmer(y_i ~ 1 + (1|Part) + (1|Obs), data = .,
             family = poisson)

```

```{r}
summary(M)
```


At the beginning of this section, I opted for observation-level random effects for  parsimony. However, there is a practical reason, too.  I frequently reminded the reader to interpret parameters quantitatively, by translating their magnitude to practical relevance. For variance parameters this is notoriously difficult to do. What can be done ease and has sufficient practical utility, is to compare variance of random effects relative to each other. With observation-level random effects, the unexplained variance is on the same scale as all the other ramndom effects, which makes them fully comparable. 


### Zero inflation


## Modelling time

### Exponential regression (and how it fails)

### ExGaussian regression


## Modelling rating scales

### Gaussian regression

The classic linear model we have mercilessly debunked at the beginning of the chapter, is a member of the GzLM framework, with some rather unusual properties. It assumes normally distributed randomness, which implies a constant variance. In contrast to all other members so far, there is no relation between mean and variance of randomness. The link function simply is *identity* and is consequential for the problem of linearity and bounded ranges. The general take is that, whenever there is a theoretically justified and reasonably pragmatic choice, one better leaves the Gaussian alone. Here it makes its re-entrance for a type of outcome measures that are so problematic that a pragmatic choice is indicated.


### Beta regression







## EATME


Consider a very basic linear regression
The linear model that has the form:


Eq.1
where y_i are the observed outcome values (e.g. response times), μ_i are the predicted values. With classic linear models it is assumed that y_i are normally distributed, with the predicted value μ_i as mean and standard deviation σ. Observation y_i  are drawn from normal distributions that have a varying mean but an equal spread throughout.  and a normally distributed error term with constant variance. These assumptions usually do not hold for usability performance measures. For example, counting errors can never take negative values. Count variables are often have severely right-skewed residual distributions [27] that increase with predicted values. As another example, task completion rates are even bounded below (zero successes) and above (number of tasks). Such variables typically are left-skewed, when approaching the lower bound, but right skewed near the upper bound. Moreover, a classic linear model fits the data by straight lines. This also holds for the prediction a linear model makes. The regression line extends between \-∞ and ∞, which can easily lead to impossible predictions for response times and other bounded measures.
Generalized Linear Models (GzLM) generalize linear models to allow for a wider range of outcome variable types. GzLM is a family of models, each member being a specialist for certain typical variable types. Three of the best known are Poisson, logistic and Gaussian regression. Poisson regression applies for count variables that theoretically have no upper limit, such as error counts. Logistic regression deals with count data that has an upper limit, such as number of successes with a given set of tasks. Gaussian regression is just the classic linear model with normally distributed residuals. All members work with their own assumption of how the measures are distributed (Poisson, binomial or Gaussian). In addition, linearity is established, preventing that impossible predictions can be made. This comes at some costs: the coefficients β_(i..k)  can be used in a linear manner, as usual, but that does no longer give the predicted value, directly, but the linear predictor η_i, which hardly has a natural interpretation. For making statements on the original scale, every GzLM  member provides their own transformation function. Throughout the reporting of results, we demonstrate the transformation and derive quantitative statements on the natural scale.



## Generalizing the concepts


The framework of Generalized Linear Models (GzLM) extends linear models by providing choices for three aspects of the outcome variable and its relation with predictors: link function, residual distribution and variance structure. In GzLM, linearity is re-established by separating the linear combination of predictors (linear term) from the predicted value μ. The linear term is tied to the linear predictor η_i that always supports the range [-∞;∞]. While this satisfies the linearity assumption, η does not have a natural interpretation. Instead, η is linked to predicted values μ by a bijective, strictly monotonous transformation, the mean function. For example, when dealing with count data, where μ has zero as lower bound, one uses Poisson regression with the exponential mean function:
μ_i=expa〖η_i 〗
η_i=β_0+ β_1 x_1i+ …+β_k x_ki	Eq.4

The inverse of the mean function, called the link function and is often referred to when characterizing a GzLM family member. In case of Poisson regression this is the logarithm, as this reverses the exponential function.
Furthermore, family members provide a residual distributions that are appropriate for the outcome variable at hand. Common choices are: Poisson distribution for counts, Binomial for successes at a number of trials, exponential distribution for waiting times and, last but not least, the normal distribution. Finally, a GzLM member defines a variance structures, which refers to the association between expected value and variance. For example, with measures of time and counts it typically occurs that variance increases by expected value. In contrast, the (Gaussian) linear model assumes that variance is constant.
Table A Members of the family of generalized linear models
Model	Range	Residuals	Variance	Link function	Mean function	Measures
Gaussian	[-∞;∞]	Normal	σ^2=c
constant	η=μ
identity	μ=η
Identity	rating scales
Poisson	[0;∞]	Poisson	σ^2=μ
proportional	η=lnaμ
logarithm	μ=e^η
exponential	sequence length,
path deviations 
Logistic	[0;k]	Binomial	μ(k-μ)/k
cigar shaped	η=lna〖μ⁄(k-μ)〗
logit
	μ=k(e^η⁄(1+e^η ))
logistic	task completion
Exponential	[0;∞]	Exponential	σ^2=μ^2
Squared	η=lnaμ
Logarithm	μ=e^(-η)
exponential	completion time
μ: predicted value; η: linear predictor; σ^2: variance; k: number of trials




## Gaussian data and linear (aka *Gaussian*) regression

First, we simulate data from the most basic sitation that we measure response time (RT) on sample of participants in just one condition (e.g., a particular design).
ANOVA situation, where one group of participants tested design A and  the other group design B. Whenever we have just one observation per participant, we can short-circuit the eight steps, in starting directly with the observation table.

```{r sim_gaussian_one_condition}
D1 = data.frame(Participant = as.factor(1:100)) %>% 
	mutate(RT = rnorm(100, 300, 50))

D1 %>% 
	ggplot(aes(x = RT)) + 
	geom_histogram()


```

Often, observations are a combination of two factors, for example, participants and conditions. The following produces an ANOVA situation, where every participant encounters two designs A and B. However, there is not (yet) a difference in the designs. Both conditions are simulated from the same distribution:

$$RT ~ Norm(\mu = 300, \sigma = 50$$

Then we try to recover the parameters by running a linear mixed-effects model (LMEM) with Design as predictor for RT, and a subject-level intercept random effect.

```{r sim_gaussian_2Designs}
n.subj = 100

D2 = expand.grid(Participant = as.factor(1:n.subj),
								 Design = c("A", "B")) %>% 
	mutate(RT = rnorm(n.subj * 2, 300, 50))

M102 = MCMCglmm(RT ~ Design, 
	 random = ~ Participant,
	 data = D2)

```

In the next simulation, 100 participants encounter three designs in a within-subject design. In the participant data frame (D3_P), the participant sample is given two properties: 

1. with a chance of $65%$, a participant is female
2. participants vary by some amount how quickly the respond, which is represented by the ramdom effect 

$$\theta ~ Norm(0, 10)$$

A second table (D3_D) represents the three designs. Designs have one property assigned: background color is either white or yellow. Yellow background adds 50s to the response time.  

The observation table D3 first creates an empty grid of all combinations of participant and designs (This is called a _complete_ design). Then the object tables are joined, to gather all predictors in one place. 

Then we compute the expected value $\mu$ by applying the linear combination of coefficients $\beta$ (designs) and  $\theta$ (participants). Finally, observed values are created as

$$RT = Norm(\mu, 50)$$

Finally, we recover $\beta$ with a (LMEM), using the same structure as before. The next logical step, is to run a model with background color as predictor. Note that you should never estimate both at a time, as they are highly correlated. The associated problem is called *multi-colinearity* and can result in weird estimates.

```{r sim_gaussian_3Designs}

Sglm[["DesignABC"]] <- 
  function(n.subj = 50){
    Participants = data.frame(Participant = as.factor(1:n.subj)) %>% 
      mutate(female = as.logical(rbinom(n.subj, 1, 0.65))) %>% 
      mutate(theta = rnorm(n.subj, 0, 10))
    
    Designs = data.frame(Design = c("A", "B", "C")) %>% 
      mutate(bgcolor = c("yellow", "white", "white")) %>% 
      mutate(beta = c(350, 300, 300))
    
    Experiment = expand.grid(Participant = as.factor(1:n.subj),
                             Design = c("A", "B", "C")) %>%
      join(Participants, by = "Participant") %>% 
      join(Designs, by = "Design") %>% 
      mutate(eta = beta + theta) %>% 
      mutate(mu_RT = identity(eta)) %>% 
      mutate(RT = rnorm(n.subj * 3, mu_RT, 50)) %>% 
      mutate(mu_Success = plogis(eta/200)) %>% # logistic 
      mutate(trials = 10) %>% 
      mutate(Success = rbinom(3 * n.subj, trials, mu_Success)) %>% 
      mutate(mu_Errors = exp(eta/200)) %>%  ## expected value mu
      mutate(Errors = rpois(3 * n.subj, mu_Errors)) ## observed variable
      Experiment
  }

Dglm[["DesignABC"]] <- Sglm[["DesignABC"]]()

D3 %>% 
	ggplot(aes(x = RT, fill = Design)) +
	facet_grid(Design ~.) +
	geom_density(alpha = .5)

M103 = MCMCglmm(RT ~ Design,
					random =~ Participant,
					data = D3)
summary(M103)

```

## Binomial data and logistic regression

Now we simulate a data set with the same research design, but the outcome variable is successes in ten tasks. For instance, the designs are three websites and participants are given ten information retrieval tasks.

Because the research design is the same, we can keep the two object data frames. The only issue is that $\beta$ and $\theta$ are on the scale of the linear predictor $\eta$, which is linked to expected values $\mu$ by the *logit link function*:

$$\eta = \log(\mu/(1-\mu))$$ 

Expected values $\mu$ are computed from $\eta$ through the so-called *mean function*, which is the inverse of the logit, the *logistic tranformation*:

$$\mu = \exp(\eta)/{1 + \exp(eta)}$$ 

Note that values of \mu from the Gaussian data set above, are in the range of 300 - 350, which would result in chances of success of virtually 100%. To get more realistic values, we first divide $\eta$ by 100. 

SNIPPET 

from the experiment on Visual EDA:

Logistic regression coefficients are on a logit scale. For the interpretation, we want to back-transform to the original scale, which is: probability the assumption to be rejected. In order to make statements on the probability scale, we backtransform using the inverse function of the logit, the logistic function:
mu_x = exp(eta_x)/(1 + exp(eta_x))
eta in the above formula is called the "linear predictor". The linear predictor is the value you get when you do operations on the coefficients. The most simple application is computing the overall probability for a rejection. This is represented by the Intercept parameter. In the table below, this parameter accounts to practically zero, on the logit scale. Back-transforming to the probability scale gives exactly a chance of 50%:
exp(0)/(1 + exp(0)) = 1/2
 So, an intercept of zero always means that chances are equal, which is the same as saying: the coefficient does not add any information. The same is true for parameters: the closer a coefficient is to zero, the less does it influence prediction of reject. Like with general linear models, parameters close to zero have "no effect" and can even be removed to simplify a model (we don't do that here).
Note, that the intercept in our model represents the situation that skew is 0 and N = 0. A more useful baseline rate for rejection rate would be at skew = 0 and the smallest sample size N = 10.
We can compute any prediction we want by combining the parameters with the predictors. But, the other coefficients in the model represent differences, not absolute values. Therefore, when predicting a certain event, let's say rejection at a skew = 0 and N = 50, we first have to compute the linear predictor eta, then transform to get predicted value mu. 
eta = 0.042 + Skew * 0 + Sampel size * 10
mu = logist(eta)
To give another example, the chance of rejection at skew = .5 and N = 50 is computed as:
eta = 0.042 + Skew * 0.5 + Sampel size * 50
mu = logist(eta)
Watch out! It is a severe mistake to transform coefficients individually, then do the linear combination.
eta = logist(Skew * 0.5) + logist(Sampel size * 50)
In any case, it is much preferred to report the estimates on the probability scale, as this is right-away interpretable. Credibility limits can be reported on the linear predictor scale. 


Group effects indicate how homogeneous the overall pattern (Table 1) is on individual units in the data set. Relevant units here are the participant and stimuli. For exanple, the relativels large coefficient Skew means that people differ a lot in how much their response is influenced by skew (which is desireable): The strong effect on stimuli means that (after taking skew and N into account) stimuli differ a lot in which response they provoke. It sounds a bit paradox, but that means that stimuli differ systematically: there exist properties other then Skew and N, that trigger a response.

SNIPPET

```{r binomial}
n.subj = 50
D4 = expand.grid(Participant = as.factor(1:n.subj),
								 Design = c("A", "B", "C")) %>%
	join(D3_P, by = "Participant") %>% 
	join(D3_D, by = "Design") %>%
	mutate(eta = (theta + beta)/100) %>%   ## linear predictor eta
	mutate(mu = plogis(eta)) %>%  ## expected value mu
	mutate(trials = 10) %>% 
	mutate(Successes10 = rbinom(3 * n.subj, trials, mu)) ## observed variable

## Distribution of linear predictor eta
D4 %>% 
	ggplot(aes(x = eta, fill = Design)) +
	geom_density(alpha = .5) +
	facet_grid(Design~.)

## Distribution of expected value mu (or p)
D4 %>% 
	ggplot(aes(x = mu, fill = Design)) +
	geom_density(alpha = .5) +
	facet_grid(Design~.) +
	xlim(0,1)

## Distribution of (simulated) observed values
D4 %>% 
	ggplot(aes(x = Successes10, fill = Design)) +
	geom_histogram(alpha = .5) +
	facet_grid(Design~.)

M104 = MCMCglmm(cbind(Successes10,trials) ~ Design,
					family = "multinomial2",
					random = ~ Participant,
					data = D4)

summary(M104)

data.frame(summary(M104)$solution) %>% 
	add_rownames() %>% 
	mutate(mu_hat = plogis(post.mean))
```

## Count data and Poisson regression

Count measures are bounded to the below. Unlike the normal distribution, the minimal count is 0. Theoretically, there is no upper bound for counts, but larger counts just get more and more unlikely.

The Poisson distribution describes count data. Different to the successes-on-trials, these counts have no natural upper limit. The distribution is left-skewed. The smaller the measured values are, the stronger the skew. Vice versa, when the average count gets larger, the distribution becomes more symmetric. For this reason, it may be practical to use the classic linear model, with normally distributed residuals (see [CLM]).

Consider a study that let's participants seek a piece of information on a governmental website. As a measure of efficiency, the number of steps are counted that takes users to find the information. Two websites are compared, website A with a very broad navigation structure (the home page presents 16 categories and the navigation structure has just two levels). Website B has a very deep information structure, presenting information in a five-level deep taxonomy.



A common misconception is that the RD of a measure approaches normal distribution with larger sample sizes. That is not true. Increasing the number of participants just results in a cleaner shape of the distribution, with less ruggedness. Examine yourself, how the shape of Poisson distributions changes by mean count, as well as sample size. The following code simulates a Poisson distribution with a given $\lambda$ and $N$. With every new run of the code, `rpois` generates a new random set of counts. By repeatedly running this code and watching the output, you can get a good idea of how varied the shape of the distribution can be.

```{r fig:exercise_poisson_dist}
data_frame(count = rpois(n = 20,
                         lambda = 2)) %>% 
  ggplot(aes(x = count)) + 
  geom_histogram()

```



```{r fig:poisson_dist}
expand.grid(lambda = c(2, 5, 10), N = 10, 50, 100) %>% 
  as_data_frame() %>% 
  mutate()
ddply()
```




Poisson regression, ...

Therefore, the link function has to transform

$$[0;\infty] \to [-\infty, \infty]$$

Accordingly, the *exponential* serves as link function, and the *logarithm* is the mean function. Note that in order to get realistic number of errors, $\eta$ has been set to a 20th of the Gaussian variable from D3. 


```{r poisson}
D5 = expand.grid(Participant = as.factor(1:n.subj),
								 Design = c("A", "B", "C")) %>%
	join(D3_P, by = "Participant") %>% 
	join(D3_D, by = "Design") %>%
	mutate(eta = (theta + beta)/20) %>%   ## linear predictor eta
	mutate(mu = exp(eta)) %>%  ## expected value mu
	mutate(trials = 10) %>% 
	mutate(Errors = rpois(3 * n.subj, mu)) ## observed variable

## Distribution of linear predictor eta
D5 %>% 
	ggplot(aes(x = eta, fill = Design)) +
	geom_density(alpha = .5) +
	facet_grid(Design~.)

## Distribution of expected value mu (or p)
D5 %>% 
	ggplot(aes(x = mu, fill = Design)) +
	geom_density(alpha = .5) +
	facet_grid(Design~.)

## Distribution of (simulated) observed values
D5 %>% 
	ggplot(aes(x = Errors, fill = Design)) +
	geom_histogram(alpha = .5) +
	facet_grid(Design~.)

M105 = MCMCglmm(Errors ~ Design,
					family = "poisson",
					random = ~ Participant,
					data = D5)

summary(M105)

data.frame(summary(M105)$solution) %>% 
	add_rownames() %>% 
	mutate(mu_hat = plogis(post.mean))
```



