---
title: "Modern regression techniques for design researchers"
author: "Martin Schmettow"
date: "October 5, 2016"
output:
  word_document:
    toc: yes
  pdf_document:
    toc: yes
  html_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
---


```{r setup, 	message = FALSE,	warning = FALSE,	include = FALSE}
## The following is for running the script through knitr
thisdir = getwd()
purp.mcmc = F


library(brms)
source("RMDR.R")
CE <- c("BrowsingAB", "Typing", "Uncanny", "CUE8", "Stroop")
load_CE(CE)

formals(stan_glmer)$chains = 2
formals(stan_glmer)$iter = 2000
```



# Linear mixed-effects models


## Average? Neverage!

Many studies in design research ignore the properties of their users. The author's personal take is that this is a heritage from experimental research in social science and it is unduly. Seminal studies in cognitive psychology and social psychology claim mental structures or processes on average, which means they assume completely homogenic responses. And indeed, several properties of human information processing have been replicated under such diverse conditions that one can assume that they are general. For example, the Stroop observation has been observed a hundred times in various population subgroups [REF], situations [REF] and variations. Research on accessibility, universal design or robust designs has a strong tradition in design research. Still, the number of studies that regard individual differences is comparably small [Schmettow & Kuurstra, 2016].

Let me get it straight: People differ! If you plan to move from design A to B, or any other design choice, its is a good idea to also regard how much people vary. If your research question refers to any mental process or its change, the only solid way is to observe how one-and-the-same individuals changes between condition. Doing a between-subject design is a potential mistake, even, and requires good justification. The assumption that the group-level averaged (or pooled) data validly reflects what happens for the individual, is daring and can be completely false, as shown by the following (thought) experiment:

Imagine you set out to determine the association between typing speed and errors. It is common sense that quality decreases with speed, so a positive correlation is expected. During the epxeriment, participants get a number of texts to type. With the first trial they are instructed to type at a relaxed speed. With every further trial they were asked to type slightly faster than at the trial before. The expectation is that

> The faster a person types, the more errors occur

In terms of statistical skills, the researcher is familiar with linear models, and is aware of the fact, that these may not be applied for repeated measures. Like many in the past (e.g., [Mathur], see [GLM]), the researcher runs the analysis on average scores of averages across participants and receives a data set with a neat 30 data points, one per trial. A first exploratory plot reveals a bizarre relation: it seems that the faster participants type, the less error they make. Efficiency and robustness to error are practically always in opposition.

```{r fig:Typing_average, opts.label = "fig.small"}
attach(Typing)

Type_1_avg %>% 
  ggplot(aes(x = speed,
             y = error)) +
  geom_point() +
  geom_smooth(se = F)

```

Could it be true that the faster a person types, the less errors happen? Of course not. The problem is how precisely we ask the question. When averaging over persons, we actually answer a different question, which is on a *group-level*: 

> Persons who can type faster, make fewer errors.

This question sees every person as one data point, representing the ability to type. Now, the above results make more sense: people, who are trained in typing are at the same time fast and more accurate.

What the experiment was out for is the trade-off between speed and accuracy, *relative* to a person's overall typing skills. The trade-off is provoked by asking every participant to increase their speed. Most importantly, the trade-off is a change in a person's mental state, so we ask the _individual-level question_:

> How does the error frequency increase when a person types faster?

When we visualize the results on an individual level, the speed-accuracy trade-off becomes immediately visible:

```{r fig:Typing_individual, ops.label = "fig.small"}
Type_1 %>% 
  ggplot(aes(x = speed,
             y = error,
             group = Part)) +
  geom_point() +
  geom_smooth(se = F, method = "lm", aes(col = "within participant")) +
  geom_smooth(data = Type_1_avg, 
              aes(col = "between participant", x = speed, y = error, group = NA), 
              se = F, method = "lm")

detach(Typing)
```


The example here is constructed to be extreme: the group-level effect has the opposite direction of individual level effects. However, the situation is fully plausible. Therefore again: treating averaged data as if it represents within-persons processes can be a severe mistake. For that reasons, studies with repeated measures should be the standard, and between-subject factors need solid justification.

*Linear mixed-effects models* (LMM) are an extension to LM. With LMM we can gracefully treat repated measures situations, highly complex, even. Let's take another good look at the Figure. There are a few things that are worth noting. We list them first, and incorporate them into the model one-by-one:

+ persons have different levels for errors at relaxed typing (trial 1)
+ persons differ in how much the error frequency goes up with speed (varying slopes)
+ the better a person performs at relaxed typing, the stronger the number of errors goes up when the person types faster.

In the remainder of the chapter, *linear mixed-effects model (LMM)* are introduced, which ultimately allows us to resolve the bizzare situation with the Typing study.




## The Human Factor: Intercept random effects

Design science fundamentally deals with interaction between  systems and humans. Every measure we take in a design study is an encounter of an individual human being with a system. It is likely that people differ.
In the previous chapter [GLM] we have already dealt with differences between users: in the BrowsingAB case, we compared two designs in how inclusive they are with respect to elderly users. The data set included further variables, education and gender, that may help to predict performance.

Imagine, you had observed massive variation in task performance. You come to the idea that the design is highly selective, where a fraction of users perform extremely well, whereas many users fail miserably.  If that were true,  you would give the advice to invest into redesign that is more inclusive. To make your point, you had to prove that individual differences are the main source of variation. A reasonable statement would be: at least 50% of observed variance is caused by variation among users.


At first, one might think that the grand mean model would do, take $\beta_0$ as the population mean and $\sigma$ as a measure for variation. Unfortunately, this will not work: the random variation captured by $\sigma$ is a sum of multiple sources, in particular:

+ inter-individual variation
+ intra-individual variation, e.g. by different levels of energy over the day
+ variations in situations, e.g. responsiveness of website
+ inaccuracy of measures

What is needed, is a way to separate the variation of participants from the rest. To make it short, endeavours into individual variation inevitably leads to repeated measures and within-subject designs.

Imagine, a pre-study was done on the existing design. Data was gathered in live operation of the website. Real users were  tracked on 10 tasks. While it was possible to identify the task a user was on, it was not possible to force them into all 10 tasks. Typically, only a handful of tasks could be observed per user, resulting in many missing observations. Another consequence of life data gathering was that recurrent users could be identified, but not a single demographic variable could be gathered. With the simulation function of the BrowsingAB case environment we generate a new data set `SOV` (sources of variance), reduce it to design A and cut out all predictors, and riddle the data set, making it incomplete.

```{r sim:sources_of_var, opts.label = "tut.nr"}

attach(BrowsingAB)

n_Part = 980
n_Task = 10
n_Obs = n_Part * n_Task
response_rate = .3 ## proportion of tasks that were observed


SOV <-
simulate(n_Part, n_Task) %>% 
filter(Design == "A") %>% 
select(Obs, Part, Task, ToT) %>% 
mutate(observed = as.logical(rbinom(n_Obs, 1, 
response_rate))) %>% ## randomly marking the observed
filter(observed) %>% ## deleting unobserved
select(-observed)

SOV %>% sample_n(6)


```

We start with a data exploration. The first question we can ask is:

> What is the average ToT in the population?

This can be approximated as:

```{r pop_mean, opts.label = "rtut"}
mean(SOV$ToT)
```


This question is rather global and does not refer to any individuals in the population. The following question does:

> What is the average ToT of every participant?

This question differs only in that it is *grouped* by participant:

```{r random_effects, opts.label = "rtut"}
SOV %>% 
group_by(Part) %>% 
summarize(mean_Part = mean(ToT)) %>% 
  sample_n(6)
```

Now, that we have the individual scores, it is almost compelling to ask:

> What is the variation of ToT in the population?

```{r re_variation, opts.label = "rtut"}
SOV %>% 
group_by(Part) %>% 
summarize(mean_Part = mean(ToT)) %>% 
ungroup() %>% 
summarize(var_Part = sd(mean_Part))
```


Mixed-effects models contain three types of parameters, and the above steps correspond with these types:

1. *fixef effect* capture the averages of arbitrary groups
1. *random effects* capture averages of individual members of a population
1. *random factor variation* (or group effects) captures the variation between members of the population.

Above, we have claimed that our data set is bare of predictors. Is it? The variable `Part` is just a variable that groups observations, in other words: a factor. For now, it completely suffices to understand random effects as levels of a *random factor*.

That being said, the terms *random effect* and *random factor* are partly misleading, as there is nothing more or less random in a random factors as compared to fixed factors. The levels are deterministic, as they are part of the data. Both explain variation, that would otherwise be random. The reader is adviced to not contemplate over what makes a factor random. It is just a name and in a later section, we will see that random factors differ from fixed factors in subtle and amazing ways. They should be called fonzy factors.

Speaking of factors: so far, we are used to *treatment contrasts*. Treatment effects represent the difference of all levels to a reference group. Analogue to a factor model, we could estimate the first participants performance level and make it the reference group, intercept $\beta_0$. All other average scores, we would express as differences to the reference participant. This seems odd and, indeed, has two disadvantages: first, whom are we to select as the reference participant? The selection would be arbitrary, unless we wanted to compare brain sizes against the gray matter of Albert Einstein. Second, the researcher is really after the factor variation, which is inconvenient to compute from treatment contrasts. 

The solution is to use a different treatment coding. *deviation contrasts* represent the individual effects as *difference ($\delta$) towards the population mean*, once again in approximation:

```{r opts.label = "rtut"}
SOV %>% 
  group_by(Part) %>% 
  summarize(delta_Part = mean(ToT) - mean(SOV$ToT)) %>% 
  sample_n(6)

detach(BrowsingAB)
```

As the population mean is a constant, the transformation does not change the variance, which can easily be computed. As the population mean is part of the model, the grand mean model is a the point to start: `ToT ~ 1`. Later, we will see that random effects can be added to any fixed effect. Here, the reference fixed effect is the intercept. We can also think of it as a conditional statement:

> The average ToT depends on the participant

In R, an *intercept random effects* is denoted as `ToT ~ 1 + (1|Part)`. The "1" represents the intercept and the random effects term can be read as: the intercept, conditional on the factor participant. In the package `rstanarm`, the command `stan_glmer()` is dedicated to estimate mixed-effects models with the extended formula syntax. The use of `brm` is virtually the same. We run the model and extract the posterior distribution.

```{r mcmc:M_1_SOV, opts.label = "mcmc"}
attach(BrowsingAB)

M_1_SOV <-
stan_glmer(ToT ~ 1 + (1|Part), data = SOV)

P_1_SOV <- posterior(M_1_SOV)


detach(BrowsingAB)

```

__Exercises__:

1. Plot the participant deviations from the population mean using ggplot.








## The encounter: cross-classified random effects

Review the SOV data set. We have not exploited it, fully, as we did nothing with the tasks. Now, that we understood that random effects are just that, factors, what keeps us from treating tasks the same way? Modern websites offer many things to do, and a few representative have been chosen for the study. Under these circumstances, we can view the ten tasks simply as members of a task population, although that sounds a bit odd, at first. Thinking of *non-human populations* is a useful generalization, as in many research contexts we encounter them:

* phonemes in psycholinguistics
* test items in a psychometric instrument
* pictures of faces in face recognition research

Design research deals with its own non-human populations. and there are as many encounter between them

* example: tasks
* example: situations
* example: designs


More formally, sampling schemes with an encounter produce so called *cross-classified random effects* (CRE) IN CRE, every observation in the data set is associated by more than one independent classification system. In the CUE8 study, we may consider the five tasks as members of a population. These have been chosen from a larger set of tasks that can be completed with the system at hand. Hence, we can specify an additional random effect for task, which will account for variance that is due to different length of these tasks. We observe that systematic differences between tasks seem to exist. These differences are comparable to those accounted for by the team level, but noticeably reduce the residual variance.

```{r opts.label = "fig.wide"}


# Plme[["CUE_eda_task_bxp"]] <-
#   D[["CUE8"]] %>% 
# ggplot(aes(x = Task, 
#            y = ToT)) + 
#   geom_boxplot()
# 
# 
# Plme[["CUE_eda_task"]] <-
#   D[["CUE8"]] %>% 
#   group_by(Task) %>% 
#   summarize(median_ToT = median(ToT, na.rm = T)) %>% 
#   ungroup() %>% 
#   mutate(ranked = percent_rank(median_ToT)) %>% 
# ggplot(aes(x = ranked, 
#            y = median_ToT)) + 
#   geom_point() +
#   xlab("Task(percentile)")

```

In a study of ours, we took it altogether, almost, and set for a claim that Egan has made, decades ago:

> Users differ more than designs

This claim has been cited in many papers that regarded individual differences. We were wondering how it would turn out in the third millenium, with its many information sites. For the convenience, we chose a very accessible user population: student users. The design population consisted of ten unversity websites, and ten representative tasks were selected. The ten websites combine with ten tasks to 100 items.

Participants encountered the items in a balanced design with planned missing observations. ...

For the convenience, here, log-trafo. 




[TODO: redo with case Egan in brms]

```{r CUE8_cross, eval = F}
attach(CUE8)


Mlme[["CUE8_2"]] <- 
  MCMCglmm(fixed = logToT ~ moderated,
							 random =~ Part + Team + Task, 
							 data = D[["CUE8"]])


#qplot(c(predict(M42) - D[["CUE8"]]$logToT))


D[["CUE8"]] %>% 
	group_by(Task) %>% 
	summarize(task_mean = mean(logToT, na.rm = T)) %>% 
	ggplot(aes(x = task_mean)) +
	geom_histogram()

D[["CUE8"]] %>% 
	group_by(Part) %>% 
	summarize(p_mean = mean(logToT, na.rm = T)) %>% 
	ggplot(aes(x = p_mean)) +
	geom_histogram()


```


### Testing Egans assumption 

```{r egan_1, eval = F}
load(file = paste0(datadir, "Case 3 Egan/Egan.Rda"))
summary(Egan)

```

```{r egan_2, eval = F}
Egan %>% 
	dplyr::group_by(participantID) %>%
	dplyr::summarize(gparticipant = mean(lTimeOnTask)) %>% 
	ggplot(aes(x = gparticipant)) +
	geom_histogram()

Egan %>% 
	dplyr::group_by(task) %>%
	dplyr::summarize(gtask = mean(lTimeOnTask)) %>% 
	ggplot(aes(x = gtask)) +
	geom_histogram()

Egan %>% 
	dplyr::group_by(task) %>%
	dplyr::summarize(gtask = mean(lTimeOnTask)) %>% 
	ggplot(aes(x = gtask)) +
	geom_histogram()

```


```{r egan_3, eval = F}

M110 = MCMCglmm(lTimeOnTask ~ 1,
					random = ~participantID + website + task,
					data = Egan,
					nitt = 55000,
					burnin = 5000)

summary(M110)

```

## Hierarchical random effects

*Hierarchical random effects* (HRE) represent nested sampling schemes. A classic example from school research is: a sample of schools is drawn and inside every school a sample of students is selected. Like cross-classified models, hierarchical models consist of multiple levels. The difference is that if one knows the lowest (or: a lower) level of an observation, the next higher level is unambiguous, like: 

+ every student is in exactly one class
+ every participant is from exactly one professional group

As we have seen above, cross-classified models play a primary role in design research, due to the user/task/design encounter. Hierarchical RE are more common in research disciplines where organisation structures or geography plays a role, such as education science (think of the international school comparison studies PISA and TIMMS), organisational psychology (although not every social psychologist has understood that, yet) or political science.

An interesting case of herarchical sampling structures are some of the Comparative Usability Evaluation (CUE) studies, pioneered by Rolf Molich [CUE8]. Different to what the name might suggest, not designs are under investigation in CUE, but professionals. The over-arching question in the CUE series is the performance and reliability of usability professionals. Earlier studies sometimes came to devastating results regarding consistency across professional groups when it comes to identifying and reporting usability problems. We always knew, qualitative analysis is much harder to do in an objective way, didn't we? The CUE8 study lowered the bar, by asking whether several professional groups are able to arrive at consistent measures for time-on-task.

The CUE8 study measured time-on-task in usability tests, which had been conducted by 14 different teams. The original research question was: how reliable are time-on-task measures across teams? All teams were responsible for recruiting their own sample of participants. The hierarchical sampling scheme is therefore: every participant is assigned to exactly one team. The original analysis [CUE8] focussed on the consistency across teams. Using hierarchical random effects, we can extend the analysis. Conjsider the research question: strong systematic variation between teams is considered a sign of low reliability in the field. But, how strong is strong? It is difficult to come up with an absolute standard for inter-team reliability. Again, we can resort to a relative standard: how does the variation between teams compare to variation between individual participants?

Under this perspective, we examine the data. This time, we have real time-on-task data and as so often, it is highly skewed. In the following chapter [GLM], alternative methods will be introduced for variables that do not "well behave" in terms of distribution. Here, we use a cheap trick: logarithmic transformation often makes left-skewed distributions more symmetric. The downside is that the outcome variable may not be zero. For time-on-task data this is not an issue. In fact, the original CUE8 data set contains several observations with unrealistically low times. Before proceeding to the model, exploring the original variable `ToT` on the two levels is in order. The mean ToT is computed for the two random effect levels, participants and teams and plotted in that order.


```{r CUE8_eda, opts.label = "fig.small"}

attach(CUE8)

D_study <-
  D_study %>% 
  mutate(logToT = ifelse(ToT > 0, log(ToT), NA))

summary(D_study)

D_part_mean <-
  D_study %>% 
  group_by(Part, Moderated) %>% 
  summarize(mean_ToT = mean(ToT), na.rm = T,
            n_Obs = n()) %>%   
  ungroup() %>% 
  rename(Unit = Part) %>% 
  mutate(percentile = percent_rank(mean_ToT),
         Level = "Part")

D_team_mean <-
  D_study %>% 
  group_by(Team, Moderated) %>% 
  summarize(mean_ToT = mean(ToT, na.rm = T),
            n_Obs = n()) %>%
  ungroup() %>% 
  rename(Unit = Team) %>% 
  mutate(percentile = percent_rank(mean_ToT),
         Level = "Team")

D_task_mean <-
  D_study %>% 
  group_by(Task, Moderated) %>% 
  summarize(mean_ToT = mean(ToT, na.rm = T),
            n_Obs = n()) %>%
  ungroup() %>% 
  rename(Unit = Task) %>% 
  mutate(percentile = percent_rank(mean_ToT),
         Level = "Task")


G_comp_SOV <- 
  bind_rows(D_team_mean,
            D_part_mean) %>%
  ggplot(aes(x = percentile, 
             y = mean_ToT,
             col = Level,
             shape = Level,
             size = Level)) + 
  geom_point()

G_comp_SOV
detach(CUE8)
```

It seems there is ample variation in ToT for participants, with mean ToT ranging from below 100 to almost 500 seconds. There is variation in team-wise ToT, too, although less pronounced. This observation is congruent with the *law of small numbers*, which will be discussed in the following section.

The model is easily specified with the familiar syntax. In fact, it is not even necessary to specify that participants are nested within teams. The only requirement is that participant identifiers are unique across the whole study (not just within a team unit). The model below contains another feature of ths CUE8 study. Participants were tested in one of two conditions: moderated and remote. Mixing fixed and random effects, we examine all influences simultaneously. 

Note that this treatment is between-subject for participants. There is also just one team that did both. So, it will not be possible to estimate a slope RE. For specifying hierarchical random effects we can use the same syntax as before. _Note_ that this requires all participants to have a unique identifier. 


```{r CUE8_hierarchical_model, opts.label = "mcmc"}
try(detach(CUE8))
attach(CUE8)

M_1 <- 
  D_study %>% 
  brm(logToT ~ Moderated + (1|Part) + (1|Team),
       data = .)

P_1 <- posterior(M_1)

detach(CUE8)
save_CE("CUE8")
```

After running the model, we compare the sources of variation. Recall, that LMM assume that the ramdom effects are drawn from a normal distribution and the amount of systematic variation is represented by $\sigma$. As this is precisely the way, normal distributed errors are represented, we can actually distinguish three levels of variation: participants, team and observations (residuals). The table below tells that the strongest variation is still on observation-level. Almost the same amount of variation is due to teams. And, surprisingly, the variation due to teams is considerably smaller than participant-level variation.


```{r tab:CUE8_sov}
attach(CUE8)
T_sov <- coef(P_1, type = c("grpef", "disp"))
T_sov %>%   kable()
```

 It is hard to deny that, at least for consumer systems, people vary greatly in performance. That is the whole point about universal design. The discordance, between professional teams is concerning. And that arises after controlling for an experimental factor, remote or moderated. By the way, the difference between moderated and remote testing is `r frm_coef(fixef(CUE8$P_1), ~fixef == "Moderatedmoderated")`. The fixed effect is rather weak and highly uncertain.

```{r tab:CUE8_fixef}
T_fixef <- fixef(P_1)
T_fixef
detach(CUE8)
```


## What are random effects? On pooling and shrinkage

When a data set contains a factor that we may wish to add to the model, the question is: fixed effect or random effect? Above I have introduced the heuristic of non-human populations. If one can conceive tasks, designs, or whatever set of items, as a population, it is best introduced as a random factor. Now, it is in order, to more formally conceive when a set of things is a population.

Obviously, we would never speak of a population, when the objects of interest are from different classes. Entities gathering on super market parking lots, persons, cars, baskets and and dropped brochures, we would never see as a population. People, we would generally see as a population, as long as what we want to observe is somewhat homogenous. When the question is, how fast persons can do a 2000 meter run at the olympic games, we would certainly want one population per discipline (swimming, running, etc). Why is that so? It is because we expect members of a population to have some similarity, in other words: if you have observed how some members of the population behave, you get an idea about the unobserved.

Reconsider the Bayesian principle of prior knowledge by an experiment of thought: Consider, a UX expert with experience in e-commerce is asked to estimate how long it takes users to do the checkout, *without being shown the system*. The expert will probably hesitate briefly, and then come up with an estimate of 45 seconds. Without any data, the expert made some reasonable assumptions, e.g. that a  disciplined design process has been followed, and then relies on experience. The experts personal experience has formed prior to the study by observing many other cases. Now, we confront the expert with an even more bizzare situation: guess the time-on-task for an unknown task at a unseen system of unknown type! The expert will probably refuse to given an answer, arguing that some systems have tasks in the millisecond range (e.g. starting a stopwatch), whereas other processes easily run for hours or days. This is agreeable, and we provide the expert with average ToT of four other tasks with the same system:

$ToT_{1-4} = {23, 45, 66, 54}$

Now, the expert is confident that ToT be around 50 seconds and that is probably a good guess. What has happened is that prior belief about the unkown task parameter has been formed not externally, but *by data* as it arrived. The likely value of one unit has been learned from the other units and this appears pretty reasonable. The same principle applies when removing outliers. When staring at a boxplot or scatterplot the mind of the observer forms a gestalt that covers the salient features of data, for example: almost all points are located in the range 100 - 500. Once this pattern has formed, deviant points stand out.

However, the salience of the gestalt may vary. Consider a situation where ToT has been measured by the same procedure, but using five different stop watches. Stop watches are so incredibly accurate that if you know one measure, you basically know them all. What many researcher do with repeated measures data is average over the repetitions. This is the one extreme called *total pooling*.  In the stopwatch case the average across the five measures would be so highly representative, that total pooling is a reasonable thing to do.

In other cases, the levels of a factor are more or less independent, for example tasks in a complex system, where procedure duration ranges from seconds to hours. Guessing the duration of one task from a set of others is highly susceptible and the average duration across tasks is not representative at all. The best choice then is to see tasks as  factor levels, that are independent. This extreme of *no pooling* is exactly represented by fixed effects factors as they have been introduced in [CLM].

Random effects sit right between these two extremes of no and partial pooling and implement *partial pooling*: the more the group mean is representative for the units of the group, the more it is taken into account. The best thing about partial pooling is that, unlike real priors, one needs not even have to determine the amount of pooling in advance. The stronger the enities vary, the less can be learned from the group level. The variation is precisely the group-level standard deviation of the random effect.

So, we can think of random factors as factors where there is a certain amount of cross-talk between levels. The random effect estimate draws on two sources of evidence: all data from the over-arching population and data that belongs just to this one entity. As that is the case, the exploratory analysis of individual performance in SOV does not resemble a true random effect, as we have only calculated the group means. How are random effects implemented to draw on both sources? Obviously, the procedure must be more refined than just taking averages. In our Bayesian framework, a remarkably simple trick suffices, and it is a familiar one. By the concept of prior distributions, we are already  know a way to restrict the range of an effect, based on prior knowledge. For example, intelligence test results have the prior distribution $IQ ~ N(100, 15)$, just because they are calibrated this way. In most other cases, we do have rough ideas about the expected magnitude an range in the population, say: healthy human adults will finish a 2000m run in the range of 5-12 minutes. 

As prior knowledge is external to the data, it often lacks systematic evidence, with the exception of a meta analysis. This is why we tend to use weakly informative priors. Like priors, random effects factor in knowledge external to the entity under question. But, they draw this knowledge from the data, which is more convincing, after all. The basic trick to establish the cross-talk between random factor levels, is to *simultaneously estimate factor levels, fixed effect and random factor variation*. This has several consequences:

All random effects get a more or less subtle trend towards the population mean. As a side-effect, the random factor variance usually is a bit smaller than variance of fixed factors, or naive group means. This effect is called *shrinkage*. When the random factor variation is small, extreme factor levels are pulled stronger towards the population mean, resulting in stronger shrinkage. Or vice versa: When random variation is large, the factor levels stand more on their own.

The random factor variation is an estimate and as such is certain only to a degree. Generally, the more entities a random factor comprises, the more precise is the estimate of random factor variation. The strongest shrinkage occurs with small and random factor variation that is highly certain.

Previously, I have stressed how important repeated measures design is, and the number of observations per entity plays a role, too. The more observations there are, the less is the group mean over-ruled by the population mean. Less shrinkage occurs. This is why mixed-effects models gracefully deal with imbalanced designs. Groups with more observations are just gradually more self-determined. Taking this to the opposite extreme: when a factor level contains no data at all, it will just be replaced by the population mean. This principle offers a very elegant solution to the problem of missing data. If you know nothing about a person, the best guess is the population.

Under the perspective of populations as a more or less similar set of entities, these principles seem to make sense. Based on these principles, we can even define what fixed effects are:

> a fixed effect is a factor where levels are regarded so unsimilar, that the factor-level distribution is taken to have a, virtually, infinite variance.

We routinely approximate such a situation when using non-informative prior distributions, like $\beta_0 ~ N(0, 10000)$. In the extreme case, a uniform distribution with an infinite upper boundary truly has an infinite variance: $\beta_0 ~ U(0, \infty)$. A finite population mean doesn't even exist with such a distribution.

So, when a design researcher has observed that with design A, ToT is approximately distributed as $ToT_A ~ N(120, 40)$, is it a realistic to assume that design B has ToT in the range of several hours? Would a cognitive psychologist see it equally likely that the difference in reaction time on two basic tasks is 200ms or 2h. Probably not. Still, using fixed effects for factors with very few levels is a justifyable approximation. First of all, at any time can priors be used to factor in reasonable assumptions about the range. Second, with very few estimates, the random factor variation cannot be estimated with any useful certainty. Very small shrinkage would occur and the results would practically not differ from a fixed factor.

In the CUE8 data set we have a perfect situation to observe shrinkage in action. We explore what happens when estimating the differences between teams as either a fixed factor, or a random factor. All teams have enough participants tested to estimate their mean with some certainty. At the same time, the group sizes varies so dramatically that we should see clear differences in tendency towards the mean. 

We estimate two models, a random effects model and a fixed effects model. For the FE model, the intercept parameter is omitted, which results in one mean estimate for every group. For the RE model, the absolute group means are calculated on the posterior. Figure XY shows the comparison of FE and RE. 



```{r fit:CUE8_fixef_vs_ranef, opts.label = "mcmc"}
try(detach(CUE8))
attach(CUE8)

M_2 <- 
  D_study %>% 
  stan_glm(logToT ~ Team - 1, data = .)

M_3 <- 
  D_study %>% 
  stan_glmer(logToT ~ 1 + (1|Team), data = .)

detach(CUE8)
save_CE("CUE8")

```

```{r fig:CUE8_fe_vs_re}
attach(CUE8)
P_2 <-  posterior(M_2)
P_3 <-  posterior(M_3)

P_4 <-
  left_join(
    filter(P_3, type == "ranef"),
    filter(P_3, type == "fixef") %>% select(chain,iter, fe_value = value),
    by = c("chain", "iter")
  ) %>%
  mutate(value = value + fe_value) %>% 
  select(-fe_value) %>% 
  posterior(model_name =  "M_3_abs")

T_fixef_ranef <- 
  D_study %>% 
  group_by(Team) %>% 
  summarize(N = n()) %>% 
  mutate(fixef = fixef(P_2)$center,
         ranef = ranef(P_4)$center,
         diff = fixef - ranef) 

G_fixef_ranef <-
  T_fixef_ranef %>% 
  ggplot(aes(x = Team, y = fixef, size = N, col = "fixef")) +
  geom_point() +
  geom_point(aes(y = ranef, col = "ranef"))

sd_fixed <- sd(T_fixef_ranef$fixef)
sd_ranef <- sd(T_fixef_ranef$ranef)

G_fixef_ranef

detach(CUE8)
```



Team H is far off the population average, but almost no shrinkage occurs due to the large number of observations. Again, no shrinkage occurs for Team L, as it is close to the population mean, and has more than enough data to speak for itself. Team B with the fewest observation (a genereous $N = 45$, still), gets noticable  shrinkage, although it is quite close to the population mean. Overall, the pattern resembles the above properties of random effects: groups that are far off the population mean and have comparably small sample size get a shrinkage correction. In the case of CUE8, these correction are overall negligible, which is due to the fact that all teams gathered ample data. Recall the SOV simulation above, where the set of tasks every user did was beyond control of th researcher. In situations with quite heterogeneous amount of missing data per participant, shrinkage should be more pronounced and provide more reasonable estimates.

At the same time, shrinkage adjusts the estimates for variation, with $sd_{ranef} = `r CUE8$sd_ranef` < sd_{fixef} = `r CUE8$sd_fixef`$. The random effects estimate is an unbiased estimate for the population mean. With fixed effects, variation would be overestimated. [REF]

Now that random effects have been de-mystified as effects with partial pooling, where the amount of pooling is a consequence of observed data, we can move on to applications. The variance in the population is an indicator for heterogeneity of participants (or any non-human population) and random effects variation is an accurate estimator  for that. For designers, it is vital to understand how varied performance with a system is. Furthermore, as we will see in the next section, with slope random effects we can even estimate the variation of how users respond to different design conditions. When comparing two designs A and B, that enables the researcher to assess in how much a desig is uniformly better, a desireable property of universal designs. There also are a useful applications for the random effects, i.e., the individual levels. In some applications the crucial issue is to get good estimates of individual performance, which falls into the domain of psychometrics.

## Variance in variation: slope random effects

So far, we have dealt with random effects that capture the gross differences between individuals and stimuli. When specifying random effects, we never went beyond statements regarding the intercept effect, which we expressed as: `(1|Part)`. In other words: our basic fixed effects model always was the GM.

More routine research questions regard the differences in performance due to one or more predictors, like the age effect in case BrowsingAB, and we used to introduce this by a fixed effect: `ToT ~ age`. In the earlier chapter (CLM) we have already seen that this does not suffice, as the age effect grossly deviated between designs. An interaction effect was needed to describe the situation more accurately: (`ToT ~ age*Design`). The interaction effect showed that the age slope of design B was stronger than A -- the slopes are not parallel (review Figure XY). The age effect interacts with factor Design. 

As we have seen so far, random effects are practically just that: factors, where the levels are members of a population. In the BrowsingAB case, we have a human population and nothing keeps us from using it as part of *slope random effect*. Practically, the slope ramdom effect on Design can be understood as a bunch of individual pairwise comparisons, as can bee seen in the *spaghetti plot* on the right-hand of Figure XY.

Previously, we have seen how central repeated measures are for estimating random intercept effects and the same holds for slope effects. For estimating individual slopes, the participant (or any non-human entity) must encounter multiple conditions. While this doesn't have to be a complete research design, in general, for the BrowsingAB case, at least a part of the participants must be measured on both designs. Key to slope ramdom effects is a *within-entity design* (as the generalization of within-subject design). For demonstration of slope random effects, we create a data set `BAB5`, which differs from `BAB1` in two aspects: all participants encounter both designs (within-entity design) and ToT was measured on five given tasks (repeated measures). We use the simulation function as provided by the BrowsingAB case environment.

```{r sim:BrowsingAB, opts.label = "sim"}
attach(BrowsingAB)
BAB5 <- simulate(n_Task = 5)
```

The left of Figure XY shows the familiar line plot on population level. Observations within both condistions have been pooled in one average. The left hand side exposes the individual trajects between design A and B. We observe that many participants get faster with design B, but not all: there are winners and loosers.

```{r fig:BrowsingAB, opts.label = "fig.wide"}
G_slope_RE_1 <-
  grid.arrange(
    BAB5 %>% 
      group_by(Design) %>% 
      summarize(ToT_popul = mean(ToT)) %>% 
      ggplot(aes(x = Design, 
                 y = ToT_popul,
                 group = 1)) +
      geom_line() + 
      ylim(70, 220)
      ,
    BAB5 %>%
      group_by(Part, Design) %>% 
      summarize(ToT_Part = mean(ToT)) %>% 
      ggplot(aes(x = Design, 
                 y = ToT_Part,
                 group = Part)) +
      geom_line() + 
      ylim(70, 220),
    ncol = 2
  )

plot(G_slope_RE_1)
detach(BrowsingAB)
```


Like with the fixed interaction effect `age:Design`, we can add the slope random effects as a conditional statement

> the effect of Design depends on the participant. 

We can add a slope random effect to the LMM by extending the random effects term, adding age to the random effects part. Mind to always put complex random effects into brackets, as the `+` operator binds like hell.


```{r fit:BAB_slope_1, opts.label = "mcmc"}
try(detach(BrowsingAB))
attach(BrowsingAB)



M_slope_1 <-
  brm(ToT ~ 1 + Design + (1 + Design|Part),
             data = BAB5)

detach(BrowsingAB)
save_CE("BrowsingAB")
```

```{r}
attach(BrowsingAB)
grpef(M_slope_1)
detach(BrowsingAB)

```



`ToT ~ 1 + Design + (1 + Design|Part)`




More generally, With slope random effects we can capture differences in how individuals respond to conditions in our studies.  


### Differential designs

* fair versus optimal designs
* Interaction effects (once again)
* BrowsingAB

### Varying factors in R



### Varying slopes in R



### Random effects correlations

Imagine a situation, where the effectiveness of a new system manual is under evaluation. The respective system is a widely used application for drawing presentation slides. The new manual was developed by a university ICT department and contains some moderately advanced procedures to more effectively create appealing presentations.

Whenever there is a training under evaluation, but initial skills differ, correlations between intercept and treatment random effect are likely. 

## Measuring entities: psychometric applications

Traditionally, psychometrics deals with the valid and reliable measurement of person chracteristics, such as individual levels of performance, motivation, socio-cognitive attitude and the like. Advanced statistical models have been devised, such as confirmatory factor analysis or item response models. Formally, these applications establish an order among population entities, from low to high. The least one is aiming for is that for any pair of entities $A$ and $B$, we can tell which entity has the more pronounced property. This is called an ordering structure. Few applications even go beyond mere ranking and establish metric interpretations. For example, in an *interval-scaled* test, the researcher may compare differences between participants. While this is a desireable property of a scale, examples in the design sciences are rare.

Psychometric models have a common structure, which we are already familiar with: the encounter of persons and test items. In design research, we usually have the participant factor, but multiple non-human populations are possible, as we have seen in the case of Egan. There, the analysis focussed on population variance, but the same model applies for a psychometric research question. Measuring participant performance is a useful application in some design research fiels, such as human resource selection, e.g. identify persons with the best talent for being an airspace controller. In the Egan case, however, we might be more interested in establishing a ranking on designs.

The most simple form is the Rasch model in IRT, where participants respond to a set of items and the response is either correct or wrong. The outcome variable response is usually coded as 0 = wrong and 1 = correct. Apparently, such a variable does nowhere near satisfy the assumption of linear models. It turns out that the Rasch model can be interpreted as a cross-classified random effects in a *logistic regression*. Logistic regression is a member of the General*ized* Linear family of models, which will be introduced in the next chapter.





## Balancing order

Here we simulate data from a virtual experiment, where two designs are compared. All participants do the same task on both designs (within-subject design) and time-on-task (ToT) is taken as a performance measure. Because the task is the same, learning can occur, such that whatever comes second has an advantage. Therefore, the researchers decided to balance order in the design: half of the participants start with design A, the rest with B. Furthermore, the order of presentation is recorded in the variable *order*. Thereby, possible order effects can be quantified.

```{r balanced_order_1, eval = F}
## true coefficients
Beta = c(Intercept = 120,
			DesignB = 60,
			order = -20) 

# We start by creating a table with identifier variables
Order = expand.grid(Part = 1:20, 
										Design = c("A", "B")) %>% 
	# encoding the order
	arrange(Part) %>% 
	mutate(order = c(rep(c(0,1), 10), 
									 rep(c(1,0), 10))) %>%
	# calculating the true values
	mutate(mu = Beta[1] + 
				 	(Design == "B") * Beta[2] + 
				 	order * Beta[3]) %>% 
	# outcome = true value + noise
	mutate(ToT = rnorm(40, mu, 10))
setwd(datadir)
save(Order, file = "Case 11 Order/Order.Rda")

```

```{r balanced_order_2, eval = F}
setwd(datadir)
load("Case 11 Order/Order.Rda")

Order %>% 
	mutate(order = as.factor(order)) %>% 
	ggplot(aes(y = ToT, x = Design, col = order)) +
	geom_boxplot()

M43 = MCMCglmm(ToT ~ order + Design,
							 data = Order)

summary(M43) 
```



### GO ON HERE



## Cross-classified models

### User-task encounters

* MMN


* Egan's assumption
* Egan data set
* selecting robust designs

### User-design encounters

### Item response theory

* tasks as non-exchangeable stimuli

### A note on reductionist experiments

* very special case that stimuli are similar by design
* counter-example: Is "tree" neutral or not in the Stroop task?


* Semantic Stroop

## Validity of instruments

* hierarchical structures
* nested structures
* multi-level modelling
* CUE8

### Hierarchical factors in R
 
* CUE8


## Advanced tricks

* non-compliant test users in CUE8
* handling missing values by priors
* unbalanced designs
* individual-level residual






## Slope Interactions, again

## Repeated measure

### Case 1: Browsing performance


```{r mcmc_BrowsingAB, opts.label = "deprecated"}
names(BrowsingAB)

summary(Dlme[["BrowsingAB"]])


Dlme[["BrowsingAB"]] %>% 
	group_by(Part) %>% 
	summarize(Participant_mean = mean(ToT)) %>% 
	ggplot(aes(x = Participant_mean)) +
	geom_histogram()

Mlme[["BrowsingAB_1"]] <- 
  MCMCglmm(fixed = ToT ~ age + education, 
					random = ~Part,
					data = Dlme[["BrowsingAB"]])

Mlme[["BrowsingAB_2"]] <- 
  MCMCglmm(fixed = ToT ~ age + education + Design, 
					random = ~Part,
					data = Dlme[["BrowsingAB"]])

Mlme[["BrowsingAB_3"]] <- 
  MCMCglmm(fixed = ToT ~ age + education + Design, 
					random = ~Part  + Task,
					data = Dlme[["BrowsingAB"]])

Mlme[["BrowsingAB_4"]] <- 
  MCMCglmm(fixed = ToT ~ age + education + Design, 
					random = ~Part  + Task + Design:Part,
					data = Dlme[["BrowsingAB"]])


```

```{r stan, opts.label = "deprecated"}
require(rstanarm)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
Mlme[["BrowsingAB_5"]] <- 
  stan_lmer(ToT ~ age + education + Design + (1|Subj)  + (1|Task),
					data = Dlme[["BrowsingAB"]])
Mlme[["BrowsingAB_5"]] <- 
  stan_lmer(ToT ~ age + education + Design + (1|Subj)  + (1|Task),
					data = Dlme[["BrowsingAB"]], 
					algorithm="meanfield")
Mlme[["BrowsingAB_5"]]
summary(Mlme[["BrowsingAB_5"]])
Mlme[["BrowsingAB_5"]]$coefficients
Mlme[["BrowsingAB_5"]]$ses
```



## Case: Stroop experiment

```{r Stroop_load_and_boxplot, opts.label = "deprecated"}
load_CE("Stroop")
attach(Stroop)

names(Stroop)

Plme[["Stroop_boxplot"]] <-
Dlme[["Stroop"]] %>%
#	filter(as.numeric(Trial)<16 ) %>%
	ggplot(aes(x = Condition, col= Condition, y = RT)) +
  geom_boxplot() +
	facet_wrap("Part", ncol = 5) +
    theme_mulifacet()
Plme[["Stroop_boxplot"]]

```

It seems that  all participants show longer RT when in the incongruent condition. But, it also seems that some participants are generally faster than others. 



Next, we use smooth plots to check whether there are any order issues. Seemingly, participants get faster over time, which indicates training effects.

```{r Stroop_training_effects, opts.label = "deprecated"}
Plme[["Stroop_training"]] <-
  Dlme[["Stroop"]] %>%
  ggplot(aes(x = trial, y = RT)) +
  #	geom_point() +
  geom_smooth(se = T) +
  facet_wrap("Part", ncol = 5) +
  theme_mulifacet()

Plme[["Stroop_training"]]
```




## Slope random effects

*Slope random effects* (SRE) are conditional estimates that represent the  degree to which a person deviates from the average response  to a treatment or predictor.

```{r Typing_group_level, opts.label = "fig.small"}

```


```{r Stroop_intercept_pv_2, opts.label = "deprecated"}
Plme[["Stroop_spaghetti"]] <-
Dlme[["Stroop"]] %>%
	ggplot(aes(x = Condition, y = RT)) +
	geom_smooth(aes(group = Part), se = F, method = "loess")
Plme[["Stroop_spaghetti"]]
```

Participants seem to differ in their response to the treatment, although not very pronounced. It seems as if there are some differences in the treatment effects as well. The slope random effect is expressed in a very consistent manner. _Note_ that the term *1 +* is implicitly added. This is the same as for fixed effects.

```{r Stroop_intercept_RE_2, opts.label = "deprecated"}

try(detach(Stroop))
attach(Stroop)

M_3_s <-
Strp_1 %>%
  stan_glmer(RT ~ age + trial + Condition + (Condition + trial|Part),
             iter = 500, chains = 2,
             data = .)

tbl_post.stanreg(M_3_s)


M_3_b <-
  Strp_1 %>%
  brm(RT ~ age + trial + Condition + (Condition + trial|Part),
      iter = 500, chains = 2,
      data = .)

P_3_b <-
  posterior(M_3_b)
Part[10,Conditionneutral]

P_3_b %>% 
  dplyr::select(parameter, type) %>% 
  filter(type == "ranef") %>% 
  tidyr::extract(parameter,
                 into = c("nonlin","group", "member", "fixef"),
                 "^(.*_){0,1}(.+)\\[(.+),(.+)\\]$",
                 remove = F)

detach(Stroop)
save_CE("Stroop")

```

Finally, using these mechanisms, we can now specify a more complex model, including:

* age effects 
* learning effects (trial number)

We might also try to incorporate ramdom effects for the additional fixed effects. However, this caused the model estimation to become unstable, which is why we exclude it. 

```{r Stroop_full_model, message=FALSE, opts.label = "deprecated"}
Mlme[["Stroop_4"]] = MCMCglmm(RT ~ Condition + age + trial,
							 random =~ idh(Condition):Part,
							 data = Dlme[["Stroop"]],
							 pr = T)

```

We observe that the intercept random effect is strongest, but differences in response to the treatment are noticable as well. What we cannot assess using the classic estimation procedures is the certainty of the variance parameters. This we leave for when Bayesian estimation is introduced.



```{r save_all, message = "purp.dbg"}
# save(Dlme, Plme, Mlme, Slme, Tlme, file = "Linear_mixed-effects_models.Rda")
save_CE(CE)
#write.bibtex(file="ref_vis.bib")
```
