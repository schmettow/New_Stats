---
title: "New statistics for the design researcher"
author: "Martin Schmettow"
date: "October 21, 2016"
output:
  word_document:
    toc: yes
  html_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
bibliography: Bayesian_Stats.bib
---

```{r setup, echo = FALSE, warnings = FALSE, eval = TRUE, message=F}
## The following is for running the script through knitr
purp.mcmc = F
source("RMDR.R")

CE <- 
  c("BrowsingAB", "Sec99", "Headache", "Reading", 
    "AR_game", "Sleep", "Uncanny")
load_CE(CE)

#formals(stan_glm)$iter <- 200
#formals(stan_glm)$chains <- 1

```


# Linear models


## Grand mean models: quantification at work

Reconsider Andrew and Jane. They were faced with the problem that potential competitors could invalidate the claim "rent a car in 99 seconds" and drag them to court. More precisely, the question was: "will users *on average* be able ...", which apparently is about the statistic of the mean. A statistical model estimating just the mean we call the *grand mean model* (GMM). There are two reasons why it's "grand": the first is that most reseachers estimate models with more than one mean, the second is that this is the most simple of all models, so in a way, we can think of it as the "grandmother of all models".


+ with medical infusion pump a decimal error (giving the tenfold or a tenth of the prescribed dose) must be below a bearable level
+ the checkout process of an e-commerce website must have a a cancel rate not higher than ...
+ the brake lights of a car must be designed to let the following driver react in a certain time frame

The GMM predicts the expected level of performance when the only thing you know is the population someone is from. Prediction here means that the estimated grand mean we take as a best guess for all realizations we have *not* observed. That includes all people we have not invited to the lab, but also potential performance in other situations, for example the daily shape people are in or their current level of motivation. Just consider the many ways people differ in abilities, experience, strategies, preferences, situations, wishes etc. All these differences may also vary for the same person from day to day and influence performance. As people differ so much, and so do situations they are in, the GMM yields a rather imperfect prediction in most cases.

The only *predictor* in the GMM is the popluation mean. If you just know that the average Dutch male has a body length of 182cm, and you are asked to guess the size of a Dutch man you have not met, yet, this is your best guess. In regression models it is common to call predictors with the greek letter $\beta$ (beta). If there are more than one predictors, these are marked with subscripts, starting at zero. The "best guess" is called the *expected value* and is denoted with $\mu$ (mu). So, for this unkown Dutch man $i$, the expected value $\mu_i$ is:

$$\mu_i = \beta_0$$

Of course, would never expect this unkown Dutch man to be exactly 182cm tall. Instead, all Dutch man are more or less clumped around the expected value. In the GMM and all other linear models, it is assumed that the observed values $Y_i$ take the form of a Normal distribution that is centered at $\mu$ and is spread out $\sigma$ standard deviations.

$$y_i \sim N(\mu_i, \sigma_{\epsilon})$$

The first term, relating expected values to predictors is called the *likelihood term*. It defines the relation between *predicted values* $mu_i$ and predictors. In the current case, there are no predictors rather than the grand mean $\beta_0$. The likelihood is strictly deterministic. Generally, $\mu_i$ can be unique for every single observation. This is why the subscript $i$ is needed. In later, more complex models we get different predicted values for every participant.  However, in the grand mean model it depends on a single coefficient $\beta_0$ which is not associated with individual measures. More complex LM usually have more parameters, one which is $\beta_0$. This is then called the *intercept coefficient*. As the  grand mean model has this one parameter only, it is also referred to as the *intercept-only model* or *null model*.

Beneath it, the *random term* specifies our assumptions on the randomness in the observations. It is given as a distribution, denoted by the $\sim$ (tilde) operator, which reads as: "is distributed". We cannot predict this random term itself, but we can assume it to follow a distribution of our choice. In the case of linear models, the assumed distribution is always the normal or *Gaussian distribution*. Gaussian distributions have a characteristic bell curve and depend on two parameters: the mean $\mu$ as the central measure and the standard deviation $\sigma$ giving the spread.

The error term captures all variation that is not explained by the likelihood term and the unkown sources of variation are manifold. Deviation can arise due to all kinds of individual differences, situational conditions and, last but not least, measurement errors. The Gaussian distribution often is a good approximation for randomness.  Still, in some recognizeable cases it is inevitably off. In chapter [GLM] we will introduce other error distributions such as binomial distributions for successes in a  number of trials (for example: task completion rates) and Poisson distributions for count variables (for example: number of errors).

So, when estimating the grand mean model, we estimate the intercept $\beta_0$ and the standard deviation of the Gaussian distributed error term $\sigma$. In R, the analysis of the 99 seconds problem unfolds as follows: completion times (ToT) are stored in a dataframe, with one observation per row. This data frame is send to the R command `stan_glm` for estimation, using `data = Ver20`. As the `stan_glm` command applies to a huge variety of regression model, the desired model needs further specification. For that purpose, R has its own formula language. The formula of the grand mean model is `ToT ~ 1`. Left of the `~` (*tilde*) operator is the outcome variable. The right hand side specifies the deterministic part. The `1`here has nothing to do with the natural number surrounded by 0 and 2. In R's formula language it represents the intercept. 

```{r fit:99_seconds, opts.label = "mcmc"}
attach(Sec99)
M_1 <- stan_glm(ToT ~ 1, data = Ver20)
P_1 <- posterior(M_1)
detach(Sec99)
```


[That was a first glance on the underlying MCMC engine and how derive conclusions from the PD.] However, much of the time a researcher doesn't want to deal with the posterior, directly, but desires a brief summary of location and uncertainty. Coefficient tables are frequently used. A typical coefficient  table is shown below. It reports the central tendency of every coefficient, which is an indicator for the magnitude of an effect. Next to that, the spread of the posterior distibution is summarized as 95% credibility  intervals and represent the degree of uncertainty: the less certain an estimate is, the wider is the spread of the posterior distribution. A 95% credibility interval gives a range of possible values where you can be 95% certain that it contains the true value.
A coefficient table is produced by the `fixef` command of the bayr library:

```{r tab:99_seconds_post_fixef, opts.label = "tab"}
attach(Sec99)
T_coef_1 <- fixef(P_1)
T_coef_1
```


The authors of Bayesian books and the various Bayesian libraries have different opinions on what to report. Some report the posterior mean, others prefer the median, as it is less influenced by skew. A second advantage of the median is that it doesn't change, when a variable is transformed monotonically. For the spread of the distributions, either simple quantiles can be used, whereas others report areas of highest posterior density (HPD).
 
In this book, we primarily use the posterior mode for magnitude of the effect, and the 2.5% and 97.5% quantiles, which resembles the 95% confidence limit in classic statistics. The posterior mode has two advantages: first, it has a rather inuitive meaning as the most likely region, which is compatibel with the concept of maximum likelihood. Second, it is invariate to (strictly monotonous) transformations, which is an issue with Generalized Linear Models in a later chaper [GLM]. The main disadvantage is that the mode does not work well when the posterior distribution is bimodal. But, when that happens, you probably have a more deeply rooted problem, then just deciding on a suitable coefficient table. For a basic summary of the posterior distribution, we can simply state: 

```{r 99_seconds_post_fixef_table, opts.label = "rtut.nr"}
P_1 %>% 

  Ver20 %>% 
  ggplot(aes(x = "Intercept", y = ToT)) +
  geom_jitter() +
  geom_crossbar(data = filter(T_coef_1, parameter == "Intercept"),
                aes(ymin = lower, y = center, ymax = upper)) +
  geom_hline(aes(yintercept = 111, col = "111 seconds"))
```



Back to Andrew and Jane: the histogram in `r figr("fig:99_seconds_post")` indicates that the average time-on-task is rather unlikely in the range of 99 seconds or better. The standard posterior summary with `fixef()` does not answer the question, strictly. The CIs give the limits of a range that contains a given certainty. In contrast, Jane needs the certainty that average time-on-task is smaller than the 99 (or 111) seconds limit.    
This can be achieved by operating on the posterior distribution, directly. We can simply count how many visited values are smaller than 99.  But, by using quantile functions on the posterior distribution, we can make a precise statement about the probability of the claim:

```{r 99_seconds_prob99}
p99 <- 
  P_1 %>% 
  filter(parameter == "Intercept") %>%
  summarize(mean(value <= 99))

p111 <- 
  P_1 %>% 
  filter(parameter == "Intercept") %>%
  summarize(mean(value <= 111))

```


```{r tab:99sec_certainties, opts.label = "rtut.nr"}
T_certainty <-
  P_1 %>% 
  filter(parameter == "Intercept") %>%
  summarize(certainty_99s = mean(value <= 99),
            certainty_111s = mean(value <= 111))

```


It turns out that the certainty for average time-on-task below the 99  is a meager `r Sec99$T_certainty[[1,1]]`. The alternative claim, that average completion time is better than 111 seconds is far more likely (`r Sec99$T_certainty[[1,2]]`).

```{r fig:99_seconds_beta, opts.label = "future"}
betaplot::iiaplot_1(fixef = c(106, 0, 0, 0, 0))
                              

```

```{r}
detach(Sec99)
```

## Meeting the posterior distribution

The `stan_glm` command returns a complex object that stores the results of estimation. The most interesting part is the posterior distribution of predictors, here the  intercept $\beta_0$. The posterior distribution gives the degree of certainty for every possible value of the parameter.

In a perfect world, we would know the analytical formula of the posterior. In most non-trivial cases, though, there is no formula for the posterior. Instead, what we get is a sample from this distribution via a process called Markov-Chain Monte Carlo sampling (MCMC). In fact, what we get from the MCMC estimation is just many draws from the posterior distribution. The following code extracts the posterior distribution from the MCMC estimation and prints a random selection of 10 lines. When calling the posterior object (class: tbl_post) directly, it provides a compact summary of all variables.

```{r lst:99_seconds_post, opts.label = "rtut"}
attach(Sec99)

P_1 <-  
  posterior(M_1)
P_1 %>% 
  sample_n(10)
P_1
```

Making statements about certainty, for example $M_{ToT] <= 99$, is very easy, once we have a basic understanding of what MCMC estimation actually does. While a full treatment of the issue is beyond the scope of this book (see [BDA]), but it suffices to understand that the MCMC algorithm is a *random walk* through *parameter space*. In the current case, the parameter space is two-dimensional, as we have two parameters. At every iteration, the MCMC algorithm stores the coordinates in parameter space.


```{r fig:99_seconds_random_walk, opts.label = "fig.small"}
G_random_walk <-
  P_1 %>% 
  filter(iter <= 50) %>% 
  select(iter, parameter, value) %>% 
  spread(parameter, value) %>% 
  ggplot(aes(x = Intercept, y = sigma_resid, label = iter)) +
  geom_text() +
  geom_path(alpha = .3) +
  ylab("residual sd") +
  xlab("intercept mu")

G_random_walk
```

The first 50 hundred steps of the MCMC random walk are shown in `r figr("fig:99_seconds_random_walk")`. Apparently, the random walk is not fully random, as the point cloud is more dense in the center area.  This is where the more probable parameter values lie. In fact, the MCMC algorithm works with simple a rule that lets it jump to more likely areas more frequently. A common way to plot the posterior distribution, is to create one histogram per parameter (alternative: density or violin plots).

In our example, in `r figr("fig:99_seconds_post")` we can spot that the most likely value for average time-on-task is  `r frm_coef(Sec99$T_coef_1, row = 1, interval = F)`. Both distributions have a certain spread. With a wider PD, far-off values have been visited by the MCMC chain more frequently. The probability mass is more evenly distributed. There is less certainty for the parameter to fall in the central region. In the current case, a risk averse decision maker would maybe take the interval  `r frm_coef(Sec99$T_coef_1, row = 1, center = F)` as "reasonably certain".


```{r fig:99_seconds_post, opts.label = "fig.small"}
P_1 %>% 
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_grid(. ~ parameter, scales = "free")
  
detach(Sec99)
```



## Walk the line: linear regression

In the previous section we have introduced the mother of all regression models: the grand mean intercept-only model. The latter term bears some resemblance to mid-school function analysis. There, the intercept is known as of intercept as *"the point where a function graph crosses the x-axis"*, or more formally, as a basic linear function:

$$f(x_1) = \beta_0 + \beta_1x_{1i}$$
$$f(x_1 = 0) = \beta_0$$

At $\beta_0$, the linear function crosses point zero on the y-axis. The second parameter, $\beta_1$ is called the *slope*. The slope determines the steepness of the function. The linear regression model can be conceived a generalization of the intercept-only model: setting $\beta_1 = 0$ produces a an orthogonal line, with the  expected value is constant over the whole range. The full  then the linear regression model equals the grand mean model, $\mu_i = \beta_0$.

```{r fig:basic_linear_function, opts.label = "fig.small"}

data_frame(x = 1:20,
           y = 3 + 2 * x) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line()

```

Linear regression requires the predictor to be metric. That gives us the opportunity to discover how ToT can be predicted by age: are older people slower on te internet? The formula specification is:

$$\mu_i = \beta_0 + \beta_1x_{1i}$$
$$Y_i = N(\mu_i, \sigma)$$

As long as we stay in the domain of linear models (LM), only the likelihood term needs to be extended: a product of the (to be estimated) parameter $\beta_1$ and recorded age $\x_{1i}$ has been added. This literally means: with every year of age, ToT increases by the $\beta_1$ seconds. Before we run a linear regression with `stan_glm`, we visually explore the association between age and ToT:

```{r fig:BAB_eda_age}
attach(BrowsingAB)

G_eda_1 <-
  BAB1 %>%
  ggplot(aes(x = age, y = ToT)) +
  geom_point()+
  geom_smooth(se = F)

G_eda_1
```

Now, we use the `stan_glm` command in much the same way as before, but add the  predictor age. The command will internally check the data type of your variable, which is numeric, here. Therefore, it is treated as a metric predictor or *covariate*.

```{r age_lm1, opts.label = "mcmc"}
M_age <-
  BAB1 %>% 
  stan_glm(ToT ~ 1 + age, 
           data = .)
```

```{r opts.label = "invisible"}
T_age <- fixef(M_age)
```

Is age associated with ToT? The coefficient table tells us that with every year of age, users get `r frm_coef(BrowsingAB$T_age, ~fixef == "age", interval = F)` seconds slower. That is not a lot.

```{r opts.label = "rtut"}
fixef(M_age)
```

### Shifting predictors

The intercept parameter tells us the expected ToT at age = 0, a new born. That is a bizarre prediction and we would never seriously put that forward in a stakeholder presentation, or in the conclusion of a scientific paper, would we not? Besides that, the intercept estimate is rather uncertain, with a wide 95% interval, `r frm_coef(fixef(M_age), ~fixef == "age")`.

Both, the unplausibility and high certainty are rooted in the same problem: the model puts a parameter, where there is no data. The broad region of the intercept is as empty as the Khali desert, because observations are impossible. Before we explore this on a deeper level, there is a pragmatic solution to the problem: *shifting the predictor*. Shifting means just that: the whole linear curve is shifted to the right (or the left), such that the intercept is in a region populated with observations. In the case, here, two options seem to make sense: either, the intercept is in the region of youngest participants, or it is the sample average, which is then called *centering*. To shift a variable, just subtract the amount of units (years) where you want the intercept to be:


```{r BAB_shift_and_center, opts.label = "mcmc"}
BAB1 <-
  BAB1 %>% 
  mutate(age_shft = age - 20,
         age_cntr = age - mean(age))

M_age_shft <- 
  stan_glm(ToT ~ 1 + age_shft, data = BAB1)
M_age_cntr <- 
  stan_glm(ToT ~ 1 + age_cntr, data = BAB1)

bind_rows(
  posterior(M_age), 
  posterior(M_age_shft), 
  posterior(M_age_cntr)) %>% 
  fixef()

```

```{r opts.label = "invisible"}
C_age <- fixef(M_age_shft)$center
```


The shifted intercept has moved to higher values, `r frm_coef(fixef(BrowsingAB$M_age), ~fixef == "Intercept")`. Surprisingly, the shift is not exactly 20 years.  This is due to the high uncertainty of the latter, as well as the relation not being exactly linear (see Figure XY). In fact, the BrowsingAB data contains what one could call a psychological model. The effect of age is partly due to far-sightedness of participants (making them slower), which more or less suddenly kicks for older participants. Still, by shifting or centering the intercept parameter gains considerable certainty. At the same time, the slope parameter practically does not change, but gains some certainty, too. Think of the intercept as the end of the lever: shifting or centering an intercept to a region with data tightens certainty and one gets an overall more determined grip.

### Endlessly linear

On a deeper level the bizarre age = 0 prediction is an example of a general rule, that will re-occur several times throughout ths book:

**In an endless universe, everything is finite.**

A fact about LR is that they allow us to fit a straight line to data. A lesser regarded consequence is that this line extends infinitly in both directions. To fulfill this assumption, the outcome variable needs to have an infinite range, too,  $y_i \in [-infty; \infty]$ (unless the slope is zero). Every scientifically trained person and many lay people know, that even elementary physics is finite: the speed of light is limited to $\approx 300.000 km/s$ and temperature has a lower limit of -276°C (or 0K). If there is neither endless speed nor cold, it would be daring to assume any psychological effect to be endlessly linear.

The endlessly linear assumption (ELA) is central to all LR models. From a formal perspective, the ELA is always violated in a universe like ours. The practical consequences are, as ever so often: it depends! Frequently, LR is a reasonably effective approximation. Review [G_eda_1]: the blue line is a so called a *smoother*, more specifically a LOESS. A smoother is a fitted line, not unlike LR. But, it is way more flexible. Where LR is a lever, LOESS is a pipe cleaner. LOESS shows a more varied picture of the relation between age and ToT. There is a rise between 20 and 40, followed by a stable plateau, and another rise starting at 60. 

[Physiologically, this makes no sense. Adapt simulation and redo BAB1 and BAB5]

It is not directly linear, still: LR provided sufficient evidence that a relationship exists, such that, generally, the older one gets, the slower one does the task. There is a possibility that the design is unfair. That is enough reason to improve the design, for example, doing a formal accessibility evaluation and fixing the major issues. A theorist may desire a more detailed picture. Disruptions of linearity often indicate interesting psychological processes. Knowing these makes us better understand design. An uncanny example of theoretical work will be given in polynomial regression.

## Predictions 

In the BrowsingAB case, we have repeatedly recorded age of participant. LR found the repeating pattern, that with every unit of age, ToT increased by  `r C_age[2]` seconds. This pattern is what the model predicts, now and forever. If we ask: what is predicted for a person of age 45? We get the predicted value $\mu_i$:

$$\mu_{age = 45} =`r C_age[1]` + `r C_age[2]` * 45\\ 
= `r C_age[1] + C_age[2] * 45`$$

The predicted value is our best guess under the LR model. It is uncertain to a degree, but we are setting this aside for the moment. What we know for sure is the *observed value* . For just a guess, we can find all persons of age 45 in the data set and take it at face value. The data set contains a few of those participants, but the situation is rather scattered and the prediction is not clear. 

```{r just_a_guess}
BAB1 %>% 
  select(Part, age, ToT) %>% 
  filter(age == 45)
```

A more disciplined way to obtain predicted values is to use the standard command `predict` on the model object:

```{r best_guess, opts.label = "rtut"}
T_pred_age <-
  BAB1 %>% 
  select(Part, age, ToT) %>% 
  mutate(mu = predict(M_age_shft))
  
T_pred_age %>%  
  filter(age == 45)

```

We see that all participants of age 45 get the same prediction. What differs is the observed value, as this contains the random term or residual. We will return to residuals in the next section.

What if we wanted to get a best guess for an age that did not occur in our data set, say 43.5? Using the LR likelihood function above, we can estimate the expectation for any value we want. By entering the  intercept and age estimates (`C_age`), we obtain:

```{r best_guess_unobserved, opts.label = "rtut"}
C_age <- fixef(M_age_shft)$center
C_age[1] + 43.5 * C_age[2]
```

With the same procedure, it is possible to plot observed and expected values next to each other:

```{r fig:expected_values}
G_pred_age <-
  BAB1 %>% 
  ggplot(aes(x = age, y = ToT)) + 
  geom_point() +
  geom_abline(intercept = C_age[1], 
              slope = C_age[2],
              col = "red")

G_pred_age
```

This figure does not look too convincing. The regression line is rather flat, indicating a small effect of age. In addition, it looks like a lonely highway in a vast forrest area. Just visually, a completely flat line or a slight negative slope would be equally credible.

```{r fig:lin_reg_modelplot, opts.label = "future"}
# betaplot::iiaplot_2(list(as.numeric(Tclm[["Browsing_AB_3"]][1,2]),
#                          as.numeric(Tclm[["Browsing_AB_3"]][2,2])),
#                     range = c(0,80)) + geom_point(data = Dclm[["Browsing_AB"]], aes(x = age, y = ToT))

```



## Residual analysis

So far, we have used a notation for LM, that defines a likelihood term for predicted values and a random term, that links observed values to predicted values. 
A slightly different notation, that is quite common, first separates the observed value $Y_i$ into predicted value and an individual *residual error* $\epsilon_i$. In this form, the GM is specified follows:

$$Y_i = \mu_i + \epsilon_i$$

Then the likelihood term links all predicted values to the predictors:

$$\mu_i = \beta_0$$

And finally, the *residual distribution* is defined as a Normal distribution, with a (to be estimated) *residual error* $\sigma_\epsilon$. Different to the specification above, which we mainly use throughout this book, the standard error distribution is *centered at zero*.

$$\epsilon_i \sim N(0, \sigma_\epsilon)$$

Arithmetically, the two specifications are equivalent, with the first being preferred for its general applicable. Still, for linear models, the second form bears a treasure  In the standard form, the error term strictly defines a different distribution for every observation. For three observations with *predicted values* 200 and 300 and 301, the error distributions are not the same:

$$
Y_1 \sim N(200, \sigma)\\
Y_2 \sim N(300, \sigma)\\
Y_3 \sim N(301, \sigma)
$$


The residual distribution is the same for all observations. The distribution center is set to zero and the estimated spread $\sigma_\epsilon$. In R, we can extract the residuals from the model object using the standard command:

```{r extract_resid, opts.label = "rtut.nr"}
residuals(M_age_shft)
```


### checking normality

This returns a vector of residual values, exactly one per observation. All LM have the *assumption that the residuals are distributed Normal *. As in practice we are dealing with data from the crazy real world, this assumption may be accurate, sometimes more, sometimes less. With the vector of residuals at hand, we can evaluate this assumption by comparing the residual distribution to its theoretical form, a perfect bell curve.

The following command chain extracts the residuals from the model and pipes them into a histogram. With `stat_function` an overlay is created with the theoretical Normal distribution. The `bayr::coef` function extracts the residual standard error, which is of type *dispersion* (disp).

```{r fig:resid_dist_1}

G_resid_age_shft <-
  data.frame(resid = residuals(M_age_shft)) %>% 
  ggplot(aes(x = resid)) + 
  geom_histogram(aes(y = ..density..), bins = 15) +
  stat_function(fun = dnorm, 
                args = c(mean = 0, 
                         sd = coef(M_age_shft, type = "disp")$center), 
                colour = "red")

G_resid_age_shft
```
  

It is highly recommended to routinely check linear models by residual analysis. Do we see the characteristic bell curve of a Gaussian distribution? In the example above, we do, sort of. To give a counter example, we examine the variable `returns`, that captures the number of times a participant had returned to the homepage for a new attempt to find the desired information.

```{r fit:age_pois, opts.label = "mcmc"}
M_age_rtrn <- 
  stan_glm(returns ~ 1 + age_shft, data = BAB1)
P_age_rtrn <- posterior(M_age_rtrn)
T_age_rtrn <- coef(M_age_rtrn)
```

```{r fig:resid_dist_2}
C_age_rtrn_disp <- 
  T_age_rtrn %>% 
  filter(type == "disp") %>% 
  select(center) %>% 
  as.numeric()

G_resid_age_rtrn <-
  data.frame(resid = residuals(M_age_rtrn)) %>% 
  ggplot(aes(x = resid)) + 
  geom_histogram(aes(y = ..density..), bins = 8) +
  stat_function(fun = dnorm, 
                args = c(mean = 0, 
                         sd = C_age_rtrn_disp), 
                colour = "red") +
  xlim(-5, 5)

G_resid_age_rtrn
```

The model runs fine and returns a parameter for the assumed Normal distributed resdiduals. However, the residuals not even remotely resemble the theoretical curve. What has happened here is another flavor of the endless-universe paradox. When discussing predictions, we had seen that the linear model is always compromised as it assumes an infinite range of the outcome variable. Often that is of no practical concern, as long as one stays in the safe area with predictions. The problem here arises from the Normal distribution extending to infinity in both directions, too. The LR with homepage returns by age estimated number of returns of a 20-year to be `r frm_coef(coef(P_age_rtrn), ~fixef == "Intercept")`, with a RSE of $`r C_age_rtrn_disp`$. As the expected value is positive, there is no linearity paradox and we are perfectly fine. 

Are we? As you can see in the plot, the theoretical RD has is quite a tail to the negative numbers. There is ample chance that the residual for a 20-year old is  smaller then `r frm_coef(coef(M_age_rtrn), ~fixef == "Intercept", interval = F, neg = T)`. 
This can only happen, if the observed number of returns is negative, which is impossible, of course. Imagine, you wanted to  simulate the performance of 20-year olds based one what we learned from the last model. You would do the following and frequently observe negative values.

```{r sim_20yrold, opts.label = "rtut"}
set.seed(42)

C_age_rtrn_20y <- 
  T_age_rtrn %>% 
  filter(fixef == "Intercept") %>% 
  select(center) %>% 
  as.numeric()

n_Part = 100

D_20yr_rtrn <- 
  data_frame(Part = 1:n_Part,
             mu = C_age_rtrn_20y,
             rse = C_age_rtrn_disp,
             returns = rnorm(n_Part, mu, rse))

negative_obs <- 
  sum(D_20yr_rtrn$returns < 0)

cat(negative_obs, " negative simulations\n")
```

This is ostentively poor behaviour of the model. What is most important, is that generally, *when a model is severely mis-spedified,  neither predictions nor estimate, nor certainty statements can be trusted *. A model that frequently fits in case of count numbers is  Poisson regression, which will enter the stage in chapter [GLM]. 

Analysis of expected values (predictions) are necessary to discover problems with the linearity. Are the predictions we want to make in the safe range? This is essential to know for serious quantification. It is logical, that prediction analysis can only be done after the model has been applied to the data. Residual analysis is an essential part of the modelling process, too. By residual analysis we reflect on whether the Normality assumption is reasonably defendable. For variables that are counts, the distribution may not look Normal at all and impossible predictions can arise when simulating from the model. Keep in mind, that neither the linearity nor the Normality assumption are truly defendable ever, in an endless universe. 

Drawing on expected values, *residual analysis is something you can only do after estimation*. It is usually part of a process called *checking assumptions*, which somehow implies you do it beforehand, which lured many into analyzing the *distribution of the outcome* variable $y_i$ before running the model. This is a false meme, because assumptions are always checked for a certain model. The distribution of residuals you only see after you have run it on the data.


__Exercises:__

1. Examine the linear parameters of model `M_age_rtrn` and derive some impossible predictions, as was done in the previous section.

1. The BAB1 data set contains another outcome variable where the number of clicks was measured until the participant found the desired information. Specify a LR with age and examine the residual distribution. Is the Normality assumption reasonably defendable? What is the difference to home returnjs, despite both variables being counts?

1. Review the two figures in the first example of [GSR]. The observations are bimodally distributed, nowhere near Gaussian. After (graphically) applying the model they are well-shaped. What does that tell you about checking residual assumptions before running the model?



### comparing models

A commonly used criterion for model fit is whether a predictor helps to reduce the error. In fact, the coefficients that are returned by the estimation do exactly that: they are "chosen" to minimize the residial standard error. As the residuals 

the estimates such that the estimates assuming that the  considerably explains variance, which reduces the spread of the residual distribution. We could ask: does the age predictor effectively reduce the residual standard error $\sigma_\epsilon$ as compared to the GM? We first fit the null model:

```{r fit:age_resid_reduction, opts.label = "mcmc"}
M_0 <-
  BAB1 %>% 
  stan_glm(ToT ~ 1, data = .)
```

Then, we extract residuals from both models and visually compare them:

```{r opts.label = "fig.small"}

T_resid <-
  data_frame(
    M_0 = residuals(M_0),
    M_age = residuals(M_age))

G_resid_0 <-
  T_resid %>%
  gather(model, error) %>% 
  ggplot(aes(col = model, x = error)) +
  geom_density()

G_resid_0
```

This is not too convincing: the error distributions overlap almost completely. And indeed, the difference of residual standard errors is minor. Adding age as predictor to the GM is practically futile. 

```{r opts.label = "fig.small"}
bind_rows(posterior(M_0),
          posterior(M_age_shft)) %>% 
coef(type = "disp")
```

In [ME] we will encounter more rigorous procedures to identify useless predictors. A predictor that does not reduce the RSE often is a disappointment for the researcher. What regards the age variable, it will have a glamorous comeback when we turn to interaction effects.


## A or B: Comparing two (or more) designs

Another basic statistical routines is the *comparison of groups* (CG), commonly known as ANOVA. In design research group comparisons are all over the place, for example:

+ comparing designs: as we have seen in the A/B testing scenario
+ comparing groups of people, like  gender or whether they have a high school degree, or not
+ comparing situations, like whether someone uses an app on the go, or sitting still behind a computer

In order to perform a CG, a variable is needed that establishes the groups. This is commonly called a *factor*. A factor is a variable that identifies members of groups,   like "A" and "B" or "male" and "female". The groups are called *factor levels*. In the BrowsingAB case, the factor Design with its levels A and B does the job.

This question could appear during an overhaul of a municipal website. With the emerge of e-government, many such websites had grown wildly over a decade. They temporarily were jungles, to the disadvantage for the  users. The prototype of a novel web design is developed and tested via A/B testing at 200 users. Every user is given the same task,  but sees only one of the two designs. The design team is interested in: *do two web designs A and B differ in user performance?*

Again, we first take a look at the raw data:

```{r fig:eda_anova}

BAB1 %>%
  ggplot(aes(x = ToT)) +
  geom_histogram() +
	facet_grid(Design~.)
```

This doesn't look too striking. We might consider a slight advantage for design B, but the overlap is immense. We perform the CG. Again, this is a two-step procedure:

1. the `stan_glm` command lets you specify a simple formula to express the dependency between predictors (education) and outcome variable (ToT). It performs the parameter estimation, using the method of _Markov-Chain Monte-Carlo Sampling_. The results are stored in a new object `M_Design`.
1. With the `fixef` command the estimates are extracted and interpreted

```{r fit:anova_lm, opts.label = "mcmc"}
# COG parameter estimation

M_Design <-
  BAB1 %>% 
  stan_glm(ToT ~ 1 + Design, 
           data = .)
```

```{r opts.label = "invisible"}

T_resid <- 
  mutate(T_resid, M_Design = residuals(M_Design))
```


```{r opts.label = "rtut"}
fixef(M_Design)
```

The model contains two parameters, the first one being the *intercept*, again. How can you have a "crossing point zero", when there is no line, but just two groups. You can't. When speaking of pure factor models, like the one above or the multifactorial models of the next section, the intercept has a different meaning, it is *mean of a reference group*. Per default, stan_glm chooses the alphabetically first group label as the reference group, clearly design A. We can therefore say that design A has an average performance of `r frm_coef(fixef(M_Design), ~fixef == "Intercept")` Given the 95% CIs one would rather say that it is around 200. 

The second parameter is *group effect*, which is the *difference to the reference group*. With design A as reference. With design B it took users `r frm_coef(fixef(M_Design), ~fixef == "DesignB", neg = T)` less time to complete the task. This effect appears rather small and there is huge uncertainty about it. It barely be justifies the effort to replace design A with B. If the BrowsingAB data set has some exciting stories to tell, the design difference is not it.

### Dummy variables (are not stupid at all)

Are we missing something so far? We have not seen the model formula, yet. The CGM is a linear model, but this is not so apparent as it contains a factor. Linear model terms are a sum of products $\beta_ix_i$, but factors cannot just enter such a term. What would be the result of $\mathrm{"DesignB"}*\beta_1$?

Factors basically answer the question: *What group does the observation belong to?* The answer is a label and cannot enter the regression formula. Dummy variables solve the dilemma by asking the simpler question: *Does this observation belong to group DesignB?* The answer is yes/no and can be coded by a Boolean variable. For every level, a separate dummy variable is constructed that answer the simple yes/no question of membership. But, can Boolean variables enter arithmetic equations? At least, R assumes that this is safe. Boolean variables can always be converted to ones and zeros using as.numeric. Routinely, we can leave the whole dummy variable business to the linear model engine. Still, it is instructive to do it yourself once. 

```{r dummy_1, opts.label = "rtut"}
BAB1 <-
  BAB1 %>% 
  mutate(d_A = as.numeric(Design == "A"),
         d_B = as.numeric((Design == "B")))
 BAB1 %>% 
   select(Obs, Design, d_A, d_B) %>% 
   sample_n(8)

```

Vice versa, numbers in a logical context are always interpreted in complete reverse.

$$ v = 1 \mapsto \mathrm{TRUE}\\ v = 0 \mapsto \mathrm{FALSE} $$

Boolean variables can be both: numerical and categorial. It is unfortunate that we belittlingly call them dummies as they are truly bridges between the world of categories and arithmetics. They identify groups in our data set and switch on or off the effect in the linear term. For a factor G with levels A and B and zero/one-coded dummy variables d_A and d_B, the likelihood is written as:

$$ \mu_i = d_A\beta_{A} + d_B\beta_{B} $$

When $d_A=1,d_B=0$, the parameter $\beta_A$ is switched on, and $\beta_B$ is switched off. An observation of group A gets the predicted value: $\mu_i = \beta_A$, vice versa for members of group B. All arithmetic commands in R do an implicit typecast when encountering a Boolean variable, e.g. `sum(TRUE, FALSE, TRUE)` (the result is 2). In contrast, regression engines interpret Boolean variables as categorial. Therefore, dummy variables have to be passed on as explicitly numeric to the regression engine. Only when a variables truly is zeroes and ones, it will be interpreted dummy. As I have already done that with a typecast above, we can right-away pipe the following into `stan_glm`.


```{r fit:dummy_2, opts.label = "mcmc"}
M_dummy <-
  stan_glm(ToT ~ 0 + d_A + d_B, 
     data = BAB1)
```

```{r dummy_3, opts.label = "rtut"}
fixef(M_dummy)
```

The model contains two effects, which are exactly the group means. This model is what we can call an *absolute group means model (AGM)*. As there is no reference group, we don't want an intercept estimate anymore. Most research deals with estimating effects as differences. All regression engines quietly assume that what you want is a model with intercept. The `0 +` term in the model formula switches the intercept off.

While the regression engines handle factors internally, when specifying the likelihood we have to use the dummy variables. When doing an AGM on a factor $x_1$ with $1,...,k$ levels, the likelihood function becomes:

$$\mu_i=d_1 \beta_{1[1]}+...+d_k \beta_{1[k]}$$

Still, much of the time the CGM with a reference group is the model of choice. With reference group A, the CGM with ToT on design is written as follows:
$μ_i=β_0+d_B β_1$. In general for a factor with $k$ levels in a CGM with intercept, $k-1$ dummy variables need to be constructed in the way shown above. As all non-reference levels are seen as difference towards the reference, *the reference level is set to always on*. To see so, we extract the dummy variables from the CGM on design with the standard command `model.matrix`. As you can see, all observations are taken for the intercept, whereas level B is switched on and off.

```{r dummy_4, opts.label = "rtut"}
BAB1 %>% 
  select(Part, Design, ToT) %>% 
  cbind(model.matrix(M_Design)) %>% ## <---
  as_data_frame() %>% 
  sample_n(8)

```

In the remainder of the book, we are dealing with more complex models (e.g., multifactorial models in the next section), as well as factors with many levels (random effects in linear mixed-effects models [LMM]). With the above notation, the likelihood can become unduly long and I will use the following abbreviation, where the $[./r]$ denotes all factor levels, except the reference level r:

$$
μ_i=β_0+x_{1[./r]} β_{1[./r]}
$$
The regression engines usually take care of dummy variables. Still, by understanding the concept, we gained additional flexibility in specifying factorial models. One variant is to estimate an AGM, leaving out the intercept. Of course, this can also be done right-away with the formula `ToT ~ 0 + Design`. Later, we will see a very practical application of AGMs, when drawing interaction plots. The second gain is that we can specify the reference level as we want. It just depends on which dummy variable one sets to always-on. In the next section we will gain even more control over what the effects mean, by setting contrasts.


Exercises:

1.	BAB1 contains the variable Education, which separates the participants by three education level (Low, Middle, High). Construct the dummy variables and run an AGM.

2.	Specify the expanded CGM likelihood for education. Construct the dummy variables and run a regression.

3.	Consider you wanted to use education level High as reference group. Create dummy variables accordingly and run the regression.




### Contrasts

The default behaviour of regression engines when encountering a factor is to select the first level as reference group and estimate all other levels relative to that. This fully makes sense when you are after the effect of a treatment and is therefore called treatment contrasts. In a moment, we will encounter different contrasts that represent different questions, for example:

+ To what extent does a group deviate from the overall mean in the sample?

+	With three ordered factor levels, how large is the rolling effect, i.e. from level 1 to level 2, and the average of 1 and 2 to level 3?

But first, we have to figure out how contrasts are coded. Treatment contrasts do not have anything special to them, in fact. They are just a default of the regression engines. One might therefore assume that we have to alter the regression formula or otherwise specify the contrasts using dedicated arguments of the regression command. It is a bit confusing, that this is not so. Instead, *contrasts are set as attributes of the respective factor  variable*, which we can retrieve by the standard command contrasts:

```{r contrasts_1, opts.label = "rtut"}
levels(BAB1$Education)
contrasts(BAB1$Education)

```

The currently set contrasts table specifies the dummy variables for the CGM. The level Low is omitted in the contrasts table as it is the reference level with an always-on intercept. In the previous section i mentioned that you can choose the reference level freely by constructing the dummy variables accordingly. However, that would mean to always bypass the regression engines dummy handling. The package `forcats` provides a set of commands to change factor levels, for example reversing the order:

```{r contrasts_2, opts.label = "rtut"}
BAB1 <-
  BAB1 %>% 
  mutate(Education_rev = forcats::fct_rev(Education))
levels(BAB1$Education_rev)
contrasts(BAB1$Education_rev)

```

When running a CGM on Education_rev the intercept represents level High, now. It is worth noting that this model would make the very same predictions and residuals. It is the same model as before, just written differently.
Sometimes, it is useful to have contrasts other than treatment, as this more closely matches the research question. A plethora of contrasts is known, I will introduce *deviation contrasts* and *successive difference contrasts*.

[Example for deviation coding]

*Successive difference coding (SDC)* applies when one is interested in effects of progressive effects, such as performance gain in a number of sessions. Consider the following research situation. Shortly after the millenium, medical infusion pumps became infamous for killing people. Infusion pumps are rather simple devices that administer medication to a patients body in a controlled manner. Being widely used in surgery and intensive care, development of these devices must comply to national and international regulations. Unfortunately, the regulations of those days almost completely overlooked the human factor. While those devices would always function "as described in the user manual", they contained all kinds of severe violations of user-interface design rules, just to name few: foil switches with poor haptic feedback, flimpsy alphanumeric LCD screenlets and a whole bunch of modes. Imagine a medical devices company has partnered with some renowned research institute to develop the infusion pump of the future. Users got interviewed and observed, guidelines were consulted, prototypes developed, tested and improved. At the end of the process the design was implemented as an interactive simulation. In the meantime, national agencies had reacted, too, and regulations now demand a routine user-oriented development cycle. One of the new rules says: "the design must undergo validation testing with trained users".

That means you have to first introduce and train your users to get fluent with the device, then test them. We [REF] thought that the training process itself is of immense importance. Why not test it, then? In the real study we tested everyone three times and traced individual progress. This requires a reapted measures analysis and we are not quite there, yet [see LMM].

[TODO: 

+ simulate IPump case, non-repeated 

+ demonstrate succ diff contr 

+ find example for deviation contrasts 

+ Contrasts extend the idea of dummy variables. 

+ Dummy variables become continuous, thereby more flexible in their effects

]


```{r opts.label = "invisible"}
try(detach(BrowsingAB))
```




__ Exercises:__

1. With BrowsingAB, simulate several data sets of very small sample size. Observe how strongly the composition of education and age varies.

## Putting it all together: multi predictor models

Design researchers are often forced to obtain their data under rather wild conditions. Users of municipal websites, consumer computing products, enterprise information systems and cars are extremely diverse. Designs vary in many attributes, affecting the user in many different ways. There are many variables in the game, and even more possible relations. With *multi predictor models* we can examine the simultaneous influence of everything we could record.

### Empirical versus statistical control

Fundamental researchers have a knack for the experimental method. An *experiment*, strictly, is a study where you measure the effects of variables you *manipulate*. Manipulation is, almost literally, that it is *in your hands*, who receives the treatment. The fantastic thing about manipulation is that it allows for *causal conclusions*. A *strictly controlled experiment* is when all influencing variables are either manipulated or kept constant. That is an ideal and would not even be the case if you test the same person over-and-over again (like researchers in psychophysics often do). You never jump into the same river twice.

Sometimes an influencing variables lends itself to be kept constant. For example in psychological experiments, environment and equipment is usually kept constant. But, this comes with a one major disadvantage: it limits the possible conclusions drawn from the study. Imagine, you tested a smartphone app with participants, all students, comfortably sitting in a quiet environment. Would you dare to make conclusions on how any users perform in real life situations, say while driving a car? When keeping things constant, *ecological validity* and *generalizability* suffer.

In most applied design studies we need ecological validity and generalizability. If performance differs under certain conditions, you certainly want to know that. The solution is to vary conditions and record them as variables, as good as possible. For example, if you were to compare two voice-controlled intelligent agent apps, you could manipulate the ambient noise level, if you are in the lab.

In practically all applied studies, there exist variables, which you cannot manipulate for one of the following reasons. The first is that you do a field study, aiming for high ecological validity. Participants are using the system in the situations they encounter. Situational variables, like ambient noise level, cannot be manipulated. Second, it usually is impossible to manipulate user traits. If someone has an extrovert character or did a lot of gaming in the past, you cannot change that. Diversity of users is a fact and people come as they are.

Fundamental lab researchers are afraid of individual differences, too. The reasons are different, though: all non-manipulated influencing factors add noise to the study, which makes it harder to find the effects of interest. While lab researchers do there best to keep the environment constant, they cannot keep all participant traits constant. Lab researchers have two solutions to the problem: matching and randomized control. 

With *matching*, potentially relevant participant traits are recorded upfront; then participants are assigned to conditions such that groups have about the same composition. For example, one makes sure that the age distribution is about the same and both genders are equally represented. When all other influencing variables are constant between groups, the lab researcher can be sure that the effect is unambigously caused by the manipulation. So they say and routinely record participants age, gender and nationality.

You can only match what you can measure and you only measure what you expect. Human behaviour in everyday life is influenced by many factors in complex ways. Although a plethory of personality inventories exists,  doing them all prior to the real study is impossible. It would probably not even be effective. Never have i seen a design research study, where even the most established personality tests explain more than a few percent of variation. As another example, take the primacy effect: what you experienced first, has the strongest influence. In real life, impressions are constantly pouring on people and you will never be able to record and match that to a reasonable extent.

*Ramdomized control* is a misleading term, as what the researcher actually does is to *let go to chance*. Indeed, if the process of assigning participants to manipulations is completely left to chance, then *in the long-term* all groups will have the same composition of traits. Again, you were able to concluzde that the manipulation is the only cause. *Randomization* works well with larger samples. With small sample, it can still easily happen that one ends up with more or less heterogenous groups. Just by chance, more higher-educated people could have ended up in condition A of BrowsingAB.

Using manipulation, matching or randomization in in-the-wild research may work in some cases. In other cases it will be ineffective or impractical. The ultimate problem is the attempt to keep things constant.  In applied design research the questions rarely come down to a "Is A better than B?". If there is an age effect, you may certainly want to know it and see how the design effect compares to it.
 But, you can only examine what is varied and recorded. The approach of *statistical control* is to record (instead of manipulate) all variables that may influence the results and add them to the statistical model. As we will see now, the linear model puts no limits on the number of predictors. If you believe that user age may play a role for performance, just record it and add it to the model. 

### Crossing lines: multiple regression models



### Crossover groups: multifactorial models

The only necessary precondition for statistical control is that you can *record the influencing variable*. This had happened in the BrowsingAB study: the primary research question regarded the design difference, but the careful researcher always records the gender of participants.

What happened to the likelihood function when we moved from GMM to CGM and LRM? The effect of age was simply added to the intercept. For the education level effects, we expanded the dummy variables and then added them all up. Indeed, the linear model is defined as a succession of linear terms  $x_i\beta_i$ and nothing keeps us from adding further predictors to the model:
```{r}
attach(BrowsingAB)
```


```{r fit:mpm_1, opts.label = "mcmc"}
M_mpm_1 <- 
  BAB1 %>% 
  stan_glm(ToT ~ Design + Gender, data = .)
T_fixef_mpm_1 <- 
  fixef(M_mpm_1)
```

```{r tab:mpm_1, opts.label = "rtut"}

T_fixef_mpm_1

```

By adding gender the model, both effects are estimated simultaneously. In a *factorial MPM* the intercept is a reference group, too. Consider that both factors have two levels, forming a $2x 2$ design wiuth groups: A-F, A-M, B-F, B-M. The first one, A-F, has been set as reference group. Women in condition A have an average ToT of `r frm_coef(BrowsingAB$T_fixef_mpm_1, ~fixef == "Intercept")` seconds.
The other two fixed effects are, once again, differences to the reference. Here, nor does gender do much to performance, nor does the design effect really change, compared to the CGM.

[interaction plot]



### Regression in groups

Recall that dummy variables make factors compatible with linear regression. No barriers are left for combining factors and covariates in one model. For example, we can estimate the effects age and design simultaneously:

```{r fit:mpm_2, opts.label = "mcmc"}

M_mpm_2 <- 
  BAB1 %>% 
  stan_glm(ToT ~ Design + age_shft, data = .)
T_fixef_mpm_2 <- 
  fixef(M_mpm_2)

```

```{r tab:mpm_2, opts.label = "rtut"}

T_fixef_mpm_2

```

Once again, we get an intercept, first. Recall, that in LRM the intercept is the the performance of a 20-year old (age shifted). In GCM it was the mean of the reference group. When *marrying factors with a covariates, the intercept is point zero in the reference group*. The predicted average performance of 20-year olds with design A is `r frm_coef(BrowsingAB$T_fixef_mpm_2, ~fixef == "Intercept")`. The age effect has the usual meaning: by year of life, participants get `r frm_coef(BrowsingAB$T_fixef_mpm_2, ~fixef == "age_shft")` seconds slower. The *factorial effect* B is a *vertical shift of the intercept*. 20-year olds in condition B are `r frm_coef(BrowsingAB$T_fixef_mpm_2, ~fixef == "DesignB", neg = T)` seconds faster. In fact, this holds for all ages, as can be seen in the following figure. The model implies that the age affect is the same with both designs, which is not true, as we will later see. 

[interaction plot]

__Exercise:__

1. The `simulate` function in case environment BrowsingAB let you change the residual error (in standard deviations). Simulate three data sets with different residual variance, and estimate them by the same model. See how the uncertainty of effects behaves.














### How to plot MPM?

Now that we have seen how we can compare on one factor, we can extend the CGMl to more factors. With linear  models, we can extend the basic CG model to incorporate multiple of such factors. This is called *multifactorial models* (MFM).

Now, we are looking at a slightly more complicated situation, where browsing is predicted by design and education level of participants at the same time.

First, with the *ggplot* system, we plot the new situation. Recall that 

1. mapping variables in the data set to elemental properties in the splot, such as: x position, y position, color, shape etc.

2. selecting an appropriate geometry that carries such properties, e.g. points, bars, boxplots

```{r fig:anova_twofact_2, opts.label = "future"}
BAB1 %>%
  ggplot(aes(x = Design, 
             y = ToT, 
             fill = Education)) +
  geom_boxplot()
```

Now we can do the two-factorial ANOVA in much the same way as before. We only have to add the predictor *Education* to the model formula. Then we run the MCMC estimation and get the estimates.


## Putting it all together: linear models

What do you think is the *lm* in `stan_glm`? Well, the letters stand for *linear model (LM)*, and as we have seen so far, this includes the case of comparison of groups (CG, MFM, predictors are factors) and linear regression (LR, predictors are covariates). But, linear models can, in fact, deal with any mix of factors and covariates at the same time. Sometimes they are therefore called general liner models, but this term is very close to general*ized* linear models (GLM), which we will encounter in a later chapter. To avoid confusion, here we speak of linear models as the family of models containing GM, CG, MFM, LR and *any combination of those*. (Speaking of such combinations as mixed models would be equally misleading, see the next chapter).

In the BrowsingAB example, we were interested in the influence of age, but we haven't regarded another demographic variable of interested: level of education. First we explore graphically how education affect ToT:




```{r fig:glm_EDA_2, opts.label = "rtut"}
BAB1 %>%
	ggplot(aes(y = ToT, 
						 x = Education)) +
	geom_boxplot()
```


And here we call `stan_glm` once again, with both predictors, simultaneously

```{r fit:glm_mixed, opts.label = "mcmc"}
# GLM parameter estimation
M_glm <- 
  BAB1 %>% 
  stan_glm(ToT ~ Design + age, 
							data = .)

# T_resid <- mutate(T_resid, M_glm = residuals(M_glm))
```

```{r opts.label = "rtut"}

fixef(M_glm)

```


```{r fig:glm_mixed, opts.label = "future"}
# fe <- as.list(c(Tclm[["Browsing_AB_3"]]$center, 0))
# 
# iiaplot_4(fixef = fe, range = c(0, 80), label = c("age", "ToT"))
```



```{r opts.label = "invisible"}
try(detach(BrowsingAB))
```



## Interaction effects

The linear regression model `M_age` showed a considerable relationship between age and ToT. However, the uncertainty was pronounced and the residual error was reduced by only a small fraction. Model `M_Design` showed a small average advantage for design B, with high uncertainty. Neither of the two models provided compelling effects.

It is commonly known that older people tend to have lower performance than younger users. A numkber of factors are responsible, such as: slower processing speed, lower working memory capacity, lower motor speed and visual problems, It is also commonly held, that universal designs are possible. Universal designs reduce the cognitive, sensory and motor obstacles for impaired users. A perfect universal design is equally usable for everyone.

Is, perhaps, one of the two designs less of a burden for elderly users? We plot the observed ToT by age, but form groups by design. For seeing the trend, a linear smoother is added to the plot.

```{r}
attach(BrowsingAB)
```


```{r fig:glm_EDA_3, opts.label = "fig"}
G_slope_ia <-
  BAB1 %>%
  ggplot(aes(x = age,
             col = Design,
             y = ToT)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)


G_slope_ia
```


With the framework of GLM, we can use an arbitrary number of predictors. These can represent properties on different levels, for example, two designs proposals for a website can differ in font size, or participants differ in age. So, with GLM we gain much greater flexibility in handling data from applied design research, which allows us to examine user-design interaction (literally) more closely. 

The catch is that if you would ask an arbitrary design researcher:

> Do you think that all users are equal? Or, could it be that one design is better for some users, but inferior for others?

you would in most cases get the answer:

> Of course users differ in many ways and it is crucial to know your target group.

Some will also refer to the concept of usability by the ISO 9241-11, which contains the famous four words:

> "... for a specified user ..."

The definition explicitly requires you to state for *for whom* you intended to design. It thereby implicitly acknowledges that usability of a design could be very different for another user group. In other words, statements on usability are by the ISO 9241-11 definition *conditional* on the target user group.

In statistical terms, conditional statements of the form:

> the effect of predictor A (the design) depends on the level of predictor B (the user group)

are called *interaction effects*. 

As we have seen, with GLM, it is  possible to investigate the effect of design properties and user properties simultaneously. For example, assume that main difference between design A and B in the web browsing example is that A uses larger letters than B. Would that create the same benefit for everybody? It is not unlikely, that larger letters only matter for users that have issues with far sightedness, which is associated with age. Maybe, there is even an adverse effect for younger users, as larger font size takes up more space on screen and more scrolling is required. 


```{r fit:BAB_ia1, opts.label = "mcmc"}

M_ia1 <- 
  BAB1 %>% 
  stan_glm(ToT ~ Design + age + Design:age,
				 data = .)

# T_resid <- mutate(T_resid, M_ia1 = residuals(M_ia1))

```

```{r opts.label = "rtut"}
fixef(M_ia1)

```


```{r fig:Browsing_interaction, opts.label = "future"}

# fe <- fixef(M_ia1)
# 
# iiaplot_4(fixef = fe, range = c(0, 80), label = c("age", "ToT"))

```

If we can do it with covariates, like age, we can do it with factors, too. For example, does the overall improvement from design A to B differ for the educational level of participants?

```{r fit:BAB_ia2, opts.label = "mcmc"}

M_ia2 <- 
  BAB1 %>% 
  stan_glm(ToT ~ Design + Education + Design:Education,
				 data = .)

# T_resid <- mutate(T_resid, M_ia2 = residuals(M_ia2))


```

```{r opts.label = "rtut"}
fixef(M_ia2)
```


Well, interaction effects occur routinely, but a `Design:Education` effect is not a feature of the BrowsingAB data set.


```{r}
detach(BrowsingAB)
```


We can distinguish between *saturation effects* and *amplification effects*.

```{r interaction_effects, message=FALSE, warning=FALSE}
	expand.grid(effect = c("saturation", "amplification"), 
							A = c(0,1),
							B = c(0,1)) %>%
		join(data.frame(effect = c("saturation","amplification"),
										beta_1 = c(.1,.1),
										beta_2 = c(.2,.2),
										beta_3 = c(-0.3, 0.3)))  %>%
		mutate(Outcome = A * beta_1 + B * beta_2 + A * B * beta_3) %>%
		mutate(B = factor(B, labels = c("low", "high")),
					 A = factor(A, labels = c("low", "high"))) %>%
		ggplot(aes(x = A, col = B, y = Outcome)) +
		geom_point(size = 3) +
		geom_smooth(aes(group = B, col= B), method = "lm") +
		facet_grid(.~effect)

Interactions <- expand.grid(effect = c("saturation", "amplification"), 
						A = seq(0,1, length.out = 11),
						B = seq(0,1, length.out = 11)) %>%
	join(data.frame(effect = c("saturation","amplification"),
									beta_1 = c(.1,.1),
									beta_2 = c(.2,.2),
									beta_3 = c(-0.3, 0.3)))  %>%
	mutate(Outcome = A * beta_1 + B * beta_2 + A * B * beta_3)

library(lattice)
grid.arrange(
	wireframe(Outcome ~ A + B, 
						data = filter(Interactions, effect == "saturation"),
						main = "saturation"),
	wireframe(Outcome ~ A + B, 
						data = filter(Interactions, effect == "amplification"),
						main = "amplification"),
	ncol = 2
)
```



### Saturation: less than the sum

Most statistically trained researchers are aware of some common assumptions of linear regression, such as the normally distributed residuals and variance homogeneity. Another assumption that is often disregarded, is the assumption of linearity, which arises from the basic regression formula:

$$y_i = \beta_0 + \beta_1 x_{1i} ...$$

The formula basically says, that if we increase $x$ (or any other influencing 
variable) by one unit, $y$ will increase by $\beta_1$. Imagine, you are testing
the influence of font size on reading performance. In your experiment, you found 
that increasing from 8pt to 12pt cuts the required reading time by 10 seconds. 
According to the linear model, you has to expect the reading time to improve 
by another 10 seconds, when enlarging to 16pt.

```{r reading_time}
D_reading_time <- 
  data_frame(font_size = c(4, 8, 12, 16, 24, 28),
           observed_time = c(NA, 40, 30, NA, NA, NA),
           predicted_time = 60 - font_size/4 * 10)
D_reading_time %>% 
  kable()
```

It is immediatly clear, that these expectations have no ground: for normally sighted persons, a font size of 12 is easy to decipher and another increase will not have the same effect. Taking this further, one would even arrive at absurdly short or 
impossible negative reading times. At the opposite, a font size of four point may just render unreadable on a computer screen. Instead of a moderate increase by 
10 seconds, participants may have to decipher and guess the individual words, which will take ages.

A major flaw with the linear model is that it presumes the regression line to 
increase and fall infinitely. However, in an endless universe everything has 
boundaries. The time someone needs to read a text is limited by fundamental cognitive processing speed. We may be able to reduce the inconvenience of deciphering small 
text, but once an optimum is reached, there is no further improvement.

Boundaries of performance measures inevitably lead to non-linear relationships with predictors. Modern statistics knows several means to deal with non-linearity, 
several of them are introduced in later chapters of this book ([GLM, NLM]). 
In the case of two (or more) interventions to improve a 

Let me explain saturation effects with an example that is intuitive enough, whether you are an design expert, or not. Consider ...


```{r prep_headache, opts.label = "invisible"}
attach(Headache)

T_means <-
  Pills %>% 
  group_by(PillA, PillB) %>% 
  summarise(reduction = mean(reduction)) %>% 
  ungroup()

T_means_cross <-
  T_means %>% 
  mutate(reduction = round(reduction,1)) %>% 
  spread(PillA, reduction)


Pills %>% 
  ggplot(aes(x = PillA, col = PillB, reduction)) +
	geom_boxplot()


```

Below, we estimate a series of models, which we are now going to compare.

```{r fit:Headache, opts.label = "mcmc"}
M_1 <-
  stan_glm(reduction ~ PillA + PillB, 
                          data = Pills)
M_2 <- 
  stan_glm(reduction ~ PillA * PillB, 
                          data = Pills)

M_3 <- 
  stan_glm(reduction ~ before * PillA + PillA * PillB, 
                          data = Pills)

M_4 <- 
  stan_glm(reduction ~ -1 + PillA:PillB, 
     data = Pills)

```

Imagine, the researcher started with a two-way ANOVA, estimating the mein effects PillA and PillsB. The result is:

```{r tab:M_1_fixef}
T_fixef_1 <- fixef(M_1)
T_fixef_1
```

Given these estimates, we conclude that pill A has an almost double reduction of headache compared to B. We further notice the Intercept, indicating that headache spontaneously diminishes at a certain rate.

Setting the spontaneous recovery aside, what would be the prediction of the model for the condition with both pills?

```{r Headache_guess}

T_guess <-
  T_means %>% 
  mutate(guess = c(0, 
                   round(T_fixef_1[[2,6]],2),
                   round(T_fixef_1[[3,6]],2),
                   "?")) %>% 
  select(-reduction) %>% 
  spread(PillB, guess)

T_guess
```

Probably, you came up with 

$$`r round(sum(Headache$T_fixef_1$center), 2)`$$

But, is that reasonable? Do the effects of headache pills simply add up like this? Consider a scenario, where five headache pills were compared in the same manner. If we assume linear addition of effects, some participants in the group with all pills could even experience the breathtaking sensation of *negative* headache. We clarify the situation by  introducing an interaction term in our second model. 

```{r tab:Headache_fixef_2}
T_fixef_2 <-  fixef(M_2)
T_fixef_2
```



We first examine the main effects for both pills. Interestingly, both effects are now considerably stronger than before. Taking one pill at a time are more effective than we thought from the first model. The fourth coefficient (`PillATRUE:PILLBTRUE`) is the interaction term. With the updated model we, again, predict the net headache reduction when both pills are given. This is as simple as summing up all coefficients from intercept to interaction. The result is a combined effect of both main effects, *reduced* by   `r frm_coef(Headache$T_fixef_2, ~fixef == "PillATRUE:PillBTRUE" , interval = F, neg = T)`. As we would expect, the effectiveness of two pills is not their sum, but less. One can have headache to a certain degree or no headache at all. If it's gone, any more pills have no additional effects.

Design of systems is a matter of compromises. A common conflict of interests is between the aesthetic appearance and the ergonomic properties. Consider the  typesetting on web pages. Many commercial websites are equally concerned about the ease of comprehending information and positive emotional response. From an ergonomic point of view, the typesettimg of longer text simply requires you to choose the best screen-readable font, find an optimal font size and maximize contrast. If you were now to suggest typesetting a website in black Arial on white background, you are in for trouble with your graphic designer or, much worse, the manager responsible for corporate impression management. In any case, someone will insist on a fancy serif font (which renders rather poorly on low resolution devices) in an understating blueish-grey tone. For creating a relaxed reading experience, the only option left is to increase the font size. This option is limited, though, as too large letters requires excessive scrolling.

The question arises: can one sufficiently compensate lack of contrast by setting the text in the maximum reasonable font size 12pt, as compared to the more typical 10pt? In the fictional study Reading, this is examined in a 2x2 between-subject design:  the same page of text is presented in four versions, with either 10pt or 12pt, and grey versus black font color. Performance is here measured as time-on-task of reading the full page.

```{r}
detach(Headache)
```


```{r tab:reading_data}
attach(Reading)

D_1 %>% 
  sample_n(8) %>% 
  select(-mu) %>% 
  arrange(Part) %>% 
  kable()
```


```{r fig:reading_expl_1}
D_1 %>% 
  ggplot(aes(col = font_color,
             x = font_size,
             y = ToT)) +
  geom_boxplot()
```

We see immediately, that both design choices have a an impact: more contrast, as well as larger letters lead to improved reading performance. But, do they add up? Or do both factors behave like headache pills, where more is more, but less than the sum. Clearly, the 12pt-black group could read fastest on average. So, neither with large font, nor with optimal contrast alone has the design reached maximum performance, i.e. saturation. Still, there could be an interaction effect, that gradually reduces the positive effect of large font in a high-contrast design. We run two regression model, one with both main effects and one that adds an interaction term. We extract the coefficients from both models and view them side-by-side:

```{r fit:reading_1, opts.label = "mcmc"}
M_1 <-
  D_1 %>% 
  stan_glm(ToT ~ font_size + font_color,
           data = .)

M_2 <-
  D_1 %>% 
  stan_glm(ToT ~ font_size + font_color + font_size : font_color,
           data = .)


T_read_fixef <-
  right_join(select(fixef(M_1), fixef, M_1 = center),
             select(fixef(M_2), fixef, M_2 = center),
             by = "fixef")



```

```{r}
T_read_fixef %>% kable()
```


The estimates confirm, that both manipulations have a considerable effect. And, there is an interaction effect as well, correcting the additive effect of font color and size. The combined effect of high contrast and large font is therefore

$$\mu_{12pt, black} = `r Reading$T_read_fixef[1,3]` + `r Reading$T_read_fixef[2,3]` + `r Reading$T_read_fixef[3,3]` + `r Reading$T_read_fixef[4,3]` = `r sum(Reading$T_read_fixef[,3])`$$

In the research scenario, that was not the question, strictly, as we were primarily interested in comparing the effects of font size and contrast. Also, if we see the credibility interval of the interaction effect it is not highly certain (`r frm_coef(fixef(Reading$M_2), row = 4)`)) Still, including the  interaction term is the right choice, for two reasons: first, both manipulations influence the same cognitive processing, as they both improve visual clatity, with the effect of better visual letter recognition. From a theoretical perspective, we would thus expect saturation.   This is simply the application of prior knowledge and in a perfect world one would  use a prior distribution for the interaction term, creating a more certain estimate. That being said, the data set is synthatic, and the simulation definitely included the interaction effect.  Second, in Table [XY], all other coefficients change considerably with introduction of the interaction effect. Especially, the effects of the two manipulations get considerably under-estimated. 

Why are the main effects under-estimated, when not including the interaction term? The pure main-effects model has three parameters. This allows it to represent the same numner of independent group means. Formally, the number of groups in the study is four. However, the three-parameter model assumes that the fourth group (black, 12pt) can sufficiently be specified by the existing three parameters. If saturation occurs, the group of participants in the 12pt group is not homogenous: in the grey group, they experience a stronger improvement than in the black group. The three parameter model is forced to make the best out of situation and adjusts the net effect of font size to be slightly lower than it actually is. The same occurs for the font color effect.

Interaction effects are notoriously neglected in research, and they are hard to grasp for audience with or without a classic statistics education. It is therefore highly recommended to illustrate interaction effects using interaction plots. 
To begin with, an interaction plot for the 2x2 design contains the four estimated group means. These can be computed from the linear model coefficients, but there is an alternative way: we modify the model, such that it represents the four group means, directly. The re-parametrized model has no intercept (if we want the group means directly, we need no reference group), and no main effects. The model contains just the raw interaction effect, which results in an estimation of the four group means:

```{r fit:reading_2, opts.label = "mcmc"}
M_3 <-
  D_1 %>% 
  stan_glm(ToT ~ 0 + font_size : font_color,
           data = .)


```

```{r opts.label = "rtut"}
fixef(M_3)
```


With some basic data transformation on the coefficient table, we create a data frame that encodes the two factors separately. Note how `separate` is used to split the coefficient names into group identifiers. Using `mutate` and `str_replace`, the group labels are stripped the factor names. The resulting coefficient table serves as input to ggplot, creating an interaction plot with overlayed credibility intervals, using `geom_errorbar`.

```{r tab:reading, opts.label = "rtut"}
T_2x2_mu <-
  fixef(M_3) %>% 
  separate(fixef, c("font_size", "font_color"), sep = ":") %>% 
  mutate(font_size = str_replace(font_size, "font_size", ""),
         font_color = str_replace(font_color, "font_color", "")) %>% 
  as.data.frame()

G_interaction <-
  T_2x2_mu %>% 
  ggplot(aes(x = font_color, 
             color = font_size, shape = font_size,
             y = center,
             ymin = lower,
             ymax = upper)) +
  geom_point() +
  geom_line(aes(group = font_size)) +
  geom_errorbar(width = .1)

G_interaction

detach(Reading)
```

From Figure XY we can easily spot the two main effects, and how they go together. The researcher would conclude that with the desired gray font, increasing font size partly compensates for the lack of contrast. At the same time, we see that some saturation occurs, but still, from a purely ergonomic perspective, large font and high contrast is the preferred design.




In the two-by-two groups case of the Reading study, the interaction plot is created 

Interaction effects can severely bias main effects, when gone unnoticed:

[TBF]

```{r headache_2, opts.label = "future"} 
## FIXME ###
# T_headache_bias <- 
#   fixef(M_1) %>% 
#   left_join(fixef(M_2), by = "parameter") %>% 
#   select(parameter, Model_1 = location.x, Model_2 = location.y) %>%
#   mutate(parameter = str_replace(parameter, "TRUE", ""), 
#          bias = Model_1 - Model_2)
```


We have seen in the Headache example that interaction effects occur 
as non-linearities. The more a participant approaches the natural boundary of zero headache, the less benefit is created by additional effort. This we call  *saturation*. Saturation is likely to occur when multiple factors influence the same cognitive or physical system or functioning. In quantitative comparative design studies, we gain a more detailed picture on the co-impact of design interventions and can come to more sophisticated decisions.

In the Reading case, overall utility is a compound of ergonomic quality and aesthetic appeal. It was assumed that a gray-on-white color scheme is more appealing. The researcher would choose the gray-12pt design: higher contrast would increase ergonomic value, but too much at the expense of appeal. Of course, in a perfect world, one would add emotional appeal as a second measure to the study.

If we don't account for saturation by introducing interaction terms, we are prone to underestimate the net effect of any of these measures, and may falsely conclude that a certain measure is ineffective. Consider a large scale study, that asseses the simultaneous impact of many demographic variables on how willing customers are to take certain energy saving actions in their homes. It is very likely that subsets of variables are associated with similar cognitive processes. For example, certain action requires little effort (such as switching off lights in unoccupied rooms), whereas others are time-consuming (drying the laundry outside). At the same time, customers may vary in the overall eagerness (motivation). For high effort actions the impact of motivation level probably makes more of a difference as when effort is low. Not including the interaction effect would result in the false conclusion that suggesting high effort actions is rather ineffective.


### Amplification: more than the sum

Saturation effect occur, when multiple factors act on the same system and can (partly) compensate each other. There is an interesting parallel to Boolean algebra. The Boolean OR operator returns `TRUE` when one of the operands is `TRUE`:

```{r tab:boolean_OR}
T_bool_interaction <-
data_frame(A = c(F, F, T, T),
           B = c(F, T, F, T)) %>%
  as_data_frame() %>% 
  mutate("A_OR_B" = A | B) %>% 
  mutate("A_AND_B" = A & B)
 
kable(T_bool_interaction)

```

The Boolean OR is an extreme case of saturation. Once factor A is present, the result is "positive" and introducing B has absolutely nor additional effect. Another basic Boolean operation is $A AND B$ with quite the opposite occuring: any of the two factors only has an effect, if the other is present, too. Rarely will we encounter non-trivial cases where the results are so crisp as in the Boolean world. The gradual version of the Boolean AND is *amplification*: the impact of one factor gets stronger with the presence of another factor.

Amplification can occur, when two (or more) factors impact different cognitive processes in a processing chain. To my mind, finding possible amplification effects is far more challenging as compared to saturation effects, because one has to deal with more complex cognitive or physiological models.

Here, a fictional study on technology acceptance will serve to illustrate amplification effects. Imagine a startup company that seeks funding for a novel augmented reality game, where groups of gamers compete for territory. For their fund raising endeavour they need numbers on the potential market. The entrepreneurs have two hypotheses they want to verify:

1. The game requires some top-notch equipment which only technophile persons will dare to use

1. The game is strongly cooperative and therefore more attractive for people with a strong motivation for social activities

In their study they collect self ratings of a larger set of participants, where they had to rate their technophily and sociophily. In addition, they were given a description of the planned game and were asked how much they intend to participate in the game.

While the example primarily serves to introduce amplification effects, it is also an opportunity to get familiar with interaction effect between metric predictors. While this is not very different to interaction effects on groups, there are a few peculiarities, one being that we cannot straight-forwardly make an exploratory plot. For factors we have used boxplots, but these do not apply for metric variables. In fact, it is very difficult to come up with a good graphical representation. One might think of 3D wireframe plots, but these transfer poorly to the 2D medium of these pages. Another option is to create a scatterplot with the predictors on axis and encode the outcome variable by shades or size of dots. These options may suffice to see any present main effects, but are too coarse to discover the subtlety of interaction effects. This is not an issue with metric interaction effects per se, but more general: while we can comprehend more complex models from coefficient tables, exploratory graphs are limited to just a few variables at the same time. Therefore, this analysis does without the usual exploratory graph. However, we will introduce a method to create interaction plots for metric predictors.


```{r fit:ARgame, opts.label = "mcmc"}
attach(AR_game)

M_1 <-
  D_1 %>% 
  stan_glm(intention ~ sociophile * technophile,
           data = .)

detach(AR_game)
```

```{r opts.label = "rtut"}
attach(AR_game)
fixef(M_1)
detach(AR_game)
```


### Theoretically interesting interaction effects

Explaining or predicting complex behaviour with psychological theory is a corner stone of design science. Unfortunately, it is not that easy. Few psychological theories cover more than three associations between external or individual conditions and behaviour. The design researcher is often forced to enter a rather narrow perspective, or knit a patchwork model from multiple theories. Such a model can either be loose, making few assumptions on how the impact factors interact which others. A more tightened model frames multiple impact factors into a conditional network, where impact of one factor can depend on the overall configuration. A classic study will now serve to show how interaction effects and theoretical reasoning go together. 

Vigilance is the ability endure in attention for rarely occuring events. Think of truck drivers on lonely night tours, where most of the time they spend keeping the truck on a straight 80km/h course. Only every now and then is the driver required to react to an event like braking lights flaring up ahead. Vigilance tasks are among the hardest thing to ask from a human operator, yet, they are safety relevant in a number of domains. Keeping up vigilance most peoaple perceive as tiring, and vigilance detoriates with tiredness.

Several studies have shown that reaction time at simple tasks increases when people are tired. The disturbing effect of noise has been documented as well. A study by Corcoran (1961) examined the simultaneous influence of sleed deprivation and noise on a rather simple reaction task. They ask:

>  will the effects of noise summate with those of loss of sleep to induce an even greater performance decrement or will noise subtract from the performance decrement caused by loss of sleep?

The central argument is that sleep deprivation detoriates a central nervous arousal system. In consequence, sleep deprived persons cannot maintain the necessary level of tension that goes with the task. Noise is a source of irritation and therefore usually reduces performance. At the same time, noise may have an arousing effect, which may compensate for the loss of arousal due to sleep deprivation. To re-iterate on the headache pills analogy, noise could be the antidote for sleepiness.

The Sleep case study is a simplified simulation of Corcoran's results. Participants were divided into 2x2 groups (quiet/noisy, rested/deprived) and has to react to five signal lamps in a succession of trials. In the original study, as performance measure gaps were counted, which is the number of delayed reactions ($>1500ms$).

```{r fig:Sleep_expl}
attach(Sleep)
G_expl <-
  D_1 %>% 
  ggplot(aes(x = Environment,
             color = Sleep,
             y = gaps)) +
  geom_boxplot()
```

Using a 2x2 model including an interaction effect, we examine the conditional association between noise and sleepiness. The `*` operator in the model formula is an abbreviation for the fully factorial term `Environment + Sleep + Environment:Sleep`.

```{r fit:Sleep, opts.label = "mcmc"}
M_1 <- 
  D_1 %>% 
  stan_glm(gaps ~ Environment * Sleep, data = .)

```

```{r tab:Sleep_fixef}
T_fixef <-
  fixef(M_1)
T_fixef %>% kable()
```

Recall, that treatment contrasts were used, where all effects are given relative to the reference group quiet-rested. The results clearly confirm the detoriating effect of sleepiness, although its exact impact is blurred by pronounced uncertainty `r frm_coef(Sleep$T_fixef, ~fixef == "SleepSleepy")`. Surprisingly, noise did not affect well-rested persons at all `r frm_coef(Sleep$T_fixef, ~fixef == "Environmentnoisy")`. Maybe that is because steady white noise was used, not a disturbing tumult. The irritating effect of noise may therefor be minimal. As suggested by Corcoran, the effect of sleepiness on performance is partly reduced in a noisy environment `r frm_coef(Sleep$T_fixef, ~fixef == "Environmentnoisy:SleepSleepy")`. This suggests the theory that the arousal system is involved in the detoriating effect of sleep deprivation, which has interesting consequences for the design of vigilance tasks in the real world. 

The finding also relates to the well known Yerkes-Dodson law in ergonomic science. The law suggests that human performance at cognitive tasks is influenced by arousal. The influence is not linear, but better approximated with a curve as shown in Figure XY. Performance is highest at a moderate level of arousal. If we assume that sleepy participants in Corcona's study showed low performance due to under-arousal, the noise perhaps has increased the arousal level, resulting in better performance. If we except that noise has an arousing effect, the null effect of noise on rested participants stands in opposition to the Yerkes-Dodson law: if rested participants were on an optimal arousal level, additional arousal would usually have a negative effect on performance. There is the slight possibility, that Corcona has hit a sweet spot: if we assume that calm/rested participants were still below an optimal  arousal level, noise could have pushed them right to the opposite point. 

Central to the Yerkes-Dodson law is that arousal is a gradually increasing property, but the current experiment only features two levels. A straight line is the only way to unambiguously connect two group means; examining any curved relationships is impossible. 
We could think of varying noise levels over a wider range for better tracing the non-linear relationship between arousal and performance. The next section introduces polynomial regression for approximating non-linear associations between metric variables.


```{r fig:Yerkes_Dodson_1}
F_Yerkes <-
  function(range = c(-2, 2)){
    data_frame(arousal = seq(-2, 2, length.out = 200)) %>% 
      mutate(performance = dlogis(arousal))
  }

D_Yerkes_Sleep <-
  D_1 %>% 
  distinct(Environment, Sleep) %>% 
  mutate(Condition = str_c(Environment, Sleep, sep = "/"),
         arousal = c(-.5, .4, -1.5, -1),
         performance = dlogis(arousal)) %>% 
  arrange(arousal)
  

G_Yerkes_1 <-
  F_Yerkes(seq(-2,2,.1)) %>% 
  ggplot(aes(x = arousal, y = performance)) +
  geom_line() +
  geom_line(aes(x = arousal, y = performance,
                color = "Sleep study"), 
            data = D_Yerkes_Sleep) +
  geom_text(aes(x = arousal, y = performance, label = Condition,
                color = "Sleep study"),
            data = D_Yerkes_Sleep) +
  theme(axis.text.y=element_blank())

G_Yerkes_1

```


```{r}
detach(Sleep)
```


## Up and down: polynomial regression [eventually move to NLM]

Robots build our cars and sometimes drive them. They mow the lawn and may soon also deliver parcels to far-off regions. Prophecy is that robots will also enter social domains, such as accompany children and care for our seniors. One can assume that in social settings emotional acceptance plays a significant role for technology adoption. Next to  our voices, our faces and mimic expressions are the main source of interpersonal messaging. Since the dawn of the very idea of robots, anthropomorph designs have been dominant. Researchers and designers all around the globe are currently pushing the limits of humanlikeness of robots. One could assume that emotional response improves with every small step towards perfection. Unfortunately, this is not so. [Mori] discovered a bizarre non-linearity in human response: people respond more positive to improvements in humanlikess, but only at the lower end. A feline robot design with recognizable but stylized facial features will always beat a faceless robot with a strong mechanical appeal. Designs on the high end, that are very humanlike, but not exactly, face a sudden drop in emotional response, which is called the *uncanny valley*.

```{r fig:uncanny_valley}
# detach(Uncanny)
attach(Uncanny)

G_uncanny_illu <-
  data_frame(hl = seq(-1, 1, length.out = 100),
             likeability = -.5 * hl + .6 * hl^3 + .2 * hl^4) %>% 
  mutate(human_likeness = (hl + 1)/2) %>% 
  ggplot(aes(x = human_likeness, y = likeability)) +
  geom_line()

G_uncanny_illu
```


[Mathur et al.] study aimed at rendering the association between humanlikeness and liking at full range. They collected 60 pictures of robots and attached a score ranging from mechanical to human appearance. Then they let more than 200 participants rate the faces on how much they liked them. Finally, they created an average score of likeability per robot picture and examined the association with human likeness using a statistical model. Owing to the curved shape of the uncanny valley, linear regression is not applicable to the problem. Instead, Mathur et al. applied a third degree polynomial term.

A polynomial function of degree $k$ has the form:

$$y_i = \beta_0 x_i^0 + \beta_1 x_i^1 + ... + \beta_{k}  x_i^{k}$$

In fact, you are already familiar with two polynomial models. The first degree polynomial is the grand mean model. This follows from $x_i^0 = 1$, which makes $\beta_0$ a constant, the intercept. Also, a second degree polynomial is just the linear model. By adding higher degrees we can introduce curvature to the association.

Mathus et al. argue that the Uncanny Valley curve possesses two stationary points, where the slope is zero. One is a local minimum and represents the deepest point in the valley. The second is a local maximum and marks the shoulder left of the valley. Such a curvature can be approximated with a polynomial of (at least) third degree, which has a constant $\beta_0$, a linear slope $\beta_1$, quadratic parameter $\beta_2$ and a cubic parameter $\beta_3$.

While R provides high-level methods to deal with polynomial regression, it is instructive to build the regression manually. The first step is to add variables to the data frame, which are the exponentiated predictors ($x_k = x^k$). These variables are then straight-forwardly added to the model term, as if they were independent predictors. For better clarity, we rename the Intercept to be $x_0$, before summarizing the fixed effects.

```{r fit:UV_1, opts.label = "mcmc"}
M_1 <-
  UV_1 %>% 
  mutate(huMech_1 = huMech,
         huMech_2 = huMech^2,
         huMech_3 = huMech^3) %>% 
  stan_glm(avg_like ~ 1 + huMech_1 + huMech_2 + huMech_3,
           data = ., iter = 500)

P_1 <-
  posterior(M_1) %>% 
  mutate(parameter = str_replace(parameter, "Intercept", "huMech_0"),
         fixef = str_replace(fixef, "Intercept", "huMech_0")) %>% 
  posterior()
  
T_fixef_1 <- fixef(P_1)
T_fixef_1
save_CE(CE)
```

```{r}
T_fixef_1
```


We can extract the fixef effects table as usual. The four coefficients specify the poynomial to approximate the average likeability responses. The polynomial parameters have little explanatory value. Neither of the parameter alone relates to a relevant property of the uncanny valley. One relevant property would be the location of the deepest point of the uncanny valley, its trough. The trough is a local minimum of the curve and With polynomial techniques, we can find this point. 

Finding a local minimum a two step procedure: first, we must find all *stationary points*, which includes local minima and maxima. Then, we determine which of the resulting points is the local minimum. Stationary points occur, where teh curve bends from a rising to falling, or vice versa. They are distinguishable by having a slope of zero, neither rising nor falling. Stationary points can be identified by the derivative of the third degree polynomial, which is a second degree polynomial:

$$f'(x) = \beta_1 + 2\beta_2x + 3\beta_2x^2$$

The derivative $f'(x)$ of a function $f(x)$ gives the slope of $f(x)$ at any given point $x$. When $f'(x) > 0$, $f(x)$ is rising at $x$, with $f'(x) < 0$ it is falling. Stationary points are precisely those points, where $f'(x) = 0$ and can be found by solving the equation. The derivative of a third degree polynomial is of the second degree, which has a quadratic part. This can produce a parabolic form, which hits point zero twice, during rise and when falling. A rising encounter of point zero indicates that $f(x)$ has a local minimum at $x$, a local maximum when falling. In consequence, solving $f'(x) = 0$ can result in two solutions, one minimum and one maximum, which needs to be distinguished further. 

If the stationary point is a local minimum, as the trough, slope switches from negative to positive; $f'(x)$ crosses $x = 0$ in a rising manner, which is a positive slope of $f'(x)$. Therefore, a stationary point is a local minimum, when of $f''(x) > 0$.

Mathur et al. followed these analytical steps to arrive at an estimate for the position of the trough. The following code uses several high-level functions from package `polynom` to estimate the location of the trough, by drawing on the first and second derivative `d[d]poly` 

```{r trough_from_fixef, opts.label = "rtut"}
library(polynom)

poly     = polynomial(T_fixef_1$center) # UC function on center
dpoly    = deriv(poly)                  # 1st derivative
ddpoly   = deriv(dpoly)                 # 2nd derivative
stat_pts = solve(dpoly)                 # finding stat points
slopes   = as.function(ddpoly)(stat_pts)# slope at stat points
trough   = stat_pts[slopes > 0]         # selecting the local minimum

cat("The trough is most likely at a huMech score of ", round(trough, 2))
```

While this procedure is instrictuve, there is an issue: drawing on the center estimates, which is a summary of the PD, we get a point estimate, only. Statements on certainty are impossible, as a CI is lacking. Recall the 99 seconds study, where we operated directly on the PD to obtain more specific statements on certainty. Another case for directly operating on the PD samples is to calculate additional statistics. It is another virtue of the MCMC method, that this is possible.

Every PD sample contains simultaneous draw of the four parameters `huMech_[0:3]`, and therefore fully specifies its own third degree polynomial. A PD for the trough parameter can be obtained by performing the above procedure on every sample separately. For the convenience, the case study Uncanny contains a function `trough(coef)` that includes all the above steps. The following code creates a data frame with one row per MCMC draw and the four huMech variables, the function `trough` acts on this data frame as a matrix of coeffients and returns one trough point per row. We have obtained the PD of the trough.


```{r trough_from_PD, opts.label = "rtut"}
P_1_trough <-
  P_1 %>%
  filter(type == "fixef") %>%
  select(chain, iter, fixef, value) %>% 
  spread(fixef, value) %>% 
  select(starts_with("huMech")) %>% 
  trough()

```


From the PD, we can derive statements on uncertainty in the usual way: the 95% credibility limits we get by:

```{r trough_CI95}
quantile(P_1_trough, c(.025, .975), na.tm = T)
```


The 95% CI is a conventional measure of uncertainty and may be more or less irrelevant for the spectator. The most generous account uncertainty is a density plot on the full posterior. The density function just smoothes over the frequency distribution of trough draws, but makes no arbitrary choices on where to cut it. 

```{r}
grid.arrange(
  UV_1 %>% 
    ggplot(aes(x = huMech, y = avg_like)) +
    geom_point(size = .3) +
    xlim(-1, 1),
  
  data_frame(trough = P_1_trough) %>% 
    ggplot(aes(x = trough)) +
    geom_density(aes(x = trough)) +
    xlim(-1, 1)
)
```

With reasonable certainty, we can say that the trough is at approximately two-thirds of the huMech score range. The illustration of the uncanny valley as they used to be perpetuated from the original source, place the trough at about four quarters of the scale. We clearly see that this is not the case in Mathur's study. This might be an artifact, for example in that the huMech score is not linearly related to the perception of human-likeness. 

```{r}
detach(Uncanny)
```



## Stub: How parameter estimation with RSS works

Here I demonstrate a brute force method to find the optimal parameters for the linear regression.
The parameter space span by $beta_0$ and $\beta_1$ is systematically explored, and for every combination the RSS is computed. The point where the RSS is at its minimum is the _least square estimate_.

```{r rss_minimization, opt.label = "deprecated"}
# ## function for RSS
# RSS = Vectorize(function(beta_0, beta_1){
#   Browsing %>%
#     mutate(Predicted = beta_0 + beta_1 * age) %>%
#     mutate(squrResid = (Predicted - Browsing)^2) %>%
#     select(squrResid) %>%
#     sum()
# })
# 
# ## exploring (part of) the parameter space
# E1 = expand.grid(beta_0 = seq(-14, -12, length.out = 100),
#                   beta_1 = seq(0.2, 0.5, length.out = 100)) %>%
#   mutate(logRSS = log(RSS(beta_0, beta_1)))
# 
# ## here is the minimum
# E1 %>%
#   filter(logRSS == min(logRSS))
# 
# ## exloring it graphically
# grid.arrange(
#   E1 %>%
#     ggplot(aes(x = beta_0, y = beta_1, col = logRSS)) +
#     geom_point(),
#   E1 %>%
#     gather(parameter, value, 1:2) %>%
#     ggplot(aes(x = value, y = logRSS)) +
#     geom_point(alpha = .5) +
#     facet_grid(.~parameter, scales = "free_x"),
#   E1 %>%
#     filter(abs(-13 - beta_0) < 0.5 & abs(0.3 - beta_1) < 0.1) %>%
#     gather(parameter, value, 1:2) %>%
#     ggplot(aes(x = value, y = logRSS)) +
#     geom_point(alpha = .5) +
#     facet_grid(.~parameter, scales = "free_x"),
#   E1 %>%
#     filter(abs(-13.25 - beta_0) < 0.1 & abs(0.31 - beta_1) < 0.03) %>%
#     gather(parameter, value, 1:2) %>%
#     ggplot(aes(x = value, y = logRSS)) +
#     geom_point(alpha = .5) +
#     facet_grid(.~parameter, scales = "free_x"),
#   ncol = 2
#   )

```


```{r save_all, opts.label = "invisible"}
save_CE(CE)

```
