---
title: "New statistics for design researchers"
author: "Martin Schmettow"
date: "October 4, 2016"
output:
  word_document:
    toc: yes
  html_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
bibliography: Bayesian_Stats.bib
---

```{r setup, echo = FALSE, warnings = FALSE, eval = TRUE, message=F}
## The following is for running the script through knitr
purp.mcmc = F
thisdir = getwd()
source("RMDR.R")

CE <- 
  c("BrowsingAB", "Sec99", "Headache", "Reading", 
    "AR_game", "Sleep", "Uncanny")
load_CE(CE)

#formals(stan_glm)$iter <- 200
#formals(stan_glm)$chains <- 1

```


# Linear models


## Grand mean models: quantification at work

Reconsider Andrew and Jane. They were faced with the problem that potential competitors could invalidate the claim "rent a car in 99 seconds" and drag them to court. More precisely, the question was: "will users *on average* be able ...", which apparently is about the statistic of the mean. A statistical model estimating just the mean we call the *grand mean model*. There are two reasons why it's "grand": the first is that most reseachers estimate models with more than one mean, the second is that this is the most simple of all models, so in a way, we can think of it as the "grandmother of all models".


+ with medical infusion pump a decimal error (giving the tenfold or a tenth of the prescribed dose) must be below a bearable level
+ the checkout process of an e-commerce website must have a a cancel rate not higher than ...
+ the brake lights of a car must be designed to let the following driver react in a certain time frame

The grand mean model predicts the level of performance to be expected from someone in the given population. Prediction here means that the estimated grand mean we take as a best guess for all realizations we have *not* observed. That includes all people we have not invited to the lab, but also potential performance in other situations, for example the daily shape people are in or their current level of motivation. In practically all cases where human behaviour is involved, this is a rather imperfect prediction. The reason is that people differ wildly in their abilities, experience, strategies, preferences, situations, wishes etc. All these differences may also vary for the same person from day to day and influence performance.

To summarize, the grand mean model notoriously predicts the population average, not drawing upon any predictors. Because of this imperfection, we can expect a good amount of *error*. The formula of the grand mean linear model is as follows:

$$\mu_i = \beta_0$$
$$Y_i = N(\mu_i, \sigma_{\epsilon})$$

The first term is called the *likelihood term*. It defines the relation between *predicted values* $mu_i$ and predictors. In the current case, there are no predictors rather than the grand mean $\beta_0$. The likelihood is strictly deterministic. Beneath it, the *random term* specifies our assumptions on the randomness in the observations.  With CLM we are always limited to a normal distribution. [...]

Note that, generally, $\mu_i$ is unique for every single observation. However, in the grand mean model it depends on a single coefficient $\beta_0$ which is not associated with individual measures. More complex CLM usually have more parameters, one which is $\beta_0$. This is then called the *intercept coefficient*. As the  grand mean model has this one parameter only, it is also referred to as the *intercept-only model* or *null model*.

The last line specifies the random term, or error. We cannot predict the error itself, but we can assume it to follow a certain distribution. In the case of linear models, the assumed distribution is the normal or *Gaussian distribution*. Gaussian distributions have a characteristic bell curve and depend on two parameters: the mean $\mu$ as the central measure and the standard deviation $\sigma$ giving the spread.

As a side note, many authors would specify the grand mean model in a slightly different manner:

$$Y_i = \mu_i + \epsilon_i$$
$$mu_i = \beta_0$$
$$\epsilon_i \sim N(0, \sigma_\epsilon)$$

This way of model formulation is intuitive in that it presents the observed values as the sum of predicted values and residuals and works for all CLM. Unfortunately, it cannot be used conveniently, when the random part is not Gaussian.  It is important to understand that the error term captures all variation that is not explained by the model and the unkown sources of variation are manifold. Deviation can arise due to all kinds of individual differences, situational conditions and, last but not least, measurement errors. The Gaussian distribution often is a good approximation for randomness.  Still, in some recognizeable cases it is inevitably off. In chapter [GLM] we will introduce other error distributions such as binomial distributions for successes in a  number of trials (for example: task completion rates) and Poisson distributions for count variables (for example: number of errors).

So, when estimating the grand mean model, we estimate the intercept $\beta_0$ and the standard deviation of the Gaussian distributed error term $\sigma$. In R, the analysis of the 99 seconds problem unfolds as follows: recall that only the completion times of the novel design are of interest. These have been extracted from the original data set and stored in a separate data frame. This data frame is send to the R command `stan_glm` for estimation, using the `%>%` operator and the argument `data = .`. As the MCMCglmm command applies to a huge variety of regression model, the desired model needs further specification. For that purpose, R has its own formula language. The formula of the grand mean model is `ToT ~ 1`. Left of the `~` (*tilde*) operator is the outcome variable. The right hand side specifies the deterministic part. The `1`here has nothing to do with the natural number surrounded by 0 and 2. In R's formula language it represents the intercept. Finally, the `family` argument states that a model with Gaussian error term is desired. This is the default in MCMCglmm (and most other regression commands), so in coming examples, we will leave it out.


```{r fit:99_seconds, opts.label = "mcmc"}
attach(Sec99)
M_1 <- stan_glm(ToT ~ 1, data = Ver20)
detach(Sec99)
```

The `stan_glm` command returns a complex object that stores the results of estimation. The most interesting part is the posterior distribution of the intercept and the standard deviation of residuals. The posterior distribution gives the degree of certainty for every possible value of the parameter. In a perfect world, we would know the analytical formula of the posterior. In most non-trivial cases, though, there is no formula for the posterior. Instead, what we get is a sample from this distribution via a process called Markov-Chain Monte Carlo sampling (MCMC). In fact, what we get from the MCMC estimation is just many draws from the posterior distribution. The following code extracts the posterior distribution from the MCMC estimation and prints a random selection of 10 lines. When calling the posterior object (class: tbl_post) directly, it provides a compact summary of all variables.

```{r lst:99_seconds_post, opts.label = "rtut"}
attach(Sec99)

P_1 <-  
  posterior(M_1)
P_1 %>% 
  sample_n(10)
P_1
```

Making statements about certainty, for example $M_{ToT] <= 99$, is very easy, once we have a basic understanding of what MCMC estimation actually does. While a full treatment of the issue is beyond the scope of this book (see [BDA]), but it suffices to understand that the MCMC algorithm is a *random walk* through *parameter space*. In the current case, the parameter space is two-dimensional, as we have two parameters. At every iteration, the MCMC algorithm stores the coordinates in parameter space.


```{r fig:99_seconds_random_walk, opts.label = "fig.small"}
G_random_walk <-
  P_1 %>% 
  filter(iter <= 50) %>% 
  select(iter, parameter, value) %>% 
  spread(parameter, value) %>% 
  ggplot(aes(x = Intercept, y = sigma_resid, label = iter)) +
  geom_text() +
  geom_path(alpha = .3) +
  ylab("residual sd") +
  xlab("intercept mu")

G_random_walk
```

The first 50 hundred steps of the MCMC random walk are shown in `r figr("fig:99_seconds_random_walk")`. Apparently, the random walk is not fully random, as the point cloud is more dense in the center area.  This is where the more probable parameter values lie. In fact, the MCMC algorithm works with simple a rule that lets it jump to more likely areas more frequently. A common way to plot the posterior distribution, is to create one histogram per parameter (alternative: density or violin plots).

In our example, in `r figr("fig:99_seconds_post")` we can spot that the most likely value for average time-on-task is  `r md_coef(Sec99$T_coef_1, row = 1, interval = F)`. Both distributions have a certain spread. With a wider PD, far-off values have been visited by the MCMC chain more frequently. The probability mass is more evenly distributed. There is less certainty for the parameter to fall in the central region. In the current case, a risk averse decision maker would maybe take the interval  $`r md_coef(Sec99$T_coef_1, row = 1, center = F)`$ as "reasonably certain".


```{r fig:99_seconds_post, opts.label = "fig.small"}
P_1 %>% 
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_grid(. ~ parameter, scales = "free")
  

```

That was a first glance on the underlying MCMC engine and how derive conclusions from the PD. However, much of the time a researcher doesn't want to deal with the posterior, directly, but desires a brief summary of location and uncertainty. Coefficient tables are frequently used. A typical coefficient  table is shown below. It reports the central tendency of every coefficient, which is an indicator for the magnitude of an effect. Next to that, the spread of the posterior distibution is summarized as 95% credibility  intervals and represent the degree of uncertainty: the less certain an estimate is, the wider is the spread of the posterior distribution. A 95% credibility interval gives a range of possible values where you can be 95% certain that it contains the true value.
A coefficient table is produced by the `fixef` command of the bayr library:

```{r tab:99_seconds_post_fixef, opts.label = "tab"}
(T_coef_1 <- fixef(P_1))

```


The authors of Bayesian books and the various Bayesian libraries have different opinions on what to report. Some report the posterior mean, others prefer the median, as it is less influenced by skew. A second advantage of the median is that it doesn't change, when a variable is transformed monotonically. For the spread of the distributions, either simple quantiles can be used, whereas others report areas of highest posterior density (HPD).
 
In this book, we primarily use the posterior mode for magnitude of the effect, and the 2.5% and 97.5% quantiles, which resembles the 95% confidence limit in classic statistics. The posterior mode has two advantages: first, it has a rather inuitive meaning as the most likely region, which is compatibel with the concept of maximum likelihood. Second, it is invariate to (strictly monotonous) transformations, which is an issue with Generalized Linear Models in a later chaper [GLM]. The main disadvantage is that the mode does not work well when the posterior distribution is bimodal. But, when that happens, you probably have a more deeply rooted problem, then just deciding on a suitable coefficient table. For a basic summary of the posterior distribution, we can simply state: 

```{r 99_seconds_post_fixef_table, opts.label = "rtut.nr"}
P_1 %>% 

  Ver20 %>% 
  ggplot(aes(x = "Intercept", y = ToT)) +
  geom_jitter() +
  geom_crossbar(data = filter(T_coef_1, parameter == "Intercept"),
                aes(ymin = lower, y = center, ymax = upper)) +
  geom_hline(aes(yintercept = 111, col = "111 seconds"))
```



Back to Andrew and Jane: the histogram in `r figr("fig:99_seconds_post")` indicates that the average time-on-task is rather unlikely in the range of 99 seconds or better. The standard posterior summary with `fixef()` does not answer the question, strictly. The CIs give the limits of a range that contains a given certainty. In contrast, Jane needs the certainty that average time-on-task is smaller than the 99 (or 111) seconds limit.    
This can be achieved by operating on the posterior distribution, directly. We can simply count how many visited values are smaller than 99.  But, by using quantile functions on the posterior distribution, we can make a precise statement about the probability of the claim:

```{r 99_seconds_prob99}
p99 <- 
  P_1 %>% 
  filter(parameter == "Intercept") %>%
  summarize(mean(value <= 99))

p111 <- 
  P_1 %>% 
  filter(parameter == "Intercept") %>%
  summarize(mean(value <= 111))

```


```{r tab:99sec_certainties, opts.label = "rtut.nr"}
T_certainty <-
  P_1 %>% 
  filter(parameter == "Intercept") %>%
  summarize(certainty_99s = mean(value <= 99),
            certainty_111s = mean(value <= 111))

```


It turns out that the certainty for average time-on-task below the 99  is a meager `r Sec99$T_certainty[[1,1]]`. The alternative claim, that average completion time is better than 111 seconds is far more likely (`r Sec99$T_certainty[[1,2]]`).

```{r fig:99_seconds_beta, opts.label = "future"}
betaplot::iiaplot_1(fixef = c(106, 0, 0, 0, 0))
                              

```

```{r}
detach(Sec99)
```




## Walk the line: linear Regression

In the previous section we have introduced the mother of all regression models: the grand mean intercept-only model. The latter term bears some resemblance to mid-school function analysis. There, the intercept is known as of intercept as *"the point where a function graph crosses the x-axis"*, or more formally, as a basic linear function:

$$f(x_1) = \beta_0 + \beta_1x_{1i}$$
$$f(x_1 = 0) = \beta_0$$

At $\beta_0$, the linear function crosses point zero on the y-axis. The second parameter, $\beta_1$ is called the *slope*. The slope determines the steepness of the function. The linear regression model can be conceived a generalization of the intercept-only model: setting $\beta_1 = 0$ produces a an orthogonal line, with the  expected value is constant over the whole range. The full  then the linear regression model equals the grand mean model, $\mu_i = \beta_0$.

```{r fig:basic_linear_function, opts.label = "fig.small"}

data_frame(x = 1:20,
           y = 3 + 2 * x) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line()

```

Linear regression requires the predictor to be metric. That gives us the opportunity to discover how ToT can be predicted by age: are older people slower on te internet? The formula specification is:

$$\mu_i = \beta_0 + \beta_1x_{1i}$$
$$Y_i = N(\mu_i, \sigma_{\epsilon})$$

As long as we do CLM, only the likelihood term needs to be extended: a product of the (to be estimated) parameter $\beta_1$ and recorded age $\x_{1i}$ has been added. This literally means: with every year of age, ToT increases by the $\beta_1$ seconds. Before we run a linear regression with `stan_glm`, we plot the association between age and ToT:

```{r fig:BAB_eda_age}
attach(BrowsingAB)

G_eda_1 <-
  BAB1 %>%
  ggplot(aes(x = age, y = ToT)) +
  geom_point()+
  geom_smooth(se = F)

G_eda_1
```

Now, we use the `stan_glm` command in much the same way as before, just adding the  predictor age. The command will internally check the data type of your variable, which is numeric, here. Therefore, it is treated as a *covariate*.

```{r age_lm1, opts.label = "mcmc"}
M_age <-
  BAB1 %>% 
  stan_glm(ToT ~ 1 + age, 
           data = .)
```

```{r opts.label = "invisible"}
T_age <- fixef(M_age)
```

Is age associated with ToT? The coefficient table tells us that with every year of age, users get `r bayr::md_coef(BrowsingAB$T_age, ~fixef == "age", interval = F)` seconds slower. That is not a lot.

```{r opts.label = "rtut"}
fixef(M_age)
```

### Shifting predictors

The intercept parameter tells us the expected ToT at age = 0, a new born. That is a bizarre prediction and we would never seriously put that forward on a stakeholder presentation, or in the conclusion of a scientific paper, would we not? Besides that, the intercept estimate is rather uncertain, with a wide 95% interval, `r md_coef(fixef(M_age), ~fixef == "age")`.

Both, the unplausibility and high certainty are rooted in the same problem: the model puts a parameter, where there is not data. The broad region of the intercept is as empty as the Khali desert, because observations are impossible. Before, we explore this on a deeper level, there is a pragmatic solution to the problem: *shifting the predictor*. Shifting means just that: the whole linear curve is shifted to the right (or the left), such that the intercept is covered with data. In the case, here, two options seem to make sense: either, the intercept is in the region of youngest participants, or it is the sample average, which is then called *centering*. To shift a variable, just subtract the amount of units (years) where you want the intercept to be:



```{r BAB_shift_and_center, opts.label = "mcmc"}
BAB1$age_shft = BAB1$age - 20
BAB1$age_cntr = BAB1$age - mean(BAB1$age)

M_age_shft <- 
  stan_glm(ToT ~ 1 + age_shft, data = BAB1)
M_age_cntr <- 
  stan_glm(ToT ~ 1 + age_cntr, data = BAB1)

# use posterior merging
# extend bayr posterior to append models
fixef(M_age)
fixef(M_age_shft)
fixef(M_age_cntr)
```

The shifted intercept has moved to higher values. We observe that intercept uncertainty `r md_coef(fixef(BrowsingAB$M_age), ~fixef == "Intercept")`.

Surprisingly, the shifted Intercept is not exactly 20 years right of the original. This is due to the high uncertainty of the latter. In addition, the slope parameter has moved a tiny bit, too. Think of the intercept as the end of the lever: shifting or centered intercepts tightens certainty and get a more determined grip on the lever. One parameter partly overrules the other.

### Endlessly linear

On a deeper level the bizarre age = 0 prediction is an example of a general rule, that i will emphasize several times throughout ths book:

> In an endless universe, everything is finite.

A fact about LR is that they allow us to fit a straight line to data. A lesser regarded consequence is that this line extends infinitly in both directions. To fulfill this assumption, the outcome variable needs to be infinite, too,  $y_i \in [-infty; \infty]$ (unless the slope is zero). Every scientifically trained person and many lay people know, that even basic physics is finite: the speed of light is limited to $\approx 300.000 km/s$ and temperature has a lower limit of -276Â°C (or 0K). If there is neither endless speed nor cold, it would be daring to assume any psychological effect to be endlessly linear.

The endlessly linear assumption (ELA) is central to all CLM and linear mixed-effects models. From a formal perspective, the ELA is always violated in a universe like ours. The practical consequences are, as ever so often: it depends! Frequently, LR is a reasonably effective approximation. Review [G_eda_1]: the blue line is a so called a *smoother*, more specifically a LOESS. A smoother is a fitted line, not unlike LR. But, it is way more flexible. Where LR is a lever, LOESS is a pipe cleaner. LOESS shows a more varied picture of the relation between age and ToT. There is a rise between 20 and 40, followed by a stable plateau, and another rise starting at 60. 

[Physiologically, this makes no sense. Adapt simulation and redo BAB1 and BAB5]

It is not directly linear, still: LR provided sufficient evidence that a relationship exists, such that, generally, the older one gets, the slower one does the task. There is a possibility that the design is unfair. That is enough reason to improve the design, for example, doing a formal accessibility evaluation and fixing the major issues. A theorist may desire a more detailed picture. Disruptions of linearity often indicate interesting psychological processes. Knowing these makes us better understand design. An uncanny example of theoretical work will be given in polynomial regression.

## Predictions and residual analysis

```{r opts.label = "invisible"}
C_age <- fixef(M_age_shft)$center

```

In the BrowsingAB case, we have repeatedly recorded age of participant. LR found the repeating pattern, that with every unit of age, ToT increased by  `r C_age[2]` seconds. This pattern is what the model predicts, now and forever. If we ask: what is predicted for a person of age 45? We get:

$$\mu_{age = 45} =`r C_age[1]` + `r C_age[2]` * 45 
= `r C_age[1] + C_age[2] * 45`$$

Parameter $\mu_i$ is called the *expected value*. 

> expected value $\mu_i$ captures the signal, that part of the observation that repeats.

The expected value is our best guess under the LR model. Therefore it is uncertain to a degree, but we are setting this aside for the moment. What we know for sure is the *observed value* . We can find a person of about age 43 in the data set and take it at face value. The data set contains a few participants in about that range. Using the estimated intercept and age coefficients `C_age` with the basic LR formula, we can compute one predicted value per observation. Here, observations are congruent with participants, but this will change down the road to [LMEM].

```{r opts.label = "rtut"}
C_age <- fixef(M_age_shft)$center

T_pred_age <-
  BAB1 %>% 
  select(age, ToT) %>% 
  mutate(mu  = C_age[1] + age * C_age[2],
         eps = ToT - mu)

T_pred_age %>%  
  filter(age >= 44, age <= 46)

```

The final line calculates the *residuals*, which is the deviation from the expected value:

$\epsilon_i = y_i - \mu_i$. 

Again, there is one residual per observation. The residuals equation contains an uncertain term $\mu_i$, therefore $\epsilon_i$ are uncertain, too. 

> residuals $\epsilon_i$ capture the random noise, that part of the observation that does not repeat. 

Residuals are routinely used to assess the quality of models. With linear models it is highly recommended to check the distribution of residuals. Do we see the characteristic bell curve of a Gaussian distribution? Sort of we do.

```{r fig:resid_age, opts.label = "rtut"}
T_pred_age %>% 
  ggplot(aes(x = eps)) +
  geom_histogram(bins = 20)

```

To give a counter example of Gaussain residual distribution, we take the variable `clicks`, that captures the number of links followed to find the desired information.

[Add Poisson clicks to BAB and make example]

Drawing on expected values, *residual analysis is something you can only do after estimation*. Indeed, this process is often called *checking assumptions*, which implies you do it beforehand. Due to the false intuition, a common beginners mistake is to analyze the distribution of the outcome variable $y_i$ before running the model. Review the two figures in the first example of [GSR]. The observations are bimodally distributed, nowhere near Gaussian. After (graphically) applying the model they are well-shaped.

Expected values $\mu_i$ and residuals $\epsilon_i$ are uncertain. With rstan_arm models, we can extract both parameters together with credibility intervals.

```{r age_resid, opts.label = "rtut.nr"}
residuals(M_age)
```

See how the command is called on the model object. This reflects the fact that 
A commonly used criterion for model fit is whether a predictor considerably explains variance, which reduces the spread of the residual distribution. We could ask: does the age predictor effectively reduce the residual variance $\sigma$ as compared to the GM?

```{r fit:age_resid_reduction, opts.label = "mcmc"}
M_0 <-
  BAB1 %>% 
  stan_glm(ToT ~ 1, data = .)

T_resid <-
  data_frame(
    M_0 = residuals(M_0),
    M_age = residuals(M_age))
```

```{r opts.label = "fig.small"}
G_resid_0 <-
  T_resid %>%
  gather(model, error) %>% 
  ggplot(aes(col = model, x = error)) +
  geom_density()

G_resid_0
```




```{r fig:lin_reg_modelplot, opts.label = "future"}
# betaplot::iiaplot_2(list(as.numeric(Tclm[["Browsing_AB_3"]][1,2]),
#                          as.numeric(Tclm[["Browsing_AB_3"]][2,2])),
#                     range = c(0,80)) + geom_point(data = Dclm[["Browsing_AB"]], aes(x = age, y = ToT))
  
```



## A or B: Comparing two (or more) designs

One of the most basic statistical routines is the *comparison of groups* (CoG), commonly known as ANOVA. In design research group comparisons are all over the place, for example:

+ designs: as we have seen in the A/B testing scenario
+ properties of people, like  gender or whether they have a high school degree, or not
+ properties of situations, like whether someone uses an app on the go, or sitting still behind a computer

In order to perform a CoG, a variable is needed that establishes the groups, which is commonly called a *factor*. A factor is a variable that puts labels "A" and "B" on observations, these are called *factor levels*. In the BrowsingAB case, the factor Design with its levels A and B does the job. A research question could be:

*Do two web designs A and B differ in user performance?*

This question could appear during an overhaul of a municipal website. With the emerge of e-government, many such websites had grown wildly over a decade. They temporarily were jungles, to the disadvantage for the  users. The prototype of a novel web design is developed and tested via A/B testing at 200 users. Every user is given the same task,  but sees only one of the two designs. As always, we first takle a look at the raw data:

```{r fig:eda_anova}

BAB1 %>% 
  select(Obs, Design, ToT) %>%
  sample_n(6) %>% 
  arrange(Obs) %>% 
  kable()


BAB1 %>%
  ggplot(aes(x = ToT)) +
  geom_histogram() +
	facet_grid(Design~.)
```

This doesn't look too striking. We might consider a slight advantage for design B, but the overlap is immense. We perform the COG. Again, this is a two-step procedure:

1. the `stan_glm` command lets you specify a simple formula to express the dependency between predictors (education) and outcome variable (ToT). It performs the parameter estimation, using the method of _Markov-Chain Monte-Carlo Sampling_. The results are stored in an object (M1).
1. With the `fixef` command the estimates are extracted and interpreted

```{r fit:anova_lm, opts.label = "mcmc"}
# COG parameter estimation

M_Design <-
  BAB1 %>% 
  stan_glm(ToT ~ 1 + Design, 
           data = .)

T_resid <- 
  mutate(T_resid, M_Design = residuals(M_Design))
```



```{r opts.label = "rtut"}
fixef(M_Design)

```

The model contains two parameters, the first one being called Intercept, again. How can you have a "crossing point zero", when there is no line, but just two groups. You can't. When speaking of pure factor models, like the one above or the multifactorial models of the next section, the Intercept has a different meaning:

> The intercept in factorial models is the mean of a reference group

Per default, stan_glm chooses the alphabetically first group label as the reference group, clearly design A. We can therefore say that design A has an average performance of `r md_coef(fixef(M_Design), ~fixef == "Intercept")` Given the 95% CIs one would rather say that it is around 200. The second parameter is the group difference, with design A as reference. With design B it took users `r md_coef(fixef(M_Design), ~fixef == "DesignB", neg = T)` *less* time to complete the task. However, the effect is small and there is huge uncertainty about it. If the BrowsingAB data set has some exciting features, the design difference is not it.

## Crossover: multifactorial ANOVA

[NEEDS SEVERE OVERHAULING]
[THINK I DID IT, where is it?]

Now that we have seen how we can compare on one factor, we can extend the model to more factors. With linear  models, we can extend the basic CoG model to incorporate multiple of such factors.



Now, we are looking at a slightly more complicated situation, where browsing is predicted by design and education level of participants. First, with the *ggplot* system, we plot the new situation. Recall that 

1. mapping variables in the data set to elemental properties in the splot, such as: x position, y position, color, shape etc.

2. selecting an appropriate geometry that carries such properties, e.g. points, bars, boxplots

```{r fig:anova_twofact_2, opts.label = "fig.tut"}
BAB1 %>%
  ggplot(aes(x = Design, 
             y = ToT, 
             fill = Education)) +
  geom_boxplot()
```

Now we can do the two-factorial ANOVA in much the same way as before. We only have to add the predictor *Education* to the model formula. Then we run the MCMC estimation and get the estimates.


```{r fit:anova_twofact_3, opts.label = "mcmc"}
M_cross <- 
  BAB1 %>% 
  stan_glm(ToT ~ 1 + Design + Education,
           data = .)

T_resid <- mutate(T_resid, M_cross = residuals(M_cross))


```


```{r optsl.label = "rtut"}
fixef(M_cross)


```


## Putting it all together: General linear models

What do you think is the *lm* in `stan_glm`? Well, the letters stand for *linear model*, and as we have just demonstrated, this includes the case of ANOVA (predictor is a factor) and linear regression (predictor is metric).

But, linear models can do more, and this is why they are often referred to as *general linear models* (GLM): with GLM you can arbitrarily mix any number of factors and covariates. 

In the BrowsingAB example, we were interested in the influence of age, but we haven't regarded another demographic variable of interested: level of education. First we explore graphically how education affect ToT:




```{r fig:glm_EDA_2}
BAB1 %>%
	ggplot(aes(y = ToT, 
						 x = Education)) +
	geom_boxplot()
```


And here we call `stan_glm` once again, with both predictors, simultaneously

```{r fit:glm_mixed, opts.label = "mcmc"}
# GLM parameter estimation
M_glm <- 
  BAB1 %>% 
  stan_glm(ToT ~ Design + age, 
							data = .)

T_resid <- mutate(T_resid, M_glm = residuals(M_glm))
```

```{r opts.label = "rtut"}

fixef(M_glm)

```


```{r fig:glm_mixed, opts.label = "future"}
# fe <- as.list(c(Tclm[["Browsing_AB_3"]]$center, 0))
# 
# iiaplot_4(fixef = fe, range = c(0, 80), label = c("age", "ToT"))
```






## Interaction effects

The linear regression model `M_age` showed a considerable relationship between age and ToT. However, the uncertainty was pronounced and the residual error was reduced by only a small fraction. Model `M_Design` showed a small average advantage for design B, with high uncertainty. Neither of the two models provided compelling effects.

It is commonly known that older people tend to have lower performance than younger users. A numkber of factors are responsible, such as: slower processing speed, lower working memory capacity, lower motor speed and visual problems, It is also commonly held, that universal designs are possible. Universal designs reduce the cognitive, sensory and motor obstacles for impaired users. A perfect universal design is equally usable for everyone.

Is, perhaps, one of the two designs less of a burden for elderly users? We plot the observed ToT by age, but form groups by design. For seeing the trend, a linear smoother is added to the plot.

```{r fig:glm_EDA_3, opts.label = "fig"}
G_slope_ia <-
  BAB1 %>%
  ggplot(aes(x = age,
             col = Design,
             y = ToT)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)


G_slope_ia
```


With the framework of GLM, we can use an arbitrary number of predictors. These can represent properties on different levels, for example, two designs proposals for a website can differ in font size, or participants differ in age. So, with GLM we gain much greater flexibility in handling data from applied design research, which allows us to examine user-design interaction (literally) more closely. 

The catch is that if you would ask an arbitrary design researcher:

> Do you think that all users are equal? Or, could it be that one design is better for some users, but inferior for others?

you would in most cases get the answer:

> Of course users differ in many ways and it is crucial to know your target group.

Some will also refer to the concept of usability by the ISO 9241-11, which contains the famous four words:

> "... for a specified user ..."

The definition explicitly requires you to state for *for whom* you intended to design. It thereby implicitly acknowledges that usability of a design could be very different for another user group. In other words, statements on usability are by the ISO 9241-11 definition *conditional* on the target user group.

In statistical terms, conditional statements of the form:

> the effect of predictor A (the design) depends on the level of predictor B (the user group)

are called *interaction effects*. 

As we have seen, with GLM, it is  possible to investigate the effect of design properties and user properties simultaneously. For example, assume that main difference between design A and B in the web browsing example is that A uses larger letters than B. Would that create the same benefit for everybody? It is not unlikely, that larger letters only matter for users that have issues with far sightedness, which is associated with age. Maybe, there is even an adverse effect for younger users, as larger font size takes up more space on screen and more scrolling is required. 


```{r fit:BAB_ia1, opts.label = "mcmc"}

M_ia1 <- 
  BAB1 %>% 
  stan_glm(ToT ~ Design + age + Design:age,
				 data = .)

T_resid <- mutate(T_resid, M_ia1 = residuals(M_ia1))

```

```{r opts.label = "rtut"}
fixef(M_ia1)

```


```{r fig:Browsing_interaction, opts.label = "future"}

# fe <- fixef(M_ia1)
# 
# iiaplot_4(fixef = fe, range = c(0, 80), label = c("age", "ToT"))

```

If we can do it with covariates, like age, we can do it with factors, too. For example, does the overall improvement from design A to B differ for the educational level of participants?

```{r fit:BAB_ia2, opts.label = "mcmc"}

M_ia2 <- 
  BAB1 %>% 
  stan_glm(ToT ~ Design + Education + Design:Education,
				 data = .)

T_resid <- mutate(T_resid, M_ia2 = residuals(M_ia2))


```

```{r opts.label = "rtut"}
fixef(M_ia2)
```


Well, interaction effects occur routinely, but a `Design:Education` effect is not a feature of the BrowsingAB data set.


```{r}
detach(BrowsingAB)
```


We can distinguish between *saturation effects* and *amplification effects*.

```{r interaction_effects, message=FALSE, warning=FALSE}
	expand.grid(effect = c("saturation", "amplification"), 
							A = c(0,1),
							B = c(0,1)) %>%
		join(data.frame(effect = c("saturation","amplification"),
										beta_1 = c(.1,.1),
										beta_2 = c(.2,.2),
										beta_3 = c(-0.3, 0.3)))  %>%
		mutate(Outcome = A * beta_1 + B * beta_2 + A * B * beta_3) %>%
		mutate(B = factor(B, labels = c("low", "high")),
					 A = factor(A, labels = c("low", "high"))) %>%
		ggplot(aes(x = A, col = B, y = Outcome)) +
		geom_point(size = 3) +
		geom_smooth(aes(group = B, col= B), method = "lm") +
		facet_grid(.~effect)

Interactions <- expand.grid(effect = c("saturation", "amplification"), 
						A = seq(0,1, length.out = 11),
						B = seq(0,1, length.out = 11)) %>%
	join(data.frame(effect = c("saturation","amplification"),
									beta_1 = c(.1,.1),
									beta_2 = c(.2,.2),
									beta_3 = c(-0.3, 0.3)))  %>%
	mutate(Outcome = A * beta_1 + B * beta_2 + A * B * beta_3)

library(lattice)
grid.arrange(
	wireframe(Outcome ~ A + B, 
						data = filter(Interactions, effect == "saturation"),
						main = "saturation"),
	wireframe(Outcome ~ A + B, 
						data = filter(Interactions, effect == "amplification"),
						main = "amplification"),
	ncol = 2
)
```



## Saturation and non-linearity

Most statistically trained researchers are aware of some common assumptions of linear regression, such as the normally distributed residuals and variance homogeneity. Another assumption that is often disregarded, is the assumption of linearity, which arises from the basic regression formula:

$$y_i = \beta_0 + \beta_1 x_{1i} ...$$

The formula basically says, that if we increase $x$ (or any other influencing 
variable) by one unit, $y$ will increase by $\beta_1$. Imagine, you are testing
the influence of font size on reading performance. In your experiment, you found 
that increasing from 8pt to 12pt cuts the required reading time by 10 seconds. 
According to the linear model, you has to expect the reading time to improve 
by another 10 seconds, when enlarging to 16pt.

```{r reading_time}
D_reading_time <- 
  data_frame(font_size = c(4, 8, 12, 16, 24, 28),
           observed_time = c(NA, 40, 30, NA, NA, NA),
           predicted_time = 60 - font_size/4 * 10)
D_reading_time %>% 
  kable()
```

It is immediatly clear, that these expectations have no ground: for normally sighted persons, a font size of 12 is easy to decipher and another increase will not have the same effect. Taking this further, one would even arrive at absurdly short or 
impossible negative reading times. At the opposite, a font size of four point may just render unreadable on a computer screen. Instead of a moderate increase by 
10 seconds, participants may have to decipher and guess the individual words, which will take ages.

A major flaw with the linear model is that it presumes the regression line to 
increase and fall infinitely. However, in an endless universe everything has 
boundaries. The time someone needs to read a text is limited by fundamental cognitive processing speed. We may be able to reduce the inconvenience of deciphering small 
text, but once an optimum is reached, there is no further improvement.

Boundaries of performance measures inevitably lead to non-linear relationships with predictors. Modern statistics knows several means to deal with non-linearity, 
several of them are introduced in later chapters of this book ([GLM, NLM]). 
In the case of two (or more) interventions to improve a 

Let me explain saturation effects with an example that is intuitive enough, whether you are an design expert, or not. Consider ...


```{r prep_headache, opts.label = "invisible"}
attach(Headache)

T_means <-
  Pills %>% 
  group_by(PillA, PillB) %>% 
  summarise(reduction = mean(reduction)) %>% 
  ungroup()

T_means_cross <-
  T_means %>% 
  mutate(reduction = round(reduction,1)) %>% 
  spread(PillA, reduction)


Pills %>% 
  ggplot(aes(x = PillA, col = PillB, reduction)) +
	geom_boxplot()


```

Below, we estimate a series of models, which we are now going to compare.

```{r fit:Headache, opts.label = "mcmc"}
M_1 <-
  stan_glm(reduction ~ PillA + PillB, 
                          data = Pills)
M_2 <- 
  stan_glm(reduction ~ PillA * PillB, 
                          data = Pills)

M_3 <- 
  stan_glm(reduction ~ before * PillA + PillA * PillB, 
                          data = Pills)

M_4 <- 
  stan_glm(reduction ~ -1 + PillA:PillB, 
     data = Pills)

```

Imagine, the researcher started with a two-way ANOVA, estimating the mein effects PillA and PillsB. The result is:

```{r tab:M_1_fixef}
T_fixef_1 <- fixef(M_1)
T_fixef_1
```

Given these estimates, we conclude that pill A has an almost double reduction of headache compared to B. We further notice the Intercept, indicating that headache spontaneously diminishes at a certain rate.

Setting the spontaneous recovery aside, what would be the prediction of the model for the condition with both pills?

```{r Headache_guess}

T_guess <-
  T_means %>% 
  mutate(guess = c(0, 
                   round(T_fixef_1[[2,6]],2),
                   round(T_fixef_1[[3,6]],2),
                   "?")) %>% 
  select(-reduction) %>% 
  spread(PillB, guess)

T_guess
```

Probably, you came up with 

$$`r round(sum(Headache$T_fixef_1$center), 2)$$

But, is that reasonable? Do the effects of headache pills simply add up like this? Consider a scenario, where five headache pills were compared in the same manner. If we assume linear addition of effects, some participants in the group with all pills could even experience the breathtaking sensation of *negative* headache. We clarify the situation by  introducing an interaction term in our second model. 

```{r tab:Headache_fixef_2}
T_fixef_2 <-  fixef(M_2)
T_fixef_2
```



We first examine the main effects for both pills. Interestingly, both effects are now considerably stronger than before. Taking one pill at a time are more effective than we thought from the first model. The fourth coefficient (`PillATRUE:PILLBTRUE`) is the interaction term. With the updated model we, again, predict the net headache reduction when both pills are given. This is as simple as summing up all coefficients from intercept to interaction. The result is a combined effect of `r round(sum(Headache$T_fixef_2$center), 2)` in reduction.

As the interaction effect is negative, it reduces the predicted combined effect. As we would expect, the effectiveness of two pills us not their sum, but less. One can have headache to a certain degree or no headache at all. If it's gone, any more pills have no additional effects.

Design of systems is a matter of compromises. A common conflict of interests is between the aesthetic appearance and the ergonomic properties. Consider the  typesetting on web pages. Many commercial websites are equally concerned about the ease of comprehending information and positive emotional response. From an ergonomic point of view, the typesettimg of longer text simply requires you to choose the best screen-readable font, find an optimal font size and maximize contrast. If you were now to suggest typesetting a website in black Arial on white background, you are in for trouble with your graphic designer or, much worse, the manager responsible for corporate impression management. In any case, someone will insist on a fancy serif font (which renders rather poorly on low resolution devices) in an understating blueish-grey tone. For creating a relaxed reading experience, the only option left is to increase the font size. This option is limited, though, as too large letters requires excessive scrolling.

The question arises: can one sufficiently compensate lack of contrast by setting the text in the maximum reasonable font size 12pt, as compared to the more typical 10pt? In the fictional study Reading, this is examined in a 2x2 between-subject design:  the same page of text is presented in four versions, with either 10pt or 12pt, and grey versus black font color. Performance is here measured as time-on-task of reading the full page.

```{r}
detach(Headache)
```


```{r tab:reading_data}
attach(Reading)

D_1 %>% 
  sample_n(8) %>% 
  select(-mu) %>% 
  arrange(Part) %>% 
  kable()
```


```{r fig:reading_expl_1}
D_1 %>% 
  ggplot(aes(col = font_color,
             x = font_size,
             y = ToT)) +
  geom_boxplot()
```

We see immediately, that both design choices have a an impact: more contrast, as well as larger letters lead to improved reading performance. But, do they add up? Or do both factors behave like headache pills, where more is more, but less than the sum. Clearly, the 12pt-black group could read fastest on average. So, neither with large font, nor with optimal contrast alone has the design reached maximum performance, i.e. saturation. Still, there could be an interaction effect, that gradually reduces the positive effect of large font in a high-contrast design. We run two regression model, one with both main effects and one that adds an interaction term. We extract the coefficients from both models and view them side-by-side:

```{r fit:reading_1, opts.label = "mcmc"}
M_1 <-
  D_1 %>% 
  stan_glm(ToT ~ font_size + font_color,
           data = .)

M_2 <-
  D_1 %>% 
  stan_glm(ToT ~ font_size + font_color + font_size : font_color,
           data = .)


T_read_fixef <-
  right_join(select(fixef(M_1), fixef, M_1 = center),
             select(fixef(M_2), fixef, M_2 = center),
             by = "fixef")



```

```{r}
T_read_fixef %>% kable()
```


The estimates confirm, that both manipulations have a considerable effect. And, there is an interaction effect as well, correcting the additive effect of font color and size. The combined effect of high contrast and large font is therefore

$$\mu_{12pt, black} = `r Reading$T_read_fixef[1,3]` + `r Reading$T_read_fixef[2,3]` + `r Reading$T_read_fixef[3,3]` + `r Reading$T_read_fixef[4,3]` = `r sum(Reading$T_read_fixef[,3])`$$

In the research scenario, that was not the question, strictly, as we were primarily interested in comparing the effects of font size and contrast. Also, if we see the credibility interval of the interaction effect it is not highly certain ($`r md_coef(fixef(Reading$M_2), row = 4)`$)) Still, including the  interaction term is the right choice, for two reasons: first, both manipulations influence the same cognitive processing, as they both improve visual clatity, with the effect of better visual letter recognition. From a theoretical perspective, we would thus expect saturation.   This is simply the application of prior knowledge and in a perfect world one would  use a prior distribution for the interaction term, creating a more certain estimate. That being said, the data set is synthatic, and the simulation definitely included the interaction effect.  Second, in Table [XY], all other coefficients change considerably with introduction of the interaction effect. Especially, the effects of the two manipulations get considerably under-estimated. 

Why are the main effects under-estimated, when not including the interaction term? The pure main-effects model has three parameters. This allows it to represent the same numner of independent group means. Formally, the number of groups in the study is four. However, the three-parameter model assumes that the fourth group (black, 12pt) can sufficiently be specified by the existing three parameters. If saturation occurs, the group of participants in the 12pt group is not homogenous: in the grey group, they experience a stronger improvement than in the black group. The three parameter model is forced to make the best out of situation and adjusts the net effect of font size to be slightly lower than it actually is. The same occurs for the font color effect.

Interaction effects are notoriously neglected in research, and they are hard to grasp for audience with or without a classic statistics education. It is therefore highly recommended to illustrate interaction effects using interaction plots. 
To begin with, an interaction plot for the 2x2 design contains the four estimated group means. These can be computed from the linear model coefficients, but there is an alternative way: we modify the model, such that it represents the four group means, directly. The re-parametrized model has no intercept (if we want the group means directly, we need no reference group), and no main effects. The model contains just the raw interaction effect, which results in an estimation of the four group means:

```{r fit:reading_2, opts.label = "mcmc"}
M_3 <-
  D_1 %>% 
  stan_glm(ToT ~ 0 + font_size : font_color,
           data = .)


```

```{r opts.label = "rtut"}
fixef(M_3)
```


With some basic data transformation on the coefficient table, we create a data frame that encodes the two factors separately. Note how `separate` is used to split the coefficient names into group identifiers. Using `mutate` and `str_replace`, the group labels are stripped the factor names. The resulting coefficient table serves as input to ggplot, creating an interaction plot with overlayed credibility intervals, using `geom_errorbar`.

```{r tab:reading, opts.label = "rtut"}
T_2x2_mu <-
  fixef(M_3) %>% 
  separate(fixef, c("font_size", "font_color"), sep = ":") %>% 
  mutate(font_size = str_replace(font_size, "font_size", ""),
         font_color = str_replace(font_color, "font_color", "")) %>% 
  as.data.frame()

G_interaction <-
  T_2x2_mu %>% 
  ggplot(aes(x = font_color, 
             color = font_size, shape = font_size,
             y = center,
             ymin = lower,
             ymax = upper)) +
  geom_point() +
  geom_line(aes(group = font_size)) +
  geom_errorbar(width = .1)

G_interaction

detach(Reading)
```

From Figure XY we can easily spot the two main effects, and how they go together. The researcher would conclude that with the desired gray font, increasing font size partly compensates for the lack of contrast. At the same time, we see that some saturation occurs, but still, from a purely ergonomic perspective, large font and high contrast is the preferred design.




In the two-by-two groups case of the Reading study, the interaction plot is created 

Interaction effects can severely bias main effects, when gone unnoticed:

```{r headache_2, opts.label = "future"} 
## FIXME ###
# T_headache_bias <- 
#   fixef(M_1) %>% 
#   left_join(fixef(M_2), by = "parameter") %>% 
#   select(parameter, Model_1 = location.x, Model_2 = location.y) %>%
#   mutate(parameter = str_replace(parameter, "TRUE", ""), 
#          bias = Model_1 - Model_2)
```


We have seen in the Headache example that interaction effects occur 
as non-linearities. The more a participant approaches the natural boundary of zero headache, the less benefit is created by additional effort. This we call  *saturation*. Saturation is likely to occur when multiple factors influence the same cognitive or physical system or functioning. In quantitative comparative design studies, we gain a more detailed picture on the co-impact of design interventions and can come to more sophisticated decisions.

In the Reading case, overall utility is a compound of ergonomic quality and aesthetic appeal. It was assumed that a gray-on-white color scheme is more appealing. The researcher would choose the gray-12pt design: higher contrast would increase ergonomic value, but too much at the expense of appeal. Of course, in a perfect world, one would add emotional appeal as a second measure to the study.

If we don't account for saturation by introducing interaction terms, we are prone to underestimate the net effect of any of these measures, and may falsely conclude that a certain measure is ineffective. Consider a large scale study, that asseses the simultaneous impact of many demographic variables on how willing customers are to take certain energy saving actions in their homes. It is very likely that subsets of variables are associated with similar cognitive processes. For example, certain action requires little effort (such as switching off lights in unoccupied rooms), whereas others are time-consuming (drying the laundry outside). At the same time, customers may vary in the overall eagerness (motivation). For high effort actions the impact of motivation level probably makes more of a difference as when effort is low. Not including the interaction effect would result in the false conclusion that suggesting high effort actions is rather ineffective.


## Amplification effects

Saturation effect occur, when multiple factors act on the same system and can (partly) compensate each other. There is an interesting parallel to Boolean algebra. The Boolean OR operator returns `TRUE` when one of the operands is `TRUE`:

```{r tab:boolean_OR}
T_bool_interaction <-
data_frame(A = c(F, F, T, T),
           B = c(F, T, F, T)) %>%
  as_data_frame() %>% 
  mutate("A_OR_B" = A | B) %>% 
  mutate("A_AND_B" = A & B)
 
kable(T_bool_interaction)

```

The Boolean OR is an extreme case of saturation. Once factor A is present, the result is "positive" and introducing B has absolutely nor additional effect. Another basic Boolean operation is $A AND B$ with quite the opposite occuring: any of the two factors only has an effect, if the other is present, too. Rarely will we encounter non-trivial cases where the results are so crisp as in the Boolean world. The gradual version of the Boolean AND is *amplification*: the impact of one factor gets stronger with the presence of another factor.

Amplification can occur, when two (or more) factors impact different cognitive processes in a processing chain. To my mind, finding possible amplification effects is far more challenging as compared to saturation effects, because one has to deal with more complex cognitive or physiological models.

Here, a fictional study on technology acceptance will serve to illustrate amplification effects. Imagine a startup company that seeks funding for a novel augmented reality game, where groups of gamers compete for territory. For their fund raising endeavour they need numbers on the potential market. The entrepreneurs have two hypotheses they want to verify:

1. The game requires some top-notch equipment which only technophile persons will dare to use
1. The game is strongly cooperative and therefore more attractive for people with a strong motivation for social activities

In their study they collect self ratings of a larger set of participants, where they had to rate their technophily and sociophily. In addition, they were given a description of the planned game and were asked how much they intend to participate in the game.

While the example primarily serves to introduce amplification effects, it is also an opportunity to get familiar with interaction effect between metric predictors. While this is not very different to interaction effects on groups, there are a few peculiarities, one being that we cannot straight-forwardly make an exploratory plot. For factors we have used boxplots, but these do not apply for metric variables. In fact, it is very difficult to come up with a good graphical representation. One might think of 3D wireframe plots, but these transfer poorly to the 2D medium of these pages. Another option is to create a scatterplot with the predictors on axis and encode the outcome variable by shades or size of dots. These options may suffice to see any present main effects, but are too coarse to discover the subtlety of interaction effects. This is not an issue with metric interaction effects per se, but more general: while we can comprehend more complex models from coefficient tables, exploratory graphs are limited to just a few variables at the same time. Therefore, this analysis does without the usual exploratory graph. However, we will introduce a method to create interaction plots for metric predictors.


```{r fit:ARgame, opts.label = "mcmc"}
attach(AR_game)

M_1 <-
  D_1 %>% 
  stan_glm(intention ~ sociophile * technophile,
           data = .)

detach(AR_game)
```

```{r opts.label = "rtut"}
attach(AR_game)
fixef(M_1)
```


## Theoretically interesting interaction effects

Explaining or predicting complex behaviour with psychological theory is a corner stone of design science. Unfortunately, it is not that easy. Few psychological theories cover more than three associations between external or individual conditions and behaviour. The design researcher is often forced to enter a rather narrow perspective, or knit a patchwork model from multiple theories. Such a model can either be loose, making few assumptions on how the impact factors interact which others. A more tightened model frames multiple impact factors into a conditional network, where impact of one factor can depend on the overall configuration. A classic study will now serve to show how interaction effects and theoretical reasoning go together. 

Vigilance is the ability endure in attention for rarely occuring events. Think of truck drivers on lonely night tours, where most of the time they spend keeping the truck on a straight 80km/h course. Only every now and then is the driver required to react to an event like braking lights flaring up ahead. Vigilance tasks are among the hardest thing to ask from a human operator, yet, they are safety relevant in a number of domains. Keeping up vigilance most peoaple perceive as tiring, and vigilance detoriates with tiredness.

Several studies have shown that reaction time at simple tasks increases when people are tired. The disturbing effect of noise has been documented as well. A study by Corcoran (1961) examined the simultaneous influence of sleed deprivation and noise on a rather simple reaction task. They ask:

>  will the effects of noise summate with those of loss of sleep to induce an even greater performance decrement or will noise subtract from the performance decrement caused by loss of sleep?

The central argument is that sleep deprivation detoriates a central nervous arousal system. In consequence, sleep deprived persons cannot maintain the necessary level of tension that goes with the task. Noise is a source of irritation and therefore usually reduces performance. At the same time, noise may have an arousing effect, which may compensate for the loss of arousal due to sleep deprivation. To re-iterate on the headache pills analogy, noise could be the antidote for sleepiness.

The Sleep case study is a simplified simulation of Corcoran's results. Participants were divided into 2x2 groups (quiet/noisy, rested/deprived) and has to react to five signal lamps in a succession of trials. In the original study, as performance measure gaps were counted, which is the number of delayed reactions ($>1500ms$).

```{r fig:Sleep_expl}
attach(Sleep)
G_expl <-
  D_1 %>% 
  ggplot(aes(x = Environment,
             color = Sleep,
             y = gaps)) +
  geom_boxplot()
```

Using a 2x2 model including an interaction effect, we examine the conditional association between noise and sleepiness. The `*` operator in the model formula is an abbreviation for the fully factorial term `Environment + Sleep + Environment:Sleep`.

```{r fit:Sleep, opts.label = "mcmc"}
M_1 <- 
  D_1 %>% 
  stan_glm(gaps ~ Environment * Sleep, data = .)

```

```{r tab:Sleep_fixef}
T_fixef <-
  fixef(M_1)
T_fixef %>% kable()
```

Recall, that treatment contrasts were used, where all effects are given relative to the reference group quiet-rested. The results clearly confirm the detoriating effect of sleepiness, although its exact impact is blurred by pronounced uncertainty `r md_coef(Sleep$T_fixef, ~fixef == "SleepSleepy")`. Surprisingly, noise did not affect well-rested persons at all `r md_coef(Sleep$T_fixef, ~fixef == "Environmentnoisy")`. Maybe that is because steady white noise was used, not a disturbing tumult. The irritating effect of noise may therefor be minimal. As suggested by Corcoran, the effect of sleepiness on performance is partly reduced in a noisy environment `r md_coef(Sleep$T_fixef, ~fixef == "Environmentnoisy:SleepSleepy")`. This suggests the theory that the arousal system is involved in the detoriating effect of sleep deprivation, which has interesting consequences for the design of vigilance tasks in the real world. 

The finding also relates to the well known Yerkes-Dodson law in ergonomic science. The law suggests that human performance at cognitive tasks is influenced by arousal. The influence is not linear, but better approximated with a curve as shown in Figure XY. Performance is highest at a moderate level of arousal. If we assume that sleepy participants in Corcona's study showed low performance due to under-arousal, the noise perhaps has increased the arousal level, resulting in better performance. If we except that noise has an arousing effect, the null effect of noise on rested participants stands in opposition to the Yerkes-Dodson law: if rested participants were on an optimal arousal level, additional arousal would usually have a negative effect on performance. There is the slight possibility, that Corcona has hit a sweet spot: if we assume that calm/rested participants were still below an optimal  arousal level, noise could have pushed them right to the opposite point. 

Central to the Yerkes-Dodson law is that arousal is a gradually increasing property, but the current experiment only features two levels. A straight line is the only way to unambiguously connect two group means; examining any curved relationships is impossible. 
We could think of varying noise levels over a wider range for better tracing the non-linear relationship between arousal and performance. The next section introduces polynomial regression for approximating non-linear associations between metric variables.


```{r fig:Yerkes_Dodson_1}
F_Yerkes <-
  function(range = c(-2, 2)){
    data_frame(arousal = seq(-2, 2, length.out = 200)) %>% 
      mutate(performance = dlogis(arousal))
  }

D_Yerkes_Sleep <-
  D_1 %>% 
  distinct(Environment, Sleep) %>% 
  mutate(Condition = str_c(Environment, Sleep, sep = "/"),
         arousal = c(-.5, .4, -1.5, -1),
         performance = dlogis(arousal)) %>% 
  arrange(arousal)
  

G_Yerkes_1 <-
  F_Yerkes(seq(-2,2,.1)) %>% 
  ggplot(aes(x = arousal, y = performance)) +
  geom_line() +
  geom_line(aes(x = arousal, y = performance,
                color = "Sleep study"), 
            data = D_Yerkes_Sleep) +
  geom_text(aes(x = arousal, y = performance, label = Condition,
                color = "Sleep study"),
            data = D_Yerkes_Sleep) +
  theme(axis.text.y=element_blank())

G_Yerkes_1

```


```{r}
detach(Sleep)
```


## Up and down: polynomial regression [eventually move to NLM]

Robots build our cars and sometimes drive them. They mow the lawn and may soon also deliver parcels to far-off regions. Prophecy is that robots will also enter social domains, such as accompany children and care for our seniors. One can assume that in social settings emotional acceptance plays a significant role for technology adoption. Next to  our voices, our faces and mimic expressions are the main source of interpersonal messaging. Since the dawn of the very idea of robots, anthropomorph designs have been dominant. Researchers and designers all around the globe are currently pushing the limits of humanlikeness of robots. One could assume that emotional response improves with every small step towards perfection. Unfortunately, this is not so. [Mori] discovered a bizarre non-linearity in human response: people respond more positive to improvements in humanlikess, but only at the lower end. A feline robot design with recognizable but stylized facial features will always beat a faceless robot with a strong mechanical appeal. Designs on the high end, that are very humanlike, but not exactly, face a sudden drop in emotional response, which is called the *uncanny valley*.

```{r fig:uncanny_valley}
# detach(Uncanny)
attach(Uncanny)

G_uncanny_illu <-
  data_frame(hl = seq(-1, 1, length.out = 100),
             likeability = -.5 * hl + .6 * hl^3 + .2 * hl^4) %>% 
  mutate(human_likeness = (hl + 1)/2) %>% 
  ggplot(aes(x = human_likeness, y = likeability)) +
  geom_line()

G_uncanny_illu
```


[Mathur et al.] study aimed at rendering the association between humanlikeness and liking at full range. They collected 60 pictures of robots and attached a score ranging from mechanical to human appearance. Then they let more than 200 participants rate the faces on how much they liked them. Finally, they created an average score of likeability per robot picture and examined the association with human likeness using a statistical model. Owing to the curved shape of the uncanny valley, linear regression is not applicable to the problem. Instead, Mathur et al. applied a third degree polynomial term.

A polynomial function of degree $k$ has the form:

$$y_i = \beta_0 * x_i^0 + \beta_1 * x_i^1 + ... + \beta_{k-1} * x_i^{k}$$

In fact, you are already familiar with two polynomial models. The first degree polynomial is the grand mean model. This follows from $x_i^0 = 1$, which makes $\beta_0$ a constant, the intercept. Also, a second degree polynomial is just the linear model. By adding higher degrees we can introduce curvature to the association.

Mathus et al. argue that the Uncanny Valley curve possesses two stationary points, where the slope is zero. One is a local minimum and represents the deepest point in the valley. The second is a local maximum and marks the shoulder left of the valley. Such a curvature can be approximated with a polynomial of (at least) third degree, which has a constant $\beta_0$, a linear slope $\beta_1$, quadratic parameter $\beta_2$ and a cubic parameter $\beta_3$.

While R provides high-level methods to deal with polynomial regression, it is instructive to build the regression manually. The first step is to add variables to the data frame, which are the exponentiated predictors ($x_k = x^k$). These variables are then straight-forwardly added to the model term, as if they were independent predictors. For better clarity, we rename the Intercept to be $x_0$, before summarizing the fixed effects.

```{r fit:UV_1, opts.label = "mcmc"}
M_1 <-
  UV_1 %>% 
  mutate(huMech_1 = huMech,
         huMech_2 = huMech^2,
         huMech_3 = huMech^3) %>% 
  stan_glm(avg_like ~ 1 + huMech_1 + huMech_2 + huMech_3,
           data = ., iter = 500)

P_1 <-
  posterior(M_1) %>% 
  mutate(parameter = str_replace(parameter, "Intercept", "huMech_0"),
         fixef = str_replace(fixef, "Intercept", "huMech_0")) %>% 
  posterior()
  
T_fixef_1 <- fixef(P_1)
T_fixef_1
save_CE(CE)
```

```{r}
T_fixef_1
```


We can extract the fixef effects table as usual. The four coefficients specify the poynomial to approximate the average likeability responses. The polynomial parameters have little explanatory value. Neither of the parameter alone relates to a relevant property of the uncanny valley. One relevant property would be the location of the deepest point of the uncanny valley, its trough. The trough is a local minimum of the curve and With polynomial techniques, we can find this point. 

Finding a local minimum a two step procedure: first, we must find all *stationary points*, which includes local minima and maxima. Then, we determine which of the resulting points is the local minimum. Stationary points occur, where teh curve bends from a rising to falling, or vice versa. They are distinguishable by having a slope of zero, neither rising nor falling. Stationary points can be identified by the derivative of the third degree polynomial, which is a second degree polynomial:

$$f'(x) = \beta_1 + 2\beta_2x + 3\beta_2x^2$$

The derivative $f'(x)$ of a function $f(x)$ gives the slope of $f(x)$ at any given point $x$. When $f'(x) > 0$, $f(x)$ is rising at $x$, with $f'(x) < 0$ it is falling. Stationary points are precisely those points, where $f'(x) = 0$ and can be found by solving the equation. The derivative of a third degree polynomial is of the second degree, which has a quadratic part. This can produce a parabolic form, which hits point zero twice, during rise and when falling. A rising encounter of point zero indicates that $f(x)$ has a local minimum at $x$, a local maximum when falling. In consequence, solving $f'(x) = 0$ can result in two solutions, one minimum and one maximum, which needs to be distinguished further. 

If the stationary point is a local minimum, as the trough, slope switches from negative to positive; $f'(x)$ crosses $x = 0$ in a rising manner, which is a positive slope of $f'(x)$. Therefore, a stationary point is a local minimum, when of $f''(x) > 0$.

Mathur et al. followed these analytical steps to arrive at an estimate for the position of the trough. The following code uses several high-level functions from package `polynom` to estimate the location of the trough, by drawing on the first and second derivative `d[d]poly` 

```{r trough_from_fixef, opts.label = "rtut"}
library(polynom)

poly     = polynomial(T_fixef_1$center) # UC function on center
dpoly    = deriv(poly)                  # 1st derivative
ddpoly   = deriv(dpoly)                 # 2nd derivative
stat_pts = solve(dpoly)                 # finding stat points
slopes   = as.function(ddpoly)(stat_pts)# slope at stat points
trough   = stat_pts[slopes > 0]         # selecting the local minimum

cat("The trough is most likely at a huMech score of ", round(trough, 2))
```

While this procedure is instrictuve, there is an issue: drawing on the center estimates, which is a summary of the PD, we get a point estimate, only. Statements on certainty are impossible, as a CI is lacking. Recall the 99 seconds study, where we operated directly on the PD to obtain more specific statements on certainty. Another case for directly operating on the PD samples is to calculate additional statistics. It is another virtue of the MCMC method, that this is possible.

Every PD sample contains simultaneous draw of the four parameters `huMech_[0:3]`, and therefore fully specifies its own third degree polynomial. A PD for the trough parameter can be obtained by performing the above procedure on every sample separately. For the convenience, the case study Uncanny contains a function `trough(coef)` that includes all the above steps. The following code creates a data frame with one row per MCMC draw and the four huMech variables, the function `trough` acts on this data frame as a matrix of coeffients and returns one trough point per row. We have obtained the PD of the trough.


```{r trough_from_PD, opts.label = "rtut"}
P_1_trough <-
  P_1 %>%
  filter(type == "fixef") %>%
  select(chain, iter, fixef, value) %>% 
  spread(fixef, value) %>% 
  select(starts_with("huMech")) %>% 
  trough()

```


From the PD, we can derive statements on uncertainty in the usual way: the 95% credibility limits we get by:

```{r trough_CI95}
quantile(P_1_trough, c(.025, .975), na.tm = T)
```


The 95% CI is a conventional measure of uncertainty and may be more or less irrelevant for the spectator. The most generous account uncertainty is a density plot on the full posterior. The density function just smoothes over the frequency distribution of trough draws, but makes no arbitrary choices on where to cut it. 

```{r}
grid.arrange(
  UV_1 %>% 
    ggplot(aes(x = huMech, y = avg_like)) +
    geom_point(size = .3) +
    xlim(-1, 1),
  
  data_frame(trough = P_1_trough) %>% 
    ggplot(aes(x = trough)) +
    geom_density(aes(x = trough)) +
    xlim(-1, 1)
)
```

With reasonable certainty, we can say that the trough is at approximately two-thirds of the huMech score range. The illustration of the uncanny valley as they used to be perpetuated from the original source, place the trough at about four quarters of the scale. We clearly see that this is not the case in Mathur's study. This might be an artifact, for example in that the huMech score is not linearly related to the perception of human-likeness. 

```{r}
detach(Uncanny)
```




## Stub: How parameter estimation with MCMC works

```{r mcmc_algorithm}
# posterior(Mclm[["Browsing_AB_1"]]) %>% 
#   spread(parameter, value) %>% 
# 	ggplot(aes(x = `Intercept`,
# 						 y = DesignB)) + 
# 	geom_path(alpha = .2) +
# 	geom_point()
```


## Stub: How parameter estimation with RSS works

Here I demonstrate a brute force method to find the optimal parameters for the linear regression.
The parameter space span by $beta_0$ and $\beta_1$ is systematically explored, and for every combination the RSS is computed. The point where the RSS is at its minimum is the _least square estimate_.

```{r rss_minimization, opt.label = "deprecated"}
# ## function for RSS
# RSS = Vectorize(function(beta_0, beta_1){
#   Browsing %>%
#     mutate(Predicted = beta_0 + beta_1 * age) %>%
#     mutate(squrResid = (Predicted - Browsing)^2) %>%
#     select(squrResid) %>%
#     sum()
# })
# 
# ## exploring (part of) the parameter space
# E1 = expand.grid(beta_0 = seq(-14, -12, length.out = 100),
#                   beta_1 = seq(0.2, 0.5, length.out = 100)) %>%
#   mutate(logRSS = log(RSS(beta_0, beta_1)))
# 
# ## here is the minimum
# E1 %>%
#   filter(logRSS == min(logRSS))
# 
# ## exloring it graphically
# grid.arrange(
#   E1 %>%
#     ggplot(aes(x = beta_0, y = beta_1, col = logRSS)) +
#     geom_point(),
#   E1 %>%
#     gather(parameter, value, 1:2) %>%
#     ggplot(aes(x = value, y = logRSS)) +
#     geom_point(alpha = .5) +
#     facet_grid(.~parameter, scales = "free_x"),
#   E1 %>%
#     filter(abs(-13 - beta_0) < 0.5 & abs(0.3 - beta_1) < 0.1) %>%
#     gather(parameter, value, 1:2) %>%
#     ggplot(aes(x = value, y = logRSS)) +
#     geom_point(alpha = .5) +
#     facet_grid(.~parameter, scales = "free_x"),
#   E1 %>%
#     filter(abs(-13.25 - beta_0) < 0.1 & abs(0.31 - beta_1) < 0.03) %>%
#     gather(parameter, value, 1:2) %>%
#     ggplot(aes(x = value, y = logRSS)) +
#     geom_point(alpha = .5) +
#     facet_grid(.~parameter, scales = "free_x"),
#   ncol = 2
#   )

```


```{r save_all, opts.label = "invisible"}
save_CE(CE)

```
